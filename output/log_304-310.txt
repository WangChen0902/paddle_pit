2022-01-07 23:12:56,594 
AMP: False
AUG:
  AUTO_AUGMENT: None
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: ./Light_ILSVRC2012
  IMAGENET_MEAN: [0.485, 0.456, 0.406]
  IMAGENET_STD: [0.229, 0.224, 0.225]
  IMAGE_SIZE: 224
  NUM_WORKERS: 8
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.0
  DISTILL: False
  DROPOUT: 0.0
  DROP_PATH: 0.1
  NAME: pit_ti
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: ./resume/PiT-Latest
  TRANS:
    BASE_DIMS: [32, 32, 32]
    DEPTH: [2, 6, 4]
    HEADS: [2, 4, 8]
    PATCH_SIZE: 16
    STRIDE: 8
  TYPE: PiT
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output/train-20220107-23-12-01
SAVE_FREQ: 10
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  COOLDOWN_EPOCHS: 10
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  DISTILLATION_ALPHA: 0.5
  DISTILLATION_TAU: 1.0
  DISTILLATION_TYPE: none
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 303
  LINEAR_SCALED_LR: None
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  MODEL_EMA: True
  MODEL_EMA_DECAY: 0.99996
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  TEACHER_MODEL: ./regnety_160
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
2022-01-07 23:12:56,594 ----- world_size = 4, local_rank = 0
2022-01-07 23:12:56,728 ----- Total # of train batch (single gpu): 1251
2022-01-07 23:12:56,728 ----- Total # of val batch (single gpu): 1563
2022-01-07 23:12:57,572 ----- Resume Training: Load model and optmizer from ./resume/PiT-Latest
2022-01-07 23:12:57,927 ----- Load model ema from ./resume/PiT-Latest-EMA.pdparams
2022-01-07 23:12:57,927 Start training from epoch 304.
2022-01-07 23:12:57,927 Now training epoch 304. LR=0.000005
2022-01-07 23:14:13,593 Epoch[304/310], Step[0000/1251], Loss: 3.4553(3.4553), Acc: 0.2637(0.2637)
2022-01-07 23:15:11,058 Epoch[304/310], Step[0050/1251], Loss: 3.4804(3.5884), Acc: 0.4043(0.3671)
2022-01-07 23:16:08,028 Epoch[304/310], Step[0100/1251], Loss: 3.6396(3.5822), Acc: 0.3242(0.3782)
2022-01-07 23:17:03,816 Epoch[304/310], Step[0150/1251], Loss: 3.6633(3.5660), Acc: 0.4160(0.3768)
2022-01-07 23:18:00,309 Epoch[304/310], Step[0200/1251], Loss: 3.2422(3.5450), Acc: 0.4365(0.3782)
2022-01-07 23:18:57,891 Epoch[304/310], Step[0250/1251], Loss: 3.3051(3.5309), Acc: 0.6025(0.3809)
2022-01-07 23:19:53,893 Epoch[304/310], Step[0300/1251], Loss: 3.4943(3.5146), Acc: 0.4365(0.3869)
2022-01-07 23:20:51,434 Epoch[304/310], Step[0350/1251], Loss: 3.9711(3.5336), Acc: 0.3643(0.3883)
2022-01-07 23:21:48,660 Epoch[304/310], Step[0400/1251], Loss: 3.7392(3.5301), Acc: 0.3936(0.3868)
2022-01-07 23:22:45,735 Epoch[304/310], Step[0450/1251], Loss: 3.7160(3.5374), Acc: 0.1826(0.3846)
2022-01-07 23:23:43,101 Epoch[304/310], Step[0500/1251], Loss: 3.0486(3.5366), Acc: 0.5713(0.3859)
2022-01-07 23:24:39,227 Epoch[304/310], Step[0550/1251], Loss: 3.4520(3.5289), Acc: 0.5068(0.3874)
2022-01-07 23:25:37,480 Epoch[304/310], Step[0600/1251], Loss: 3.3528(3.5311), Acc: 0.5234(0.3886)
2022-01-07 23:26:35,335 Epoch[304/310], Step[0650/1251], Loss: 4.3323(3.5337), Acc: 0.1885(0.3867)
2022-01-07 23:27:33,449 Epoch[304/310], Step[0700/1251], Loss: 4.0528(3.5335), Acc: 0.4033(0.3880)
2022-01-07 23:28:32,436 Epoch[304/310], Step[0750/1251], Loss: 3.4230(3.5332), Acc: 0.5596(0.3896)
2022-01-07 23:29:31,068 Epoch[304/310], Step[0800/1251], Loss: 3.8454(3.5332), Acc: 0.3652(0.3893)
2022-01-07 23:30:31,609 Epoch[304/310], Step[0850/1251], Loss: 3.9749(3.5283), Acc: 0.1895(0.3884)
2022-01-07 23:31:31,720 Epoch[304/310], Step[0900/1251], Loss: 3.8707(3.5284), Acc: 0.3252(0.3892)
2022-01-07 23:32:31,876 Epoch[304/310], Step[0950/1251], Loss: 3.5347(3.5265), Acc: 0.3701(0.3900)
2022-01-07 23:33:31,945 Epoch[304/310], Step[1000/1251], Loss: 3.5035(3.5274), Acc: 0.1836(0.3890)
2022-01-07 23:34:31,463 Epoch[304/310], Step[1050/1251], Loss: 3.3481(3.5277), Acc: 0.3320(0.3870)
2022-01-07 23:35:30,998 Epoch[304/310], Step[1100/1251], Loss: 3.7045(3.5276), Acc: 0.3184(0.3862)
2022-01-07 23:36:30,518 Epoch[304/310], Step[1150/1251], Loss: 3.8267(3.5285), Acc: 0.2637(0.3859)
2022-01-07 23:37:31,139 Epoch[304/310], Step[1200/1251], Loss: 3.9488(3.5293), Acc: 0.4160(0.3855)
2022-01-07 23:38:30,453 Epoch[304/310], Step[1250/1251], Loss: 3.5174(3.5300), Acc: 0.3662(0.3850)
2022-01-07 23:38:31,792 ----- Validation after Epoch: 304
2022-01-07 23:39:40,284 Val Step[0000/1563], Loss: 0.6749 (0.6749), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 23:39:41,788 Val Step[0050/1563], Loss: 2.2590 (0.7102), Acc@1: 0.4062 (0.8634), Acc@5: 0.8750 (0.9632)
2022-01-07 23:39:43,080 Val Step[0100/1563], Loss: 1.8899 (0.9622), Acc@1: 0.5000 (0.7890), Acc@5: 0.8438 (0.9406)
2022-01-07 23:39:44,403 Val Step[0150/1563], Loss: 0.3672 (0.9128), Acc@1: 0.9688 (0.8019), Acc@5: 1.0000 (0.9454)
2022-01-07 23:39:45,663 Val Step[0200/1563], Loss: 0.9104 (0.9204), Acc@1: 0.8125 (0.8033), Acc@5: 0.9375 (0.9437)
2022-01-07 23:39:47,108 Val Step[0250/1563], Loss: 0.4773 (0.8734), Acc@1: 0.9062 (0.8145), Acc@5: 1.0000 (0.9491)
2022-01-07 23:39:48,635 Val Step[0300/1563], Loss: 1.0063 (0.9295), Acc@1: 0.7188 (0.7990), Acc@5: 1.0000 (0.9450)
2022-01-07 23:39:50,114 Val Step[0350/1563], Loss: 0.9277 (0.9363), Acc@1: 0.7812 (0.7945), Acc@5: 0.9062 (0.9463)
2022-01-07 23:39:51,505 Val Step[0400/1563], Loss: 0.8639 (0.9436), Acc@1: 0.8125 (0.7884), Acc@5: 0.9688 (0.9463)
2022-01-07 23:39:52,905 Val Step[0450/1563], Loss: 0.9913 (0.9507), Acc@1: 0.7188 (0.7857), Acc@5: 1.0000 (0.9471)
2022-01-07 23:39:54,309 Val Step[0500/1563], Loss: 0.4434 (0.9426), Acc@1: 0.9062 (0.7884), Acc@5: 1.0000 (0.9480)
2022-01-07 23:39:55,761 Val Step[0550/1563], Loss: 0.7587 (0.9223), Acc@1: 0.8125 (0.7930), Acc@5: 0.9688 (0.9498)
2022-01-07 23:39:57,147 Val Step[0600/1563], Loss: 0.7794 (0.9306), Acc@1: 0.8438 (0.7917), Acc@5: 0.9375 (0.9491)
2022-01-07 23:39:58,542 Val Step[0650/1563], Loss: 0.5064 (0.9501), Acc@1: 0.8750 (0.7876), Acc@5: 1.0000 (0.9465)
2022-01-07 23:39:59,918 Val Step[0700/1563], Loss: 0.9615 (0.9764), Acc@1: 0.8125 (0.7811), Acc@5: 0.9688 (0.9435)
2022-01-07 23:40:01,340 Val Step[0750/1563], Loss: 1.1656 (1.0072), Acc@1: 0.8125 (0.7749), Acc@5: 0.9062 (0.9395)
2022-01-07 23:40:02,746 Val Step[0800/1563], Loss: 0.6248 (1.0427), Acc@1: 0.8750 (0.7662), Acc@5: 1.0000 (0.9351)
2022-01-07 23:40:04,149 Val Step[0850/1563], Loss: 1.2396 (1.0667), Acc@1: 0.6250 (0.7597), Acc@5: 0.9375 (0.9324)
2022-01-07 23:40:05,600 Val Step[0900/1563], Loss: 0.2379 (1.0661), Acc@1: 0.9688 (0.7609), Acc@5: 1.0000 (0.9321)
2022-01-07 23:40:07,049 Val Step[0950/1563], Loss: 1.2682 (1.0854), Acc@1: 0.7812 (0.7575), Acc@5: 0.8750 (0.9287)
2022-01-07 23:40:08,293 Val Step[1000/1563], Loss: 0.5849 (1.1077), Acc@1: 0.9375 (0.7516), Acc@5: 1.0000 (0.9259)
2022-01-07 23:40:09,544 Val Step[1050/1563], Loss: 0.3337 (1.1215), Acc@1: 0.9688 (0.7483), Acc@5: 1.0000 (0.9242)
2022-01-07 23:40:10,780 Val Step[1100/1563], Loss: 0.6670 (1.1350), Acc@1: 0.9375 (0.7457), Acc@5: 1.0000 (0.9225)
2022-01-07 23:40:12,113 Val Step[1150/1563], Loss: 1.2019 (1.1481), Acc@1: 0.7812 (0.7429), Acc@5: 0.8438 (0.9207)
2022-01-07 23:40:13,352 Val Step[1200/1563], Loss: 1.1877 (1.1614), Acc@1: 0.7812 (0.7396), Acc@5: 0.8438 (0.9186)
2022-01-07 23:40:14,668 Val Step[1250/1563], Loss: 0.7035 (1.1719), Acc@1: 0.8750 (0.7377), Acc@5: 0.9375 (0.9171)
2022-01-07 23:40:15,887 Val Step[1300/1563], Loss: 0.7793 (1.1798), Acc@1: 0.9062 (0.7364), Acc@5: 0.9375 (0.9161)
2022-01-07 23:40:17,122 Val Step[1350/1563], Loss: 1.7713 (1.1959), Acc@1: 0.5000 (0.7324), Acc@5: 0.8438 (0.9139)
2022-01-07 23:40:18,426 Val Step[1400/1563], Loss: 0.9516 (1.2022), Acc@1: 0.7500 (0.7310), Acc@5: 0.9375 (0.9130)
2022-01-07 23:40:19,731 Val Step[1450/1563], Loss: 1.3748 (1.2083), Acc@1: 0.7500 (0.7292), Acc@5: 0.9062 (0.9127)
2022-01-07 23:40:21,044 Val Step[1500/1563], Loss: 1.6411 (1.1985), Acc@1: 0.6250 (0.7315), Acc@5: 0.8438 (0.9139)
2022-01-07 23:40:22,421 Val Step[1550/1563], Loss: 0.8516 (1.2003), Acc@1: 0.8750 (0.7307), Acc@5: 0.9062 (0.9136)
2022-01-07 23:40:23,171 ----- Epoch[304/310], Validation Loss: 1.1989, Validation Acc@1: 0.7309, Validation Acc@5: 0.9138, time: 111.38
2022-01-07 23:40:23,171 ----- Epoch[304/310], Train Loss: 3.5300, Train Acc: 0.3850, time: 1533.86, Best Val(epoch302) Acc@1: 0.7313
2022-01-07 23:40:23,333 ----- Save model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdparams
2022-01-07 23:40:23,334 ----- Save optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdopt
2022-01-07 23:40:23,399 ----- Save ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest-EMA.pdparams
2022-01-07 23:40:23,399 Now training epoch 305. LR=0.000005
2022-01-07 23:41:48,691 Epoch[305/310], Step[0000/1251], Loss: 3.3580(3.3580), Acc: 0.1689(0.1689)
2022-01-07 23:42:46,510 Epoch[305/310], Step[0050/1251], Loss: 3.0015(3.5947), Acc: 0.2959(0.3775)
2022-01-07 23:43:43,824 Epoch[305/310], Step[0100/1251], Loss: 3.6992(3.5488), Acc: 0.0762(0.3754)
2022-01-07 23:44:40,654 Epoch[305/310], Step[0150/1251], Loss: 3.3811(3.5646), Acc: 0.4014(0.3783)
2022-01-07 23:45:38,071 Epoch[305/310], Step[0200/1251], Loss: 3.6982(3.5582), Acc: 0.3730(0.3820)
2022-01-07 23:46:35,961 Epoch[305/310], Step[0250/1251], Loss: 3.8062(3.5497), Acc: 0.3232(0.3854)
2022-01-07 23:47:31,712 Epoch[305/310], Step[0300/1251], Loss: 3.4634(3.5528), Acc: 0.2031(0.3884)
2022-01-07 23:48:29,683 Epoch[305/310], Step[0350/1251], Loss: 4.0048(3.5558), Acc: 0.3652(0.3871)
2022-01-07 23:49:27,419 Epoch[305/310], Step[0400/1251], Loss: 3.1925(3.5508), Acc: 0.4375(0.3867)
2022-01-07 23:50:25,000 Epoch[305/310], Step[0450/1251], Loss: 3.0433(3.5492), Acc: 0.5537(0.3860)
2022-01-07 23:51:22,384 Epoch[305/310], Step[0500/1251], Loss: 3.5989(3.5476), Acc: 0.1768(0.3839)
2022-01-07 23:52:19,081 Epoch[305/310], Step[0550/1251], Loss: 3.3220(3.5461), Acc: 0.3096(0.3845)
2022-01-07 23:53:15,822 Epoch[305/310], Step[0600/1251], Loss: 3.6124(3.5405), Acc: 0.1885(0.3875)
2022-01-07 23:54:14,598 Epoch[305/310], Step[0650/1251], Loss: 3.6341(3.5346), Acc: 0.3438(0.3857)
2022-01-07 23:55:10,489 Epoch[305/310], Step[0700/1251], Loss: 3.2835(3.5256), Acc: 0.5791(0.3873)
2022-01-07 23:56:07,902 Epoch[305/310], Step[0750/1251], Loss: 3.6224(3.5238), Acc: 0.3135(0.3884)
2022-01-07 23:57:04,635 Epoch[305/310], Step[0800/1251], Loss: 3.4980(3.5264), Acc: 0.4434(0.3868)
2022-01-07 23:58:02,477 Epoch[305/310], Step[0850/1251], Loss: 3.6250(3.5263), Acc: 0.4434(0.3853)
2022-01-07 23:58:59,312 Epoch[305/310], Step[0900/1251], Loss: 3.3239(3.5292), Acc: 0.3779(0.3855)
2022-01-07 23:59:56,045 Epoch[305/310], Step[0950/1251], Loss: 3.3847(3.5323), Acc: 0.5264(0.3853)
2022-01-08 00:00:54,376 Epoch[305/310], Step[1000/1251], Loss: 4.0135(3.5328), Acc: 0.2568(0.3847)
2022-01-08 00:01:53,674 Epoch[305/310], Step[1050/1251], Loss: 3.6630(3.5312), Acc: 0.2080(0.3833)
2022-01-08 00:02:52,636 Epoch[305/310], Step[1100/1251], Loss: 3.6199(3.5335), Acc: 0.1846(0.3835)
2022-01-08 00:03:51,398 Epoch[305/310], Step[1150/1251], Loss: 3.4746(3.5333), Acc: 0.4248(0.3831)
2022-01-08 00:04:50,307 Epoch[305/310], Step[1200/1251], Loss: 3.6983(3.5334), Acc: 0.4209(0.3845)
2022-01-08 00:05:47,316 Epoch[305/310], Step[1250/1251], Loss: 2.9118(3.5343), Acc: 0.6172(0.3853)
2022-01-08 00:05:48,740 ----- Epoch[305/310], Train Loss: 3.5343, Train Acc: 0.3853, time: 1525.34, Best Val(epoch302) Acc@1: 0.7313
2022-01-08 00:05:48,918 ----- Save model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdparams
2022-01-08 00:05:48,918 ----- Save optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdopt
2022-01-08 00:05:49,007 ----- Save ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest-EMA.pdparams
2022-01-08 00:05:49,008 Now training epoch 306. LR=0.000005
2022-01-08 00:07:10,446 Epoch[306/310], Step[0000/1251], Loss: 3.4135(3.4135), Acc: 0.4189(0.4189)
2022-01-08 00:08:07,855 Epoch[306/310], Step[0050/1251], Loss: 3.3852(3.4881), Acc: 0.4180(0.4120)
2022-01-08 00:09:05,545 Epoch[306/310], Step[0100/1251], Loss: 3.2809(3.5062), Acc: 0.4688(0.4098)
2022-01-08 00:10:02,419 Epoch[306/310], Step[0150/1251], Loss: 3.7054(3.5069), Acc: 0.4951(0.3971)
2022-01-08 00:10:58,903 Epoch[306/310], Step[0200/1251], Loss: 3.3013(3.5201), Acc: 0.5127(0.3990)
2022-01-08 00:11:56,838 Epoch[306/310], Step[0250/1251], Loss: 3.1180(3.5186), Acc: 0.6230(0.4006)
2022-01-08 00:12:55,283 Epoch[306/310], Step[0300/1251], Loss: 3.4909(3.5130), Acc: 0.4912(0.3999)
2022-01-08 00:13:52,664 Epoch[306/310], Step[0350/1251], Loss: 3.9276(3.5165), Acc: 0.4688(0.4015)
2022-01-08 00:14:48,895 Epoch[306/310], Step[0400/1251], Loss: 3.5517(3.5181), Acc: 0.5264(0.4004)
2022-01-08 00:15:47,158 Epoch[306/310], Step[0450/1251], Loss: 3.7110(3.5164), Acc: 0.4102(0.3972)
2022-01-08 00:16:44,484 Epoch[306/310], Step[0500/1251], Loss: 3.7460(3.5156), Acc: 0.1934(0.3946)
2022-01-08 00:17:41,470 Epoch[306/310], Step[0550/1251], Loss: 3.0943(3.5212), Acc: 0.4600(0.3933)
2022-01-08 00:18:39,767 Epoch[306/310], Step[0600/1251], Loss: 3.5723(3.5216), Acc: 0.5518(0.3951)
2022-01-08 00:19:38,304 Epoch[306/310], Step[0650/1251], Loss: 4.1508(3.5222), Acc: 0.3926(0.3944)
2022-01-08 00:20:36,049 Epoch[306/310], Step[0700/1251], Loss: 3.6372(3.5243), Acc: 0.1602(0.3925)
2022-01-08 00:21:33,788 Epoch[306/310], Step[0750/1251], Loss: 3.4962(3.5291), Acc: 0.2148(0.3903)
2022-01-08 00:22:30,831 Epoch[306/310], Step[0800/1251], Loss: 3.7408(3.5321), Acc: 0.3867(0.3915)
2022-01-08 00:23:27,542 Epoch[306/310], Step[0850/1251], Loss: 3.6787(3.5337), Acc: 0.3359(0.3904)
2022-01-08 00:24:24,989 Epoch[306/310], Step[0900/1251], Loss: 3.4927(3.5335), Acc: 0.3457(0.3878)
2022-01-08 00:25:21,345 Epoch[306/310], Step[0950/1251], Loss: 3.4368(3.5304), Acc: 0.1582(0.3883)
2022-01-08 00:26:18,351 Epoch[306/310], Step[1000/1251], Loss: 3.8010(3.5309), Acc: 0.4209(0.3885)
2022-01-08 00:27:16,331 Epoch[306/310], Step[1050/1251], Loss: 3.4807(3.5289), Acc: 0.2764(0.3879)
2022-01-08 00:28:14,157 Epoch[306/310], Step[1100/1251], Loss: 3.4719(3.5288), Acc: 0.2812(0.3874)
2022-01-08 00:29:10,632 Epoch[306/310], Step[1150/1251], Loss: 3.8680(3.5337), Acc: 0.3896(0.3874)
2022-01-08 00:30:07,869 Epoch[306/310], Step[1200/1251], Loss: 3.4048(3.5307), Acc: 0.4746(0.3873)
2022-01-08 00:31:05,772 Epoch[306/310], Step[1250/1251], Loss: 3.6190(3.5344), Acc: 0.4355(0.3863)
2022-01-08 00:31:07,244 ----- Validation after Epoch: 306
2022-01-08 00:32:11,192 Val Step[0000/1563], Loss: 0.6855 (0.6855), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-08 00:32:12,561 Val Step[0050/1563], Loss: 2.2107 (0.7189), Acc@1: 0.4062 (0.8597), Acc@5: 0.8750 (0.9614)
2022-01-08 00:32:13,787 Val Step[0100/1563], Loss: 1.8877 (0.9734), Acc@1: 0.5000 (0.7874), Acc@5: 0.8438 (0.9375)
2022-01-08 00:32:15,079 Val Step[0150/1563], Loss: 0.3676 (0.9230), Acc@1: 0.9688 (0.8011), Acc@5: 1.0000 (0.9429)
2022-01-08 00:32:16,479 Val Step[0200/1563], Loss: 0.9557 (0.9302), Acc@1: 0.7812 (0.8025), Acc@5: 0.9375 (0.9426)
2022-01-08 00:32:17,793 Val Step[0250/1563], Loss: 0.4610 (0.8828), Acc@1: 0.9375 (0.8146), Acc@5: 1.0000 (0.9477)
2022-01-08 00:32:19,044 Val Step[0300/1563], Loss: 1.0527 (0.9400), Acc@1: 0.7188 (0.7982), Acc@5: 1.0000 (0.9434)
2022-01-08 00:32:20,289 Val Step[0350/1563], Loss: 0.9357 (0.9470), Acc@1: 0.7812 (0.7934), Acc@5: 0.9062 (0.9449)
2022-01-08 00:32:21,619 Val Step[0400/1563], Loss: 0.8227 (0.9536), Acc@1: 0.8438 (0.7879), Acc@5: 0.9688 (0.9451)
2022-01-08 00:32:22,876 Val Step[0450/1563], Loss: 0.9743 (0.9600), Acc@1: 0.7188 (0.7854), Acc@5: 1.0000 (0.9462)
2022-01-08 00:32:24,135 Val Step[0500/1563], Loss: 0.4501 (0.9513), Acc@1: 0.9062 (0.7882), Acc@5: 1.0000 (0.9473)
2022-01-08 00:32:25,498 Val Step[0550/1563], Loss: 0.7993 (0.9314), Acc@1: 0.8125 (0.7923), Acc@5: 0.9688 (0.9491)
2022-01-08 00:32:26,750 Val Step[0600/1563], Loss: 0.7818 (0.9393), Acc@1: 0.8438 (0.7911), Acc@5: 0.9375 (0.9486)
2022-01-08 00:32:27,990 Val Step[0650/1563], Loss: 0.4947 (0.9579), Acc@1: 0.9375 (0.7875), Acc@5: 1.0000 (0.9462)
2022-01-08 00:32:29,229 Val Step[0700/1563], Loss: 0.9412 (0.9837), Acc@1: 0.8125 (0.7810), Acc@5: 0.9688 (0.9433)
2022-01-08 00:32:30,546 Val Step[0750/1563], Loss: 1.1988 (1.0146), Acc@1: 0.8125 (0.7748), Acc@5: 0.9062 (0.9394)
2022-01-08 00:32:31,756 Val Step[0800/1563], Loss: 0.6281 (1.0500), Acc@1: 0.9062 (0.7659), Acc@5: 1.0000 (0.9352)
2022-01-08 00:32:32,974 Val Step[0850/1563], Loss: 1.2589 (1.0736), Acc@1: 0.6562 (0.7598), Acc@5: 0.9375 (0.9324)
2022-01-08 00:32:34,224 Val Step[0900/1563], Loss: 0.2551 (1.0725), Acc@1: 0.9688 (0.7612), Acc@5: 1.0000 (0.9320)
2022-01-08 00:32:35,640 Val Step[0950/1563], Loss: 1.2590 (1.0914), Acc@1: 0.7500 (0.7575), Acc@5: 0.9062 (0.9289)
2022-01-08 00:32:36,874 Val Step[1000/1563], Loss: 0.5940 (1.1141), Acc@1: 0.9375 (0.7515), Acc@5: 1.0000 (0.9258)
2022-01-08 00:32:38,153 Val Step[1050/1563], Loss: 0.3320 (1.1274), Acc@1: 0.9688 (0.7485), Acc@5: 1.0000 (0.9243)
2022-01-08 00:32:39,410 Val Step[1100/1563], Loss: 0.7169 (1.1406), Acc@1: 0.9062 (0.7459), Acc@5: 0.9688 (0.9225)
2022-01-08 00:32:40,657 Val Step[1150/1563], Loss: 1.1653 (1.1537), Acc@1: 0.7812 (0.7432), Acc@5: 0.8438 (0.9206)
2022-01-08 00:32:42,051 Val Step[1200/1563], Loss: 1.2426 (1.1671), Acc@1: 0.7812 (0.7400), Acc@5: 0.8438 (0.9183)
2022-01-08 00:32:43,365 Val Step[1250/1563], Loss: 0.7148 (1.1778), Acc@1: 0.8750 (0.7383), Acc@5: 0.9375 (0.9168)
2022-01-08 00:32:44,626 Val Step[1300/1563], Loss: 0.7931 (1.1859), Acc@1: 0.8750 (0.7367), Acc@5: 0.9375 (0.9157)
2022-01-08 00:32:45,958 Val Step[1350/1563], Loss: 1.8724 (1.2019), Acc@1: 0.5000 (0.7329), Acc@5: 0.8125 (0.9134)
2022-01-08 00:32:47,210 Val Step[1400/1563], Loss: 0.9730 (1.2086), Acc@1: 0.7500 (0.7314), Acc@5: 0.9375 (0.9125)
2022-01-08 00:32:48,530 Val Step[1450/1563], Loss: 1.4036 (1.2151), Acc@1: 0.7188 (0.7295), Acc@5: 0.9375 (0.9122)
2022-01-08 00:32:49,899 Val Step[1500/1563], Loss: 1.6117 (1.2049), Acc@1: 0.6562 (0.7319), Acc@5: 0.8438 (0.9135)
2022-01-08 00:32:51,292 Val Step[1550/1563], Loss: 0.8673 (1.2069), Acc@1: 0.8750 (0.7311), Acc@5: 0.9062 (0.9133)
2022-01-08 00:32:52,069 ----- Epoch[306/310], Validation Loss: 1.2054, Validation Acc@1: 0.7314, Validation Acc@5: 0.9135, time: 104.82
2022-01-08 00:32:52,069 ----- Epoch[306/310], Train Loss: 3.5344, Train Acc: 0.3863, time: 1518.23, Best Val(epoch306) Acc@1: 0.7314
2022-01-08 00:32:52,235 Max accuracy so far: 0.7314 at epoch_306
2022-01-08 00:32:52,235 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220107-23-12-01/Best_PiT.pdparams
2022-01-08 00:32:52,235 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/Best_PiT.pdopt
2022-01-08 00:32:52,298 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/Best_PiT-EMA.pdparams
2022-01-08 00:32:52,474 ----- Save model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdparams
2022-01-08 00:32:52,475 ----- Save optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdopt
2022-01-08 00:32:52,564 ----- Save ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest-EMA.pdparams
2022-01-08 00:32:52,565 Now training epoch 307. LR=0.000005
2022-01-08 00:34:14,101 Epoch[307/310], Step[0000/1251], Loss: 3.6372(3.6372), Acc: 0.0635(0.0635)
2022-01-08 00:35:11,898 Epoch[307/310], Step[0050/1251], Loss: 3.5122(3.5720), Acc: 0.4902(0.3871)
2022-01-08 00:36:09,134 Epoch[307/310], Step[0100/1251], Loss: 3.4237(3.5714), Acc: 0.3936(0.3905)
2022-01-08 00:37:06,287 Epoch[307/310], Step[0150/1251], Loss: 3.4754(3.5470), Acc: 0.5527(0.4001)
2022-01-08 00:38:03,833 Epoch[307/310], Step[0200/1251], Loss: 3.9231(3.5330), Acc: 0.3281(0.3947)
2022-01-08 00:39:00,340 Epoch[307/310], Step[0250/1251], Loss: 3.5299(3.5360), Acc: 0.2832(0.3863)
2022-01-08 00:39:58,126 Epoch[307/310], Step[0300/1251], Loss: 3.4646(3.5361), Acc: 0.2139(0.3899)
2022-01-08 00:40:56,147 Epoch[307/310], Step[0350/1251], Loss: 3.3487(3.5273), Acc: 0.3145(0.3923)
2022-01-08 00:41:54,287 Epoch[307/310], Step[0400/1251], Loss: 3.4646(3.5243), Acc: 0.2510(0.3928)
2022-01-08 00:42:52,646 Epoch[307/310], Step[0450/1251], Loss: 3.0752(3.5237), Acc: 0.6680(0.3920)
2022-01-08 00:43:51,067 Epoch[307/310], Step[0500/1251], Loss: 3.6759(3.5296), Acc: 0.3799(0.3914)
2022-01-08 00:44:49,533 Epoch[307/310], Step[0550/1251], Loss: 3.6366(3.5278), Acc: 0.3184(0.3924)
2022-01-08 00:45:48,849 Epoch[307/310], Step[0600/1251], Loss: 3.4754(3.5282), Acc: 0.3691(0.3923)
2022-01-08 00:46:46,329 Epoch[307/310], Step[0650/1251], Loss: 3.4096(3.5311), Acc: 0.3682(0.3910)
2022-01-08 00:47:45,530 Epoch[307/310], Step[0700/1251], Loss: 3.5976(3.5343), Acc: 0.5039(0.3897)
2022-01-08 00:48:44,139 Epoch[307/310], Step[0750/1251], Loss: 3.2189(3.5360), Acc: 0.4180(0.3887)
2022-01-08 00:49:43,262 Epoch[307/310], Step[0800/1251], Loss: 3.2751(3.5416), Acc: 0.4307(0.3862)
2022-01-08 00:50:41,877 Epoch[307/310], Step[0850/1251], Loss: 3.7205(3.5466), Acc: 0.4844(0.3844)
2022-01-08 00:51:41,563 Epoch[307/310], Step[0900/1251], Loss: 3.6749(3.5464), Acc: 0.2363(0.3855)
2022-01-08 00:52:41,684 Epoch[307/310], Step[0950/1251], Loss: 3.5428(3.5472), Acc: 0.2207(0.3848)
2022-01-08 00:53:40,102 Epoch[307/310], Step[1000/1251], Loss: 3.4477(3.5447), Acc: 0.5000(0.3848)
2022-01-08 00:54:38,695 Epoch[307/310], Step[1050/1251], Loss: 3.8916(3.5455), Acc: 0.3027(0.3860)
2022-01-08 00:55:37,892 Epoch[307/310], Step[1100/1251], Loss: 4.1772(3.5446), Acc: 0.2393(0.3862)
2022-01-08 00:56:34,922 Epoch[307/310], Step[1150/1251], Loss: 2.7539(3.5408), Acc: 0.6572(0.3874)
2022-01-08 00:57:33,587 Epoch[307/310], Step[1200/1251], Loss: 3.5306(3.5421), Acc: 0.4346(0.3878)
2022-01-08 00:58:32,296 Epoch[307/310], Step[1250/1251], Loss: 3.9329(3.5426), Acc: 0.4424(0.3872)
2022-01-08 00:58:33,965 ----- Epoch[307/310], Train Loss: 3.5426, Train Acc: 0.3872, time: 1541.40, Best Val(epoch306) Acc@1: 0.7314
2022-01-08 00:58:34,143 ----- Save model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdparams
2022-01-08 00:58:34,143 ----- Save optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdopt
2022-01-08 00:58:34,233 ----- Save ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest-EMA.pdparams
2022-01-08 00:58:34,234 Now training epoch 308. LR=0.000005
2022-01-08 00:59:55,776 Epoch[308/310], Step[0000/1251], Loss: 3.3010(3.3010), Acc: 0.4404(0.4404)
2022-01-08 01:00:53,140 Epoch[308/310], Step[0050/1251], Loss: 3.9961(3.5381), Acc: 0.3047(0.3684)
2022-01-08 01:01:48,634 Epoch[308/310], Step[0100/1251], Loss: 3.8942(3.5392), Acc: 0.3750(0.4001)
2022-01-08 01:02:46,994 Epoch[308/310], Step[0150/1251], Loss: 3.2769(3.5101), Acc: 0.6045(0.4102)
2022-01-08 01:03:44,186 Epoch[308/310], Step[0200/1251], Loss: 3.6129(3.5185), Acc: 0.3818(0.3975)
2022-01-08 01:04:42,031 Epoch[308/310], Step[0250/1251], Loss: 3.8779(3.5323), Acc: 0.4346(0.3964)
2022-01-08 01:05:40,112 Epoch[308/310], Step[0300/1251], Loss: 3.6801(3.5318), Acc: 0.3350(0.3945)
2022-01-08 01:06:37,422 Epoch[308/310], Step[0350/1251], Loss: 3.6402(3.5295), Acc: 0.3828(0.3938)
2022-01-08 01:07:35,762 Epoch[308/310], Step[0400/1251], Loss: 3.7452(3.5351), Acc: 0.4141(0.3943)
2022-01-08 01:08:33,737 Epoch[308/310], Step[0450/1251], Loss: 3.1443(3.5413), Acc: 0.6328(0.3929)
2022-01-08 01:09:30,564 Epoch[308/310], Step[0500/1251], Loss: 3.7778(3.5371), Acc: 0.5156(0.3924)
2022-01-08 01:10:27,114 Epoch[308/310], Step[0550/1251], Loss: 3.4733(3.5334), Acc: 0.2070(0.3928)
2022-01-08 01:11:26,465 Epoch[308/310], Step[0600/1251], Loss: 3.1015(3.5355), Acc: 0.2432(0.3907)
2022-01-08 01:12:23,595 Epoch[308/310], Step[0650/1251], Loss: 3.8068(3.5372), Acc: 0.5068(0.3881)
2022-01-08 01:13:22,676 Epoch[308/310], Step[0700/1251], Loss: 3.1385(3.5395), Acc: 0.5928(0.3869)
2022-01-08 01:14:20,956 Epoch[308/310], Step[0750/1251], Loss: 3.7205(3.5356), Acc: 0.3428(0.3878)
2022-01-08 01:15:19,128 Epoch[308/310], Step[0800/1251], Loss: 3.5175(3.5323), Acc: 0.5010(0.3879)
2022-01-08 01:16:18,220 Epoch[308/310], Step[0850/1251], Loss: 3.9564(3.5340), Acc: 0.3584(0.3877)
2022-01-08 01:17:15,968 Epoch[308/310], Step[0900/1251], Loss: 3.3290(3.5365), Acc: 0.2637(0.3874)
2022-01-08 01:18:14,321 Epoch[308/310], Step[0950/1251], Loss: 2.8682(3.5373), Acc: 0.4932(0.3887)
2022-01-08 01:19:12,194 Epoch[308/310], Step[1000/1251], Loss: 3.9392(3.5392), Acc: 0.3350(0.3880)
2022-01-08 01:20:10,517 Epoch[308/310], Step[1050/1251], Loss: 3.5975(3.5357), Acc: 0.3711(0.3888)
2022-01-08 01:21:09,800 Epoch[308/310], Step[1100/1251], Loss: 3.2675(3.5359), Acc: 0.2314(0.3891)
2022-01-08 01:22:08,280 Epoch[308/310], Step[1150/1251], Loss: 3.7126(3.5347), Acc: 0.4473(0.3892)
2022-01-08 01:23:06,802 Epoch[308/310], Step[1200/1251], Loss: 3.3353(3.5341), Acc: 0.6016(0.3893)
2022-01-08 01:24:06,237 Epoch[308/310], Step[1250/1251], Loss: 3.9249(3.5360), Acc: 0.4082(0.3878)
2022-01-08 01:24:07,672 ----- Validation after Epoch: 308
2022-01-08 01:25:12,624 Val Step[0000/1563], Loss: 0.6436 (0.6436), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-08 01:25:13,873 Val Step[0050/1563], Loss: 2.1276 (0.7124), Acc@1: 0.4375 (0.8585), Acc@5: 0.8750 (0.9614)
2022-01-08 01:25:15,109 Val Step[0100/1563], Loss: 1.8225 (0.9665), Acc@1: 0.5312 (0.7877), Acc@5: 0.8125 (0.9394)
2022-01-08 01:25:16,358 Val Step[0150/1563], Loss: 0.3684 (0.9160), Acc@1: 0.9688 (0.8013), Acc@5: 1.0000 (0.9435)
2022-01-08 01:25:17,578 Val Step[0200/1563], Loss: 0.9403 (0.9226), Acc@1: 0.8125 (0.8036), Acc@5: 0.9375 (0.9419)
2022-01-08 01:25:18,789 Val Step[0250/1563], Loss: 0.4822 (0.8763), Acc@1: 0.9375 (0.8144), Acc@5: 1.0000 (0.9472)
2022-01-08 01:25:20,014 Val Step[0300/1563], Loss: 1.0486 (0.9314), Acc@1: 0.6875 (0.7987), Acc@5: 1.0000 (0.9432)
2022-01-08 01:25:21,224 Val Step[0350/1563], Loss: 0.9064 (0.9386), Acc@1: 0.7812 (0.7942), Acc@5: 0.9062 (0.9450)
2022-01-08 01:25:22,440 Val Step[0400/1563], Loss: 0.8650 (0.9457), Acc@1: 0.8125 (0.7886), Acc@5: 0.9688 (0.9451)
2022-01-08 01:25:23,672 Val Step[0450/1563], Loss: 0.9726 (0.9530), Acc@1: 0.7188 (0.7860), Acc@5: 1.0000 (0.9460)
2022-01-08 01:25:25,043 Val Step[0500/1563], Loss: 0.4053 (0.9443), Acc@1: 0.9375 (0.7888), Acc@5: 1.0000 (0.9474)
2022-01-08 01:25:26,494 Val Step[0550/1563], Loss: 0.7517 (0.9240), Acc@1: 0.8438 (0.7934), Acc@5: 0.9688 (0.9492)
2022-01-08 01:25:27,871 Val Step[0600/1563], Loss: 0.7541 (0.9323), Acc@1: 0.8438 (0.7924), Acc@5: 0.9375 (0.9484)
2022-01-08 01:25:29,092 Val Step[0650/1563], Loss: 0.5005 (0.9516), Acc@1: 0.8750 (0.7884), Acc@5: 1.0000 (0.9458)
2022-01-08 01:25:30,326 Val Step[0700/1563], Loss: 0.9967 (0.9780), Acc@1: 0.8125 (0.7818), Acc@5: 0.9688 (0.9426)
2022-01-08 01:25:31,573 Val Step[0750/1563], Loss: 1.1890 (1.0083), Acc@1: 0.8125 (0.7754), Acc@5: 0.9062 (0.9389)
2022-01-08 01:25:32,817 Val Step[0800/1563], Loss: 0.6384 (1.0441), Acc@1: 0.9062 (0.7664), Acc@5: 1.0000 (0.9344)
2022-01-08 01:25:34,055 Val Step[0850/1563], Loss: 1.1757 (1.0679), Acc@1: 0.6562 (0.7601), Acc@5: 0.9375 (0.9316)
2022-01-08 01:25:35,299 Val Step[0900/1563], Loss: 0.2480 (1.0674), Acc@1: 0.9688 (0.7614), Acc@5: 1.0000 (0.9313)
2022-01-08 01:25:36,646 Val Step[0950/1563], Loss: 1.2392 (1.0864), Acc@1: 0.7500 (0.7578), Acc@5: 0.9062 (0.9283)
2022-01-08 01:25:37,936 Val Step[1000/1563], Loss: 0.5972 (1.1082), Acc@1: 0.9375 (0.7522), Acc@5: 0.9688 (0.9252)
2022-01-08 01:25:39,219 Val Step[1050/1563], Loss: 0.3333 (1.1217), Acc@1: 0.9688 (0.7493), Acc@5: 1.0000 (0.9237)
2022-01-08 01:25:40,458 Val Step[1100/1563], Loss: 0.6852 (1.1343), Acc@1: 0.8750 (0.7467), Acc@5: 1.0000 (0.9220)
2022-01-08 01:25:41,739 Val Step[1150/1563], Loss: 1.1823 (1.1474), Acc@1: 0.7812 (0.7441), Acc@5: 0.8438 (0.9202)
2022-01-08 01:25:43,051 Val Step[1200/1563], Loss: 1.2538 (1.1603), Acc@1: 0.7812 (0.7409), Acc@5: 0.8438 (0.9181)
2022-01-08 01:25:44,299 Val Step[1250/1563], Loss: 0.7685 (1.1711), Acc@1: 0.8750 (0.7390), Acc@5: 0.9375 (0.9165)
2022-01-08 01:25:45,539 Val Step[1300/1563], Loss: 0.7915 (1.1789), Acc@1: 0.9062 (0.7374), Acc@5: 0.9375 (0.9155)
2022-01-08 01:25:46,777 Val Step[1350/1563], Loss: 1.6625 (1.1952), Acc@1: 0.5625 (0.7334), Acc@5: 0.8750 (0.9133)
2022-01-08 01:25:48,024 Val Step[1400/1563], Loss: 1.0212 (1.2017), Acc@1: 0.7500 (0.7319), Acc@5: 0.9375 (0.9124)
2022-01-08 01:25:49,266 Val Step[1450/1563], Loss: 1.4092 (1.2081), Acc@1: 0.7500 (0.7299), Acc@5: 0.9375 (0.9121)
2022-01-08 01:25:50,576 Val Step[1500/1563], Loss: 1.6646 (1.1983), Acc@1: 0.5938 (0.7322), Acc@5: 0.8438 (0.9134)
2022-01-08 01:25:51,976 Val Step[1550/1563], Loss: 0.8603 (1.2005), Acc@1: 0.8750 (0.7314), Acc@5: 0.9062 (0.9131)
2022-01-08 01:25:52,729 ----- Epoch[308/310], Validation Loss: 1.1990, Validation Acc@1: 0.7317, Validation Acc@5: 0.9133, time: 105.05
2022-01-08 01:25:52,729 ----- Epoch[308/310], Train Loss: 3.5360, Train Acc: 0.3878, time: 1533.43, Best Val(epoch308) Acc@1: 0.7317
2022-01-08 01:25:52,920 Max accuracy so far: 0.7317 at epoch_308
2022-01-08 01:25:52,921 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220107-23-12-01/Best_PiT.pdparams
2022-01-08 01:25:52,921 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/Best_PiT.pdopt
2022-01-08 01:25:53,014 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/Best_PiT-EMA.pdparams
2022-01-08 01:25:53,407 ----- Save model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdparams
2022-01-08 01:25:53,407 ----- Save optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdopt
2022-01-08 01:25:53,542 ----- Save ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest-EMA.pdparams
2022-01-08 01:25:53,543 Now training epoch 309. LR=0.000005
2022-01-08 01:27:12,400 Epoch[309/310], Step[0000/1251], Loss: 3.6983(3.6983), Acc: 0.3037(0.3037)
2022-01-08 01:28:10,285 Epoch[309/310], Step[0050/1251], Loss: 3.9556(3.6110), Acc: 0.3320(0.3844)
2022-01-08 01:29:07,952 Epoch[309/310], Step[0100/1251], Loss: 2.7710(3.5879), Acc: 0.4824(0.3906)
2022-01-08 01:30:05,778 Epoch[309/310], Step[0150/1251], Loss: 3.9882(3.6099), Acc: 0.2949(0.3877)
2022-01-08 01:31:02,379 Epoch[309/310], Step[0200/1251], Loss: 3.2490(3.5931), Acc: 0.5469(0.3823)
2022-01-08 01:32:00,057 Epoch[309/310], Step[0250/1251], Loss: 3.7405(3.5794), Acc: 0.3047(0.3813)
2022-01-08 01:32:57,344 Epoch[309/310], Step[0300/1251], Loss: 3.4376(3.5835), Acc: 0.4033(0.3805)
2022-01-08 01:33:55,163 Epoch[309/310], Step[0350/1251], Loss: 3.2096(3.5759), Acc: 0.1836(0.3821)
2022-01-08 01:34:50,540 Epoch[309/310], Step[0400/1251], Loss: 3.8422(3.5716), Acc: 0.4189(0.3849)
2022-01-08 01:35:47,605 Epoch[309/310], Step[0450/1251], Loss: 3.1550(3.5584), Acc: 0.2588(0.3883)
2022-01-08 01:36:45,322 Epoch[309/310], Step[0500/1251], Loss: 3.9070(3.5565), Acc: 0.2510(0.3878)
2022-01-08 01:37:43,070 Epoch[309/310], Step[0550/1251], Loss: 3.5860(3.5570), Acc: 0.4307(0.3865)
2022-01-08 01:38:40,042 Epoch[309/310], Step[0600/1251], Loss: 3.5663(3.5482), Acc: 0.3760(0.3860)
2022-01-08 01:39:37,215 Epoch[309/310], Step[0650/1251], Loss: 3.1370(3.5472), Acc: 0.3652(0.3857)
2022-01-08 01:40:36,087 Epoch[309/310], Step[0700/1251], Loss: 3.4180(3.5466), Acc: 0.2949(0.3860)
2022-01-08 01:41:34,622 Epoch[309/310], Step[0750/1251], Loss: 3.5863(3.5424), Acc: 0.5312(0.3867)
2022-01-08 01:42:30,075 Epoch[309/310], Step[0800/1251], Loss: 3.8371(3.5416), Acc: 0.4014(0.3886)
2022-01-08 01:43:27,424 Epoch[309/310], Step[0850/1251], Loss: 3.6309(3.5430), Acc: 0.2891(0.3889)
2022-01-08 01:44:25,952 Epoch[309/310], Step[0900/1251], Loss: 3.3646(3.5422), Acc: 0.3330(0.3886)
2022-01-08 01:45:24,779 Epoch[309/310], Step[0950/1251], Loss: 3.2079(3.5399), Acc: 0.5098(0.3878)
2022-01-08 01:46:23,646 Epoch[309/310], Step[1000/1251], Loss: 3.4821(3.5395), Acc: 0.4883(0.3872)
2022-01-08 01:47:22,449 Epoch[309/310], Step[1050/1251], Loss: 3.3898(3.5394), Acc: 0.5967(0.3851)
2022-01-08 01:48:20,965 Epoch[309/310], Step[1100/1251], Loss: 3.6912(3.5378), Acc: 0.3857(0.3852)
2022-01-08 01:49:18,781 Epoch[309/310], Step[1150/1251], Loss: 3.5886(3.5349), Acc: 0.5635(0.3860)
2022-01-08 01:50:17,640 Epoch[309/310], Step[1200/1251], Loss: 4.1117(3.5361), Acc: 0.2871(0.3857)
2022-01-08 01:51:16,422 Epoch[309/310], Step[1250/1251], Loss: 3.5520(3.5383), Acc: 0.2803(0.3870)
2022-01-08 01:51:17,836 ----- Epoch[309/310], Train Loss: 3.5383, Train Acc: 0.3870, time: 1524.29, Best Val(epoch308) Acc@1: 0.7317
2022-01-08 01:51:18,109 ----- Save model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdparams
2022-01-08 01:51:18,110 ----- Save optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest.pdopt
2022-01-08 01:51:18,186 ----- Save ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Latest-EMA.pdparams
2022-01-08 01:51:18,187 Now training epoch 310. LR=0.000005
2022-01-08 01:52:31,681 Epoch[310/310], Step[0000/1251], Loss: 3.0614(3.0614), Acc: 0.4141(0.4141)
2022-01-08 01:53:27,604 Epoch[310/310], Step[0050/1251], Loss: 3.7635(3.5542), Acc: 0.2930(0.3966)
2022-01-08 01:54:23,419 Epoch[310/310], Step[0100/1251], Loss: 3.6274(3.5401), Acc: 0.4561(0.3867)
2022-01-08 01:55:19,540 Epoch[310/310], Step[0150/1251], Loss: 3.4408(3.5359), Acc: 0.5459(0.3805)
2022-01-08 01:56:16,491 Epoch[310/310], Step[0200/1251], Loss: 3.5338(3.5393), Acc: 0.4043(0.3869)
2022-01-08 01:57:12,801 Epoch[310/310], Step[0250/1251], Loss: 3.7014(3.5314), Acc: 0.3252(0.3902)
2022-01-08 01:58:10,860 Epoch[310/310], Step[0300/1251], Loss: 3.5231(3.5267), Acc: 0.4209(0.3876)
2022-01-08 01:59:08,372 Epoch[310/310], Step[0350/1251], Loss: 3.7760(3.5227), Acc: 0.3154(0.3903)
2022-01-08 02:00:05,677 Epoch[310/310], Step[0400/1251], Loss: 3.3480(3.5354), Acc: 0.2686(0.3880)
2022-01-08 02:01:03,758 Epoch[310/310], Step[0450/1251], Loss: 3.4201(3.5335), Acc: 0.2773(0.3882)
2022-01-08 02:02:01,474 Epoch[310/310], Step[0500/1251], Loss: 3.7659(3.5324), Acc: 0.4004(0.3895)
2022-01-08 02:02:57,808 Epoch[310/310], Step[0550/1251], Loss: 3.2386(3.5290), Acc: 0.3984(0.3890)
2022-01-08 02:03:54,248 Epoch[310/310], Step[0600/1251], Loss: 4.0034(3.5319), Acc: 0.2812(0.3898)
2022-01-08 02:04:52,127 Epoch[310/310], Step[0650/1251], Loss: 3.9664(3.5288), Acc: 0.3379(0.3911)
2022-01-08 02:05:49,370 Epoch[310/310], Step[0700/1251], Loss: 3.8917(3.5265), Acc: 0.1680(0.3916)
2022-01-08 02:06:47,793 Epoch[310/310], Step[0750/1251], Loss: 4.1462(3.5275), Acc: 0.4023(0.3918)
2022-01-08 02:07:44,878 Epoch[310/310], Step[0800/1251], Loss: 3.1856(3.5291), Acc: 0.3057(0.3917)
2022-01-08 02:08:43,102 Epoch[310/310], Step[0850/1251], Loss: 3.8217(3.5307), Acc: 0.4277(0.3920)
2022-01-08 02:09:40,687 Epoch[310/310], Step[0900/1251], Loss: 3.3563(3.5306), Acc: 0.2930(0.3914)
2022-01-08 02:10:38,356 Epoch[310/310], Step[0950/1251], Loss: 3.5665(3.5296), Acc: 0.5391(0.3907)
2022-01-08 02:11:34,434 Epoch[310/310], Step[1000/1251], Loss: 3.2336(3.5311), Acc: 0.3770(0.3917)
2022-01-08 02:12:32,516 Epoch[310/310], Step[1050/1251], Loss: 3.6167(3.5319), Acc: 0.3887(0.3918)
2022-01-08 02:13:29,403 Epoch[310/310], Step[1100/1251], Loss: 4.0433(3.5335), Acc: 0.1943(0.3917)
2022-01-08 02:14:26,847 Epoch[310/310], Step[1150/1251], Loss: 3.5850(3.5372), Acc: 0.4629(0.3908)
2022-01-08 02:15:23,533 Epoch[310/310], Step[1200/1251], Loss: 3.5577(3.5358), Acc: 0.3359(0.3909)
2022-01-08 02:16:20,840 Epoch[310/310], Step[1250/1251], Loss: 3.8564(3.5357), Acc: 0.3740(0.3916)
2022-01-08 02:16:22,291 ----- Validation after Epoch: 310
2022-01-08 02:17:25,535 Val Step[0000/1563], Loss: 0.6700 (0.6700), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-08 02:17:26,842 Val Step[0050/1563], Loss: 2.1717 (0.7071), Acc@1: 0.4062 (0.8585), Acc@5: 0.8750 (0.9632)
2022-01-08 02:17:28,187 Val Step[0100/1563], Loss: 1.8843 (0.9663), Acc@1: 0.5000 (0.7850), Acc@5: 0.8125 (0.9403)
2022-01-08 02:17:29,554 Val Step[0150/1563], Loss: 0.3901 (0.9145), Acc@1: 0.9375 (0.7990), Acc@5: 1.0000 (0.9441)
2022-01-08 02:17:30,793 Val Step[0200/1563], Loss: 0.9315 (0.9212), Acc@1: 0.8125 (0.8008), Acc@5: 0.9375 (0.9429)
2022-01-08 02:17:32,032 Val Step[0250/1563], Loss: 0.4839 (0.8744), Acc@1: 0.9062 (0.8124), Acc@5: 1.0000 (0.9483)
2022-01-08 02:17:33,328 Val Step[0300/1563], Loss: 1.0400 (0.9310), Acc@1: 0.7188 (0.7979), Acc@5: 1.0000 (0.9442)
2022-01-08 02:17:34,577 Val Step[0350/1563], Loss: 0.9079 (0.9387), Acc@1: 0.7812 (0.7931), Acc@5: 0.9062 (0.9456)
2022-01-08 02:17:35,849 Val Step[0400/1563], Loss: 0.8476 (0.9455), Acc@1: 0.8438 (0.7879), Acc@5: 0.9688 (0.9458)
2022-01-08 02:17:37,109 Val Step[0450/1563], Loss: 0.9142 (0.9522), Acc@1: 0.7500 (0.7852), Acc@5: 1.0000 (0.9466)
2022-01-08 02:17:38,364 Val Step[0500/1563], Loss: 0.4323 (0.9433), Acc@1: 0.9062 (0.7879), Acc@5: 1.0000 (0.9478)
2022-01-08 02:17:39,787 Val Step[0550/1563], Loss: 0.7918 (0.9229), Acc@1: 0.7500 (0.7921), Acc@5: 0.9688 (0.9495)
2022-01-08 02:17:41,035 Val Step[0600/1563], Loss: 0.7817 (0.9310), Acc@1: 0.8438 (0.7909), Acc@5: 0.9375 (0.9489)
2022-01-08 02:17:42,323 Val Step[0650/1563], Loss: 0.4936 (0.9504), Acc@1: 0.9062 (0.7866), Acc@5: 1.0000 (0.9461)
2022-01-08 02:17:43,561 Val Step[0700/1563], Loss: 0.9873 (0.9771), Acc@1: 0.8125 (0.7801), Acc@5: 0.9688 (0.9433)
2022-01-08 02:17:44,845 Val Step[0750/1563], Loss: 1.1997 (1.0076), Acc@1: 0.8438 (0.7740), Acc@5: 0.9062 (0.9392)
2022-01-08 02:17:46,240 Val Step[0800/1563], Loss: 0.5905 (1.0434), Acc@1: 0.9062 (0.7651), Acc@5: 1.0000 (0.9347)
2022-01-08 02:17:47,513 Val Step[0850/1563], Loss: 1.2678 (1.0668), Acc@1: 0.6250 (0.7590), Acc@5: 0.9375 (0.9321)
2022-01-08 02:17:48,810 Val Step[0900/1563], Loss: 0.2464 (1.0661), Acc@1: 0.9688 (0.7605), Acc@5: 1.0000 (0.9317)
2022-01-08 02:17:50,176 Val Step[0950/1563], Loss: 1.2176 (1.0854), Acc@1: 0.7812 (0.7569), Acc@5: 0.9062 (0.9285)
2022-01-08 02:17:51,421 Val Step[1000/1563], Loss: 0.5949 (1.1078), Acc@1: 0.9375 (0.7510), Acc@5: 0.9688 (0.9254)
2022-01-08 02:17:52,666 Val Step[1050/1563], Loss: 0.3473 (1.1212), Acc@1: 0.9688 (0.7478), Acc@5: 1.0000 (0.9239)
2022-01-08 02:17:53,902 Val Step[1100/1563], Loss: 0.7241 (1.1341), Acc@1: 0.8750 (0.7453), Acc@5: 1.0000 (0.9221)
2022-01-08 02:17:55,233 Val Step[1150/1563], Loss: 1.1932 (1.1472), Acc@1: 0.7812 (0.7426), Acc@5: 0.8438 (0.9203)
2022-01-08 02:17:56,480 Val Step[1200/1563], Loss: 1.2420 (1.1606), Acc@1: 0.7812 (0.7394), Acc@5: 0.8438 (0.9182)
2022-01-08 02:17:57,737 Val Step[1250/1563], Loss: 0.7512 (1.1712), Acc@1: 0.8750 (0.7377), Acc@5: 0.9375 (0.9165)
2022-01-08 02:17:58,987 Val Step[1300/1563], Loss: 0.7599 (1.1795), Acc@1: 0.9062 (0.7361), Acc@5: 0.9375 (0.9155)
2022-01-08 02:18:00,236 Val Step[1350/1563], Loss: 1.7735 (1.1959), Acc@1: 0.5312 (0.7321), Acc@5: 0.8750 (0.9132)
2022-01-08 02:18:01,557 Val Step[1400/1563], Loss: 0.9721 (1.2022), Acc@1: 0.7500 (0.7308), Acc@5: 0.9375 (0.9123)
2022-01-08 02:18:02,784 Val Step[1450/1563], Loss: 1.4125 (1.2086), Acc@1: 0.7500 (0.7289), Acc@5: 0.9375 (0.9120)
2022-01-08 02:18:04,017 Val Step[1500/1563], Loss: 1.5978 (1.1984), Acc@1: 0.6562 (0.7313), Acc@5: 0.8438 (0.9132)
2022-01-08 02:18:05,272 Val Step[1550/1563], Loss: 0.8727 (1.2004), Acc@1: 0.8750 (0.7305), Acc@5: 0.9062 (0.9128)
2022-01-08 02:18:05,990 ----- Epoch[310/310], Validation Loss: 1.1989, Validation Acc@1: 0.7308, Validation Acc@5: 0.9131, time: 103.70
2022-01-08 02:18:05,991 ----- Epoch[310/310], Train Loss: 3.5357, Train Acc: 0.3916, time: 1504.10, Best Val(epoch308) Acc@1: 0.7317
2022-01-08 02:18:06,158 ----- Save model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Epoch-310-Loss-3.532398318786987.pdparams
2022-01-08 02:18:06,158 ----- Save optim: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Epoch-310-Loss-3.532398318786987.pdopt
2022-01-08 02:18:06,220 ----- Save ema model: /root/paddlejob/workspace/output/train-20220107-23-12-01/PiT-Epoch-310-Loss-3.532398318786987-EMA.pdparams
2022-01-08 02:18:06,220 Training completed.
2022-01-08 02:18:06,220 Top-1 test accuracy: 0.7317
