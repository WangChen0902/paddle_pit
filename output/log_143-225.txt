2022-01-04 15:20:54,801 
AMP: False
AUG:
  AUTO_AUGMENT: None
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: ./Light_ILSVRC2012
  IMAGENET_MEAN: [0.485, 0.456, 0.406]
  IMAGENET_STD: [0.229, 0.224, 0.225]
  IMAGE_SIZE: 224
  NUM_WORKERS: 8
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.0
  DISTILL: False
  DROPOUT: 0.0
  DROP_PATH: 0.1
  NAME: pit_ti
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: ./resume/PiT-Latest
  TRANS:
    BASE_DIMS: [32, 32, 32]
    DEPTH: [2, 6, 4]
    HEADS: [2, 4, 8]
    PATCH_SIZE: 16
    STRIDE: 8
  TYPE: PiT
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output/train-20220104-15-19-59
SAVE_FREQ: 10
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  COOLDOWN_EPOCHS: 10
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  DISTILLATION_ALPHA: 0.5
  DISTILLATION_TAU: 1.0
  DISTILLATION_TYPE: none
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 142
  LINEAR_SCALED_LR: None
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  MODEL_EMA: True
  MODEL_EMA_DECAY: 0.99996
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  TEACHER_MODEL: ./regnety_160
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
2022-01-04 15:20:54,801 ----- world_size = 4, local_rank = 0
2022-01-04 15:20:54,930 ----- Total # of train batch (single gpu): 1251
2022-01-04 15:20:54,930 ----- Total # of val batch (single gpu): 1563
2022-01-04 15:20:55,792 ----- Resume Training: Load model and optmizer from ./resume/PiT-Latest
2022-01-04 15:20:56,128 ----- Load model ema from ./resume/PiT-Latest-EMA.pdparams
2022-01-04 15:20:56,129 Start training from epoch 143.
2022-01-04 15:20:56,129 Now training epoch 143. LR=0.000539
2022-01-04 15:22:20,884 Epoch[143/310], Step[0000/1251], Loss: 4.0592(4.0592), Acc: 0.2070(0.2070)
2022-01-04 15:23:18,511 Epoch[143/310], Step[0050/1251], Loss: 3.9414(4.0613), Acc: 0.3330(0.2986)
2022-01-04 15:24:15,453 Epoch[143/310], Step[0100/1251], Loss: 4.1416(4.0471), Acc: 0.2598(0.3111)
2022-01-04 15:25:11,600 Epoch[143/310], Step[0150/1251], Loss: 4.1056(4.0287), Acc: 0.3291(0.3106)
2022-01-04 15:26:07,940 Epoch[143/310], Step[0200/1251], Loss: 3.7024(4.0040), Acc: 0.3584(0.3117)
2022-01-04 15:27:05,541 Epoch[143/310], Step[0250/1251], Loss: 3.8783(3.9903), Acc: 0.4756(0.3138)
2022-01-04 15:28:01,729 Epoch[143/310], Step[0300/1251], Loss: 3.8795(3.9734), Acc: 0.3652(0.3194)
2022-01-04 15:28:59,673 Epoch[143/310], Step[0350/1251], Loss: 4.4101(3.9954), Acc: 0.3047(0.3199)
2022-01-04 15:29:56,944 Epoch[143/310], Step[0400/1251], Loss: 4.2203(3.9942), Acc: 0.3057(0.3184)
2022-01-04 15:30:54,553 Epoch[143/310], Step[0450/1251], Loss: 4.2518(4.0034), Acc: 0.1426(0.3161)
2022-01-04 15:31:51,198 Epoch[143/310], Step[0500/1251], Loss: 3.5611(4.0020), Acc: 0.4727(0.3170)
2022-01-04 15:32:46,802 Epoch[143/310], Step[0550/1251], Loss: 3.9461(3.9937), Acc: 0.4180(0.3183)
2022-01-04 15:33:42,742 Epoch[143/310], Step[0600/1251], Loss: 3.7434(3.9961), Acc: 0.4541(0.3193)
2022-01-04 15:34:39,634 Epoch[143/310], Step[0650/1251], Loss: 4.8770(3.9993), Acc: 0.1377(0.3175)
2022-01-04 15:35:36,998 Epoch[143/310], Step[0700/1251], Loss: 4.5714(3.9998), Acc: 0.3018(0.3184)
2022-01-04 15:36:35,903 Epoch[143/310], Step[0750/1251], Loss: 3.9112(4.0009), Acc: 0.4648(0.3196)
2022-01-04 15:37:32,492 Epoch[143/310], Step[0800/1251], Loss: 4.2768(4.0009), Acc: 0.3193(0.3192)
2022-01-04 15:38:30,866 Epoch[143/310], Step[0850/1251], Loss: 4.4245(3.9966), Acc: 0.1514(0.3183)
2022-01-04 15:39:29,144 Epoch[143/310], Step[0900/1251], Loss: 4.4637(3.9973), Acc: 0.2549(0.3187)
2022-01-04 15:40:27,680 Epoch[143/310], Step[0950/1251], Loss: 3.9063(3.9952), Acc: 0.2988(0.3194)
2022-01-04 15:41:25,933 Epoch[143/310], Step[1000/1251], Loss: 3.9785(3.9958), Acc: 0.1367(0.3187)
2022-01-04 15:42:23,558 Epoch[143/310], Step[1050/1251], Loss: 3.8405(3.9969), Acc: 0.2627(0.3168)
2022-01-04 15:43:20,625 Epoch[143/310], Step[1100/1251], Loss: 4.2093(3.9970), Acc: 0.2646(0.3163)
2022-01-04 15:44:16,166 Epoch[143/310], Step[1150/1251], Loss: 4.3634(3.9989), Acc: 0.2002(0.3158)
2022-01-04 15:45:14,144 Epoch[143/310], Step[1200/1251], Loss: 4.5138(4.0004), Acc: 0.3213(0.3154)
2022-01-04 15:46:11,213 Epoch[143/310], Step[1250/1251], Loss: 3.9281(4.0008), Acc: 0.3105(0.3150)
2022-01-04 15:46:13,421 ----- Epoch[143/310], Train Loss: 4.0008, Train Acc: 0.3150, time: 1517.29
2022-01-04 15:46:13,573 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 15:46:13,573 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 15:46:13,612 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 15:46:13,612 Now training epoch 144. LR=0.000534
2022-01-04 15:47:28,982 Epoch[144/310], Step[0000/1251], Loss: 3.8255(3.8255), Acc: 0.1338(0.1338)
2022-01-04 15:48:25,625 Epoch[144/310], Step[0050/1251], Loss: 3.3735(4.0509), Acc: 0.2598(0.3102)
2022-01-04 15:49:21,993 Epoch[144/310], Step[0100/1251], Loss: 4.1265(4.0096), Acc: 0.0693(0.3077)
2022-01-04 15:50:18,196 Epoch[144/310], Step[0150/1251], Loss: 3.8333(4.0236), Acc: 0.3350(0.3100)
2022-01-04 15:51:15,422 Epoch[144/310], Step[0200/1251], Loss: 4.1978(4.0170), Acc: 0.3184(0.3140)
2022-01-04 15:52:12,970 Epoch[144/310], Step[0250/1251], Loss: 4.2226(4.0084), Acc: 0.2812(0.3166)
2022-01-04 15:53:08,280 Epoch[144/310], Step[0300/1251], Loss: 3.8075(4.0111), Acc: 0.1631(0.3193)
2022-01-04 15:54:06,293 Epoch[144/310], Step[0350/1251], Loss: 4.5737(4.0157), Acc: 0.2705(0.3178)
2022-01-04 15:55:03,498 Epoch[144/310], Step[0400/1251], Loss: 3.6173(4.0110), Acc: 0.3799(0.3175)
2022-01-04 15:56:00,851 Epoch[144/310], Step[0450/1251], Loss: 3.4868(4.0099), Acc: 0.4688(0.3169)
2022-01-04 15:56:57,934 Epoch[144/310], Step[0500/1251], Loss: 4.0807(4.0089), Acc: 0.1562(0.3151)
2022-01-04 15:57:54,293 Epoch[144/310], Step[0550/1251], Loss: 3.7901(4.0085), Acc: 0.2451(0.3157)
2022-01-04 15:58:50,959 Epoch[144/310], Step[0600/1251], Loss: 4.1355(4.0035), Acc: 0.1631(0.3182)
2022-01-04 15:59:49,316 Epoch[144/310], Step[0650/1251], Loss: 4.1199(3.9971), Acc: 0.2910(0.3166)
2022-01-04 16:00:44,667 Epoch[144/310], Step[0700/1251], Loss: 3.8077(3.9876), Acc: 0.4707(0.3181)
2022-01-04 16:01:41,624 Epoch[144/310], Step[0750/1251], Loss: 4.1696(3.9868), Acc: 0.2451(0.3192)
2022-01-04 16:02:37,506 Epoch[144/310], Step[0800/1251], Loss: 4.0402(3.9900), Acc: 0.3379(0.3176)
2022-01-04 16:03:35,020 Epoch[144/310], Step[0850/1251], Loss: 4.1516(3.9902), Acc: 0.3408(0.3164)
2022-01-04 16:04:31,465 Epoch[144/310], Step[0900/1251], Loss: 3.7279(3.9936), Acc: 0.3262(0.3165)
2022-01-04 16:05:27,793 Epoch[144/310], Step[0950/1251], Loss: 3.9038(3.9973), Acc: 0.3887(0.3161)
2022-01-04 16:06:24,583 Epoch[144/310], Step[1000/1251], Loss: 4.5608(3.9986), Acc: 0.1924(0.3154)
2022-01-04 16:07:22,472 Epoch[144/310], Step[1050/1251], Loss: 4.0031(3.9971), Acc: 0.1768(0.3143)
2022-01-04 16:08:20,043 Epoch[144/310], Step[1100/1251], Loss: 4.1004(4.0006), Acc: 0.1592(0.3143)
2022-01-04 16:09:17,326 Epoch[144/310], Step[1150/1251], Loss: 4.0521(4.0005), Acc: 0.3555(0.3140)
2022-01-04 16:10:14,909 Epoch[144/310], Step[1200/1251], Loss: 4.1363(4.0009), Acc: 0.3447(0.3151)
2022-01-04 16:11:10,023 Epoch[144/310], Step[1250/1251], Loss: 3.3852(4.0020), Acc: 0.5049(0.3157)
2022-01-04 16:11:11,829 ----- Validation after Epoch: 144
2022-01-04 16:12:08,443 Val Step[0000/1563], Loss: 0.8722 (0.8722), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-04 16:12:09,947 Val Step[0050/1563], Loss: 2.9005 (0.9907), Acc@1: 0.3438 (0.7996), Acc@5: 0.8125 (0.9363)
2022-01-04 16:12:11,322 Val Step[0100/1563], Loss: 1.7389 (1.2746), Acc@1: 0.6562 (0.7160), Acc@5: 0.8438 (0.9100)
2022-01-04 16:12:12,698 Val Step[0150/1563], Loss: 0.6199 (1.2020), Acc@1: 0.9062 (0.7382), Acc@5: 0.9688 (0.9149)
2022-01-04 16:12:14,156 Val Step[0200/1563], Loss: 1.2128 (1.2229), Acc@1: 0.7500 (0.7404), Acc@5: 0.9062 (0.9115)
2022-01-04 16:12:15,631 Val Step[0250/1563], Loss: 0.6777 (1.1507), Acc@1: 0.9062 (0.7567), Acc@5: 1.0000 (0.9206)
2022-01-04 16:12:17,059 Val Step[0300/1563], Loss: 1.1112 (1.2146), Acc@1: 0.7500 (0.7372), Acc@5: 0.9062 (0.9150)
2022-01-04 16:12:18,530 Val Step[0350/1563], Loss: 1.3697 (1.2144), Acc@1: 0.7188 (0.7334), Acc@5: 0.9375 (0.9186)
2022-01-04 16:12:19,970 Val Step[0400/1563], Loss: 1.3632 (1.2213), Acc@1: 0.6875 (0.7271), Acc@5: 0.9375 (0.9200)
2022-01-04 16:12:21,372 Val Step[0450/1563], Loss: 1.2683 (1.2271), Acc@1: 0.4688 (0.7238), Acc@5: 1.0000 (0.9202)
2022-01-04 16:12:22,865 Val Step[0500/1563], Loss: 0.5909 (1.2203), Acc@1: 0.9375 (0.7258), Acc@5: 1.0000 (0.9218)
2022-01-04 16:12:24,453 Val Step[0550/1563], Loss: 0.9057 (1.1958), Acc@1: 0.7812 (0.7325), Acc@5: 0.9688 (0.9243)
2022-01-04 16:12:25,916 Val Step[0600/1563], Loss: 1.0012 (1.2010), Acc@1: 0.7812 (0.7313), Acc@5: 0.9688 (0.9235)
2022-01-04 16:12:27,412 Val Step[0650/1563], Loss: 1.1417 (1.2257), Acc@1: 0.7812 (0.7267), Acc@5: 0.9688 (0.9201)
2022-01-04 16:12:28,879 Val Step[0700/1563], Loss: 1.5238 (1.2649), Acc@1: 0.7188 (0.7178), Acc@5: 0.8438 (0.9141)
2022-01-04 16:12:30,296 Val Step[0750/1563], Loss: 1.7730 (1.3028), Acc@1: 0.6250 (0.7099), Acc@5: 0.8438 (0.9085)
2022-01-04 16:12:31,730 Val Step[0800/1563], Loss: 1.5193 (1.3432), Acc@1: 0.6250 (0.7010), Acc@5: 0.9688 (0.9031)
2022-01-04 16:12:33,150 Val Step[0850/1563], Loss: 1.6597 (1.3677), Acc@1: 0.5938 (0.6958), Acc@5: 0.8438 (0.8997)
2022-01-04 16:12:34,531 Val Step[0900/1563], Loss: 0.3409 (1.3667), Acc@1: 0.9375 (0.6974), Acc@5: 1.0000 (0.8989)
2022-01-04 16:12:36,049 Val Step[0950/1563], Loss: 1.8363 (1.3901), Acc@1: 0.7188 (0.6933), Acc@5: 0.8125 (0.8955)
2022-01-04 16:12:37,455 Val Step[1000/1563], Loss: 0.8738 (1.4171), Acc@1: 0.8750 (0.6876), Acc@5: 0.9688 (0.8910)
2022-01-04 16:12:38,870 Val Step[1050/1563], Loss: 0.5195 (1.4322), Acc@1: 0.9375 (0.6844), Acc@5: 0.9688 (0.8895)
2022-01-04 16:12:40,350 Val Step[1100/1563], Loss: 1.2279 (1.4495), Acc@1: 0.7500 (0.6809), Acc@5: 0.9375 (0.8866)
2022-01-04 16:12:41,828 Val Step[1150/1563], Loss: 1.4946 (1.4681), Acc@1: 0.7188 (0.6778), Acc@5: 0.7812 (0.8842)
2022-01-04 16:12:43,210 Val Step[1200/1563], Loss: 1.3788 (1.4824), Acc@1: 0.7500 (0.6751), Acc@5: 0.8438 (0.8817)
2022-01-04 16:12:44,655 Val Step[1250/1563], Loss: 0.6283 (1.4974), Acc@1: 0.9062 (0.6722), Acc@5: 0.9375 (0.8793)
2022-01-04 16:12:46,060 Val Step[1300/1563], Loss: 1.1230 (1.5074), Acc@1: 0.7500 (0.6703), Acc@5: 0.9062 (0.8782)
2022-01-04 16:12:47,450 Val Step[1350/1563], Loss: 2.2163 (1.5288), Acc@1: 0.3438 (0.6655), Acc@5: 0.7188 (0.8749)
2022-01-04 16:12:48,925 Val Step[1400/1563], Loss: 1.4355 (1.5394), Acc@1: 0.7188 (0.6628), Acc@5: 0.8438 (0.8733)
2022-01-04 16:12:50,312 Val Step[1450/1563], Loss: 1.7435 (1.5456), Acc@1: 0.5625 (0.6611), Acc@5: 0.9062 (0.8729)
2022-01-04 16:12:51,706 Val Step[1500/1563], Loss: 1.7721 (1.5319), Acc@1: 0.5938 (0.6641), Acc@5: 0.8750 (0.8747)
2022-01-04 16:12:53,089 Val Step[1550/1563], Loss: 1.0194 (1.5315), Acc@1: 0.8750 (0.6642), Acc@5: 0.9062 (0.8746)
2022-01-04 16:12:53,886 ----- Epoch[144/310], Validation Loss: 1.5297, Validation Acc@1: 0.6644, Validation Acc@5: 0.8749, time: 102.05
2022-01-04 16:12:53,886 ----- Epoch[144/310], Train Loss: 4.0020, Train Acc: 0.3157, time: 1498.21, Best Val(epoch144) Acc@1: 0.6644
2022-01-04 16:12:54,040 Max accuracy so far: 0.6644 at epoch_144
2022-01-04 16:12:54,040 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-04 16:12:54,040 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-04 16:12:54,079 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-04 16:12:54,221 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 16:12:54,221 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 16:12:54,350 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 16:12:54,351 Now training epoch 145. LR=0.000529
2022-01-04 16:14:13,327 Epoch[145/310], Step[0000/1251], Loss: 3.8180(3.8180), Acc: 0.3594(0.3594)
2022-01-04 16:15:09,584 Epoch[145/310], Step[0050/1251], Loss: 3.7933(3.9377), Acc: 0.3447(0.3401)
2022-01-04 16:16:06,636 Epoch[145/310], Step[0100/1251], Loss: 3.7118(3.9589), Acc: 0.3828(0.3388)
2022-01-04 16:17:02,363 Epoch[145/310], Step[0150/1251], Loss: 4.0569(3.9637), Acc: 0.4277(0.3272)
2022-01-04 16:17:57,706 Epoch[145/310], Step[0200/1251], Loss: 3.7591(3.9779), Acc: 0.4307(0.3287)
2022-01-04 16:18:55,423 Epoch[145/310], Step[0250/1251], Loss: 3.5698(3.9797), Acc: 0.5107(0.3296)
2022-01-04 16:19:53,262 Epoch[145/310], Step[0300/1251], Loss: 3.9639(3.9754), Acc: 0.3809(0.3284)
2022-01-04 16:20:50,478 Epoch[145/310], Step[0350/1251], Loss: 4.5588(3.9794), Acc: 0.3350(0.3297)
2022-01-04 16:21:47,256 Epoch[145/310], Step[0400/1251], Loss: 4.0414(3.9808), Acc: 0.4258(0.3293)
2022-01-04 16:22:45,614 Epoch[145/310], Step[0450/1251], Loss: 4.1156(3.9782), Acc: 0.3506(0.3267)
2022-01-04 16:23:43,253 Epoch[145/310], Step[0500/1251], Loss: 4.1816(3.9768), Acc: 0.1611(0.3246)
2022-01-04 16:24:40,750 Epoch[145/310], Step[0550/1251], Loss: 3.5401(3.9833), Acc: 0.3740(0.3233)
2022-01-04 16:25:38,796 Epoch[145/310], Step[0600/1251], Loss: 3.9827(3.9836), Acc: 0.4590(0.3247)
2022-01-04 16:26:36,654 Epoch[145/310], Step[0650/1251], Loss: 4.5495(3.9844), Acc: 0.3330(0.3240)
2022-01-04 16:27:33,718 Epoch[145/310], Step[0700/1251], Loss: 4.1324(3.9871), Acc: 0.1270(0.3223)
2022-01-04 16:28:31,327 Epoch[145/310], Step[0750/1251], Loss: 3.9855(3.9924), Acc: 0.1758(0.3201)
2022-01-04 16:29:28,542 Epoch[145/310], Step[0800/1251], Loss: 4.2633(3.9951), Acc: 0.3135(0.3210)
2022-01-04 16:30:25,420 Epoch[145/310], Step[0850/1251], Loss: 4.0700(3.9963), Acc: 0.2861(0.3202)
2022-01-04 16:31:21,757 Epoch[145/310], Step[0900/1251], Loss: 3.9646(3.9967), Acc: 0.2959(0.3180)
2022-01-04 16:32:17,899 Epoch[145/310], Step[0950/1251], Loss: 3.9346(3.9938), Acc: 0.1230(0.3185)
2022-01-04 16:33:15,491 Epoch[145/310], Step[1000/1251], Loss: 4.2590(3.9942), Acc: 0.3408(0.3188)
2022-01-04 16:34:12,836 Epoch[145/310], Step[1050/1251], Loss: 3.9367(3.9927), Acc: 0.2158(0.3183)
2022-01-04 16:35:10,926 Epoch[145/310], Step[1100/1251], Loss: 4.0441(3.9936), Acc: 0.2217(0.3179)
2022-01-04 16:36:07,491 Epoch[145/310], Step[1150/1251], Loss: 4.3144(3.9985), Acc: 0.3467(0.3178)
2022-01-04 16:37:04,334 Epoch[145/310], Step[1200/1251], Loss: 3.8630(3.9949), Acc: 0.3994(0.3178)
2022-01-04 16:38:01,006 Epoch[145/310], Step[1250/1251], Loss: 4.1375(3.9988), Acc: 0.3213(0.3168)
2022-01-04 16:38:02,863 ----- Epoch[145/310], Train Loss: 3.9988, Train Acc: 0.3168, time: 1508.51, Best Val(epoch144) Acc@1: 0.6644
2022-01-04 16:38:03,030 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 16:38:03,030 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 16:38:03,156 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 16:38:03,157 Now training epoch 146. LR=0.000523
2022-01-04 16:39:22,559 Epoch[146/310], Step[0000/1251], Loss: 4.0688(4.0688), Acc: 0.0410(0.0410)
2022-01-04 16:40:19,722 Epoch[146/310], Step[0050/1251], Loss: 3.9285(4.0277), Acc: 0.4209(0.3198)
2022-01-04 16:41:15,035 Epoch[146/310], Step[0100/1251], Loss: 3.9111(4.0263), Acc: 0.3174(0.3218)
2022-01-04 16:42:10,890 Epoch[146/310], Step[0150/1251], Loss: 3.9024(4.0028), Acc: 0.4736(0.3299)
2022-01-04 16:43:06,478 Epoch[146/310], Step[0200/1251], Loss: 4.3727(3.9888), Acc: 0.2676(0.3265)
2022-01-04 16:44:01,050 Epoch[146/310], Step[0250/1251], Loss: 4.0693(3.9935), Acc: 0.2275(0.3194)
2022-01-04 16:44:57,096 Epoch[146/310], Step[0300/1251], Loss: 3.7980(3.9938), Acc: 0.1738(0.3225)
2022-01-04 16:45:53,462 Epoch[146/310], Step[0350/1251], Loss: 3.7784(3.9861), Acc: 0.2656(0.3245)
2022-01-04 16:46:48,904 Epoch[146/310], Step[0400/1251], Loss: 4.0414(3.9837), Acc: 0.1875(0.3250)
2022-01-04 16:47:45,042 Epoch[146/310], Step[0450/1251], Loss: 3.4154(3.9837), Acc: 0.5713(0.3242)
2022-01-04 16:48:41,659 Epoch[146/310], Step[0500/1251], Loss: 4.1832(3.9905), Acc: 0.2979(0.3229)
2022-01-04 16:49:39,058 Epoch[146/310], Step[0550/1251], Loss: 4.1555(3.9887), Acc: 0.2705(0.3239)
2022-01-04 16:50:37,655 Epoch[146/310], Step[0600/1251], Loss: 3.9230(3.9891), Acc: 0.3115(0.3241)
2022-01-04 16:51:33,941 Epoch[146/310], Step[0650/1251], Loss: 3.8478(3.9924), Acc: 0.3291(0.3228)
2022-01-04 16:52:32,091 Epoch[146/310], Step[0700/1251], Loss: 3.9902(3.9967), Acc: 0.4238(0.3215)
2022-01-04 16:53:29,539 Epoch[146/310], Step[0750/1251], Loss: 3.6522(3.9994), Acc: 0.3545(0.3204)
2022-01-04 16:54:26,456 Epoch[146/310], Step[0800/1251], Loss: 3.8095(4.0049), Acc: 0.3506(0.3182)
2022-01-04 16:55:23,526 Epoch[146/310], Step[0850/1251], Loss: 4.1500(4.0103), Acc: 0.4072(0.3167)
2022-01-04 16:56:21,671 Epoch[146/310], Step[0900/1251], Loss: 4.0910(4.0095), Acc: 0.1826(0.3175)
2022-01-04 16:57:19,766 Epoch[146/310], Step[0950/1251], Loss: 4.0277(4.0113), Acc: 0.1758(0.3167)
2022-01-04 16:58:16,353 Epoch[146/310], Step[1000/1251], Loss: 3.9469(4.0089), Acc: 0.3916(0.3167)
2022-01-04 16:59:13,249 Epoch[146/310], Step[1050/1251], Loss: 4.3466(4.0097), Acc: 0.2539(0.3175)
2022-01-04 17:00:10,949 Epoch[146/310], Step[1100/1251], Loss: 4.6709(4.0087), Acc: 0.1855(0.3177)
2022-01-04 17:01:06,645 Epoch[146/310], Step[1150/1251], Loss: 3.0612(4.0044), Acc: 0.5830(0.3187)
2022-01-04 17:02:03,423 Epoch[146/310], Step[1200/1251], Loss: 4.0108(4.0063), Acc: 0.3623(0.3190)
2022-01-04 17:02:59,516 Epoch[146/310], Step[1250/1251], Loss: 4.4531(4.0074), Acc: 0.3906(0.3184)
2022-01-04 17:03:01,379 ----- Validation after Epoch: 146
2022-01-04 17:04:00,855 Val Step[0000/1563], Loss: 0.8081 (0.8081), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-04 17:04:02,496 Val Step[0050/1563], Loss: 2.2534 (0.9426), Acc@1: 0.5000 (0.8051), Acc@5: 0.7812 (0.9387)
2022-01-04 17:04:03,971 Val Step[0100/1563], Loss: 1.9726 (1.2330), Acc@1: 0.5938 (0.7262), Acc@5: 0.8125 (0.9155)
2022-01-04 17:04:05,340 Val Step[0150/1563], Loss: 0.5248 (1.1641), Acc@1: 0.9062 (0.7434), Acc@5: 1.0000 (0.9216)
2022-01-04 17:04:06,798 Val Step[0200/1563], Loss: 1.3310 (1.1846), Acc@1: 0.7500 (0.7444), Acc@5: 0.9688 (0.9173)
2022-01-04 17:04:08,197 Val Step[0250/1563], Loss: 0.6111 (1.1280), Acc@1: 0.9375 (0.7591), Acc@5: 1.0000 (0.9246)
2022-01-04 17:04:09,599 Val Step[0300/1563], Loss: 1.2600 (1.1966), Acc@1: 0.6562 (0.7395), Acc@5: 0.9062 (0.9194)
2022-01-04 17:04:10,984 Val Step[0350/1563], Loss: 1.1725 (1.2118), Acc@1: 0.6875 (0.7332), Acc@5: 0.9375 (0.9210)
2022-01-04 17:04:12,368 Val Step[0400/1563], Loss: 1.1829 (1.2228), Acc@1: 0.7500 (0.7270), Acc@5: 0.9375 (0.9214)
2022-01-04 17:04:13,841 Val Step[0450/1563], Loss: 1.1543 (1.2343), Acc@1: 0.6562 (0.7226), Acc@5: 0.9688 (0.9215)
2022-01-04 17:04:15,325 Val Step[0500/1563], Loss: 0.4720 (1.2295), Acc@1: 0.9062 (0.7251), Acc@5: 1.0000 (0.9223)
2022-01-04 17:04:16,772 Val Step[0550/1563], Loss: 0.7862 (1.2075), Acc@1: 0.8125 (0.7316), Acc@5: 0.9375 (0.9244)
2022-01-04 17:04:18,299 Val Step[0600/1563], Loss: 0.8831 (1.2111), Acc@1: 0.8125 (0.7299), Acc@5: 0.9688 (0.9242)
2022-01-04 17:04:19,721 Val Step[0650/1563], Loss: 0.5807 (1.2333), Acc@1: 0.8438 (0.7252), Acc@5: 1.0000 (0.9205)
2022-01-04 17:04:21,142 Val Step[0700/1563], Loss: 1.1549 (1.2644), Acc@1: 0.7812 (0.7188), Acc@5: 0.9375 (0.9163)
2022-01-04 17:04:22,612 Val Step[0750/1563], Loss: 1.6711 (1.3006), Acc@1: 0.6562 (0.7109), Acc@5: 0.8125 (0.9112)
2022-01-04 17:04:24,011 Val Step[0800/1563], Loss: 1.3360 (1.3442), Acc@1: 0.7188 (0.7002), Acc@5: 0.9688 (0.9052)
2022-01-04 17:04:25,427 Val Step[0850/1563], Loss: 1.6799 (1.3774), Acc@1: 0.6562 (0.6932), Acc@5: 0.8750 (0.9006)
2022-01-04 17:04:26,797 Val Step[0900/1563], Loss: 0.4961 (1.3764), Acc@1: 0.9375 (0.6947), Acc@5: 0.9688 (0.9001)
2022-01-04 17:04:28,321 Val Step[0950/1563], Loss: 1.2365 (1.3984), Acc@1: 0.7500 (0.6906), Acc@5: 0.9375 (0.8966)
2022-01-04 17:04:29,714 Val Step[1000/1563], Loss: 0.5946 (1.4219), Acc@1: 0.9375 (0.6854), Acc@5: 1.0000 (0.8929)
2022-01-04 17:04:31,240 Val Step[1050/1563], Loss: 0.5209 (1.4352), Acc@1: 0.9375 (0.6819), Acc@5: 0.9688 (0.8912)
2022-01-04 17:04:32,753 Val Step[1100/1563], Loss: 1.3986 (1.4526), Acc@1: 0.7500 (0.6781), Acc@5: 0.8750 (0.8884)
2022-01-04 17:04:34,163 Val Step[1150/1563], Loss: 1.4964 (1.4694), Acc@1: 0.6875 (0.6750), Acc@5: 0.7812 (0.8857)
2022-01-04 17:04:35,576 Val Step[1200/1563], Loss: 1.6499 (1.4859), Acc@1: 0.7188 (0.6724), Acc@5: 0.8438 (0.8828)
2022-01-04 17:04:37,014 Val Step[1250/1563], Loss: 0.8513 (1.4993), Acc@1: 0.8750 (0.6702), Acc@5: 0.9375 (0.8802)
2022-01-04 17:04:38,394 Val Step[1300/1563], Loss: 1.0646 (1.5101), Acc@1: 0.8125 (0.6681), Acc@5: 0.9062 (0.8789)
2022-01-04 17:04:39,808 Val Step[1350/1563], Loss: 2.4615 (1.5297), Acc@1: 0.2812 (0.6635), Acc@5: 0.6875 (0.8759)
2022-01-04 17:04:41,228 Val Step[1400/1563], Loss: 1.0663 (1.5376), Acc@1: 0.7812 (0.6620), Acc@5: 0.9062 (0.8745)
2022-01-04 17:04:42,548 Val Step[1450/1563], Loss: 1.9677 (1.5427), Acc@1: 0.5000 (0.6606), Acc@5: 0.9062 (0.8740)
2022-01-04 17:04:43,925 Val Step[1500/1563], Loss: 2.3074 (1.5291), Acc@1: 0.5000 (0.6636), Acc@5: 0.7812 (0.8757)
2022-01-04 17:04:45,318 Val Step[1550/1563], Loss: 1.1012 (1.5281), Acc@1: 0.8750 (0.6638), Acc@5: 0.9062 (0.8761)
2022-01-04 17:04:46,166 ----- Epoch[146/310], Validation Loss: 1.5268, Validation Acc@1: 0.6640, Validation Acc@5: 0.8762, time: 104.78
2022-01-04 17:04:46,166 ----- Epoch[146/310], Train Loss: 4.0074, Train Acc: 0.3184, time: 1498.22, Best Val(epoch144) Acc@1: 0.6644
2022-01-04 17:04:46,344 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 17:04:46,345 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 17:04:46,465 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 17:04:46,466 Now training epoch 147. LR=0.000518
2022-01-04 17:06:04,834 Epoch[147/310], Step[0000/1251], Loss: 3.7217(3.7217), Acc: 0.3662(0.3662)
2022-01-04 17:07:01,444 Epoch[147/310], Step[0050/1251], Loss: 4.3155(3.9876), Acc: 0.2539(0.3026)
2022-01-04 17:07:55,598 Epoch[147/310], Step[0100/1251], Loss: 4.3979(3.9816), Acc: 0.3096(0.3320)
2022-01-04 17:08:52,594 Epoch[147/310], Step[0150/1251], Loss: 3.7760(3.9503), Acc: 0.4893(0.3392)
2022-01-04 17:09:48,879 Epoch[147/310], Step[0200/1251], Loss: 4.1123(3.9589), Acc: 0.3076(0.3295)
2022-01-04 17:10:45,844 Epoch[147/310], Step[0250/1251], Loss: 4.4082(3.9733), Acc: 0.3623(0.3288)
2022-01-04 17:11:44,439 Epoch[147/310], Step[0300/1251], Loss: 4.1230(3.9761), Acc: 0.2607(0.3269)
2022-01-04 17:12:41,856 Epoch[147/310], Step[0350/1251], Loss: 4.0211(3.9761), Acc: 0.3164(0.3255)
2022-01-04 17:13:39,936 Epoch[147/310], Step[0400/1251], Loss: 4.2124(3.9825), Acc: 0.3223(0.3256)
2022-01-04 17:14:37,855 Epoch[147/310], Step[0450/1251], Loss: 3.5569(3.9885), Acc: 0.5244(0.3245)
2022-01-04 17:15:33,969 Epoch[147/310], Step[0500/1251], Loss: 4.2116(3.9838), Acc: 0.4346(0.3242)
2022-01-04 17:16:29,551 Epoch[147/310], Step[0550/1251], Loss: 3.8523(3.9814), Acc: 0.1660(0.3245)
2022-01-04 17:17:27,116 Epoch[147/310], Step[0600/1251], Loss: 3.5591(3.9856), Acc: 0.2100(0.3224)
2022-01-04 17:18:22,979 Epoch[147/310], Step[0650/1251], Loss: 4.2240(3.9880), Acc: 0.4238(0.3200)
2022-01-04 17:19:19,922 Epoch[147/310], Step[0700/1251], Loss: 3.5388(3.9918), Acc: 0.5205(0.3186)
2022-01-04 17:20:16,975 Epoch[147/310], Step[0750/1251], Loss: 4.1912(3.9875), Acc: 0.3037(0.3195)
2022-01-04 17:21:13,639 Epoch[147/310], Step[0800/1251], Loss: 3.9229(3.9832), Acc: 0.4365(0.3198)
2022-01-04 17:22:11,128 Epoch[147/310], Step[0850/1251], Loss: 4.4256(3.9856), Acc: 0.2959(0.3193)
2022-01-04 17:23:07,211 Epoch[147/310], Step[0900/1251], Loss: 3.7443(3.9881), Acc: 0.2207(0.3190)
2022-01-04 17:24:04,339 Epoch[147/310], Step[0950/1251], Loss: 3.2940(3.9897), Acc: 0.4072(0.3201)
2022-01-04 17:25:00,372 Epoch[147/310], Step[1000/1251], Loss: 4.3924(3.9916), Acc: 0.2715(0.3194)
2022-01-04 17:25:56,987 Epoch[147/310], Step[1050/1251], Loss: 4.1384(3.9889), Acc: 0.2754(0.3197)
2022-01-04 17:26:54,069 Epoch[147/310], Step[1100/1251], Loss: 3.6874(3.9900), Acc: 0.1787(0.3198)
2022-01-04 17:27:50,724 Epoch[147/310], Step[1150/1251], Loss: 4.2297(3.9893), Acc: 0.3594(0.3200)
2022-01-04 17:28:47,342 Epoch[147/310], Step[1200/1251], Loss: 3.8299(3.9895), Acc: 0.4785(0.3197)
2022-01-04 17:29:44,775 Epoch[147/310], Step[1250/1251], Loss: 4.4148(3.9914), Acc: 0.3223(0.3186)
2022-01-04 17:29:46,706 ----- Epoch[147/310], Train Loss: 3.9914, Train Acc: 0.3186, time: 1500.24, Best Val(epoch144) Acc@1: 0.6644
2022-01-04 17:29:46,881 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 17:29:46,881 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 17:29:47,245 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 17:29:47,246 Now training epoch 148. LR=0.000513
2022-01-04 17:31:06,112 Epoch[148/310], Step[0000/1251], Loss: 4.1730(4.1730), Acc: 0.2207(0.2207)
2022-01-04 17:32:04,293 Epoch[148/310], Step[0050/1251], Loss: 4.3878(4.0674), Acc: 0.2734(0.3185)
2022-01-04 17:33:00,932 Epoch[148/310], Step[0100/1251], Loss: 3.2247(4.0429), Acc: 0.4053(0.3228)
2022-01-04 17:33:57,479 Epoch[148/310], Step[0150/1251], Loss: 4.3324(4.0648), Acc: 0.2471(0.3197)
2022-01-04 17:34:54,365 Epoch[148/310], Step[0200/1251], Loss: 3.7507(4.0461), Acc: 0.4492(0.3154)
2022-01-04 17:35:51,816 Epoch[148/310], Step[0250/1251], Loss: 4.1629(4.0332), Acc: 0.2568(0.3140)
2022-01-04 17:36:49,106 Epoch[148/310], Step[0300/1251], Loss: 3.8632(4.0368), Acc: 0.3271(0.3131)
2022-01-04 17:37:46,035 Epoch[148/310], Step[0350/1251], Loss: 3.7209(4.0286), Acc: 0.1309(0.3145)
2022-01-04 17:38:41,904 Epoch[148/310], Step[0400/1251], Loss: 4.3052(4.0244), Acc: 0.3486(0.3173)
2022-01-04 17:39:39,017 Epoch[148/310], Step[0450/1251], Loss: 3.5750(4.0096), Acc: 0.2129(0.3207)
2022-01-04 17:40:36,931 Epoch[148/310], Step[0500/1251], Loss: 4.4533(4.0085), Acc: 0.1768(0.3200)
2022-01-04 17:41:34,554 Epoch[148/310], Step[0550/1251], Loss: 4.1534(4.0096), Acc: 0.3320(0.3190)
2022-01-04 17:42:31,531 Epoch[148/310], Step[0600/1251], Loss: 4.1029(4.0011), Acc: 0.3008(0.3184)
2022-01-04 17:43:28,335 Epoch[148/310], Step[0650/1251], Loss: 3.5382(4.0019), Acc: 0.3037(0.3177)
2022-01-04 17:44:26,027 Epoch[148/310], Step[0700/1251], Loss: 3.8411(4.0023), Acc: 0.2734(0.3177)
2022-01-04 17:45:23,043 Epoch[148/310], Step[0750/1251], Loss: 4.0032(3.9983), Acc: 0.4482(0.3183)
2022-01-04 17:46:16,936 Epoch[148/310], Step[0800/1251], Loss: 4.3221(3.9970), Acc: 0.2881(0.3201)
2022-01-04 17:47:12,737 Epoch[148/310], Step[0850/1251], Loss: 4.0863(3.9984), Acc: 0.2549(0.3202)
2022-01-04 17:48:08,665 Epoch[148/310], Step[0900/1251], Loss: 3.7379(3.9980), Acc: 0.2783(0.3199)
2022-01-04 17:49:06,300 Epoch[148/310], Step[0950/1251], Loss: 3.7197(3.9967), Acc: 0.4180(0.3191)
2022-01-04 17:50:02,312 Epoch[148/310], Step[1000/1251], Loss: 3.9492(3.9962), Acc: 0.4014(0.3186)
2022-01-04 17:50:59,215 Epoch[148/310], Step[1050/1251], Loss: 3.8847(3.9961), Acc: 0.5020(0.3169)
2022-01-04 17:51:57,034 Epoch[148/310], Step[1100/1251], Loss: 4.1036(3.9947), Acc: 0.3213(0.3170)
2022-01-04 17:52:54,421 Epoch[148/310], Step[1150/1251], Loss: 4.0560(3.9919), Acc: 0.4688(0.3176)
2022-01-04 17:53:52,258 Epoch[148/310], Step[1200/1251], Loss: 4.6313(3.9930), Acc: 0.2529(0.3174)
2022-01-04 17:54:49,779 Epoch[148/310], Step[1250/1251], Loss: 4.0035(3.9947), Acc: 0.2334(0.3186)
2022-01-04 17:54:51,772 ----- Validation after Epoch: 148
2022-01-04 17:55:48,589 Val Step[0000/1563], Loss: 0.7406 (0.7406), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-04 17:55:50,084 Val Step[0050/1563], Loss: 2.3987 (0.9600), Acc@1: 0.4375 (0.8039), Acc@5: 0.8125 (0.9381)
2022-01-04 17:55:51,529 Val Step[0100/1563], Loss: 2.0174 (1.2588), Acc@1: 0.5312 (0.7144), Acc@5: 0.7812 (0.9072)
2022-01-04 17:55:53,053 Val Step[0150/1563], Loss: 0.5137 (1.1747), Acc@1: 0.8750 (0.7351), Acc@5: 0.9688 (0.9176)
2022-01-04 17:55:54,462 Val Step[0200/1563], Loss: 1.1428 (1.1905), Acc@1: 0.6875 (0.7383), Acc@5: 0.9375 (0.9137)
2022-01-04 17:55:55,818 Val Step[0250/1563], Loss: 0.6088 (1.1349), Acc@1: 1.0000 (0.7526), Acc@5: 1.0000 (0.9223)
2022-01-04 17:55:57,243 Val Step[0300/1563], Loss: 1.4686 (1.1993), Acc@1: 0.6875 (0.7340), Acc@5: 0.8750 (0.9163)
2022-01-04 17:55:58,787 Val Step[0350/1563], Loss: 1.3921 (1.2207), Acc@1: 0.6875 (0.7264), Acc@5: 0.9062 (0.9164)
2022-01-04 17:56:00,284 Val Step[0400/1563], Loss: 1.1125 (1.2251), Acc@1: 0.7812 (0.7216), Acc@5: 0.9688 (0.9180)
2022-01-04 17:56:01,884 Val Step[0450/1563], Loss: 1.0309 (1.2278), Acc@1: 0.6250 (0.7197), Acc@5: 1.0000 (0.9190)
2022-01-04 17:56:03,393 Val Step[0500/1563], Loss: 0.4270 (1.2152), Acc@1: 0.9375 (0.7226), Acc@5: 1.0000 (0.9205)
2022-01-04 17:56:04,978 Val Step[0550/1563], Loss: 0.8604 (1.1918), Acc@1: 0.8438 (0.7300), Acc@5: 0.9688 (0.9229)
2022-01-04 17:56:06,509 Val Step[0600/1563], Loss: 0.9850 (1.1979), Acc@1: 0.7812 (0.7305), Acc@5: 0.9688 (0.9220)
2022-01-04 17:56:07,982 Val Step[0650/1563], Loss: 0.4759 (1.2226), Acc@1: 1.0000 (0.7257), Acc@5: 1.0000 (0.9182)
2022-01-04 17:56:09,361 Val Step[0700/1563], Loss: 1.1415 (1.2541), Acc@1: 0.7500 (0.7193), Acc@5: 0.9375 (0.9139)
2022-01-04 17:56:10,848 Val Step[0750/1563], Loss: 1.4638 (1.2878), Acc@1: 0.6875 (0.7128), Acc@5: 0.8438 (0.9091)
2022-01-04 17:56:12,281 Val Step[0800/1563], Loss: 1.4620 (1.3325), Acc@1: 0.6562 (0.7025), Acc@5: 1.0000 (0.9027)
2022-01-04 17:56:13,691 Val Step[0850/1563], Loss: 1.2326 (1.3587), Acc@1: 0.6875 (0.6969), Acc@5: 0.9375 (0.8994)
2022-01-04 17:56:15,141 Val Step[0900/1563], Loss: 0.4019 (1.3601), Acc@1: 0.9375 (0.6981), Acc@5: 0.9688 (0.8989)
2022-01-04 17:56:16,642 Val Step[0950/1563], Loss: 1.7174 (1.3819), Acc@1: 0.5938 (0.6934), Acc@5: 0.8438 (0.8959)
2022-01-04 17:56:18,009 Val Step[1000/1563], Loss: 0.6955 (1.4054), Acc@1: 0.8750 (0.6878), Acc@5: 0.9688 (0.8922)
2022-01-04 17:56:19,373 Val Step[1050/1563], Loss: 0.3607 (1.4162), Acc@1: 0.9688 (0.6857), Acc@5: 0.9688 (0.8908)
2022-01-04 17:56:20,761 Val Step[1100/1563], Loss: 1.5192 (1.4330), Acc@1: 0.7812 (0.6821), Acc@5: 0.8750 (0.8878)
2022-01-04 17:56:22,156 Val Step[1150/1563], Loss: 1.2963 (1.4496), Acc@1: 0.7188 (0.6784), Acc@5: 0.8438 (0.8850)
2022-01-04 17:56:23,560 Val Step[1200/1563], Loss: 1.5817 (1.4673), Acc@1: 0.7188 (0.6746), Acc@5: 0.8438 (0.8822)
2022-01-04 17:56:24,960 Val Step[1250/1563], Loss: 0.8231 (1.4842), Acc@1: 0.8750 (0.6716), Acc@5: 0.9062 (0.8796)
2022-01-04 17:56:26,340 Val Step[1300/1563], Loss: 1.4926 (1.4951), Acc@1: 0.6562 (0.6695), Acc@5: 0.8750 (0.8782)
2022-01-04 17:56:27,715 Val Step[1350/1563], Loss: 2.2068 (1.5147), Acc@1: 0.4062 (0.6648), Acc@5: 0.7500 (0.8750)
2022-01-04 17:56:29,155 Val Step[1400/1563], Loss: 1.4015 (1.5240), Acc@1: 0.7500 (0.6623), Acc@5: 0.9375 (0.8738)
2022-01-04 17:56:30,552 Val Step[1450/1563], Loss: 1.7290 (1.5322), Acc@1: 0.6562 (0.6607), Acc@5: 0.8438 (0.8730)
2022-01-04 17:56:32,056 Val Step[1500/1563], Loss: 1.6753 (1.5226), Acc@1: 0.6875 (0.6628), Acc@5: 0.8750 (0.8745)
2022-01-04 17:56:33,484 Val Step[1550/1563], Loss: 1.0568 (1.5231), Acc@1: 0.8750 (0.6626), Acc@5: 0.9062 (0.8746)
2022-01-04 17:56:34,913 ----- Epoch[148/310], Validation Loss: 1.5203, Validation Acc@1: 0.6632, Validation Acc@5: 0.8748, time: 103.14
2022-01-04 17:56:34,914 ----- Epoch[148/310], Train Loss: 3.9947, Train Acc: 0.3186, time: 1504.52, Best Val(epoch144) Acc@1: 0.6644
2022-01-04 17:56:35,094 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 17:56:35,094 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 17:56:35,201 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 17:56:35,202 Now training epoch 149. LR=0.000508
2022-01-04 17:57:53,129 Epoch[149/310], Step[0000/1251], Loss: 3.6310(3.6310), Acc: 0.3486(0.3486)
2022-01-04 17:58:48,274 Epoch[149/310], Step[0050/1251], Loss: 4.1790(3.9833), Acc: 0.2305(0.3321)
2022-01-04 17:59:44,400 Epoch[149/310], Step[0100/1251], Loss: 4.1865(3.9734), Acc: 0.3691(0.3226)
2022-01-04 18:00:41,462 Epoch[149/310], Step[0150/1251], Loss: 3.9101(3.9713), Acc: 0.4580(0.3167)
2022-01-04 18:01:39,387 Epoch[149/310], Step[0200/1251], Loss: 3.8735(3.9759), Acc: 0.3262(0.3210)
2022-01-04 18:02:36,734 Epoch[149/310], Step[0250/1251], Loss: 4.1885(3.9716), Acc: 0.2539(0.3233)
2022-01-04 18:03:35,153 Epoch[149/310], Step[0300/1251], Loss: 3.9516(3.9685), Acc: 0.3252(0.3209)
2022-01-04 18:04:32,470 Epoch[149/310], Step[0350/1251], Loss: 4.2342(3.9662), Acc: 0.2480(0.3231)
2022-01-04 18:05:28,415 Epoch[149/310], Step[0400/1251], Loss: 3.8059(3.9800), Acc: 0.2324(0.3209)
2022-01-04 18:06:26,365 Epoch[149/310], Step[0450/1251], Loss: 3.8459(3.9810), Acc: 0.2305(0.3206)
2022-01-04 18:07:23,788 Epoch[149/310], Step[0500/1251], Loss: 4.3132(3.9801), Acc: 0.3125(0.3216)
2022-01-04 18:08:20,828 Epoch[149/310], Step[0550/1251], Loss: 3.6598(3.9762), Acc: 0.3291(0.3214)
2022-01-04 18:09:17,132 Epoch[149/310], Step[0600/1251], Loss: 4.4284(3.9789), Acc: 0.2061(0.3220)
2022-01-04 18:10:14,082 Epoch[149/310], Step[0650/1251], Loss: 4.4589(3.9770), Acc: 0.2744(0.3231)
2022-01-04 18:11:09,743 Epoch[149/310], Step[0700/1251], Loss: 4.4225(3.9760), Acc: 0.1514(0.3233)
2022-01-04 18:12:08,216 Epoch[149/310], Step[0750/1251], Loss: 4.6434(3.9772), Acc: 0.3076(0.3234)
2022-01-04 18:13:05,874 Epoch[149/310], Step[0800/1251], Loss: 3.6458(3.9791), Acc: 0.2236(0.3231)
2022-01-04 18:14:04,497 Epoch[149/310], Step[0850/1251], Loss: 4.3261(3.9801), Acc: 0.3213(0.3236)
2022-01-04 18:15:01,912 Epoch[149/310], Step[0900/1251], Loss: 3.8333(3.9798), Acc: 0.2305(0.3230)
2022-01-04 18:15:59,729 Epoch[149/310], Step[0950/1251], Loss: 4.0029(3.9788), Acc: 0.4727(0.3225)
2022-01-04 18:16:55,295 Epoch[149/310], Step[1000/1251], Loss: 3.6516(3.9805), Acc: 0.2988(0.3232)
2022-01-04 18:17:52,612 Epoch[149/310], Step[1050/1251], Loss: 4.0447(3.9819), Acc: 0.3271(0.3230)
2022-01-04 18:18:48,149 Epoch[149/310], Step[1100/1251], Loss: 4.4290(3.9837), Acc: 0.1748(0.3228)
2022-01-04 18:19:44,746 Epoch[149/310], Step[1150/1251], Loss: 4.0639(3.9875), Acc: 0.3887(0.3221)
2022-01-04 18:20:41,637 Epoch[149/310], Step[1200/1251], Loss: 4.0349(3.9861), Acc: 0.2549(0.3222)
2022-01-04 18:21:38,335 Epoch[149/310], Step[1250/1251], Loss: 4.3310(3.9872), Acc: 0.3086(0.3226)
2022-01-04 18:21:40,398 ----- Epoch[149/310], Train Loss: 3.9872, Train Acc: 0.3226, time: 1505.19, Best Val(epoch144) Acc@1: 0.6644
2022-01-04 18:21:40,573 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 18:21:40,573 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 18:21:40,686 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 18:21:40,687 Now training epoch 150. LR=0.000503
2022-01-04 18:22:55,001 Epoch[150/310], Step[0000/1251], Loss: 4.0554(4.0554), Acc: 0.3379(0.3379)
2022-01-04 18:23:50,574 Epoch[150/310], Step[0050/1251], Loss: 3.8249(3.9242), Acc: 0.4697(0.3656)
2022-01-04 18:24:47,031 Epoch[150/310], Step[0100/1251], Loss: 3.4371(3.9663), Acc: 0.2949(0.3363)
2022-01-04 18:25:44,407 Epoch[150/310], Step[0150/1251], Loss: 4.5110(3.9664), Acc: 0.3623(0.3280)
2022-01-04 18:26:40,930 Epoch[150/310], Step[0200/1251], Loss: 4.2902(3.9712), Acc: 0.2344(0.3242)
2022-01-04 18:27:39,041 Epoch[150/310], Step[0250/1251], Loss: 3.4899(3.9596), Acc: 0.2402(0.3227)
2022-01-04 18:28:36,184 Epoch[150/310], Step[0300/1251], Loss: 4.0167(3.9498), Acc: 0.4619(0.3261)
2022-01-04 18:29:33,682 Epoch[150/310], Step[0350/1251], Loss: 3.9901(3.9574), Acc: 0.4023(0.3258)
2022-01-04 18:30:30,842 Epoch[150/310], Step[0400/1251], Loss: 4.3746(3.9614), Acc: 0.2578(0.3265)
2022-01-04 18:31:28,593 Epoch[150/310], Step[0450/1251], Loss: 4.0753(3.9616), Acc: 0.4424(0.3299)
2022-01-04 18:32:25,320 Epoch[150/310], Step[0500/1251], Loss: 3.9116(3.9587), Acc: 0.4473(0.3294)
2022-01-04 18:33:23,119 Epoch[150/310], Step[0550/1251], Loss: 4.0216(3.9621), Acc: 0.2275(0.3255)
2022-01-04 18:34:19,626 Epoch[150/310], Step[0600/1251], Loss: 4.1554(3.9621), Acc: 0.4365(0.3272)
2022-01-04 18:35:16,932 Epoch[150/310], Step[0650/1251], Loss: 3.7724(3.9674), Acc: 0.2852(0.3286)
2022-01-04 18:36:15,160 Epoch[150/310], Step[0700/1251], Loss: 4.2675(3.9729), Acc: 0.4023(0.3282)
2022-01-04 18:37:12,758 Epoch[150/310], Step[0750/1251], Loss: 3.5887(3.9739), Acc: 0.3027(0.3272)
2022-01-04 18:38:09,389 Epoch[150/310], Step[0800/1251], Loss: 4.1929(3.9702), Acc: 0.3027(0.3284)
2022-01-04 18:39:06,548 Epoch[150/310], Step[0850/1251], Loss: 3.9697(3.9678), Acc: 0.3691(0.3285)
2022-01-04 18:40:04,935 Epoch[150/310], Step[0900/1251], Loss: 4.3479(3.9700), Acc: 0.2412(0.3276)
2022-01-04 18:41:02,552 Epoch[150/310], Step[0950/1251], Loss: 4.3631(3.9728), Acc: 0.3662(0.3271)
2022-01-04 18:41:59,055 Epoch[150/310], Step[1000/1251], Loss: 4.3381(3.9749), Acc: 0.2715(0.3258)
2022-01-04 18:42:56,737 Epoch[150/310], Step[1050/1251], Loss: 3.2299(3.9783), Acc: 0.4326(0.3247)
2022-01-04 18:43:54,116 Epoch[150/310], Step[1100/1251], Loss: 4.1460(3.9765), Acc: 0.3672(0.3257)
2022-01-04 18:44:51,194 Epoch[150/310], Step[1150/1251], Loss: 3.8239(3.9765), Acc: 0.4326(0.3254)
2022-01-04 18:45:48,281 Epoch[150/310], Step[1200/1251], Loss: 4.0147(3.9766), Acc: 0.1475(0.3244)
2022-01-04 18:46:45,335 Epoch[150/310], Step[1250/1251], Loss: 4.1630(3.9726), Acc: 0.4102(0.3253)
2022-01-04 18:46:47,630 ----- Validation after Epoch: 150
2022-01-04 18:47:44,330 Val Step[0000/1563], Loss: 0.6764 (0.6764), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-04 18:47:45,787 Val Step[0050/1563], Loss: 2.2530 (0.8617), Acc@1: 0.3438 (0.8094), Acc@5: 0.8750 (0.9553)
2022-01-04 18:47:47,244 Val Step[0100/1563], Loss: 2.0348 (1.1836), Acc@1: 0.5000 (0.7256), Acc@5: 0.8125 (0.9174)
2022-01-04 18:47:48,789 Val Step[0150/1563], Loss: 0.6606 (1.1192), Acc@1: 0.8125 (0.7444), Acc@5: 0.9375 (0.9220)
2022-01-04 18:47:50,298 Val Step[0200/1563], Loss: 1.6904 (1.1428), Acc@1: 0.5938 (0.7453), Acc@5: 0.7812 (0.9162)
2022-01-04 18:47:51,756 Val Step[0250/1563], Loss: 1.3158 (1.0837), Acc@1: 0.6250 (0.7598), Acc@5: 0.9688 (0.9232)
2022-01-04 18:47:53,239 Val Step[0300/1563], Loss: 1.1687 (1.1445), Acc@1: 0.7188 (0.7414), Acc@5: 0.9375 (0.9184)
2022-01-04 18:47:54,778 Val Step[0350/1563], Loss: 1.1990 (1.1493), Acc@1: 0.6562 (0.7390), Acc@5: 0.9375 (0.9206)
2022-01-04 18:47:56,266 Val Step[0400/1563], Loss: 1.3456 (1.1615), Acc@1: 0.6875 (0.7336), Acc@5: 0.9062 (0.9214)
2022-01-04 18:47:57,851 Val Step[0450/1563], Loss: 1.5954 (1.1667), Acc@1: 0.5000 (0.7312), Acc@5: 1.0000 (0.9224)
2022-01-04 18:47:59,388 Val Step[0500/1563], Loss: 0.4020 (1.1606), Acc@1: 0.9375 (0.7330), Acc@5: 1.0000 (0.9235)
2022-01-04 18:48:00,870 Val Step[0550/1563], Loss: 1.1097 (1.1384), Acc@1: 0.7500 (0.7391), Acc@5: 0.9375 (0.9258)
2022-01-04 18:48:02,357 Val Step[0600/1563], Loss: 0.9084 (1.1452), Acc@1: 0.8125 (0.7381), Acc@5: 0.9062 (0.9249)
2022-01-04 18:48:03,764 Val Step[0650/1563], Loss: 0.7619 (1.1662), Acc@1: 0.7812 (0.7338), Acc@5: 1.0000 (0.9211)
2022-01-04 18:48:05,248 Val Step[0700/1563], Loss: 1.6191 (1.2017), Acc@1: 0.6875 (0.7259), Acc@5: 0.8125 (0.9161)
2022-01-04 18:48:06,793 Val Step[0750/1563], Loss: 2.2452 (1.2400), Acc@1: 0.5625 (0.7192), Acc@5: 0.7500 (0.9111)
2022-01-04 18:48:08,213 Val Step[0800/1563], Loss: 1.2817 (1.2797), Acc@1: 0.7500 (0.7108), Acc@5: 0.9688 (0.9055)
2022-01-04 18:48:09,657 Val Step[0850/1563], Loss: 1.7995 (1.3088), Acc@1: 0.4688 (0.7037), Acc@5: 0.8750 (0.9016)
2022-01-04 18:48:11,154 Val Step[0900/1563], Loss: 0.3598 (1.3103), Acc@1: 0.9375 (0.7046), Acc@5: 1.0000 (0.9005)
2022-01-04 18:48:12,632 Val Step[0950/1563], Loss: 1.5085 (1.3340), Acc@1: 0.7500 (0.6998), Acc@5: 0.8750 (0.8971)
2022-01-04 18:48:13,993 Val Step[1000/1563], Loss: 0.5882 (1.3582), Acc@1: 0.9375 (0.6941), Acc@5: 1.0000 (0.8939)
2022-01-04 18:48:15,461 Val Step[1050/1563], Loss: 0.3827 (1.3747), Acc@1: 0.9688 (0.6904), Acc@5: 0.9688 (0.8922)
2022-01-04 18:48:16,872 Val Step[1100/1563], Loss: 1.1468 (1.3916), Acc@1: 0.7500 (0.6871), Acc@5: 0.9062 (0.8895)
2022-01-04 18:48:18,307 Val Step[1150/1563], Loss: 1.5344 (1.4105), Acc@1: 0.6562 (0.6839), Acc@5: 0.8125 (0.8867)
2022-01-04 18:48:19,741 Val Step[1200/1563], Loss: 1.6356 (1.4276), Acc@1: 0.6875 (0.6807), Acc@5: 0.8438 (0.8842)
2022-01-04 18:48:21,141 Val Step[1250/1563], Loss: 0.8418 (1.4431), Acc@1: 0.8750 (0.6785), Acc@5: 0.9062 (0.8816)
2022-01-04 18:48:22,595 Val Step[1300/1563], Loss: 1.1953 (1.4551), Acc@1: 0.7812 (0.6755), Acc@5: 0.9062 (0.8804)
2022-01-04 18:48:23,999 Val Step[1350/1563], Loss: 2.3044 (1.4739), Acc@1: 0.3125 (0.6712), Acc@5: 0.7812 (0.8773)
2022-01-04 18:48:25,444 Val Step[1400/1563], Loss: 1.0264 (1.4803), Acc@1: 0.8125 (0.6700), Acc@5: 0.9688 (0.8766)
2022-01-04 18:48:26,879 Val Step[1450/1563], Loss: 1.6003 (1.4874), Acc@1: 0.5000 (0.6682), Acc@5: 0.9062 (0.8758)
2022-01-04 18:48:28,383 Val Step[1500/1563], Loss: 1.5963 (1.4755), Acc@1: 0.6250 (0.6707), Acc@5: 0.8438 (0.8775)
2022-01-04 18:48:29,888 Val Step[1550/1563], Loss: 1.1306 (1.4759), Acc@1: 0.8750 (0.6704), Acc@5: 0.9062 (0.8778)
2022-01-04 18:48:30,732 ----- Epoch[150/310], Validation Loss: 1.4739, Validation Acc@1: 0.6707, Validation Acc@5: 0.8780, time: 103.10
2022-01-04 18:48:30,732 ----- Epoch[150/310], Train Loss: 3.9726, Train Acc: 0.3253, time: 1506.94, Best Val(epoch150) Acc@1: 0.6707
2022-01-04 18:48:31,016 Max accuracy so far: 0.6707 at epoch_150
2022-01-04 18:48:31,016 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-04 18:48:31,016 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-04 18:48:31,237 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-04 18:48:31,360 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-150-Loss-3.9790194215629695.pdparams
2022-01-04 18:48:31,360 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-150-Loss-3.9790194215629695.pdopt
2022-01-04 18:48:31,400 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-150-Loss-3.9790194215629695-EMA.pdparams
2022-01-04 18:48:31,400 Now training epoch 151. LR=0.000497
2022-01-04 18:49:47,268 Epoch[151/310], Step[0000/1251], Loss: 4.0659(4.0659), Acc: 0.4082(0.4082)
2022-01-04 18:50:46,227 Epoch[151/310], Step[0050/1251], Loss: 4.5878(4.0239), Acc: 0.2539(0.3186)
2022-01-04 18:51:42,607 Epoch[151/310], Step[0100/1251], Loss: 3.9604(4.0086), Acc: 0.3320(0.3269)
2022-01-04 18:52:40,165 Epoch[151/310], Step[0150/1251], Loss: 4.0088(3.9906), Acc: 0.3828(0.3269)
2022-01-04 18:53:36,134 Epoch[151/310], Step[0200/1251], Loss: 3.9399(3.9839), Acc: 0.4189(0.3274)
2022-01-04 18:54:31,838 Epoch[151/310], Step[0250/1251], Loss: 3.6923(3.9740), Acc: 0.4180(0.3239)
2022-01-04 18:55:28,154 Epoch[151/310], Step[0300/1251], Loss: 3.6884(3.9758), Acc: 0.4023(0.3271)
2022-01-04 18:56:26,199 Epoch[151/310], Step[0350/1251], Loss: 3.8402(3.9795), Acc: 0.1729(0.3272)
2022-01-04 18:57:22,068 Epoch[151/310], Step[0400/1251], Loss: 3.9213(3.9853), Acc: 0.3926(0.3240)
2022-01-04 18:58:19,059 Epoch[151/310], Step[0450/1251], Loss: 3.4522(3.9802), Acc: 0.5059(0.3245)
2022-01-04 18:59:15,224 Epoch[151/310], Step[0500/1251], Loss: 3.9840(3.9765), Acc: 0.4014(0.3233)
2022-01-04 19:00:12,186 Epoch[151/310], Step[0550/1251], Loss: 3.6467(3.9751), Acc: 0.3828(0.3242)
2022-01-04 19:01:08,390 Epoch[151/310], Step[0600/1251], Loss: 4.0449(3.9767), Acc: 0.3779(0.3256)
2022-01-04 19:02:05,294 Epoch[151/310], Step[0650/1251], Loss: 4.3050(3.9784), Acc: 0.2051(0.3238)
2022-01-04 19:03:02,292 Epoch[151/310], Step[0700/1251], Loss: 3.5662(3.9818), Acc: 0.4668(0.3217)
2022-01-04 19:03:59,411 Epoch[151/310], Step[0750/1251], Loss: 3.7316(3.9834), Acc: 0.4404(0.3231)
2022-01-04 19:04:55,871 Epoch[151/310], Step[0800/1251], Loss: 3.9429(3.9835), Acc: 0.4229(0.3251)
2022-01-04 19:05:52,859 Epoch[151/310], Step[0850/1251], Loss: 3.7586(3.9862), Acc: 0.2080(0.3256)
2022-01-04 19:06:50,625 Epoch[151/310], Step[0900/1251], Loss: 3.9316(3.9838), Acc: 0.4023(0.3269)
2022-01-04 19:07:48,789 Epoch[151/310], Step[0950/1251], Loss: 4.0583(3.9811), Acc: 0.1865(0.3275)
2022-01-04 19:08:45,369 Epoch[151/310], Step[1000/1251], Loss: 3.8326(3.9841), Acc: 0.2490(0.3264)
2022-01-04 19:09:43,040 Epoch[151/310], Step[1050/1251], Loss: 4.0271(3.9831), Acc: 0.2373(0.3269)
2022-01-04 19:10:40,491 Epoch[151/310], Step[1100/1251], Loss: 3.9228(3.9813), Acc: 0.3838(0.3270)
2022-01-04 19:11:38,462 Epoch[151/310], Step[1150/1251], Loss: 3.6827(3.9809), Acc: 0.1455(0.3267)
2022-01-04 19:12:35,742 Epoch[151/310], Step[1200/1251], Loss: 4.2147(3.9792), Acc: 0.1982(0.3262)
2022-01-04 19:13:33,782 Epoch[151/310], Step[1250/1251], Loss: 4.2657(3.9779), Acc: 0.1504(0.3269)
2022-01-04 19:13:35,623 ----- Epoch[151/310], Train Loss: 3.9779, Train Acc: 0.3269, time: 1504.22, Best Val(epoch150) Acc@1: 0.6707
2022-01-04 19:13:35,795 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 19:13:35,796 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 19:13:35,901 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 19:13:35,901 Now training epoch 152. LR=0.000492
2022-01-04 19:14:50,935 Epoch[152/310], Step[0000/1251], Loss: 3.8374(3.8374), Acc: 0.3828(0.3828)
2022-01-04 19:15:46,197 Epoch[152/310], Step[0050/1251], Loss: 3.7066(4.0391), Acc: 0.2275(0.3117)
2022-01-04 19:16:41,663 Epoch[152/310], Step[0100/1251], Loss: 4.3237(4.0229), Acc: 0.3486(0.3176)
2022-01-04 19:17:38,587 Epoch[152/310], Step[0150/1251], Loss: 3.3865(4.0046), Acc: 0.4180(0.3224)
2022-01-04 19:18:36,330 Epoch[152/310], Step[0200/1251], Loss: 4.1208(4.0202), Acc: 0.2715(0.3149)
2022-01-04 19:19:34,583 Epoch[152/310], Step[0250/1251], Loss: 3.3018(4.0270), Acc: 0.3965(0.3137)
2022-01-04 19:20:32,009 Epoch[152/310], Step[0300/1251], Loss: 3.6576(4.0157), Acc: 0.3623(0.3134)
2022-01-04 19:21:29,964 Epoch[152/310], Step[0350/1251], Loss: 4.2574(4.0145), Acc: 0.3105(0.3164)
2022-01-04 19:22:27,471 Epoch[152/310], Step[0400/1251], Loss: 4.2860(4.0047), Acc: 0.4053(0.3174)
2022-01-04 19:23:25,173 Epoch[152/310], Step[0450/1251], Loss: 3.7769(4.0056), Acc: 0.4814(0.3191)
2022-01-04 19:24:22,818 Epoch[152/310], Step[0500/1251], Loss: 4.5106(3.9989), Acc: 0.3135(0.3211)
2022-01-04 19:25:19,040 Epoch[152/310], Step[0550/1251], Loss: 3.8957(3.9967), Acc: 0.2295(0.3234)
2022-01-04 19:26:16,243 Epoch[152/310], Step[0600/1251], Loss: 3.6588(3.9995), Acc: 0.3008(0.3238)
2022-01-04 19:27:13,669 Epoch[152/310], Step[0650/1251], Loss: 3.9367(4.0010), Acc: 0.4375(0.3222)
2022-01-04 19:28:10,375 Epoch[152/310], Step[0700/1251], Loss: 4.4048(4.0047), Acc: 0.3906(0.3217)
2022-01-04 19:29:07,501 Epoch[152/310], Step[0750/1251], Loss: 3.7271(3.9991), Acc: 0.4580(0.3202)
2022-01-04 19:30:03,982 Epoch[152/310], Step[0800/1251], Loss: 3.6771(3.9991), Acc: 0.2100(0.3208)
2022-01-04 19:31:00,698 Epoch[152/310], Step[0850/1251], Loss: 4.1667(3.9958), Acc: 0.3252(0.3220)
2022-01-04 19:31:57,558 Epoch[152/310], Step[0900/1251], Loss: 3.8312(3.9915), Acc: 0.4453(0.3231)
2022-01-04 19:32:55,502 Epoch[152/310], Step[0950/1251], Loss: 4.3561(3.9906), Acc: 0.3242(0.3229)
2022-01-04 19:33:52,228 Epoch[152/310], Step[1000/1251], Loss: 4.5118(3.9921), Acc: 0.3037(0.3213)
2022-01-04 19:34:48,974 Epoch[152/310], Step[1050/1251], Loss: 3.6836(3.9926), Acc: 0.3789(0.3204)
2022-01-04 19:35:47,213 Epoch[152/310], Step[1100/1251], Loss: 4.3936(3.9926), Acc: 0.3193(0.3200)
2022-01-04 19:36:44,300 Epoch[152/310], Step[1150/1251], Loss: 4.0727(3.9910), Acc: 0.2217(0.3200)
2022-01-04 19:37:40,705 Epoch[152/310], Step[1200/1251], Loss: 3.5380(3.9898), Acc: 0.4414(0.3217)
2022-01-04 19:38:36,497 Epoch[152/310], Step[1250/1251], Loss: 4.3695(3.9915), Acc: 0.2578(0.3215)
2022-01-04 19:38:38,428 ----- Validation after Epoch: 152
2022-01-04 19:39:34,400 Val Step[0000/1563], Loss: 0.7415 (0.7415), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-04 19:39:35,974 Val Step[0050/1563], Loss: 2.1372 (0.8923), Acc@1: 0.5000 (0.8180), Acc@5: 0.8438 (0.9442)
2022-01-04 19:39:37,462 Val Step[0100/1563], Loss: 1.7785 (1.2020), Acc@1: 0.5000 (0.7237), Acc@5: 0.8750 (0.9158)
2022-01-04 19:39:38,936 Val Step[0150/1563], Loss: 0.6534 (1.1471), Acc@1: 0.8438 (0.7397), Acc@5: 0.9688 (0.9199)
2022-01-04 19:39:40,534 Val Step[0200/1563], Loss: 1.2410 (1.1678), Acc@1: 0.7188 (0.7414), Acc@5: 0.9062 (0.9162)
2022-01-04 19:39:42,018 Val Step[0250/1563], Loss: 0.8431 (1.1184), Acc@1: 0.8750 (0.7521), Acc@5: 1.0000 (0.9219)
2022-01-04 19:39:43,434 Val Step[0300/1563], Loss: 1.1946 (1.1760), Acc@1: 0.7188 (0.7356), Acc@5: 0.9375 (0.9184)
2022-01-04 19:39:44,836 Val Step[0350/1563], Loss: 1.1105 (1.1842), Acc@1: 0.8750 (0.7314), Acc@5: 0.9062 (0.9209)
2022-01-04 19:39:46,243 Val Step[0400/1563], Loss: 1.1256 (1.1839), Acc@1: 0.7812 (0.7274), Acc@5: 0.9688 (0.9227)
2022-01-04 19:39:47,754 Val Step[0450/1563], Loss: 1.3329 (1.1913), Acc@1: 0.5625 (0.7246), Acc@5: 1.0000 (0.9237)
2022-01-04 19:39:49,315 Val Step[0500/1563], Loss: 0.4284 (1.1801), Acc@1: 0.9062 (0.7274), Acc@5: 1.0000 (0.9251)
2022-01-04 19:39:50,843 Val Step[0550/1563], Loss: 0.9637 (1.1555), Acc@1: 0.7188 (0.7339), Acc@5: 0.9375 (0.9272)
2022-01-04 19:39:52,497 Val Step[0600/1563], Loss: 1.0041 (1.1629), Acc@1: 0.8125 (0.7332), Acc@5: 0.9062 (0.9264)
2022-01-04 19:39:54,076 Val Step[0650/1563], Loss: 0.6176 (1.1896), Acc@1: 0.8438 (0.7274), Acc@5: 1.0000 (0.9223)
2022-01-04 19:39:55,593 Val Step[0700/1563], Loss: 1.5386 (1.2258), Acc@1: 0.6562 (0.7203), Acc@5: 0.8750 (0.9173)
2022-01-04 19:39:57,097 Val Step[0750/1563], Loss: 2.0976 (1.2664), Acc@1: 0.5625 (0.7125), Acc@5: 0.7812 (0.9116)
2022-01-04 19:39:58,666 Val Step[0800/1563], Loss: 1.1356 (1.3122), Acc@1: 0.6875 (0.7024), Acc@5: 0.9688 (0.9054)
2022-01-04 19:40:00,208 Val Step[0850/1563], Loss: 1.8051 (1.3393), Acc@1: 0.5000 (0.6965), Acc@5: 0.8750 (0.9018)
2022-01-04 19:40:01,820 Val Step[0900/1563], Loss: 0.4899 (1.3412), Acc@1: 0.9062 (0.6977), Acc@5: 0.9375 (0.9009)
2022-01-04 19:40:03,355 Val Step[0950/1563], Loss: 1.5677 (1.3652), Acc@1: 0.7500 (0.6934), Acc@5: 0.8438 (0.8971)
2022-01-04 19:40:04,700 Val Step[1000/1563], Loss: 0.7863 (1.3940), Acc@1: 0.9062 (0.6868), Acc@5: 1.0000 (0.8934)
2022-01-04 19:40:06,135 Val Step[1050/1563], Loss: 0.3880 (1.4123), Acc@1: 0.9688 (0.6826), Acc@5: 0.9688 (0.8909)
2022-01-04 19:40:07,588 Val Step[1100/1563], Loss: 1.2577 (1.4324), Acc@1: 0.7188 (0.6785), Acc@5: 0.9688 (0.8877)
2022-01-04 19:40:09,086 Val Step[1150/1563], Loss: 1.2334 (1.4508), Acc@1: 0.7812 (0.6753), Acc@5: 0.8125 (0.8853)
2022-01-04 19:40:10,485 Val Step[1200/1563], Loss: 1.5349 (1.4649), Acc@1: 0.7500 (0.6731), Acc@5: 0.8438 (0.8829)
2022-01-04 19:40:11,935 Val Step[1250/1563], Loss: 0.8224 (1.4775), Acc@1: 0.8438 (0.6712), Acc@5: 0.9062 (0.8804)
2022-01-04 19:40:13,280 Val Step[1300/1563], Loss: 1.3488 (1.4873), Acc@1: 0.7500 (0.6693), Acc@5: 0.9062 (0.8790)
2022-01-04 19:40:14,773 Val Step[1350/1563], Loss: 1.8653 (1.5048), Acc@1: 0.4688 (0.6652), Acc@5: 0.8750 (0.8760)
2022-01-04 19:40:16,168 Val Step[1400/1563], Loss: 1.2587 (1.5140), Acc@1: 0.7812 (0.6632), Acc@5: 0.9062 (0.8750)
2022-01-04 19:40:17,576 Val Step[1450/1563], Loss: 1.5944 (1.5217), Acc@1: 0.6875 (0.6615), Acc@5: 0.9062 (0.8740)
2022-01-04 19:40:19,078 Val Step[1500/1563], Loss: 1.8354 (1.5096), Acc@1: 0.5938 (0.6638), Acc@5: 0.8750 (0.8757)
2022-01-04 19:40:20,597 Val Step[1550/1563], Loss: 1.2085 (1.5102), Acc@1: 0.7188 (0.6632), Acc@5: 0.9062 (0.8756)
2022-01-04 19:40:21,425 ----- Epoch[152/310], Validation Loss: 1.5077, Validation Acc@1: 0.6637, Validation Acc@5: 0.8758, time: 102.99
2022-01-04 19:40:21,425 ----- Epoch[152/310], Train Loss: 3.9915, Train Acc: 0.3215, time: 1502.52, Best Val(epoch150) Acc@1: 0.6707
2022-01-04 19:40:21,594 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 19:40:21,595 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 19:40:21,726 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 19:40:21,727 Now training epoch 153. LR=0.000487
2022-01-04 19:41:38,399 Epoch[153/310], Step[0000/1251], Loss: 3.8861(3.8861), Acc: 0.3516(0.3516)
2022-01-04 19:42:34,572 Epoch[153/310], Step[0050/1251], Loss: 3.6704(3.9800), Acc: 0.4619(0.3214)
2022-01-04 19:43:30,606 Epoch[153/310], Step[0100/1251], Loss: 3.7206(4.0097), Acc: 0.3682(0.3112)
2022-01-04 19:44:28,064 Epoch[153/310], Step[0150/1251], Loss: 4.5323(3.9969), Acc: 0.1475(0.3100)
2022-01-04 19:45:24,558 Epoch[153/310], Step[0200/1251], Loss: 4.2079(3.9954), Acc: 0.2627(0.3162)
2022-01-04 19:46:21,412 Epoch[153/310], Step[0250/1251], Loss: 4.0238(4.0012), Acc: 0.1611(0.3149)
2022-01-04 19:47:17,900 Epoch[153/310], Step[0300/1251], Loss: 4.4298(3.9915), Acc: 0.2832(0.3151)
2022-01-04 19:48:13,388 Epoch[153/310], Step[0350/1251], Loss: 3.1033(3.9866), Acc: 0.4277(0.3175)
2022-01-04 19:49:09,376 Epoch[153/310], Step[0400/1251], Loss: 4.0057(3.9905), Acc: 0.2480(0.3165)
2022-01-04 19:50:06,455 Epoch[153/310], Step[0450/1251], Loss: 4.3047(3.9875), Acc: 0.2637(0.3199)
2022-01-04 19:51:03,006 Epoch[153/310], Step[0500/1251], Loss: 4.3440(3.9837), Acc: 0.3848(0.3219)
2022-01-04 19:52:00,543 Epoch[153/310], Step[0550/1251], Loss: 3.9149(3.9843), Acc: 0.4180(0.3220)
2022-01-04 19:52:58,545 Epoch[153/310], Step[0600/1251], Loss: 3.6964(3.9863), Acc: 0.2939(0.3215)
2022-01-04 19:53:55,791 Epoch[153/310], Step[0650/1251], Loss: 4.2444(3.9869), Acc: 0.3125(0.3205)
2022-01-04 19:54:53,109 Epoch[153/310], Step[0700/1251], Loss: 4.0457(3.9843), Acc: 0.4629(0.3219)
2022-01-04 19:55:50,819 Epoch[153/310], Step[0750/1251], Loss: 3.6949(3.9870), Acc: 0.3271(0.3205)
2022-01-04 19:56:47,874 Epoch[153/310], Step[0800/1251], Loss: 3.2870(3.9854), Acc: 0.5479(0.3214)
2022-01-04 19:57:44,495 Epoch[153/310], Step[0850/1251], Loss: 3.7945(3.9835), Acc: 0.2969(0.3221)
2022-01-04 19:58:41,267 Epoch[153/310], Step[0900/1251], Loss: 3.8090(3.9838), Acc: 0.2822(0.3220)
2022-01-04 19:59:36,581 Epoch[153/310], Step[0950/1251], Loss: 4.1763(3.9848), Acc: 0.2051(0.3214)
2022-01-04 20:00:34,429 Epoch[153/310], Step[1000/1251], Loss: 4.1847(3.9848), Acc: 0.4297(0.3212)
2022-01-04 20:01:30,973 Epoch[153/310], Step[1050/1251], Loss: 3.8177(3.9806), Acc: 0.4346(0.3222)
2022-01-04 20:02:28,579 Epoch[153/310], Step[1100/1251], Loss: 4.4112(3.9746), Acc: 0.3350(0.3237)
2022-01-04 20:03:23,806 Epoch[153/310], Step[1150/1251], Loss: 4.0722(3.9729), Acc: 0.2842(0.3245)
2022-01-04 20:04:20,983 Epoch[153/310], Step[1200/1251], Loss: 4.1513(3.9743), Acc: 0.3125(0.3241)
2022-01-04 20:05:18,020 Epoch[153/310], Step[1250/1251], Loss: 4.3230(3.9760), Acc: 0.3818(0.3250)
2022-01-04 20:05:19,890 ----- Epoch[153/310], Train Loss: 3.9760, Train Acc: 0.3250, time: 1498.16, Best Val(epoch150) Acc@1: 0.6707
2022-01-04 20:05:20,108 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 20:05:20,108 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 20:05:20,181 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 20:05:20,182 Now training epoch 154. LR=0.000482
2022-01-04 20:06:45,565 Epoch[154/310], Step[0000/1251], Loss: 4.3777(4.3777), Acc: 0.2188(0.2188)
2022-01-04 20:07:40,542 Epoch[154/310], Step[0050/1251], Loss: 4.1058(3.9275), Acc: 0.4453(0.3426)
2022-01-04 20:08:37,035 Epoch[154/310], Step[0100/1251], Loss: 3.8582(3.9329), Acc: 0.4150(0.3340)
2022-01-04 20:09:34,702 Epoch[154/310], Step[0150/1251], Loss: 3.7981(3.9283), Acc: 0.2236(0.3352)
2022-01-04 20:10:32,467 Epoch[154/310], Step[0200/1251], Loss: 3.5430(3.9254), Acc: 0.3701(0.3308)
2022-01-04 20:11:30,490 Epoch[154/310], Step[0250/1251], Loss: 3.9919(3.9298), Acc: 0.3184(0.3312)
2022-01-04 20:12:26,900 Epoch[154/310], Step[0300/1251], Loss: 4.0484(3.9232), Acc: 0.3994(0.3312)
2022-01-04 20:13:22,626 Epoch[154/310], Step[0350/1251], Loss: 3.9766(3.9346), Acc: 0.1641(0.3321)
2022-01-04 20:14:18,953 Epoch[154/310], Step[0400/1251], Loss: 4.5461(3.9443), Acc: 0.1279(0.3286)
2022-01-04 20:15:13,778 Epoch[154/310], Step[0450/1251], Loss: 3.9375(3.9517), Acc: 0.1582(0.3264)
2022-01-04 20:16:08,918 Epoch[154/310], Step[0500/1251], Loss: 4.1151(3.9507), Acc: 0.4014(0.3271)
2022-01-04 20:17:06,054 Epoch[154/310], Step[0550/1251], Loss: 3.5525(3.9441), Acc: 0.5137(0.3269)
2022-01-04 20:18:02,901 Epoch[154/310], Step[0600/1251], Loss: 3.9593(3.9436), Acc: 0.1523(0.3289)
2022-01-04 20:19:00,390 Epoch[154/310], Step[0650/1251], Loss: 3.7643(3.9530), Acc: 0.3330(0.3283)
2022-01-04 20:19:58,407 Epoch[154/310], Step[0700/1251], Loss: 4.1037(3.9540), Acc: 0.2217(0.3287)
2022-01-04 20:20:56,299 Epoch[154/310], Step[0750/1251], Loss: 4.1755(3.9606), Acc: 0.2480(0.3256)
2022-01-04 20:21:53,711 Epoch[154/310], Step[0800/1251], Loss: 3.6668(3.9608), Acc: 0.3604(0.3251)
2022-01-04 20:22:50,753 Epoch[154/310], Step[0850/1251], Loss: 4.1131(3.9633), Acc: 0.1494(0.3247)
2022-01-04 20:23:48,330 Epoch[154/310], Step[0900/1251], Loss: 4.0814(3.9666), Acc: 0.3320(0.3245)
2022-01-04 20:24:45,595 Epoch[154/310], Step[0950/1251], Loss: 3.4779(3.9617), Acc: 0.2832(0.3248)
2022-01-04 20:25:43,356 Epoch[154/310], Step[1000/1251], Loss: 3.9052(3.9631), Acc: 0.3818(0.3251)
2022-01-04 20:26:40,456 Epoch[154/310], Step[1050/1251], Loss: 3.7158(3.9619), Acc: 0.4736(0.3252)
2022-01-04 20:27:37,960 Epoch[154/310], Step[1100/1251], Loss: 4.4354(3.9658), Acc: 0.3887(0.3245)
2022-01-04 20:28:34,103 Epoch[154/310], Step[1150/1251], Loss: 4.3231(3.9664), Acc: 0.2822(0.3244)
2022-01-04 20:29:30,339 Epoch[154/310], Step[1200/1251], Loss: 3.9678(3.9634), Acc: 0.4854(0.3246)
2022-01-04 20:30:27,687 Epoch[154/310], Step[1250/1251], Loss: 4.3754(3.9652), Acc: 0.2666(0.3245)
2022-01-04 20:30:29,567 ----- Validation after Epoch: 154
2022-01-04 20:31:32,822 Val Step[0000/1563], Loss: 0.7419 (0.7419), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-04 20:31:34,431 Val Step[0050/1563], Loss: 1.9690 (0.9116), Acc@1: 0.5000 (0.8094), Acc@5: 0.8438 (0.9449)
2022-01-04 20:31:35,852 Val Step[0100/1563], Loss: 1.6890 (1.2002), Acc@1: 0.5938 (0.7218), Acc@5: 0.8125 (0.9137)
2022-01-04 20:31:37,318 Val Step[0150/1563], Loss: 0.6488 (1.1337), Acc@1: 0.8750 (0.7421), Acc@5: 0.9688 (0.9197)
2022-01-04 20:31:38,712 Val Step[0200/1563], Loss: 0.9957 (1.1705), Acc@1: 0.7188 (0.7387), Acc@5: 0.9688 (0.9151)
2022-01-04 20:31:40,092 Val Step[0250/1563], Loss: 0.6194 (1.1042), Acc@1: 0.8750 (0.7540), Acc@5: 1.0000 (0.9232)
2022-01-04 20:31:41,641 Val Step[0300/1563], Loss: 1.2753 (1.1675), Acc@1: 0.6250 (0.7366), Acc@5: 0.9688 (0.9178)
2022-01-04 20:31:43,086 Val Step[0350/1563], Loss: 1.1685 (1.1791), Acc@1: 0.6250 (0.7309), Acc@5: 0.9062 (0.9198)
2022-01-04 20:31:44,589 Val Step[0400/1563], Loss: 1.3171 (1.1920), Acc@1: 0.6875 (0.7261), Acc@5: 0.9688 (0.9196)
2022-01-04 20:31:46,200 Val Step[0450/1563], Loss: 1.1954 (1.1962), Acc@1: 0.6250 (0.7239), Acc@5: 1.0000 (0.9208)
2022-01-04 20:31:47,731 Val Step[0500/1563], Loss: 0.3973 (1.1908), Acc@1: 0.9688 (0.7260), Acc@5: 1.0000 (0.9222)
2022-01-04 20:31:49,220 Val Step[0550/1563], Loss: 0.8344 (1.1640), Acc@1: 0.8125 (0.7332), Acc@5: 1.0000 (0.9247)
2022-01-04 20:31:50,733 Val Step[0600/1563], Loss: 0.8961 (1.1664), Acc@1: 0.7500 (0.7331), Acc@5: 0.9375 (0.9247)
2022-01-04 20:31:52,246 Val Step[0650/1563], Loss: 0.7353 (1.1866), Acc@1: 0.8438 (0.7292), Acc@5: 1.0000 (0.9215)
2022-01-04 20:31:53,738 Val Step[0700/1563], Loss: 1.3388 (1.2243), Acc@1: 0.7500 (0.7204), Acc@5: 0.8750 (0.9163)
2022-01-04 20:31:55,173 Val Step[0750/1563], Loss: 1.6582 (1.2563), Acc@1: 0.6250 (0.7141), Acc@5: 0.8438 (0.9116)
2022-01-04 20:31:56,634 Val Step[0800/1563], Loss: 0.9374 (1.2947), Acc@1: 0.7500 (0.7056), Acc@5: 1.0000 (0.9062)
2022-01-04 20:31:58,117 Val Step[0850/1563], Loss: 1.2675 (1.3214), Acc@1: 0.6875 (0.6995), Acc@5: 0.9375 (0.9028)
2022-01-04 20:31:59,580 Val Step[0900/1563], Loss: 0.6192 (1.3231), Acc@1: 0.8750 (0.7005), Acc@5: 0.9688 (0.9018)
2022-01-04 20:32:01,079 Val Step[0950/1563], Loss: 1.7897 (1.3467), Acc@1: 0.5938 (0.6958), Acc@5: 0.8438 (0.8986)
2022-01-04 20:32:02,470 Val Step[1000/1563], Loss: 0.5715 (1.3713), Acc@1: 0.9375 (0.6901), Acc@5: 1.0000 (0.8952)
2022-01-04 20:32:03,865 Val Step[1050/1563], Loss: 0.4712 (1.3880), Acc@1: 0.9688 (0.6863), Acc@5: 0.9688 (0.8932)
2022-01-04 20:32:05,369 Val Step[1100/1563], Loss: 1.4649 (1.4030), Acc@1: 0.7188 (0.6837), Acc@5: 0.8750 (0.8910)
2022-01-04 20:32:06,760 Val Step[1150/1563], Loss: 1.2334 (1.4194), Acc@1: 0.7500 (0.6804), Acc@5: 0.8438 (0.8885)
2022-01-04 20:32:08,221 Val Step[1200/1563], Loss: 1.2698 (1.4389), Acc@1: 0.8125 (0.6767), Acc@5: 0.8438 (0.8852)
2022-01-04 20:32:09,699 Val Step[1250/1563], Loss: 0.6889 (1.4551), Acc@1: 0.8438 (0.6743), Acc@5: 0.9062 (0.8821)
2022-01-04 20:32:11,279 Val Step[1300/1563], Loss: 1.2490 (1.4645), Acc@1: 0.7812 (0.6727), Acc@5: 0.8750 (0.8810)
2022-01-04 20:32:12,711 Val Step[1350/1563], Loss: 2.2349 (1.4851), Acc@1: 0.3750 (0.6683), Acc@5: 0.8438 (0.8781)
2022-01-04 20:32:14,106 Val Step[1400/1563], Loss: 1.1408 (1.4919), Acc@1: 0.6875 (0.6669), Acc@5: 0.9688 (0.8773)
2022-01-04 20:32:15,745 Val Step[1450/1563], Loss: 1.5572 (1.4972), Acc@1: 0.5625 (0.6656), Acc@5: 0.9375 (0.8767)
2022-01-04 20:32:17,254 Val Step[1500/1563], Loss: 1.6313 (1.4851), Acc@1: 0.6562 (0.6686), Acc@5: 0.8750 (0.8785)
2022-01-04 20:32:18,706 Val Step[1550/1563], Loss: 0.9034 (1.4858), Acc@1: 0.8750 (0.6685), Acc@5: 0.9062 (0.8786)
2022-01-04 20:32:20,222 ----- Epoch[154/310], Validation Loss: 1.4835, Validation Acc@1: 0.6691, Validation Acc@5: 0.8788, time: 110.65
2022-01-04 20:32:20,222 ----- Epoch[154/310], Train Loss: 3.9652, Train Acc: 0.3245, time: 1509.38, Best Val(epoch150) Acc@1: 0.6707
2022-01-04 20:32:20,449 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 20:32:20,450 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 20:32:20,530 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 20:32:20,530 Now training epoch 155. LR=0.000476
2022-01-04 20:33:42,386 Epoch[155/310], Step[0000/1251], Loss: 3.8712(3.8712), Acc: 0.3154(0.3154)
2022-01-04 20:34:38,771 Epoch[155/310], Step[0050/1251], Loss: 3.7364(3.8943), Acc: 0.2754(0.3529)
2022-01-04 20:35:36,249 Epoch[155/310], Step[0100/1251], Loss: 3.9822(3.9333), Acc: 0.4746(0.3335)
2022-01-04 20:36:31,352 Epoch[155/310], Step[0150/1251], Loss: 3.5577(3.9626), Acc: 0.5234(0.3230)
2022-01-04 20:37:27,813 Epoch[155/310], Step[0200/1251], Loss: 3.8640(3.9626), Acc: 0.4736(0.3255)
2022-01-04 20:38:24,674 Epoch[155/310], Step[0250/1251], Loss: 4.1372(3.9667), Acc: 0.3027(0.3233)
2022-01-04 20:39:21,731 Epoch[155/310], Step[0300/1251], Loss: 4.0855(3.9787), Acc: 0.1611(0.3192)
2022-01-04 20:40:18,245 Epoch[155/310], Step[0350/1251], Loss: 4.2298(3.9778), Acc: 0.2676(0.3187)
2022-01-04 20:41:15,376 Epoch[155/310], Step[0400/1251], Loss: 4.0797(3.9767), Acc: 0.3145(0.3197)
2022-01-04 20:42:12,985 Epoch[155/310], Step[0450/1251], Loss: 3.3832(3.9725), Acc: 0.3008(0.3225)
2022-01-04 20:43:09,384 Epoch[155/310], Step[0500/1251], Loss: 4.7196(3.9651), Acc: 0.2139(0.3247)
2022-01-04 20:44:06,460 Epoch[155/310], Step[0550/1251], Loss: 4.5183(3.9743), Acc: 0.3242(0.3243)
2022-01-04 20:45:04,137 Epoch[155/310], Step[0600/1251], Loss: 4.1933(3.9752), Acc: 0.3574(0.3210)
2022-01-04 20:46:00,539 Epoch[155/310], Step[0650/1251], Loss: 3.7738(3.9759), Acc: 0.1260(0.3205)
2022-01-04 20:46:57,675 Epoch[155/310], Step[0700/1251], Loss: 3.5547(3.9788), Acc: 0.2686(0.3193)
2022-01-04 20:47:54,331 Epoch[155/310], Step[0750/1251], Loss: 3.9252(3.9798), Acc: 0.1777(0.3209)
2022-01-04 20:48:52,115 Epoch[155/310], Step[0800/1251], Loss: 3.4826(3.9784), Acc: 0.5488(0.3219)
2022-01-04 20:49:49,330 Epoch[155/310], Step[0850/1251], Loss: 4.0160(3.9757), Acc: 0.2773(0.3226)
2022-01-04 20:50:47,244 Epoch[155/310], Step[0900/1251], Loss: 3.6100(3.9776), Acc: 0.3652(0.3216)
2022-01-04 20:51:44,945 Epoch[155/310], Step[0950/1251], Loss: 4.2381(3.9768), Acc: 0.2197(0.3203)
2022-01-04 20:52:40,892 Epoch[155/310], Step[1000/1251], Loss: 4.4272(3.9748), Acc: 0.4248(0.3220)
2022-01-04 20:53:37,122 Epoch[155/310], Step[1050/1251], Loss: 4.1765(3.9783), Acc: 0.2969(0.3216)
2022-01-04 20:54:32,582 Epoch[155/310], Step[1100/1251], Loss: 3.9251(3.9769), Acc: 0.4229(0.3207)
2022-01-04 20:55:30,845 Epoch[155/310], Step[1150/1251], Loss: 3.6341(3.9807), Acc: 0.3926(0.3210)
2022-01-04 20:56:29,655 Epoch[155/310], Step[1200/1251], Loss: 3.8699(3.9789), Acc: 0.4561(0.3209)
2022-01-04 20:57:26,885 Epoch[155/310], Step[1250/1251], Loss: 4.3441(3.9780), Acc: 0.1885(0.3205)
2022-01-04 20:57:28,969 ----- Epoch[155/310], Train Loss: 3.9780, Train Acc: 0.3205, time: 1508.44, Best Val(epoch150) Acc@1: 0.6707
2022-01-04 20:57:29,135 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 20:57:29,135 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 20:57:29,241 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 20:57:29,242 Now training epoch 156. LR=0.000471
2022-01-04 20:58:40,946 Epoch[156/310], Step[0000/1251], Loss: 3.5878(3.5878), Acc: 0.2061(0.2061)
2022-01-04 20:59:38,050 Epoch[156/310], Step[0050/1251], Loss: 3.7657(3.9050), Acc: 0.3643(0.3481)
2022-01-04 21:00:34,707 Epoch[156/310], Step[0100/1251], Loss: 3.5924(3.9033), Acc: 0.1865(0.3274)
2022-01-04 21:01:30,751 Epoch[156/310], Step[0150/1251], Loss: 4.0809(3.9166), Acc: 0.3730(0.3189)
2022-01-04 21:02:26,525 Epoch[156/310], Step[0200/1251], Loss: 3.9278(3.9156), Acc: 0.3047(0.3238)
2022-01-04 21:03:23,548 Epoch[156/310], Step[0250/1251], Loss: 4.1687(3.9345), Acc: 0.2070(0.3236)
2022-01-04 21:04:20,233 Epoch[156/310], Step[0300/1251], Loss: 3.8714(3.9332), Acc: 0.2285(0.3196)
2022-01-04 21:05:17,505 Epoch[156/310], Step[0350/1251], Loss: 3.5946(3.9463), Acc: 0.4883(0.3200)
2022-01-04 21:06:15,319 Epoch[156/310], Step[0400/1251], Loss: 3.6938(3.9497), Acc: 0.2676(0.3206)
2022-01-04 21:07:12,461 Epoch[156/310], Step[0450/1251], Loss: 3.9004(3.9565), Acc: 0.4482(0.3212)
2022-01-04 21:08:09,910 Epoch[156/310], Step[0500/1251], Loss: 4.6433(3.9539), Acc: 0.2031(0.3238)
2022-01-04 21:09:08,223 Epoch[156/310], Step[0550/1251], Loss: 4.4955(3.9570), Acc: 0.3271(0.3246)
2022-01-04 21:10:06,667 Epoch[156/310], Step[0600/1251], Loss: 3.7151(3.9624), Acc: 0.2793(0.3250)
2022-01-04 21:11:04,849 Epoch[156/310], Step[0650/1251], Loss: 4.0290(3.9639), Acc: 0.2520(0.3245)
2022-01-04 21:12:02,465 Epoch[156/310], Step[0700/1251], Loss: 3.8946(3.9623), Acc: 0.4219(0.3243)
2022-01-04 21:12:58,851 Epoch[156/310], Step[0750/1251], Loss: 4.1563(3.9642), Acc: 0.2686(0.3247)
2022-01-04 21:13:56,676 Epoch[156/310], Step[0800/1251], Loss: 3.8407(3.9662), Acc: 0.1621(0.3225)
2022-01-04 21:14:52,183 Epoch[156/310], Step[0850/1251], Loss: 3.9625(3.9616), Acc: 0.3271(0.3220)
2022-01-04 21:15:50,281 Epoch[156/310], Step[0900/1251], Loss: 3.6804(3.9647), Acc: 0.4834(0.3217)
2022-01-04 21:16:48,092 Epoch[156/310], Step[0950/1251], Loss: 3.8582(3.9633), Acc: 0.3809(0.3211)
2022-01-04 21:17:45,101 Epoch[156/310], Step[1000/1251], Loss: 4.1710(3.9632), Acc: 0.3398(0.3213)
2022-01-04 21:18:41,814 Epoch[156/310], Step[1050/1251], Loss: 4.2279(3.9622), Acc: 0.4082(0.3216)
2022-01-04 21:19:37,776 Epoch[156/310], Step[1100/1251], Loss: 4.0800(3.9636), Acc: 0.3887(0.3211)
2022-01-04 21:20:34,326 Epoch[156/310], Step[1150/1251], Loss: 3.6905(3.9605), Acc: 0.5098(0.3218)
2022-01-04 21:21:29,854 Epoch[156/310], Step[1200/1251], Loss: 3.9166(3.9592), Acc: 0.2783(0.3218)
2022-01-04 21:22:27,195 Epoch[156/310], Step[1250/1251], Loss: 4.3245(3.9563), Acc: 0.4053(0.3229)
2022-01-04 21:22:29,100 ----- Validation after Epoch: 156
2022-01-04 21:23:27,065 Val Step[0000/1563], Loss: 0.7816 (0.7816), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-04 21:23:28,546 Val Step[0050/1563], Loss: 2.2234 (0.8869), Acc@1: 0.3438 (0.8211), Acc@5: 0.9062 (0.9522)
2022-01-04 21:23:30,010 Val Step[0100/1563], Loss: 1.7990 (1.2142), Acc@1: 0.5625 (0.7367), Acc@5: 0.8438 (0.9171)
2022-01-04 21:23:31,387 Val Step[0150/1563], Loss: 0.7470 (1.1644), Acc@1: 0.8438 (0.7502), Acc@5: 0.9688 (0.9197)
2022-01-04 21:23:32,887 Val Step[0200/1563], Loss: 1.2209 (1.1703), Acc@1: 0.7188 (0.7525), Acc@5: 0.9062 (0.9174)
2022-01-04 21:23:34,411 Val Step[0250/1563], Loss: 0.6432 (1.0971), Acc@1: 0.8750 (0.7695), Acc@5: 1.0000 (0.9269)
2022-01-04 21:23:35,906 Val Step[0300/1563], Loss: 1.2601 (1.1636), Acc@1: 0.6875 (0.7504), Acc@5: 0.9375 (0.9208)
2022-01-04 21:23:37,409 Val Step[0350/1563], Loss: 1.3897 (1.1729), Acc@1: 0.6562 (0.7446), Acc@5: 0.9062 (0.9233)
2022-01-04 21:23:38,903 Val Step[0400/1563], Loss: 1.3738 (1.1840), Acc@1: 0.6562 (0.7359), Acc@5: 0.9375 (0.9242)
2022-01-04 21:23:40,446 Val Step[0450/1563], Loss: 1.1562 (1.1926), Acc@1: 0.6875 (0.7315), Acc@5: 1.0000 (0.9253)
2022-01-04 21:23:42,004 Val Step[0500/1563], Loss: 0.4081 (1.1865), Acc@1: 0.9375 (0.7336), Acc@5: 1.0000 (0.9266)
2022-01-04 21:23:43,543 Val Step[0550/1563], Loss: 0.8763 (1.1641), Acc@1: 0.7500 (0.7396), Acc@5: 0.9688 (0.9284)
2022-01-04 21:23:45,110 Val Step[0600/1563], Loss: 0.8918 (1.1674), Acc@1: 0.8438 (0.7400), Acc@5: 0.9375 (0.9279)
2022-01-04 21:23:46,512 Val Step[0650/1563], Loss: 1.0199 (1.1900), Acc@1: 0.6875 (0.7353), Acc@5: 1.0000 (0.9244)
2022-01-04 21:23:47,976 Val Step[0700/1563], Loss: 1.3159 (1.2230), Acc@1: 0.7188 (0.7269), Acc@5: 0.9062 (0.9192)
2022-01-04 21:23:49,471 Val Step[0750/1563], Loss: 1.8392 (1.2548), Acc@1: 0.6875 (0.7201), Acc@5: 0.8438 (0.9152)
2022-01-04 21:23:50,895 Val Step[0800/1563], Loss: 1.0138 (1.2960), Acc@1: 0.7500 (0.7105), Acc@5: 0.9688 (0.9094)
2022-01-04 21:23:52,322 Val Step[0850/1563], Loss: 1.6995 (1.3253), Acc@1: 0.4688 (0.7035), Acc@5: 0.9375 (0.9051)
2022-01-04 21:23:53,883 Val Step[0900/1563], Loss: 0.4696 (1.3264), Acc@1: 0.9375 (0.7043), Acc@5: 1.0000 (0.9043)
2022-01-04 21:23:55,549 Val Step[0950/1563], Loss: 1.5051 (1.3489), Acc@1: 0.7500 (0.7002), Acc@5: 0.8438 (0.9008)
2022-01-04 21:23:56,985 Val Step[1000/1563], Loss: 0.6864 (1.3744), Acc@1: 0.9688 (0.6942), Acc@5: 1.0000 (0.8975)
2022-01-04 21:23:58,479 Val Step[1050/1563], Loss: 0.4516 (1.3920), Acc@1: 0.9688 (0.6901), Acc@5: 0.9688 (0.8952)
2022-01-04 21:23:59,977 Val Step[1100/1563], Loss: 1.1656 (1.4103), Acc@1: 0.7188 (0.6864), Acc@5: 0.9062 (0.8922)
2022-01-04 21:24:01,540 Val Step[1150/1563], Loss: 1.3885 (1.4251), Acc@1: 0.7188 (0.6838), Acc@5: 0.8125 (0.8897)
2022-01-04 21:24:03,076 Val Step[1200/1563], Loss: 1.6174 (1.4422), Acc@1: 0.6875 (0.6806), Acc@5: 0.8438 (0.8864)
2022-01-04 21:24:04,581 Val Step[1250/1563], Loss: 0.8560 (1.4564), Acc@1: 0.8750 (0.6785), Acc@5: 0.9375 (0.8841)
2022-01-04 21:24:05,996 Val Step[1300/1563], Loss: 1.1103 (1.4682), Acc@1: 0.8125 (0.6758), Acc@5: 0.9062 (0.8828)
2022-01-04 21:24:07,484 Val Step[1350/1563], Loss: 1.9947 (1.4856), Acc@1: 0.4688 (0.6719), Acc@5: 0.8125 (0.8803)
2022-01-04 21:24:08,917 Val Step[1400/1563], Loss: 1.2662 (1.4935), Acc@1: 0.6875 (0.6707), Acc@5: 0.9375 (0.8792)
2022-01-04 21:24:10,377 Val Step[1450/1563], Loss: 1.8085 (1.4998), Acc@1: 0.6562 (0.6695), Acc@5: 0.8438 (0.8787)
2022-01-04 21:24:11,826 Val Step[1500/1563], Loss: 2.0560 (1.4887), Acc@1: 0.5938 (0.6720), Acc@5: 0.8438 (0.8804)
2022-01-04 21:24:13,225 Val Step[1550/1563], Loss: 0.9972 (1.4896), Acc@1: 0.8750 (0.6718), Acc@5: 0.9062 (0.8801)
2022-01-04 21:24:14,126 ----- Epoch[156/310], Validation Loss: 1.4876, Validation Acc@1: 0.6722, Validation Acc@5: 0.8803, time: 105.02
2022-01-04 21:24:14,126 ----- Epoch[156/310], Train Loss: 3.9563, Train Acc: 0.3229, time: 1499.85, Best Val(epoch156) Acc@1: 0.6722
2022-01-04 21:24:14,302 Max accuracy so far: 0.6722 at epoch_156
2022-01-04 21:24:14,302 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-04 21:24:14,302 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-04 21:24:14,412 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-04 21:24:14,790 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 21:24:14,791 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 21:24:14,931 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 21:24:14,932 Now training epoch 157. LR=0.000466
2022-01-04 21:25:28,452 Epoch[157/310], Step[0000/1251], Loss: 3.7999(3.7999), Acc: 0.4248(0.4248)
2022-01-04 21:26:25,576 Epoch[157/310], Step[0050/1251], Loss: 3.9250(3.8851), Acc: 0.3203(0.3149)
2022-01-04 21:27:21,638 Epoch[157/310], Step[0100/1251], Loss: 3.1384(3.9079), Acc: 0.2852(0.3167)
2022-01-04 21:28:18,739 Epoch[157/310], Step[0150/1251], Loss: 3.8460(3.8973), Acc: 0.4541(0.3213)
2022-01-04 21:29:15,361 Epoch[157/310], Step[0200/1251], Loss: 4.0173(3.9123), Acc: 0.1982(0.3247)
2022-01-04 21:30:11,523 Epoch[157/310], Step[0250/1251], Loss: 3.9902(3.9214), Acc: 0.2393(0.3232)
2022-01-04 21:31:09,113 Epoch[157/310], Step[0300/1251], Loss: 3.4966(3.9205), Acc: 0.4004(0.3251)
2022-01-04 21:32:06,131 Epoch[157/310], Step[0350/1251], Loss: 3.4762(3.9223), Acc: 0.2031(0.3278)
2022-01-04 21:33:03,612 Epoch[157/310], Step[0400/1251], Loss: 3.9243(3.9309), Acc: 0.1621(0.3267)
2022-01-04 21:33:59,931 Epoch[157/310], Step[0450/1251], Loss: 4.3161(3.9340), Acc: 0.1357(0.3276)
2022-01-04 21:34:56,388 Epoch[157/310], Step[0500/1251], Loss: 3.8404(3.9273), Acc: 0.0869(0.3276)
2022-01-04 21:35:53,447 Epoch[157/310], Step[0550/1251], Loss: 3.3912(3.9316), Acc: 0.5107(0.3275)
2022-01-04 21:36:51,509 Epoch[157/310], Step[0600/1251], Loss: 4.4806(3.9430), Acc: 0.3027(0.3246)
2022-01-04 21:37:49,082 Epoch[157/310], Step[0650/1251], Loss: 3.7888(3.9445), Acc: 0.2734(0.3271)
2022-01-04 21:38:46,156 Epoch[157/310], Step[0700/1251], Loss: 3.9605(3.9490), Acc: 0.3008(0.3268)
2022-01-04 21:39:42,646 Epoch[157/310], Step[0750/1251], Loss: 3.8261(3.9525), Acc: 0.4072(0.3251)
2022-01-04 21:40:39,208 Epoch[157/310], Step[0800/1251], Loss: 3.8055(3.9525), Acc: 0.1885(0.3252)
2022-01-04 21:41:36,040 Epoch[157/310], Step[0850/1251], Loss: 3.9943(3.9529), Acc: 0.3555(0.3252)
2022-01-04 21:42:31,852 Epoch[157/310], Step[0900/1251], Loss: 3.7938(3.9563), Acc: 0.4121(0.3255)
2022-01-04 21:43:29,435 Epoch[157/310], Step[0950/1251], Loss: 3.8083(3.9579), Acc: 0.4766(0.3264)
2022-01-04 21:44:26,258 Epoch[157/310], Step[1000/1251], Loss: 3.6967(3.9574), Acc: 0.2598(0.3265)
2022-01-04 21:45:22,545 Epoch[157/310], Step[1050/1251], Loss: 3.9879(3.9579), Acc: 0.2441(0.3261)
2022-01-04 21:46:19,192 Epoch[157/310], Step[1100/1251], Loss: 4.3846(3.9590), Acc: 0.2939(0.3264)
2022-01-04 21:47:16,907 Epoch[157/310], Step[1150/1251], Loss: 4.1808(3.9557), Acc: 0.3564(0.3272)
2022-01-04 21:48:12,957 Epoch[157/310], Step[1200/1251], Loss: 3.8797(3.9557), Acc: 0.4189(0.3275)
2022-01-04 21:49:07,202 Epoch[157/310], Step[1250/1251], Loss: 4.3187(3.9565), Acc: 0.2275(0.3273)
2022-01-04 21:49:09,305 ----- Epoch[157/310], Train Loss: 3.9565, Train Acc: 0.3273, time: 1494.37, Best Val(epoch156) Acc@1: 0.6722
2022-01-04 21:49:09,506 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 21:49:09,507 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 21:49:09,593 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 21:49:09,594 Now training epoch 158. LR=0.000461
2022-01-04 21:50:25,536 Epoch[158/310], Step[0000/1251], Loss: 4.3964(4.3964), Acc: 0.2861(0.2861)
2022-01-04 21:51:21,371 Epoch[158/310], Step[0050/1251], Loss: 3.5385(3.9454), Acc: 0.3301(0.3205)
2022-01-04 21:52:17,535 Epoch[158/310], Step[0100/1251], Loss: 4.4166(3.9313), Acc: 0.3418(0.3311)
2022-01-04 21:53:15,326 Epoch[158/310], Step[0150/1251], Loss: 3.4102(3.9267), Acc: 0.2959(0.3348)
2022-01-04 21:54:12,396 Epoch[158/310], Step[0200/1251], Loss: 4.0258(3.9338), Acc: 0.4463(0.3311)
2022-01-04 21:55:09,023 Epoch[158/310], Step[0250/1251], Loss: 3.9493(3.9441), Acc: 0.4756(0.3309)
2022-01-04 21:56:04,598 Epoch[158/310], Step[0300/1251], Loss: 3.9472(3.9405), Acc: 0.2861(0.3299)
2022-01-04 21:57:02,507 Epoch[158/310], Step[0350/1251], Loss: 3.8425(3.9460), Acc: 0.2490(0.3274)
2022-01-04 21:58:00,618 Epoch[158/310], Step[0400/1251], Loss: 3.5582(3.9416), Acc: 0.3418(0.3262)
2022-01-04 21:58:57,964 Epoch[158/310], Step[0450/1251], Loss: 4.6263(3.9465), Acc: 0.2559(0.3236)
2022-01-04 21:59:54,192 Epoch[158/310], Step[0500/1251], Loss: 4.1220(3.9501), Acc: 0.2627(0.3266)
2022-01-04 22:00:51,871 Epoch[158/310], Step[0550/1251], Loss: 4.4233(3.9523), Acc: 0.3428(0.3253)
2022-01-04 22:01:49,459 Epoch[158/310], Step[0600/1251], Loss: 3.7142(3.9570), Acc: 0.3066(0.3248)
2022-01-04 22:02:45,931 Epoch[158/310], Step[0650/1251], Loss: 4.3871(3.9546), Acc: 0.3604(0.3261)
2022-01-04 22:03:43,592 Epoch[158/310], Step[0700/1251], Loss: 3.9737(3.9581), Acc: 0.3613(0.3254)
2022-01-04 22:04:40,230 Epoch[158/310], Step[0750/1251], Loss: 4.6903(3.9549), Acc: 0.2617(0.3273)
2022-01-04 22:05:37,745 Epoch[158/310], Step[0800/1251], Loss: 3.9064(3.9564), Acc: 0.4648(0.3280)
2022-01-04 22:06:36,311 Epoch[158/310], Step[0850/1251], Loss: 3.2959(3.9567), Acc: 0.4238(0.3281)
2022-01-04 22:07:34,866 Epoch[158/310], Step[0900/1251], Loss: 3.6025(3.9537), Acc: 0.4668(0.3282)
2022-01-04 22:08:31,609 Epoch[158/310], Step[0950/1251], Loss: 3.8946(3.9533), Acc: 0.3281(0.3283)
2022-01-04 22:09:29,145 Epoch[158/310], Step[1000/1251], Loss: 4.3706(3.9533), Acc: 0.2822(0.3282)
2022-01-04 22:10:26,162 Epoch[158/310], Step[1050/1251], Loss: 3.7012(3.9512), Acc: 0.2441(0.3284)
2022-01-04 22:11:22,841 Epoch[158/310], Step[1100/1251], Loss: 4.1619(3.9528), Acc: 0.3760(0.3279)
2022-01-04 22:12:21,837 Epoch[158/310], Step[1150/1251], Loss: 4.1565(3.9516), Acc: 0.2148(0.3281)
2022-01-04 22:13:19,548 Epoch[158/310], Step[1200/1251], Loss: 4.0014(3.9512), Acc: 0.2559(0.3288)
2022-01-04 22:14:16,259 Epoch[158/310], Step[1250/1251], Loss: 4.0604(3.9523), Acc: 0.2891(0.3285)
2022-01-04 22:14:18,103 ----- Validation after Epoch: 158
2022-01-04 22:15:16,913 Val Step[0000/1563], Loss: 0.8423 (0.8423), Acc@1: 0.8125 (0.8125), Acc@5: 0.9688 (0.9688)
2022-01-04 22:15:18,433 Val Step[0050/1563], Loss: 2.4752 (0.9153), Acc@1: 0.3438 (0.8248), Acc@5: 0.8125 (0.9504)
2022-01-04 22:15:19,855 Val Step[0100/1563], Loss: 2.1137 (1.2366), Acc@1: 0.5312 (0.7345), Acc@5: 0.8125 (0.9152)
2022-01-04 22:15:21,280 Val Step[0150/1563], Loss: 0.7973 (1.1617), Acc@1: 0.8750 (0.7506), Acc@5: 0.9688 (0.9230)
2022-01-04 22:15:22,749 Val Step[0200/1563], Loss: 1.2107 (1.1808), Acc@1: 0.7188 (0.7492), Acc@5: 0.9375 (0.9201)
2022-01-04 22:15:24,174 Val Step[0250/1563], Loss: 0.9826 (1.1185), Acc@1: 0.8438 (0.7633), Acc@5: 1.0000 (0.9277)
2022-01-04 22:15:25,606 Val Step[0300/1563], Loss: 1.0016 (1.1875), Acc@1: 0.8125 (0.7426), Acc@5: 1.0000 (0.9221)
2022-01-04 22:15:26,966 Val Step[0350/1563], Loss: 1.4404 (1.1974), Acc@1: 0.6562 (0.7381), Acc@5: 0.9062 (0.9236)
2022-01-04 22:15:28,387 Val Step[0400/1563], Loss: 1.3323 (1.2040), Acc@1: 0.7188 (0.7321), Acc@5: 0.9375 (0.9242)
2022-01-04 22:15:29,910 Val Step[0450/1563], Loss: 1.1422 (1.2074), Acc@1: 0.5625 (0.7294), Acc@5: 0.9688 (0.9258)
2022-01-04 22:15:31,475 Val Step[0500/1563], Loss: 0.4975 (1.1972), Acc@1: 0.9375 (0.7325), Acc@5: 1.0000 (0.9275)
2022-01-04 22:15:33,067 Val Step[0550/1563], Loss: 0.9675 (1.1718), Acc@1: 0.7500 (0.7396), Acc@5: 0.9375 (0.9297)
2022-01-04 22:15:34,610 Val Step[0600/1563], Loss: 0.9146 (1.1774), Acc@1: 0.7812 (0.7391), Acc@5: 0.9375 (0.9291)
2022-01-04 22:15:36,117 Val Step[0650/1563], Loss: 0.7963 (1.2009), Acc@1: 0.8438 (0.7351), Acc@5: 1.0000 (0.9252)
2022-01-04 22:15:37,618 Val Step[0700/1563], Loss: 1.1674 (1.2374), Acc@1: 0.7812 (0.7262), Acc@5: 0.9062 (0.9197)
2022-01-04 22:15:39,112 Val Step[0750/1563], Loss: 1.1340 (1.2723), Acc@1: 0.8125 (0.7186), Acc@5: 0.9062 (0.9142)
2022-01-04 22:15:40,625 Val Step[0800/1563], Loss: 1.3798 (1.3116), Acc@1: 0.6875 (0.7094), Acc@5: 0.9062 (0.9087)
2022-01-04 22:15:42,002 Val Step[0850/1563], Loss: 1.2691 (1.3371), Acc@1: 0.7188 (0.7040), Acc@5: 0.9375 (0.9051)
2022-01-04 22:15:43,485 Val Step[0900/1563], Loss: 0.6378 (1.3383), Acc@1: 0.9062 (0.7046), Acc@5: 0.9688 (0.9048)
2022-01-04 22:15:45,035 Val Step[0950/1563], Loss: 1.6012 (1.3612), Acc@1: 0.6875 (0.7007), Acc@5: 0.8750 (0.9012)
2022-01-04 22:15:46,437 Val Step[1000/1563], Loss: 0.8673 (1.3868), Acc@1: 0.9062 (0.6950), Acc@5: 0.9688 (0.8974)
2022-01-04 22:15:47,885 Val Step[1050/1563], Loss: 0.6208 (1.4016), Acc@1: 0.9375 (0.6920), Acc@5: 0.9688 (0.8956)
2022-01-04 22:15:49,373 Val Step[1100/1563], Loss: 1.1715 (1.4157), Acc@1: 0.7188 (0.6893), Acc@5: 0.9062 (0.8932)
2022-01-04 22:15:50,971 Val Step[1150/1563], Loss: 1.5072 (1.4331), Acc@1: 0.6875 (0.6859), Acc@5: 0.8125 (0.8904)
2022-01-04 22:15:52,590 Val Step[1200/1563], Loss: 1.5231 (1.4503), Acc@1: 0.7188 (0.6823), Acc@5: 0.8438 (0.8876)
2022-01-04 22:15:54,140 Val Step[1250/1563], Loss: 0.7592 (1.4628), Acc@1: 0.9062 (0.6807), Acc@5: 0.9375 (0.8853)
2022-01-04 22:15:55,606 Val Step[1300/1563], Loss: 1.1062 (1.4732), Acc@1: 0.8438 (0.6792), Acc@5: 0.9062 (0.8840)
2022-01-04 22:15:57,002 Val Step[1350/1563], Loss: 2.7944 (1.4931), Acc@1: 0.3125 (0.6748), Acc@5: 0.5938 (0.8808)
2022-01-04 22:15:58,502 Val Step[1400/1563], Loss: 1.4255 (1.5018), Acc@1: 0.6875 (0.6727), Acc@5: 0.9375 (0.8797)
2022-01-04 22:15:59,927 Val Step[1450/1563], Loss: 1.6346 (1.5106), Acc@1: 0.6875 (0.6707), Acc@5: 0.9062 (0.8788)
2022-01-04 22:16:01,409 Val Step[1500/1563], Loss: 2.0927 (1.5006), Acc@1: 0.6250 (0.6731), Acc@5: 0.7812 (0.8801)
2022-01-04 22:16:02,904 Val Step[1550/1563], Loss: 1.1421 (1.5021), Acc@1: 0.8750 (0.6726), Acc@5: 0.9062 (0.8800)
2022-01-04 22:16:03,780 ----- Epoch[158/310], Validation Loss: 1.4998, Validation Acc@1: 0.6729, Validation Acc@5: 0.8802, time: 105.67
2022-01-04 22:16:03,780 ----- Epoch[158/310], Train Loss: 3.9523, Train Acc: 0.3285, time: 1508.51, Best Val(epoch158) Acc@1: 0.6729
2022-01-04 22:16:03,950 Max accuracy so far: 0.6729 at epoch_158
2022-01-04 22:16:03,950 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-04 22:16:03,951 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-04 22:16:04,073 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-04 22:16:04,459 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 22:16:04,459 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 22:16:04,590 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 22:16:04,590 Now training epoch 159. LR=0.000456
2022-01-04 22:17:18,976 Epoch[159/310], Step[0000/1251], Loss: 3.7211(3.7211), Acc: 0.4023(0.4023)
2022-01-04 22:18:14,978 Epoch[159/310], Step[0050/1251], Loss: 3.7374(3.9998), Acc: 0.3164(0.3397)
2022-01-04 22:19:11,281 Epoch[159/310], Step[0100/1251], Loss: 4.3165(3.9717), Acc: 0.1377(0.3372)
2022-01-04 22:20:06,882 Epoch[159/310], Step[0150/1251], Loss: 3.8699(3.9334), Acc: 0.3506(0.3434)
2022-01-04 22:21:04,235 Epoch[159/310], Step[0200/1251], Loss: 4.2081(3.9188), Acc: 0.3037(0.3398)
2022-01-04 22:22:02,549 Epoch[159/310], Step[0250/1251], Loss: 3.9166(3.9198), Acc: 0.4561(0.3404)
2022-01-04 22:22:59,946 Epoch[159/310], Step[0300/1251], Loss: 3.7864(3.9211), Acc: 0.2979(0.3418)
2022-01-04 22:23:56,177 Epoch[159/310], Step[0350/1251], Loss: 3.7657(3.9281), Acc: 0.2646(0.3418)
2022-01-04 22:24:52,397 Epoch[159/310], Step[0400/1251], Loss: 3.4172(3.9277), Acc: 0.3535(0.3422)
2022-01-04 22:25:47,829 Epoch[159/310], Step[0450/1251], Loss: 3.8895(3.9356), Acc: 0.2314(0.3396)
2022-01-04 22:26:44,849 Epoch[159/310], Step[0500/1251], Loss: 3.8461(3.9424), Acc: 0.3125(0.3389)
2022-01-04 22:27:42,325 Epoch[159/310], Step[0550/1251], Loss: 3.5309(3.9424), Acc: 0.1787(0.3387)
2022-01-04 22:28:39,048 Epoch[159/310], Step[0600/1251], Loss: 3.4349(3.9396), Acc: 0.3887(0.3382)
2022-01-04 22:29:36,767 Epoch[159/310], Step[0650/1251], Loss: 3.8935(3.9460), Acc: 0.0957(0.3365)
2022-01-04 22:30:34,290 Epoch[159/310], Step[0700/1251], Loss: 4.5350(3.9503), Acc: 0.2646(0.3345)
2022-01-04 22:31:31,346 Epoch[159/310], Step[0750/1251], Loss: 3.9539(3.9502), Acc: 0.2207(0.3327)
2022-01-04 22:32:28,415 Epoch[159/310], Step[0800/1251], Loss: 3.8208(3.9480), Acc: 0.4492(0.3323)
2022-01-04 22:33:26,278 Epoch[159/310], Step[0850/1251], Loss: 3.6619(3.9499), Acc: 0.2715(0.3322)
2022-01-04 22:34:24,388 Epoch[159/310], Step[0900/1251], Loss: 4.4822(3.9540), Acc: 0.2373(0.3317)
2022-01-04 22:35:22,838 Epoch[159/310], Step[0950/1251], Loss: 4.1746(3.9530), Acc: 0.1084(0.3313)
2022-01-04 22:36:20,303 Epoch[159/310], Step[1000/1251], Loss: 4.2723(3.9525), Acc: 0.4473(0.3310)
2022-01-04 22:37:17,675 Epoch[159/310], Step[1050/1251], Loss: 4.0416(3.9493), Acc: 0.2051(0.3311)
2022-01-04 22:38:14,824 Epoch[159/310], Step[1100/1251], Loss: 4.2069(3.9488), Acc: 0.3105(0.3307)
2022-01-04 22:39:10,076 Epoch[159/310], Step[1150/1251], Loss: 3.7805(3.9501), Acc: 0.4951(0.3310)
2022-01-04 22:40:08,372 Epoch[159/310], Step[1200/1251], Loss: 4.3883(3.9504), Acc: 0.3809(0.3315)
2022-01-04 22:41:04,789 Epoch[159/310], Step[1250/1251], Loss: 4.2215(3.9515), Acc: 0.2861(0.3310)
2022-01-04 22:41:07,259 ----- Epoch[159/310], Train Loss: 3.9515, Train Acc: 0.3310, time: 1502.67, Best Val(epoch158) Acc@1: 0.6729
2022-01-04 22:41:07,429 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 22:41:07,429 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 22:41:07,546 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 22:41:07,547 Now training epoch 160. LR=0.000450
2022-01-04 22:42:26,018 Epoch[160/310], Step[0000/1251], Loss: 3.8782(3.8782), Acc: 0.4893(0.4893)
2022-01-04 22:43:23,818 Epoch[160/310], Step[0050/1251], Loss: 4.3962(3.9376), Acc: 0.2822(0.3059)
2022-01-04 22:44:20,218 Epoch[160/310], Step[0100/1251], Loss: 3.7123(3.9703), Acc: 0.2305(0.3158)
2022-01-04 22:45:18,019 Epoch[160/310], Step[0150/1251], Loss: 3.2426(3.9630), Acc: 0.2891(0.3160)
2022-01-04 22:46:15,814 Epoch[160/310], Step[0200/1251], Loss: 4.2390(3.9440), Acc: 0.2236(0.3219)
2022-01-04 22:47:11,801 Epoch[160/310], Step[0250/1251], Loss: 3.7892(3.9413), Acc: 0.4307(0.3267)
2022-01-04 22:48:08,939 Epoch[160/310], Step[0300/1251], Loss: 4.1221(3.9387), Acc: 0.3955(0.3290)
2022-01-04 22:49:05,366 Epoch[160/310], Step[0350/1251], Loss: 3.3989(3.9382), Acc: 0.2207(0.3286)
2022-01-04 22:50:02,288 Epoch[160/310], Step[0400/1251], Loss: 3.8148(3.9448), Acc: 0.0381(0.3250)
2022-01-04 22:50:59,122 Epoch[160/310], Step[0450/1251], Loss: 3.7405(3.9494), Acc: 0.3242(0.3256)
2022-01-04 22:51:56,937 Epoch[160/310], Step[0500/1251], Loss: 3.5525(3.9474), Acc: 0.3623(0.3266)
2022-01-04 22:52:54,310 Epoch[160/310], Step[0550/1251], Loss: 4.1024(3.9492), Acc: 0.3828(0.3266)
2022-01-04 22:53:52,737 Epoch[160/310], Step[0600/1251], Loss: 3.9528(3.9488), Acc: 0.4443(0.3257)
2022-01-04 22:54:49,808 Epoch[160/310], Step[0650/1251], Loss: 3.7509(3.9509), Acc: 0.3887(0.3266)
2022-01-04 22:55:47,080 Epoch[160/310], Step[0700/1251], Loss: 3.7712(3.9498), Acc: 0.2832(0.3278)
2022-01-04 22:56:44,233 Epoch[160/310], Step[0750/1251], Loss: 3.6470(3.9526), Acc: 0.3223(0.3273)
2022-01-04 22:57:42,407 Epoch[160/310], Step[0800/1251], Loss: 4.0321(3.9502), Acc: 0.4453(0.3272)
2022-01-04 22:58:38,289 Epoch[160/310], Step[0850/1251], Loss: 4.1439(3.9503), Acc: 0.3076(0.3278)
2022-01-04 22:59:35,375 Epoch[160/310], Step[0900/1251], Loss: 4.1821(3.9512), Acc: 0.1846(0.3256)
2022-01-04 23:00:31,068 Epoch[160/310], Step[0950/1251], Loss: 3.8576(3.9584), Acc: 0.2412(0.3239)
2022-01-04 23:01:28,173 Epoch[160/310], Step[1000/1251], Loss: 4.2187(3.9591), Acc: 0.1465(0.3238)
2022-01-04 23:02:25,082 Epoch[160/310], Step[1050/1251], Loss: 3.5850(3.9606), Acc: 0.5254(0.3229)
2022-01-04 23:03:21,875 Epoch[160/310], Step[1100/1251], Loss: 4.2160(3.9634), Acc: 0.2910(0.3229)
2022-01-04 23:04:19,312 Epoch[160/310], Step[1150/1251], Loss: 4.4305(3.9609), Acc: 0.2051(0.3227)
2022-01-04 23:05:16,981 Epoch[160/310], Step[1200/1251], Loss: 3.9388(3.9626), Acc: 0.3389(0.3229)
2022-01-04 23:06:13,520 Epoch[160/310], Step[1250/1251], Loss: 4.1418(3.9604), Acc: 0.2949(0.3234)
2022-01-04 23:06:15,519 ----- Validation after Epoch: 160
2022-01-04 23:07:12,940 Val Step[0000/1563], Loss: 0.7342 (0.7342), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-04 23:07:14,446 Val Step[0050/1563], Loss: 2.5430 (0.8886), Acc@1: 0.3750 (0.8088), Acc@5: 0.7500 (0.9485)
2022-01-04 23:07:15,822 Val Step[0100/1563], Loss: 1.9175 (1.2139), Acc@1: 0.5938 (0.7222), Acc@5: 0.8438 (0.9140)
2022-01-04 23:07:17,218 Val Step[0150/1563], Loss: 0.6351 (1.1276), Acc@1: 0.8125 (0.7454), Acc@5: 0.9688 (0.9222)
2022-01-04 23:07:18,658 Val Step[0200/1563], Loss: 0.9885 (1.1371), Acc@1: 0.7188 (0.7455), Acc@5: 0.9688 (0.9206)
2022-01-04 23:07:20,208 Val Step[0250/1563], Loss: 0.5930 (1.0725), Acc@1: 0.9375 (0.7611), Acc@5: 1.0000 (0.9290)
2022-01-04 23:07:21,686 Val Step[0300/1563], Loss: 1.0875 (1.1428), Acc@1: 0.7812 (0.7415), Acc@5: 0.9062 (0.9234)
2022-01-04 23:07:23,159 Val Step[0350/1563], Loss: 1.4036 (1.1554), Acc@1: 0.6250 (0.7363), Acc@5: 0.9062 (0.9247)
2022-01-04 23:07:24,574 Val Step[0400/1563], Loss: 1.0458 (1.1680), Acc@1: 0.7188 (0.7293), Acc@5: 0.9375 (0.9253)
2022-01-04 23:07:26,099 Val Step[0450/1563], Loss: 1.1749 (1.1716), Acc@1: 0.5625 (0.7269), Acc@5: 1.0000 (0.9258)
2022-01-04 23:07:27,560 Val Step[0500/1563], Loss: 0.8130 (1.1645), Acc@1: 0.8125 (0.7291), Acc@5: 0.9688 (0.9268)
2022-01-04 23:07:29,175 Val Step[0550/1563], Loss: 1.1450 (1.1359), Acc@1: 0.6562 (0.7367), Acc@5: 0.9375 (0.9299)
2022-01-04 23:07:30,750 Val Step[0600/1563], Loss: 0.8606 (1.1384), Acc@1: 0.8125 (0.7371), Acc@5: 0.9375 (0.9294)
2022-01-04 23:07:32,221 Val Step[0650/1563], Loss: 0.6683 (1.1589), Acc@1: 0.8438 (0.7323), Acc@5: 0.9688 (0.9261)
2022-01-04 23:07:33,610 Val Step[0700/1563], Loss: 1.6066 (1.1917), Acc@1: 0.7188 (0.7257), Acc@5: 0.8750 (0.9212)
2022-01-04 23:07:35,112 Val Step[0750/1563], Loss: 1.4486 (1.2275), Acc@1: 0.6875 (0.7182), Acc@5: 0.8750 (0.9162)
2022-01-04 23:07:36,600 Val Step[0800/1563], Loss: 1.1239 (1.2717), Acc@1: 0.7812 (0.7079), Acc@5: 0.9688 (0.9104)
2022-01-04 23:07:38,087 Val Step[0850/1563], Loss: 1.6096 (1.3008), Acc@1: 0.5938 (0.7013), Acc@5: 0.9062 (0.9062)
2022-01-04 23:07:39,520 Val Step[0900/1563], Loss: 0.4739 (1.2999), Acc@1: 0.9062 (0.7030), Acc@5: 0.9688 (0.9051)
2022-01-04 23:07:41,030 Val Step[0950/1563], Loss: 1.4823 (1.3236), Acc@1: 0.6562 (0.6988), Acc@5: 0.9062 (0.9013)
2022-01-04 23:07:42,413 Val Step[1000/1563], Loss: 0.5890 (1.3460), Acc@1: 0.9062 (0.6945), Acc@5: 1.0000 (0.8979)
2022-01-04 23:07:43,799 Val Step[1050/1563], Loss: 0.4259 (1.3630), Acc@1: 0.9688 (0.6910), Acc@5: 0.9688 (0.8960)
2022-01-04 23:07:45,215 Val Step[1100/1563], Loss: 1.0628 (1.3816), Acc@1: 0.7500 (0.6872), Acc@5: 0.9375 (0.8929)
2022-01-04 23:07:46,609 Val Step[1150/1563], Loss: 1.2400 (1.3977), Acc@1: 0.7812 (0.6846), Acc@5: 0.8438 (0.8902)
2022-01-04 23:07:48,027 Val Step[1200/1563], Loss: 1.2166 (1.4151), Acc@1: 0.8125 (0.6809), Acc@5: 0.8438 (0.8877)
2022-01-04 23:07:49,522 Val Step[1250/1563], Loss: 0.8759 (1.4301), Acc@1: 0.8750 (0.6784), Acc@5: 0.9062 (0.8854)
2022-01-04 23:07:50,936 Val Step[1300/1563], Loss: 1.0885 (1.4410), Acc@1: 0.7812 (0.6762), Acc@5: 0.9062 (0.8837)
2022-01-04 23:07:52,398 Val Step[1350/1563], Loss: 2.1811 (1.4582), Acc@1: 0.3750 (0.6726), Acc@5: 0.7812 (0.8809)
2022-01-04 23:07:53,866 Val Step[1400/1563], Loss: 1.2624 (1.4672), Acc@1: 0.6875 (0.6705), Acc@5: 0.9062 (0.8795)
2022-01-04 23:07:55,386 Val Step[1450/1563], Loss: 1.2822 (1.4769), Acc@1: 0.7812 (0.6684), Acc@5: 0.9062 (0.8783)
2022-01-04 23:07:56,868 Val Step[1500/1563], Loss: 1.8377 (1.4644), Acc@1: 0.5312 (0.6711), Acc@5: 0.8438 (0.8800)
2022-01-04 23:07:58,273 Val Step[1550/1563], Loss: 1.0771 (1.4649), Acc@1: 0.8750 (0.6709), Acc@5: 0.9062 (0.8798)
2022-01-04 23:07:59,093 ----- Epoch[160/310], Validation Loss: 1.4627, Validation Acc@1: 0.6713, Validation Acc@5: 0.8800, time: 103.57
2022-01-04 23:07:59,093 ----- Epoch[160/310], Train Loss: 3.9604, Train Acc: 0.3234, time: 1507.97, Best Val(epoch158) Acc@1: 0.6729
2022-01-04 23:07:59,252 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-160-Loss-3.951942048389182.pdparams
2022-01-04 23:07:59,252 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-160-Loss-3.951942048389182.pdopt
2022-01-04 23:07:59,294 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-160-Loss-3.951942048389182-EMA.pdparams
2022-01-04 23:07:59,294 Now training epoch 161. LR=0.000445
2022-01-04 23:09:13,589 Epoch[161/310], Step[0000/1251], Loss: 4.2125(4.2125), Acc: 0.2119(0.2119)
2022-01-04 23:10:10,073 Epoch[161/310], Step[0050/1251], Loss: 3.5108(3.9129), Acc: 0.2793(0.3367)
2022-01-04 23:11:08,120 Epoch[161/310], Step[0100/1251], Loss: 3.1308(3.9236), Acc: 0.4307(0.3382)
2022-01-04 23:12:06,002 Epoch[161/310], Step[0150/1251], Loss: 3.7530(3.9089), Acc: 0.2578(0.3354)
2022-01-04 23:13:02,007 Epoch[161/310], Step[0200/1251], Loss: 4.2790(3.9172), Acc: 0.3096(0.3387)
2022-01-04 23:13:59,329 Epoch[161/310], Step[0250/1251], Loss: 3.7933(3.9283), Acc: 0.3516(0.3349)
2022-01-04 23:14:55,577 Epoch[161/310], Step[0300/1251], Loss: 3.9663(3.9321), Acc: 0.3506(0.3351)
2022-01-04 23:15:52,504 Epoch[161/310], Step[0350/1251], Loss: 3.5970(3.9268), Acc: 0.2090(0.3362)
2022-01-04 23:16:49,501 Epoch[161/310], Step[0400/1251], Loss: 4.0244(3.9327), Acc: 0.4209(0.3338)
2022-01-04 23:17:46,759 Epoch[161/310], Step[0450/1251], Loss: 3.6969(3.9306), Acc: 0.3086(0.3326)
2022-01-04 23:18:43,376 Epoch[161/310], Step[0500/1251], Loss: 3.8528(3.9363), Acc: 0.5049(0.3341)
2022-01-04 23:19:41,060 Epoch[161/310], Step[0550/1251], Loss: 4.6406(3.9374), Acc: 0.2627(0.3340)
2022-01-04 23:20:38,896 Epoch[161/310], Step[0600/1251], Loss: 4.5295(3.9365), Acc: 0.3125(0.3340)
2022-01-04 23:21:35,636 Epoch[161/310], Step[0650/1251], Loss: 3.5950(3.9364), Acc: 0.4932(0.3335)
2022-01-04 23:22:32,070 Epoch[161/310], Step[0700/1251], Loss: 3.6669(3.9367), Acc: 0.2676(0.3336)
2022-01-04 23:23:28,887 Epoch[161/310], Step[0750/1251], Loss: 4.0199(3.9405), Acc: 0.1689(0.3332)
2022-01-04 23:24:27,222 Epoch[161/310], Step[0800/1251], Loss: 3.7528(3.9427), Acc: 0.0244(0.3311)
2022-01-04 23:25:25,631 Epoch[161/310], Step[0850/1251], Loss: 4.3716(3.9409), Acc: 0.3057(0.3310)
2022-01-04 23:26:23,498 Epoch[161/310], Step[0900/1251], Loss: 4.1398(3.9364), Acc: 0.1875(0.3299)
2022-01-04 23:27:21,234 Epoch[161/310], Step[0950/1251], Loss: 4.6611(3.9393), Acc: 0.2910(0.3296)
2022-01-04 23:28:19,017 Epoch[161/310], Step[1000/1251], Loss: 3.9104(3.9394), Acc: 0.3770(0.3289)
2022-01-04 23:29:15,614 Epoch[161/310], Step[1050/1251], Loss: 4.0902(3.9417), Acc: 0.3975(0.3292)
2022-01-04 23:30:13,310 Epoch[161/310], Step[1100/1251], Loss: 4.1136(3.9444), Acc: 0.3926(0.3288)
2022-01-04 23:31:10,589 Epoch[161/310], Step[1150/1251], Loss: 4.0551(3.9443), Acc: 0.3965(0.3287)
2022-01-04 23:32:07,197 Epoch[161/310], Step[1200/1251], Loss: 3.7353(3.9435), Acc: 0.4453(0.3292)
2022-01-04 23:33:03,828 Epoch[161/310], Step[1250/1251], Loss: 4.3687(3.9449), Acc: 0.2891(0.3287)
2022-01-04 23:33:05,800 ----- Epoch[161/310], Train Loss: 3.9449, Train Acc: 0.3287, time: 1506.50, Best Val(epoch158) Acc@1: 0.6729
2022-01-04 23:33:05,969 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 23:33:05,970 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 23:33:06,082 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 23:33:06,082 Now training epoch 162. LR=0.000440
2022-01-04 23:34:18,969 Epoch[162/310], Step[0000/1251], Loss: 3.6449(3.6449), Acc: 0.3740(0.3740)
2022-01-04 23:35:14,845 Epoch[162/310], Step[0050/1251], Loss: 3.8666(3.8902), Acc: 0.3262(0.3344)
2022-01-04 23:36:11,417 Epoch[162/310], Step[0100/1251], Loss: 3.9748(3.9582), Acc: 0.5088(0.3254)
2022-01-04 23:37:08,293 Epoch[162/310], Step[0150/1251], Loss: 3.6322(3.9531), Acc: 0.5098(0.3239)
2022-01-04 23:38:05,428 Epoch[162/310], Step[0200/1251], Loss: 3.8024(3.9551), Acc: 0.4297(0.3271)
2022-01-04 23:39:03,402 Epoch[162/310], Step[0250/1251], Loss: 3.5976(3.9507), Acc: 0.4980(0.3320)
2022-01-04 23:39:59,900 Epoch[162/310], Step[0300/1251], Loss: 3.6654(3.9392), Acc: 0.3613(0.3335)
2022-01-04 23:40:56,486 Epoch[162/310], Step[0350/1251], Loss: 4.2394(3.9386), Acc: 0.3066(0.3307)
2022-01-04 23:41:52,285 Epoch[162/310], Step[0400/1251], Loss: 3.3832(3.9490), Acc: 0.3965(0.3302)
2022-01-04 23:42:49,167 Epoch[162/310], Step[0450/1251], Loss: 3.9530(3.9471), Acc: 0.2822(0.3274)
2022-01-04 23:43:47,293 Epoch[162/310], Step[0500/1251], Loss: 3.9887(3.9553), Acc: 0.3350(0.3275)
2022-01-04 23:44:43,025 Epoch[162/310], Step[0550/1251], Loss: 3.9515(3.9557), Acc: 0.4268(0.3287)
2022-01-04 23:45:40,296 Epoch[162/310], Step[0600/1251], Loss: 4.2492(3.9569), Acc: 0.2822(0.3282)
2022-01-04 23:46:38,785 Epoch[162/310], Step[0650/1251], Loss: 3.9390(3.9530), Acc: 0.2031(0.3276)
2022-01-04 23:47:36,100 Epoch[162/310], Step[0700/1251], Loss: 4.0988(3.9489), Acc: 0.3877(0.3267)
2022-01-04 23:48:32,908 Epoch[162/310], Step[0750/1251], Loss: 3.7575(3.9450), Acc: 0.4316(0.3254)
2022-01-04 23:49:30,332 Epoch[162/310], Step[0800/1251], Loss: 3.6037(3.9465), Acc: 0.2334(0.3250)
2022-01-04 23:50:27,623 Epoch[162/310], Step[0850/1251], Loss: 4.1634(3.9482), Acc: 0.3652(0.3248)
2022-01-04 23:51:25,693 Epoch[162/310], Step[0900/1251], Loss: 4.1469(3.9479), Acc: 0.3701(0.3258)
2022-01-04 23:52:21,927 Epoch[162/310], Step[0950/1251], Loss: 4.5271(3.9481), Acc: 0.2832(0.3247)
2022-01-04 23:53:17,266 Epoch[162/310], Step[1000/1251], Loss: 4.2006(3.9513), Acc: 0.1885(0.3248)
2022-01-04 23:54:14,957 Epoch[162/310], Step[1050/1251], Loss: 4.2316(3.9492), Acc: 0.2305(0.3252)
2022-01-04 23:55:11,969 Epoch[162/310], Step[1100/1251], Loss: 4.2128(3.9489), Acc: 0.2725(0.3254)
2022-01-04 23:56:08,354 Epoch[162/310], Step[1150/1251], Loss: 3.7940(3.9454), Acc: 0.5146(0.3255)
2022-01-04 23:57:05,541 Epoch[162/310], Step[1200/1251], Loss: 4.0463(3.9469), Acc: 0.2109(0.3258)
2022-01-04 23:58:01,689 Epoch[162/310], Step[1250/1251], Loss: 4.2406(3.9475), Acc: 0.2266(0.3256)
2022-01-04 23:58:03,615 ----- Validation after Epoch: 162
2022-01-04 23:59:00,253 Val Step[0000/1563], Loss: 0.9123 (0.9123), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-04 23:59:01,800 Val Step[0050/1563], Loss: 2.5434 (0.9082), Acc@1: 0.3750 (0.8229), Acc@5: 0.8125 (0.9430)
2022-01-04 23:59:03,215 Val Step[0100/1563], Loss: 2.0253 (1.2065), Acc@1: 0.4688 (0.7305), Acc@5: 0.8438 (0.9162)
2022-01-04 23:59:04,634 Val Step[0150/1563], Loss: 0.4969 (1.1323), Acc@1: 0.8750 (0.7465), Acc@5: 1.0000 (0.9218)
2022-01-04 23:59:06,076 Val Step[0200/1563], Loss: 1.3606 (1.1608), Acc@1: 0.6875 (0.7436), Acc@5: 0.9062 (0.9156)
2022-01-04 23:59:07,487 Val Step[0250/1563], Loss: 1.0365 (1.1025), Acc@1: 0.8125 (0.7560), Acc@5: 0.9688 (0.9223)
2022-01-04 23:59:08,930 Val Step[0300/1563], Loss: 1.0377 (1.1603), Acc@1: 0.7500 (0.7368), Acc@5: 1.0000 (0.9202)
2022-01-04 23:59:10,342 Val Step[0350/1563], Loss: 1.3070 (1.1621), Acc@1: 0.7500 (0.7350), Acc@5: 0.8750 (0.9227)
2022-01-04 23:59:11,756 Val Step[0400/1563], Loss: 1.2396 (1.1656), Acc@1: 0.7188 (0.7306), Acc@5: 0.9375 (0.9240)
2022-01-04 23:59:13,272 Val Step[0450/1563], Loss: 0.5950 (1.1703), Acc@1: 0.9375 (0.7270), Acc@5: 1.0000 (0.9249)
2022-01-04 23:59:14,822 Val Step[0500/1563], Loss: 0.5199 (1.1615), Acc@1: 0.9062 (0.7296), Acc@5: 1.0000 (0.9264)
2022-01-04 23:59:16,335 Val Step[0550/1563], Loss: 0.8125 (1.1353), Acc@1: 0.7812 (0.7370), Acc@5: 1.0000 (0.9290)
2022-01-04 23:59:17,945 Val Step[0600/1563], Loss: 0.8509 (1.1374), Acc@1: 0.8438 (0.7376), Acc@5: 0.9062 (0.9285)
2022-01-04 23:59:19,528 Val Step[0650/1563], Loss: 0.9818 (1.1642), Acc@1: 0.7812 (0.7323), Acc@5: 1.0000 (0.9249)
2022-01-04 23:59:20,991 Val Step[0700/1563], Loss: 1.5152 (1.2014), Acc@1: 0.7188 (0.7245), Acc@5: 0.8750 (0.9198)
2022-01-04 23:59:22,431 Val Step[0750/1563], Loss: 1.4731 (1.2413), Acc@1: 0.6562 (0.7165), Acc@5: 0.8750 (0.9142)
2022-01-04 23:59:23,970 Val Step[0800/1563], Loss: 1.1050 (1.2846), Acc@1: 0.7500 (0.7067), Acc@5: 0.9688 (0.9086)
2022-01-04 23:59:25,388 Val Step[0850/1563], Loss: 1.4751 (1.3148), Acc@1: 0.5938 (0.7004), Acc@5: 0.8750 (0.9045)
2022-01-04 23:59:26,827 Val Step[0900/1563], Loss: 0.4694 (1.3149), Acc@1: 0.9688 (0.7020), Acc@5: 0.9688 (0.9042)
2022-01-04 23:59:28,313 Val Step[0950/1563], Loss: 1.4460 (1.3365), Acc@1: 0.7500 (0.6979), Acc@5: 0.8750 (0.9011)
2022-01-04 23:59:29,668 Val Step[1000/1563], Loss: 0.6668 (1.3618), Acc@1: 0.9375 (0.6921), Acc@5: 0.9688 (0.8974)
2022-01-04 23:59:31,090 Val Step[1050/1563], Loss: 0.3739 (1.3776), Acc@1: 0.9688 (0.6882), Acc@5: 0.9688 (0.8951)
2022-01-04 23:59:32,554 Val Step[1100/1563], Loss: 1.4745 (1.3928), Acc@1: 0.6250 (0.6854), Acc@5: 0.8438 (0.8928)
2022-01-04 23:59:33,965 Val Step[1150/1563], Loss: 1.1191 (1.4083), Acc@1: 0.7500 (0.6825), Acc@5: 0.8750 (0.8905)
2022-01-04 23:59:35,342 Val Step[1200/1563], Loss: 1.2810 (1.4243), Acc@1: 0.8125 (0.6795), Acc@5: 0.8438 (0.8877)
2022-01-04 23:59:36,762 Val Step[1250/1563], Loss: 0.8835 (1.4373), Acc@1: 0.8750 (0.6779), Acc@5: 0.9062 (0.8857)
2022-01-04 23:59:38,228 Val Step[1300/1563], Loss: 1.2168 (1.4489), Acc@1: 0.7812 (0.6758), Acc@5: 0.8750 (0.8842)
2022-01-04 23:59:39,602 Val Step[1350/1563], Loss: 2.4470 (1.4706), Acc@1: 0.3438 (0.6706), Acc@5: 0.7812 (0.8812)
2022-01-04 23:59:41,060 Val Step[1400/1563], Loss: 1.0823 (1.4785), Acc@1: 0.7812 (0.6688), Acc@5: 0.9688 (0.8800)
2022-01-04 23:59:42,438 Val Step[1450/1563], Loss: 1.6076 (1.4835), Acc@1: 0.6250 (0.6679), Acc@5: 0.9062 (0.8798)
2022-01-04 23:59:43,805 Val Step[1500/1563], Loss: 2.1215 (1.4722), Acc@1: 0.5312 (0.6708), Acc@5: 0.8438 (0.8812)
2022-01-04 23:59:45,169 Val Step[1550/1563], Loss: 1.0067 (1.4725), Acc@1: 0.8750 (0.6705), Acc@5: 0.9062 (0.8814)
2022-01-04 23:59:46,088 ----- Epoch[162/310], Validation Loss: 1.4700, Validation Acc@1: 0.6709, Validation Acc@5: 0.8818, time: 102.47
2022-01-04 23:59:46,089 ----- Epoch[162/310], Train Loss: 3.9475, Train Acc: 0.3256, time: 1497.53, Best Val(epoch158) Acc@1: 0.6729
2022-01-04 23:59:46,260 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-04 23:59:46,260 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-04 23:59:46,367 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-04 23:59:46,368 Now training epoch 163. LR=0.000435
2022-01-05 00:00:55,752 Epoch[163/310], Step[0000/1251], Loss: 3.8637(3.8637), Acc: 0.3477(0.3477)
2022-01-05 00:01:51,925 Epoch[163/310], Step[0050/1251], Loss: 3.7162(3.9193), Acc: 0.1895(0.3251)
2022-01-05 00:02:47,519 Epoch[163/310], Step[0100/1251], Loss: 4.1262(3.9008), Acc: 0.3545(0.3448)
2022-01-05 00:03:44,285 Epoch[163/310], Step[0150/1251], Loss: 3.8811(3.9155), Acc: 0.3008(0.3447)
2022-01-05 00:04:40,705 Epoch[163/310], Step[0200/1251], Loss: 4.2812(3.9350), Acc: 0.3740(0.3368)
2022-01-05 00:05:37,062 Epoch[163/310], Step[0250/1251], Loss: 4.1763(3.9562), Acc: 0.4043(0.3370)
2022-01-05 00:06:35,203 Epoch[163/310], Step[0300/1251], Loss: 3.8043(3.9486), Acc: 0.2764(0.3395)
2022-01-05 00:07:31,247 Epoch[163/310], Step[0350/1251], Loss: 4.4057(3.9473), Acc: 0.1162(0.3390)
2022-01-05 00:08:26,440 Epoch[163/310], Step[0400/1251], Loss: 3.9414(3.9473), Acc: 0.4873(0.3402)
2022-01-05 00:09:23,310 Epoch[163/310], Step[0450/1251], Loss: 4.2867(3.9484), Acc: 0.2773(0.3398)
2022-01-05 00:10:20,328 Epoch[163/310], Step[0500/1251], Loss: 3.8035(3.9490), Acc: 0.4648(0.3375)
2022-01-05 00:11:15,462 Epoch[163/310], Step[0550/1251], Loss: 4.0600(3.9394), Acc: 0.4170(0.3391)
2022-01-05 00:12:12,603 Epoch[163/310], Step[0600/1251], Loss: 4.1495(3.9379), Acc: 0.2627(0.3391)
2022-01-05 00:13:10,073 Epoch[163/310], Step[0650/1251], Loss: 3.9741(3.9374), Acc: 0.1006(0.3374)
2022-01-05 00:14:07,483 Epoch[163/310], Step[0700/1251], Loss: 4.2301(3.9415), Acc: 0.2705(0.3361)
2022-01-05 00:15:04,816 Epoch[163/310], Step[0750/1251], Loss: 4.0108(3.9426), Acc: 0.1240(0.3353)
2022-01-05 00:16:01,825 Epoch[163/310], Step[0800/1251], Loss: 4.1172(3.9415), Acc: 0.4023(0.3364)
2022-01-05 00:16:59,471 Epoch[163/310], Step[0850/1251], Loss: 4.0701(3.9427), Acc: 0.3887(0.3358)
2022-01-05 00:17:55,744 Epoch[163/310], Step[0900/1251], Loss: 4.4757(3.9356), Acc: 0.3320(0.3370)
2022-01-05 00:18:52,206 Epoch[163/310], Step[0950/1251], Loss: 3.6681(3.9348), Acc: 0.2783(0.3362)
2022-01-05 00:19:48,794 Epoch[163/310], Step[1000/1251], Loss: 3.8226(3.9346), Acc: 0.1660(0.3361)
2022-01-05 00:20:46,233 Epoch[163/310], Step[1050/1251], Loss: 3.8320(3.9367), Acc: 0.3428(0.3355)
2022-01-05 00:21:43,984 Epoch[163/310], Step[1100/1251], Loss: 4.1173(3.9383), Acc: 0.3740(0.3352)
2022-01-05 00:22:41,405 Epoch[163/310], Step[1150/1251], Loss: 4.7431(3.9411), Acc: 0.2871(0.3339)
2022-01-05 00:23:39,754 Epoch[163/310], Step[1200/1251], Loss: 4.3375(3.9403), Acc: 0.2549(0.3337)
2022-01-05 00:24:36,425 Epoch[163/310], Step[1250/1251], Loss: 3.8314(3.9390), Acc: 0.3086(0.3335)
2022-01-05 00:24:38,662 ----- Epoch[163/310], Train Loss: 3.9390, Train Acc: 0.3335, time: 1492.29, Best Val(epoch158) Acc@1: 0.6729
2022-01-05 00:24:38,829 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 00:24:38,829 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 00:24:38,934 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 00:24:38,935 Now training epoch 164. LR=0.000430
2022-01-05 00:25:53,408 Epoch[164/310], Step[0000/1251], Loss: 3.4686(3.4686), Acc: 0.3027(0.3027)
2022-01-05 00:26:49,824 Epoch[164/310], Step[0050/1251], Loss: 4.0074(3.8001), Acc: 0.4111(0.3273)
2022-01-05 00:27:46,373 Epoch[164/310], Step[0100/1251], Loss: 4.0251(3.8734), Acc: 0.3965(0.3353)
2022-01-05 00:28:43,473 Epoch[164/310], Step[0150/1251], Loss: 3.6466(3.8852), Acc: 0.1777(0.3385)
2022-01-05 00:29:40,404 Epoch[164/310], Step[0200/1251], Loss: 3.8134(3.9005), Acc: 0.2900(0.3311)
2022-01-05 00:30:36,940 Epoch[164/310], Step[0250/1251], Loss: 4.0992(3.9023), Acc: 0.1816(0.3341)
2022-01-05 00:31:33,310 Epoch[164/310], Step[0300/1251], Loss: 3.8801(3.9182), Acc: 0.3496(0.3369)
2022-01-05 00:32:28,808 Epoch[164/310], Step[0350/1251], Loss: 4.1163(3.9156), Acc: 0.3066(0.3377)
2022-01-05 00:33:24,892 Epoch[164/310], Step[0400/1251], Loss: 3.9474(3.9300), Acc: 0.1875(0.3341)
2022-01-05 00:34:21,058 Epoch[164/310], Step[0450/1251], Loss: 3.5201(3.9382), Acc: 0.4766(0.3339)
2022-01-05 00:35:17,944 Epoch[164/310], Step[0500/1251], Loss: 3.6849(3.9357), Acc: 0.3877(0.3330)
2022-01-05 00:36:14,831 Epoch[164/310], Step[0550/1251], Loss: 4.1602(3.9418), Acc: 0.2900(0.3329)
2022-01-05 00:37:11,767 Epoch[164/310], Step[0600/1251], Loss: 3.9657(3.9418), Acc: 0.2578(0.3317)
2022-01-05 00:38:09,456 Epoch[164/310], Step[0650/1251], Loss: 4.2625(3.9408), Acc: 0.2812(0.3320)
2022-01-05 00:39:06,308 Epoch[164/310], Step[0700/1251], Loss: 3.4917(3.9378), Acc: 0.1504(0.3316)
2022-01-05 00:40:02,630 Epoch[164/310], Step[0750/1251], Loss: 3.9935(3.9319), Acc: 0.2373(0.3309)
2022-01-05 00:40:59,452 Epoch[164/310], Step[0800/1251], Loss: 3.1812(3.9305), Acc: 0.2607(0.3312)
2022-01-05 00:41:57,602 Epoch[164/310], Step[0850/1251], Loss: 3.9538(3.9340), Acc: 0.3184(0.3305)
2022-01-05 00:42:55,101 Epoch[164/310], Step[0900/1251], Loss: 4.2253(3.9355), Acc: 0.3525(0.3305)
2022-01-05 00:43:52,652 Epoch[164/310], Step[0950/1251], Loss: 3.7258(3.9374), Acc: 0.4912(0.3295)
2022-01-05 00:44:49,904 Epoch[164/310], Step[1000/1251], Loss: 4.1937(3.9374), Acc: 0.3262(0.3296)
2022-01-05 00:45:46,946 Epoch[164/310], Step[1050/1251], Loss: 4.5128(3.9384), Acc: 0.3828(0.3295)
2022-01-05 00:46:44,184 Epoch[164/310], Step[1100/1251], Loss: 4.1505(3.9395), Acc: 0.3066(0.3303)
2022-01-05 00:47:42,224 Epoch[164/310], Step[1150/1251], Loss: 3.2712(3.9373), Acc: 0.5586(0.3302)
2022-01-05 00:48:39,422 Epoch[164/310], Step[1200/1251], Loss: 3.4328(3.9360), Acc: 0.2754(0.3314)
2022-01-05 00:49:36,592 Epoch[164/310], Step[1250/1251], Loss: 3.7279(3.9362), Acc: 0.2812(0.3309)
2022-01-05 00:49:39,081 ----- Validation after Epoch: 164
2022-01-05 00:50:33,782 Val Step[0000/1563], Loss: 0.7916 (0.7916), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2022-01-05 00:50:35,300 Val Step[0050/1563], Loss: 2.3721 (0.8750), Acc@1: 0.4375 (0.8150), Acc@5: 0.8125 (0.9461)
2022-01-05 00:50:36,721 Val Step[0100/1563], Loss: 2.1374 (1.1694), Acc@1: 0.5000 (0.7336), Acc@5: 0.8125 (0.9189)
2022-01-05 00:50:38,127 Val Step[0150/1563], Loss: 0.6358 (1.0967), Acc@1: 0.8750 (0.7508), Acc@5: 1.0000 (0.9245)
2022-01-05 00:50:39,645 Val Step[0200/1563], Loss: 1.2933 (1.1135), Acc@1: 0.6875 (0.7519), Acc@5: 0.9375 (0.9215)
2022-01-05 00:50:41,059 Val Step[0250/1563], Loss: 0.6816 (1.0612), Acc@1: 0.9688 (0.7654), Acc@5: 1.0000 (0.9294)
2022-01-05 00:50:42,467 Val Step[0300/1563], Loss: 1.2622 (1.1193), Acc@1: 0.6875 (0.7499), Acc@5: 0.9062 (0.9255)
2022-01-05 00:50:43,820 Val Step[0350/1563], Loss: 1.4748 (1.1459), Acc@1: 0.6250 (0.7409), Acc@5: 0.9062 (0.9252)
2022-01-05 00:50:45,242 Val Step[0400/1563], Loss: 0.9956 (1.1614), Acc@1: 0.9062 (0.7342), Acc@5: 0.9688 (0.9258)
2022-01-05 00:50:46,758 Val Step[0450/1563], Loss: 1.1032 (1.1674), Acc@1: 0.6250 (0.7321), Acc@5: 1.0000 (0.9260)
2022-01-05 00:50:48,249 Val Step[0500/1563], Loss: 0.5166 (1.1590), Acc@1: 0.9062 (0.7348), Acc@5: 1.0000 (0.9271)
2022-01-05 00:50:49,628 Val Step[0550/1563], Loss: 0.8584 (1.1316), Acc@1: 0.8125 (0.7422), Acc@5: 0.9375 (0.9298)
2022-01-05 00:50:51,100 Val Step[0600/1563], Loss: 0.9115 (1.1371), Acc@1: 0.8125 (0.7419), Acc@5: 0.9688 (0.9291)
2022-01-05 00:50:52,662 Val Step[0650/1563], Loss: 0.9538 (1.1578), Acc@1: 0.6562 (0.7367), Acc@5: 1.0000 (0.9262)
2022-01-05 00:50:54,136 Val Step[0700/1563], Loss: 1.6113 (1.1946), Acc@1: 0.6875 (0.7287), Acc@5: 0.8438 (0.9209)
2022-01-05 00:50:55,633 Val Step[0750/1563], Loss: 1.4273 (1.2283), Acc@1: 0.7188 (0.7222), Acc@5: 0.9062 (0.9158)
2022-01-05 00:50:57,044 Val Step[0800/1563], Loss: 0.9511 (1.2671), Acc@1: 0.7500 (0.7132), Acc@5: 0.9688 (0.9107)
2022-01-05 00:50:58,481 Val Step[0850/1563], Loss: 1.5847 (1.2958), Acc@1: 0.5625 (0.7066), Acc@5: 0.9062 (0.9074)
2022-01-05 00:50:59,988 Val Step[0900/1563], Loss: 0.2909 (1.2977), Acc@1: 0.9688 (0.7075), Acc@5: 1.0000 (0.9065)
2022-01-05 00:51:01,546 Val Step[0950/1563], Loss: 1.6094 (1.3187), Acc@1: 0.6562 (0.7032), Acc@5: 0.8750 (0.9032)
2022-01-05 00:51:03,041 Val Step[1000/1563], Loss: 0.7675 (1.3438), Acc@1: 0.8438 (0.6976), Acc@5: 1.0000 (0.8995)
2022-01-05 00:51:04,519 Val Step[1050/1563], Loss: 0.4087 (1.3617), Acc@1: 0.9688 (0.6936), Acc@5: 0.9688 (0.8969)
2022-01-05 00:51:05,926 Val Step[1100/1563], Loss: 1.3576 (1.3775), Acc@1: 0.6875 (0.6905), Acc@5: 0.8438 (0.8944)
2022-01-05 00:51:07,351 Val Step[1150/1563], Loss: 1.3900 (1.3956), Acc@1: 0.7500 (0.6871), Acc@5: 0.8125 (0.8917)
2022-01-05 00:51:08,769 Val Step[1200/1563], Loss: 1.3937 (1.4112), Acc@1: 0.6875 (0.6839), Acc@5: 0.8438 (0.8891)
2022-01-05 00:51:10,183 Val Step[1250/1563], Loss: 0.8358 (1.4239), Acc@1: 0.8438 (0.6823), Acc@5: 0.9062 (0.8867)
2022-01-05 00:51:11,665 Val Step[1300/1563], Loss: 1.3059 (1.4332), Acc@1: 0.8125 (0.6803), Acc@5: 0.8750 (0.8855)
2022-01-05 00:51:13,073 Val Step[1350/1563], Loss: 2.2162 (1.4530), Acc@1: 0.3438 (0.6757), Acc@5: 0.7500 (0.8827)
2022-01-05 00:51:14,499 Val Step[1400/1563], Loss: 1.4949 (1.4627), Acc@1: 0.6562 (0.6736), Acc@5: 0.9062 (0.8811)
2022-01-05 00:51:15,935 Val Step[1450/1563], Loss: 1.6772 (1.4720), Acc@1: 0.5625 (0.6712), Acc@5: 0.9062 (0.8803)
2022-01-05 00:51:17,351 Val Step[1500/1563], Loss: 1.8372 (1.4609), Acc@1: 0.5625 (0.6738), Acc@5: 0.8438 (0.8818)
2022-01-05 00:51:18,744 Val Step[1550/1563], Loss: 0.9283 (1.4599), Acc@1: 0.8750 (0.6742), Acc@5: 0.9062 (0.8821)
2022-01-05 00:51:20,088 ----- Epoch[164/310], Validation Loss: 1.4578, Validation Acc@1: 0.6744, Validation Acc@5: 0.8824, time: 101.00
2022-01-05 00:51:20,088 ----- Epoch[164/310], Train Loss: 3.9362, Train Acc: 0.3309, time: 1500.14, Best Val(epoch164) Acc@1: 0.6744
2022-01-05 00:51:20,261 Max accuracy so far: 0.6744 at epoch_164
2022-01-05 00:51:20,261 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 00:51:20,261 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 00:51:20,370 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 00:51:20,762 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 00:51:20,762 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 00:51:20,886 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 00:51:21,187 Now training epoch 165. LR=0.000425
2022-01-05 00:52:37,274 Epoch[165/310], Step[0000/1251], Loss: 4.0489(4.0489), Acc: 0.2842(0.2842)
2022-01-05 00:53:32,066 Epoch[165/310], Step[0050/1251], Loss: 4.0906(3.9504), Acc: 0.3711(0.3221)
2022-01-05 00:54:28,187 Epoch[165/310], Step[0100/1251], Loss: 3.6722(3.9340), Acc: 0.1729(0.3204)
2022-01-05 00:55:25,679 Epoch[165/310], Step[0150/1251], Loss: 3.9256(3.9311), Acc: 0.2764(0.3219)
2022-01-05 00:56:22,308 Epoch[165/310], Step[0200/1251], Loss: 4.3084(3.9323), Acc: 0.2275(0.3268)
2022-01-05 00:57:19,161 Epoch[165/310], Step[0250/1251], Loss: 4.0820(3.9318), Acc: 0.4150(0.3218)
2022-01-05 00:58:15,934 Epoch[165/310], Step[0300/1251], Loss: 3.9751(3.9345), Acc: 0.4756(0.3271)
2022-01-05 00:59:13,266 Epoch[165/310], Step[0350/1251], Loss: 3.4383(3.9265), Acc: 0.2148(0.3250)
2022-01-05 01:00:10,371 Epoch[165/310], Step[0400/1251], Loss: 4.1782(3.9220), Acc: 0.4023(0.3253)
2022-01-05 01:01:08,149 Epoch[165/310], Step[0450/1251], Loss: 3.8546(3.9225), Acc: 0.2988(0.3268)
2022-01-05 01:02:05,150 Epoch[165/310], Step[0500/1251], Loss: 3.8609(3.9238), Acc: 0.2773(0.3269)
2022-01-05 01:03:01,067 Epoch[165/310], Step[0550/1251], Loss: 3.9905(3.9293), Acc: 0.3896(0.3288)
2022-01-05 01:03:58,255 Epoch[165/310], Step[0600/1251], Loss: 4.0916(3.9294), Acc: 0.1836(0.3281)
2022-01-05 01:04:54,936 Epoch[165/310], Step[0650/1251], Loss: 4.1968(3.9289), Acc: 0.1846(0.3272)
2022-01-05 01:05:52,166 Epoch[165/310], Step[0700/1251], Loss: 3.7787(3.9274), Acc: 0.5166(0.3279)
2022-01-05 01:06:48,332 Epoch[165/310], Step[0750/1251], Loss: 3.7006(3.9262), Acc: 0.4883(0.3305)
2022-01-05 01:07:43,992 Epoch[165/310], Step[0800/1251], Loss: 3.7659(3.9272), Acc: 0.4688(0.3297)
2022-01-05 01:08:39,701 Epoch[165/310], Step[0850/1251], Loss: 3.8293(3.9264), Acc: 0.3662(0.3311)
2022-01-05 01:09:35,242 Epoch[165/310], Step[0900/1251], Loss: 4.1746(3.9222), Acc: 0.3848(0.3317)
2022-01-05 01:10:32,196 Epoch[165/310], Step[0950/1251], Loss: 3.5365(3.9216), Acc: 0.5020(0.3324)
2022-01-05 01:11:30,264 Epoch[165/310], Step[1000/1251], Loss: 3.4890(3.9220), Acc: 0.2939(0.3312)
2022-01-05 01:12:28,662 Epoch[165/310], Step[1050/1251], Loss: 3.8194(3.9253), Acc: 0.2617(0.3301)
2022-01-05 01:13:26,190 Epoch[165/310], Step[1100/1251], Loss: 4.1309(3.9291), Acc: 0.3027(0.3294)
2022-01-05 01:14:23,103 Epoch[165/310], Step[1150/1251], Loss: 3.2877(3.9283), Acc: 0.4141(0.3283)
2022-01-05 01:15:20,159 Epoch[165/310], Step[1200/1251], Loss: 4.3503(3.9304), Acc: 0.2568(0.3289)
2022-01-05 01:16:17,020 Epoch[165/310], Step[1250/1251], Loss: 3.8299(3.9317), Acc: 0.4736(0.3280)
2022-01-05 01:16:19,034 ----- Epoch[165/310], Train Loss: 3.9317, Train Acc: 0.3280, time: 1497.84, Best Val(epoch164) Acc@1: 0.6744
2022-01-05 01:16:19,220 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 01:16:19,220 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 01:16:19,333 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 01:16:19,334 Now training epoch 166. LR=0.000420
2022-01-05 01:17:32,198 Epoch[166/310], Step[0000/1251], Loss: 3.8304(3.8304), Acc: 0.2148(0.2148)
2022-01-05 01:18:29,326 Epoch[166/310], Step[0050/1251], Loss: 4.1694(3.9774), Acc: 0.4531(0.3375)
2022-01-05 01:19:27,011 Epoch[166/310], Step[0100/1251], Loss: 3.7839(3.9707), Acc: 0.4766(0.3189)
2022-01-05 01:20:25,024 Epoch[166/310], Step[0150/1251], Loss: 3.6975(3.9287), Acc: 0.3545(0.3296)
2022-01-05 01:21:21,451 Epoch[166/310], Step[0200/1251], Loss: 3.7465(3.9197), Acc: 0.3281(0.3340)
2022-01-05 01:22:18,776 Epoch[166/310], Step[0250/1251], Loss: 3.5635(3.9186), Acc: 0.0986(0.3297)
2022-01-05 01:23:14,930 Epoch[166/310], Step[0300/1251], Loss: 3.5596(3.9186), Acc: 0.3643(0.3279)
2022-01-05 01:24:11,028 Epoch[166/310], Step[0350/1251], Loss: 3.1887(3.9173), Acc: 0.4141(0.3291)
2022-01-05 01:25:08,783 Epoch[166/310], Step[0400/1251], Loss: 3.6273(3.9138), Acc: 0.4023(0.3327)
2022-01-05 01:26:06,233 Epoch[166/310], Step[0450/1251], Loss: 4.0050(3.9153), Acc: 0.3428(0.3336)
2022-01-05 01:27:02,864 Epoch[166/310], Step[0500/1251], Loss: 4.2004(3.9181), Acc: 0.3867(0.3332)
2022-01-05 01:27:59,750 Epoch[166/310], Step[0550/1251], Loss: 3.6973(3.9209), Acc: 0.4951(0.3320)
2022-01-05 01:28:57,352 Epoch[166/310], Step[0600/1251], Loss: 4.3122(3.9239), Acc: 0.3301(0.3318)
2022-01-05 01:29:53,677 Epoch[166/310], Step[0650/1251], Loss: 3.8246(3.9225), Acc: 0.1924(0.3321)
2022-01-05 01:30:51,944 Epoch[166/310], Step[0700/1251], Loss: 4.0918(3.9185), Acc: 0.2930(0.3320)
2022-01-05 01:31:49,824 Epoch[166/310], Step[0750/1251], Loss: 4.1046(3.9199), Acc: 0.2676(0.3325)
2022-01-05 01:32:45,893 Epoch[166/310], Step[0800/1251], Loss: 4.1787(3.9168), Acc: 0.3838(0.3321)
2022-01-05 01:33:41,243 Epoch[166/310], Step[0850/1251], Loss: 3.8798(3.9131), Acc: 0.4111(0.3320)
2022-01-05 01:34:39,094 Epoch[166/310], Step[0900/1251], Loss: 4.4344(3.9174), Acc: 0.3701(0.3308)
2022-01-05 01:35:35,290 Epoch[166/310], Step[0950/1251], Loss: 3.6612(3.9178), Acc: 0.4482(0.3300)
2022-01-05 01:36:32,272 Epoch[166/310], Step[1000/1251], Loss: 3.4273(3.9192), Acc: 0.5244(0.3300)
2022-01-05 01:37:29,769 Epoch[166/310], Step[1050/1251], Loss: 4.1433(3.9180), Acc: 0.1592(0.3295)
2022-01-05 01:38:27,547 Epoch[166/310], Step[1100/1251], Loss: 3.6686(3.9196), Acc: 0.5137(0.3298)
2022-01-05 01:39:24,384 Epoch[166/310], Step[1150/1251], Loss: 4.3067(3.9189), Acc: 0.2969(0.3312)
2022-01-05 01:40:21,444 Epoch[166/310], Step[1200/1251], Loss: 3.5884(3.9195), Acc: 0.3496(0.3313)
2022-01-05 01:41:18,464 Epoch[166/310], Step[1250/1251], Loss: 3.7375(3.9233), Acc: 0.3633(0.3307)
2022-01-05 01:41:20,461 ----- Validation after Epoch: 166
2022-01-05 01:42:16,516 Val Step[0000/1563], Loss: 0.8156 (0.8156), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 01:42:18,014 Val Step[0050/1563], Loss: 2.3708 (0.9017), Acc@1: 0.4062 (0.8229), Acc@5: 0.8438 (0.9485)
2022-01-05 01:42:19,512 Val Step[0100/1563], Loss: 2.0548 (1.2254), Acc@1: 0.5938 (0.7392), Acc@5: 0.8125 (0.9162)
2022-01-05 01:42:20,956 Val Step[0150/1563], Loss: 0.7115 (1.1688), Acc@1: 0.8438 (0.7531), Acc@5: 1.0000 (0.9214)
2022-01-05 01:42:22,471 Val Step[0200/1563], Loss: 1.3442 (1.1945), Acc@1: 0.6875 (0.7505), Acc@5: 0.9062 (0.9167)
2022-01-05 01:42:23,925 Val Step[0250/1563], Loss: 1.0017 (1.1291), Acc@1: 0.8125 (0.7641), Acc@5: 1.0000 (0.9244)
2022-01-05 01:42:25,400 Val Step[0300/1563], Loss: 1.5105 (1.1987), Acc@1: 0.5938 (0.7435), Acc@5: 0.9375 (0.9182)
2022-01-05 01:42:26,899 Val Step[0350/1563], Loss: 1.4905 (1.1993), Acc@1: 0.6250 (0.7402), Acc@5: 0.9062 (0.9210)
2022-01-05 01:42:28,287 Val Step[0400/1563], Loss: 0.8448 (1.2083), Acc@1: 0.8750 (0.7325), Acc@5: 0.9688 (0.9227)
2022-01-05 01:42:29,871 Val Step[0450/1563], Loss: 1.1206 (1.2166), Acc@1: 0.6875 (0.7296), Acc@5: 0.9688 (0.9236)
2022-01-05 01:42:31,377 Val Step[0500/1563], Loss: 0.7841 (1.2085), Acc@1: 0.8750 (0.7319), Acc@5: 1.0000 (0.9252)
2022-01-05 01:42:32,882 Val Step[0550/1563], Loss: 0.7148 (1.1883), Acc@1: 0.8438 (0.7371), Acc@5: 0.9375 (0.9269)
2022-01-05 01:42:34,472 Val Step[0600/1563], Loss: 1.1850 (1.1924), Acc@1: 0.7500 (0.7369), Acc@5: 0.9062 (0.9264)
2022-01-05 01:42:35,912 Val Step[0650/1563], Loss: 0.9291 (1.2107), Acc@1: 0.7812 (0.7335), Acc@5: 0.9688 (0.9230)
2022-01-05 01:42:37,312 Val Step[0700/1563], Loss: 1.0819 (1.2416), Acc@1: 0.7500 (0.7263), Acc@5: 0.8750 (0.9192)
2022-01-05 01:42:38,852 Val Step[0750/1563], Loss: 1.7588 (1.2809), Acc@1: 0.6875 (0.7184), Acc@5: 0.8438 (0.9136)
2022-01-05 01:42:40,294 Val Step[0800/1563], Loss: 0.6721 (1.3174), Acc@1: 0.8750 (0.7099), Acc@5: 1.0000 (0.9084)
2022-01-05 01:42:41,763 Val Step[0850/1563], Loss: 1.5627 (1.3460), Acc@1: 0.5625 (0.7034), Acc@5: 0.9062 (0.9044)
2022-01-05 01:42:43,155 Val Step[0900/1563], Loss: 0.4220 (1.3475), Acc@1: 0.9375 (0.7039), Acc@5: 1.0000 (0.9033)
2022-01-05 01:42:44,665 Val Step[0950/1563], Loss: 1.9011 (1.3690), Acc@1: 0.5938 (0.7001), Acc@5: 0.8750 (0.8997)
2022-01-05 01:42:46,097 Val Step[1000/1563], Loss: 0.6948 (1.3952), Acc@1: 0.9375 (0.6937), Acc@5: 0.9375 (0.8957)
2022-01-05 01:42:47,628 Val Step[1050/1563], Loss: 0.4345 (1.4093), Acc@1: 0.9375 (0.6908), Acc@5: 0.9688 (0.8940)
2022-01-05 01:42:49,102 Val Step[1100/1563], Loss: 1.1071 (1.4245), Acc@1: 0.7500 (0.6878), Acc@5: 0.9688 (0.8918)
2022-01-05 01:42:50,719 Val Step[1150/1563], Loss: 1.1840 (1.4404), Acc@1: 0.7812 (0.6847), Acc@5: 0.8125 (0.8894)
2022-01-05 01:42:52,171 Val Step[1200/1563], Loss: 1.2374 (1.4546), Acc@1: 0.8438 (0.6819), Acc@5: 0.8750 (0.8868)
2022-01-05 01:42:53,635 Val Step[1250/1563], Loss: 1.1627 (1.4707), Acc@1: 0.8438 (0.6793), Acc@5: 0.9062 (0.8842)
2022-01-05 01:42:55,164 Val Step[1300/1563], Loss: 1.1328 (1.4816), Acc@1: 0.7812 (0.6771), Acc@5: 0.8750 (0.8829)
2022-01-05 01:42:56,694 Val Step[1350/1563], Loss: 2.0103 (1.5008), Acc@1: 0.4375 (0.6731), Acc@5: 0.7812 (0.8797)
2022-01-05 01:42:58,308 Val Step[1400/1563], Loss: 1.2656 (1.5081), Acc@1: 0.7188 (0.6715), Acc@5: 0.9062 (0.8788)
2022-01-05 01:42:59,767 Val Step[1450/1563], Loss: 1.4599 (1.5168), Acc@1: 0.5938 (0.6694), Acc@5: 0.9062 (0.8777)
2022-01-05 01:43:01,321 Val Step[1500/1563], Loss: 1.6356 (1.5048), Acc@1: 0.6250 (0.6719), Acc@5: 0.9062 (0.8795)
2022-01-05 01:43:02,837 Val Step[1550/1563], Loss: 0.9635 (1.5047), Acc@1: 0.8750 (0.6719), Acc@5: 0.9062 (0.8795)
2022-01-05 01:43:03,771 ----- Epoch[166/310], Validation Loss: 1.5025, Validation Acc@1: 0.6723, Validation Acc@5: 0.8797, time: 103.31
2022-01-05 01:43:03,771 ----- Epoch[166/310], Train Loss: 3.9233, Train Acc: 0.3307, time: 1501.12, Best Val(epoch164) Acc@1: 0.6744
2022-01-05 01:43:03,952 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 01:43:03,953 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 01:43:04,063 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 01:43:04,063 Now training epoch 167. LR=0.000414
2022-01-05 01:44:14,733 Epoch[167/310], Step[0000/1251], Loss: 4.0907(4.0907), Acc: 0.3652(0.3652)
2022-01-05 01:45:10,963 Epoch[167/310], Step[0050/1251], Loss: 4.0259(3.9309), Acc: 0.2451(0.3305)
2022-01-05 01:46:06,945 Epoch[167/310], Step[0100/1251], Loss: 4.1858(3.9450), Acc: 0.2861(0.3254)
2022-01-05 01:47:03,527 Epoch[167/310], Step[0150/1251], Loss: 4.2647(3.9486), Acc: 0.3760(0.3259)
2022-01-05 01:47:59,189 Epoch[167/310], Step[0200/1251], Loss: 3.1212(3.9478), Acc: 0.3789(0.3263)
2022-01-05 01:48:55,172 Epoch[167/310], Step[0250/1251], Loss: 3.5922(3.9544), Acc: 0.4932(0.3277)
2022-01-05 01:49:52,336 Epoch[167/310], Step[0300/1251], Loss: 3.9517(3.9507), Acc: 0.4268(0.3250)
2022-01-05 01:50:49,393 Epoch[167/310], Step[0350/1251], Loss: 3.7632(3.9470), Acc: 0.2051(0.3253)
2022-01-05 01:51:46,067 Epoch[167/310], Step[0400/1251], Loss: 4.1071(3.9486), Acc: 0.4307(0.3277)
2022-01-05 01:52:41,527 Epoch[167/310], Step[0450/1251], Loss: 3.8331(3.9567), Acc: 0.5000(0.3283)
2022-01-05 01:53:37,551 Epoch[167/310], Step[0500/1251], Loss: 3.9411(3.9524), Acc: 0.3525(0.3289)
2022-01-05 01:54:34,838 Epoch[167/310], Step[0550/1251], Loss: 3.8889(3.9471), Acc: 0.4863(0.3281)
2022-01-05 01:55:33,056 Epoch[167/310], Step[0600/1251], Loss: 4.1827(3.9422), Acc: 0.1650(0.3288)
2022-01-05 01:56:29,034 Epoch[167/310], Step[0650/1251], Loss: 4.5528(3.9377), Acc: 0.2637(0.3294)
2022-01-05 01:57:25,030 Epoch[167/310], Step[0700/1251], Loss: 3.6231(3.9380), Acc: 0.3389(0.3274)
2022-01-05 01:58:21,104 Epoch[167/310], Step[0750/1251], Loss: 4.0541(3.9373), Acc: 0.3896(0.3266)
2022-01-05 01:59:17,937 Epoch[167/310], Step[0800/1251], Loss: 4.6079(3.9420), Acc: 0.2637(0.3266)
2022-01-05 02:00:13,931 Epoch[167/310], Step[0850/1251], Loss: 3.7408(3.9440), Acc: 0.3418(0.3269)
2022-01-05 02:01:10,560 Epoch[167/310], Step[0900/1251], Loss: 3.8444(3.9427), Acc: 0.1846(0.3265)
2022-01-05 02:02:07,747 Epoch[167/310], Step[0950/1251], Loss: 3.6857(3.9409), Acc: 0.5146(0.3274)
2022-01-05 02:03:05,294 Epoch[167/310], Step[1000/1251], Loss: 4.6573(3.9380), Acc: 0.2676(0.3272)
2022-01-05 02:04:02,532 Epoch[167/310], Step[1050/1251], Loss: 3.5728(3.9386), Acc: 0.4092(0.3275)
2022-01-05 02:05:00,556 Epoch[167/310], Step[1100/1251], Loss: 3.8588(3.9402), Acc: 0.1982(0.3264)
2022-01-05 02:05:56,413 Epoch[167/310], Step[1150/1251], Loss: 4.2946(3.9434), Acc: 0.1826(0.3266)
2022-01-05 02:06:53,702 Epoch[167/310], Step[1200/1251], Loss: 3.4028(3.9415), Acc: 0.1240(0.3269)
2022-01-05 02:07:51,624 Epoch[167/310], Step[1250/1251], Loss: 3.2254(3.9441), Acc: 0.3037(0.3267)
2022-01-05 02:07:53,581 ----- Epoch[167/310], Train Loss: 3.9441, Train Acc: 0.3267, time: 1489.51, Best Val(epoch164) Acc@1: 0.6744
2022-01-05 02:07:53,756 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 02:07:53,756 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 02:07:53,861 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 02:07:53,862 Now training epoch 168. LR=0.000409
2022-01-05 02:09:08,891 Epoch[168/310], Step[0000/1251], Loss: 3.9007(3.9007), Acc: 0.4482(0.4482)
2022-01-05 02:10:06,169 Epoch[168/310], Step[0050/1251], Loss: 3.8951(3.9710), Acc: 0.2471(0.3408)
2022-01-05 02:11:02,207 Epoch[168/310], Step[0100/1251], Loss: 4.0634(3.9494), Acc: 0.4316(0.3363)
2022-01-05 02:11:57,796 Epoch[168/310], Step[0150/1251], Loss: 4.0361(3.9263), Acc: 0.2109(0.3371)
2022-01-05 02:12:54,388 Epoch[168/310], Step[0200/1251], Loss: 4.0782(3.9247), Acc: 0.1787(0.3289)
2022-01-05 02:13:50,991 Epoch[168/310], Step[0250/1251], Loss: 3.8890(3.9181), Acc: 0.3916(0.3282)
2022-01-05 02:14:47,842 Epoch[168/310], Step[0300/1251], Loss: 3.7935(3.9118), Acc: 0.3906(0.3308)
2022-01-05 02:15:45,245 Epoch[168/310], Step[0350/1251], Loss: 3.8249(3.9100), Acc: 0.4150(0.3315)
2022-01-05 02:16:42,715 Epoch[168/310], Step[0400/1251], Loss: 3.6898(3.9113), Acc: 0.4795(0.3310)
2022-01-05 02:17:40,115 Epoch[168/310], Step[0450/1251], Loss: 3.8718(3.9252), Acc: 0.3320(0.3313)
2022-01-05 02:18:37,796 Epoch[168/310], Step[0500/1251], Loss: 4.0379(3.9256), Acc: 0.3438(0.3269)
2022-01-05 02:19:35,166 Epoch[168/310], Step[0550/1251], Loss: 3.9066(3.9293), Acc: 0.2939(0.3263)
2022-01-05 02:20:32,536 Epoch[168/310], Step[0600/1251], Loss: 4.3778(3.9274), Acc: 0.2295(0.3268)
2022-01-05 02:21:29,835 Epoch[168/310], Step[0650/1251], Loss: 3.8729(3.9302), Acc: 0.1445(0.3260)
2022-01-05 02:22:26,171 Epoch[168/310], Step[0700/1251], Loss: 3.8094(3.9303), Acc: 0.1689(0.3256)
2022-01-05 02:23:23,423 Epoch[168/310], Step[0750/1251], Loss: 4.0390(3.9259), Acc: 0.2803(0.3272)
2022-01-05 02:24:19,852 Epoch[168/310], Step[0800/1251], Loss: 3.5646(3.9278), Acc: 0.5225(0.3271)
2022-01-05 02:25:16,465 Epoch[168/310], Step[0850/1251], Loss: 3.7395(3.9318), Acc: 0.3408(0.3273)
2022-01-05 02:26:13,576 Epoch[168/310], Step[0900/1251], Loss: 4.1103(3.9322), Acc: 0.4600(0.3277)
2022-01-05 02:27:09,570 Epoch[168/310], Step[0950/1251], Loss: 3.8904(3.9365), Acc: 0.2402(0.3264)
2022-01-05 02:28:07,069 Epoch[168/310], Step[1000/1251], Loss: 3.8626(3.9337), Acc: 0.4209(0.3256)
2022-01-05 02:29:04,293 Epoch[168/310], Step[1050/1251], Loss: 3.6736(3.9353), Acc: 0.3535(0.3259)
2022-01-05 02:30:00,869 Epoch[168/310], Step[1100/1251], Loss: 3.5688(3.9358), Acc: 0.3906(0.3252)
2022-01-05 02:30:57,513 Epoch[168/310], Step[1150/1251], Loss: 3.7663(3.9357), Acc: 0.2129(0.3256)
2022-01-05 02:31:54,600 Epoch[168/310], Step[1200/1251], Loss: 4.2270(3.9354), Acc: 0.1748(0.3258)
2022-01-05 02:32:50,734 Epoch[168/310], Step[1250/1251], Loss: 4.4800(3.9323), Acc: 0.3574(0.3272)
2022-01-05 02:32:52,927 ----- Validation after Epoch: 168
2022-01-05 02:33:48,731 Val Step[0000/1563], Loss: 0.8258 (0.8258), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 02:33:50,329 Val Step[0050/1563], Loss: 2.4247 (0.8457), Acc@1: 0.4062 (0.8199), Acc@5: 0.8125 (0.9504)
2022-01-05 02:33:51,751 Val Step[0100/1563], Loss: 2.3382 (1.1774), Acc@1: 0.4062 (0.7327), Acc@5: 0.7500 (0.9146)
2022-01-05 02:33:53,196 Val Step[0150/1563], Loss: 0.6631 (1.1133), Acc@1: 0.8750 (0.7500), Acc@5: 0.9688 (0.9212)
2022-01-05 02:33:54,592 Val Step[0200/1563], Loss: 1.2829 (1.1289), Acc@1: 0.6562 (0.7520), Acc@5: 0.9375 (0.9187)
2022-01-05 02:33:56,019 Val Step[0250/1563], Loss: 0.7569 (1.0691), Acc@1: 0.7812 (0.7661), Acc@5: 1.0000 (0.9269)
2022-01-05 02:33:57,445 Val Step[0300/1563], Loss: 1.3226 (1.1293), Acc@1: 0.6875 (0.7473), Acc@5: 0.8438 (0.9228)
2022-01-05 02:33:58,885 Val Step[0350/1563], Loss: 1.1217 (1.1383), Acc@1: 0.8125 (0.7423), Acc@5: 0.9375 (0.9238)
2022-01-05 02:34:00,320 Val Step[0400/1563], Loss: 1.0444 (1.1404), Acc@1: 0.7812 (0.7374), Acc@5: 0.9688 (0.9253)
2022-01-05 02:34:01,782 Val Step[0450/1563], Loss: 0.8102 (1.1458), Acc@1: 0.8125 (0.7342), Acc@5: 1.0000 (0.9266)
2022-01-05 02:34:03,286 Val Step[0500/1563], Loss: 0.4993 (1.1360), Acc@1: 0.8438 (0.7368), Acc@5: 1.0000 (0.9283)
2022-01-05 02:34:04,799 Val Step[0550/1563], Loss: 0.9357 (1.1158), Acc@1: 0.7500 (0.7426), Acc@5: 1.0000 (0.9300)
2022-01-05 02:34:06,266 Val Step[0600/1563], Loss: 0.6663 (1.1218), Acc@1: 0.8438 (0.7420), Acc@5: 1.0000 (0.9288)
2022-01-05 02:34:07,721 Val Step[0650/1563], Loss: 0.7368 (1.1430), Acc@1: 0.7500 (0.7368), Acc@5: 1.0000 (0.9258)
2022-01-05 02:34:09,166 Val Step[0700/1563], Loss: 0.9317 (1.1748), Acc@1: 0.8125 (0.7293), Acc@5: 0.8750 (0.9218)
2022-01-05 02:34:10,635 Val Step[0750/1563], Loss: 1.5339 (1.2089), Acc@1: 0.7188 (0.7219), Acc@5: 0.8750 (0.9171)
2022-01-05 02:34:12,070 Val Step[0800/1563], Loss: 1.1725 (1.2473), Acc@1: 0.6875 (0.7135), Acc@5: 1.0000 (0.9119)
2022-01-05 02:34:13,503 Val Step[0850/1563], Loss: 1.5630 (1.2756), Acc@1: 0.6250 (0.7068), Acc@5: 0.9375 (0.9083)
2022-01-05 02:34:14,987 Val Step[0900/1563], Loss: 0.4840 (1.2776), Acc@1: 0.9062 (0.7076), Acc@5: 0.9688 (0.9076)
2022-01-05 02:34:16,496 Val Step[0950/1563], Loss: 1.4376 (1.3008), Acc@1: 0.6875 (0.7032), Acc@5: 0.8750 (0.9044)
2022-01-05 02:34:17,923 Val Step[1000/1563], Loss: 0.6285 (1.3273), Acc@1: 0.9062 (0.6973), Acc@5: 1.0000 (0.9003)
2022-01-05 02:34:19,401 Val Step[1050/1563], Loss: 0.3886 (1.3439), Acc@1: 0.9688 (0.6938), Acc@5: 0.9688 (0.8984)
2022-01-05 02:34:20,861 Val Step[1100/1563], Loss: 1.3091 (1.3610), Acc@1: 0.6562 (0.6908), Acc@5: 0.9375 (0.8959)
2022-01-05 02:34:22,276 Val Step[1150/1563], Loss: 1.2702 (1.3804), Acc@1: 0.7188 (0.6875), Acc@5: 0.8438 (0.8929)
2022-01-05 02:34:23,609 Val Step[1200/1563], Loss: 1.3900 (1.3985), Acc@1: 0.7500 (0.6838), Acc@5: 0.8438 (0.8900)
2022-01-05 02:34:24,984 Val Step[1250/1563], Loss: 0.7311 (1.4124), Acc@1: 0.8750 (0.6819), Acc@5: 0.9062 (0.8876)
2022-01-05 02:34:26,434 Val Step[1300/1563], Loss: 1.2532 (1.4248), Acc@1: 0.7500 (0.6794), Acc@5: 0.9062 (0.8861)
2022-01-05 02:34:27,851 Val Step[1350/1563], Loss: 2.1370 (1.4443), Acc@1: 0.3125 (0.6751), Acc@5: 0.8125 (0.8828)
2022-01-05 02:34:29,289 Val Step[1400/1563], Loss: 1.1955 (1.4537), Acc@1: 0.6875 (0.6729), Acc@5: 0.9375 (0.8814)
2022-01-05 02:34:30,652 Val Step[1450/1563], Loss: 1.3713 (1.4595), Acc@1: 0.6875 (0.6716), Acc@5: 0.9062 (0.8806)
2022-01-05 02:34:32,118 Val Step[1500/1563], Loss: 1.9182 (1.4462), Acc@1: 0.5000 (0.6745), Acc@5: 0.8438 (0.8826)
2022-01-05 02:34:33,635 Val Step[1550/1563], Loss: 0.9986 (1.4446), Acc@1: 0.8750 (0.6746), Acc@5: 0.9062 (0.8827)
2022-01-05 02:34:34,488 ----- Epoch[168/310], Validation Loss: 1.4425, Validation Acc@1: 0.6751, Validation Acc@5: 0.8829, time: 101.56
2022-01-05 02:34:34,489 ----- Epoch[168/310], Train Loss: 3.9323, Train Acc: 0.3272, time: 1499.06, Best Val(epoch168) Acc@1: 0.6751
2022-01-05 02:34:34,669 Max accuracy so far: 0.6751 at epoch_168
2022-01-05 02:34:34,669 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 02:34:34,669 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 02:34:34,778 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 02:34:35,167 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 02:34:35,167 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 02:34:35,294 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 02:34:35,294 Now training epoch 169. LR=0.000404
2022-01-05 02:35:51,229 Epoch[169/310], Step[0000/1251], Loss: 3.6028(3.6028), Acc: 0.4287(0.4287)
2022-01-05 02:36:49,106 Epoch[169/310], Step[0050/1251], Loss: 4.4209(3.9474), Acc: 0.2969(0.3423)
2022-01-05 02:37:45,647 Epoch[169/310], Step[0100/1251], Loss: 4.0531(3.9220), Acc: 0.3047(0.3497)
2022-01-05 02:38:41,585 Epoch[169/310], Step[0150/1251], Loss: 3.3077(3.9226), Acc: 0.5625(0.3437)
2022-01-05 02:39:38,811 Epoch[169/310], Step[0200/1251], Loss: 4.1086(3.9247), Acc: 0.4209(0.3408)
2022-01-05 02:40:36,411 Epoch[169/310], Step[0250/1251], Loss: 4.3883(3.9186), Acc: 0.3184(0.3390)
2022-01-05 02:41:34,342 Epoch[169/310], Step[0300/1251], Loss: 4.7151(3.9205), Acc: 0.1797(0.3386)
2022-01-05 02:42:31,775 Epoch[169/310], Step[0350/1251], Loss: 3.8493(3.9239), Acc: 0.3359(0.3372)
2022-01-05 02:43:28,656 Epoch[169/310], Step[0400/1251], Loss: 4.2445(3.9169), Acc: 0.3369(0.3363)
2022-01-05 02:44:26,223 Epoch[169/310], Step[0450/1251], Loss: 4.1248(3.9152), Acc: 0.4434(0.3350)
2022-01-05 02:45:24,194 Epoch[169/310], Step[0500/1251], Loss: 4.3514(3.9187), Acc: 0.2754(0.3329)
2022-01-05 02:46:21,440 Epoch[169/310], Step[0550/1251], Loss: 3.5025(3.9214), Acc: 0.5225(0.3341)
2022-01-05 02:47:19,463 Epoch[169/310], Step[0600/1251], Loss: 4.0494(3.9246), Acc: 0.3975(0.3322)
2022-01-05 02:48:17,080 Epoch[169/310], Step[0650/1251], Loss: 4.0472(3.9242), Acc: 0.0811(0.3307)
2022-01-05 02:49:14,114 Epoch[169/310], Step[0700/1251], Loss: 3.6870(3.9261), Acc: 0.4131(0.3302)
2022-01-05 02:50:11,627 Epoch[169/310], Step[0750/1251], Loss: 3.8058(3.9275), Acc: 0.1650(0.3307)
2022-01-05 02:51:09,608 Epoch[169/310], Step[0800/1251], Loss: 4.1819(3.9326), Acc: 0.2549(0.3291)
2022-01-05 02:52:07,580 Epoch[169/310], Step[0850/1251], Loss: 4.2001(3.9334), Acc: 0.1436(0.3281)
2022-01-05 02:53:05,237 Epoch[169/310], Step[0900/1251], Loss: 4.1306(3.9335), Acc: 0.3105(0.3283)
2022-01-05 02:54:03,309 Epoch[169/310], Step[0950/1251], Loss: 4.1569(3.9323), Acc: 0.3154(0.3296)
2022-01-05 02:54:59,182 Epoch[169/310], Step[1000/1251], Loss: 3.8446(3.9320), Acc: 0.1895(0.3296)
2022-01-05 02:55:56,786 Epoch[169/310], Step[1050/1251], Loss: 3.5208(3.9334), Acc: 0.3857(0.3300)
2022-01-05 02:56:53,843 Epoch[169/310], Step[1100/1251], Loss: 4.0666(3.9340), Acc: 0.4775(0.3298)
2022-01-05 02:57:52,079 Epoch[169/310], Step[1150/1251], Loss: 3.3426(3.9313), Acc: 0.4395(0.3310)
2022-01-05 02:58:49,066 Epoch[169/310], Step[1200/1251], Loss: 4.1468(3.9292), Acc: 0.3652(0.3310)
2022-01-05 02:59:45,092 Epoch[169/310], Step[1250/1251], Loss: 3.9778(3.9286), Acc: 0.3691(0.3318)
2022-01-05 02:59:47,121 ----- Epoch[169/310], Train Loss: 3.9286, Train Acc: 0.3318, time: 1511.82, Best Val(epoch168) Acc@1: 0.6751
2022-01-05 02:59:47,289 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 02:59:47,289 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 02:59:47,407 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 02:59:47,407 Now training epoch 170. LR=0.000399
2022-01-05 03:01:03,943 Epoch[170/310], Step[0000/1251], Loss: 4.0943(4.0943), Acc: 0.1660(0.1660)
2022-01-05 03:02:01,045 Epoch[170/310], Step[0050/1251], Loss: 4.3453(3.9113), Acc: 0.1533(0.3221)
2022-01-05 03:02:57,650 Epoch[170/310], Step[0100/1251], Loss: 4.2337(3.9530), Acc: 0.2705(0.3256)
2022-01-05 03:03:53,378 Epoch[170/310], Step[0150/1251], Loss: 4.1438(3.9229), Acc: 0.2920(0.3235)
2022-01-05 03:04:50,295 Epoch[170/310], Step[0200/1251], Loss: 4.1354(3.9260), Acc: 0.2578(0.3226)
2022-01-05 03:05:48,020 Epoch[170/310], Step[0250/1251], Loss: 3.9228(3.9181), Acc: 0.3027(0.3290)
2022-01-05 03:06:45,734 Epoch[170/310], Step[0300/1251], Loss: 4.3546(3.9227), Acc: 0.3857(0.3296)
2022-01-05 03:07:41,949 Epoch[170/310], Step[0350/1251], Loss: 3.7137(3.9115), Acc: 0.3662(0.3292)
2022-01-05 03:08:38,505 Epoch[170/310], Step[0400/1251], Loss: 3.4148(3.9050), Acc: 0.5381(0.3301)
2022-01-05 03:09:36,659 Epoch[170/310], Step[0450/1251], Loss: 3.5121(3.9027), Acc: 0.2764(0.3299)
2022-01-05 03:10:35,121 Epoch[170/310], Step[0500/1251], Loss: 3.4664(3.9037), Acc: 0.3760(0.3302)
2022-01-05 03:11:31,187 Epoch[170/310], Step[0550/1251], Loss: 4.0192(3.9038), Acc: 0.3311(0.3322)
2022-01-05 03:12:27,916 Epoch[170/310], Step[0600/1251], Loss: 4.1567(3.9062), Acc: 0.2363(0.3329)
2022-01-05 03:13:24,991 Epoch[170/310], Step[0650/1251], Loss: 4.0372(3.9103), Acc: 0.2344(0.3337)
2022-01-05 03:14:22,169 Epoch[170/310], Step[0700/1251], Loss: 3.9349(3.9086), Acc: 0.4756(0.3316)
2022-01-05 03:15:20,466 Epoch[170/310], Step[0750/1251], Loss: 4.1692(3.9097), Acc: 0.2588(0.3309)
2022-01-05 03:16:16,739 Epoch[170/310], Step[0800/1251], Loss: 4.0567(3.9111), Acc: 0.2871(0.3318)
2022-01-05 03:17:13,994 Epoch[170/310], Step[0850/1251], Loss: 3.9362(3.9098), Acc: 0.3613(0.3325)
2022-01-05 03:18:11,791 Epoch[170/310], Step[0900/1251], Loss: 3.8883(3.9088), Acc: 0.2783(0.3321)
2022-01-05 03:19:09,375 Epoch[170/310], Step[0950/1251], Loss: 3.7746(3.9131), Acc: 0.2930(0.3315)
2022-01-05 03:20:06,870 Epoch[170/310], Step[1000/1251], Loss: 4.0130(3.9152), Acc: 0.4424(0.3320)
2022-01-05 03:21:04,522 Epoch[170/310], Step[1050/1251], Loss: 4.1593(3.9164), Acc: 0.2490(0.3320)
2022-01-05 03:22:01,731 Epoch[170/310], Step[1100/1251], Loss: 3.6417(3.9173), Acc: 0.3584(0.3320)
2022-01-05 03:22:57,299 Epoch[170/310], Step[1150/1251], Loss: 4.0342(3.9147), Acc: 0.3887(0.3319)
2022-01-05 03:23:54,530 Epoch[170/310], Step[1200/1251], Loss: 3.7608(3.9161), Acc: 0.4824(0.3319)
2022-01-05 03:24:51,613 Epoch[170/310], Step[1250/1251], Loss: 3.6503(3.9139), Acc: 0.3564(0.3329)
2022-01-05 03:24:53,452 ----- Validation after Epoch: 170
2022-01-05 03:25:48,131 Val Step[0000/1563], Loss: 0.7845 (0.7845), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 03:25:49,704 Val Step[0050/1563], Loss: 2.2761 (0.8986), Acc@1: 0.4062 (0.8137), Acc@5: 0.8125 (0.9553)
2022-01-05 03:25:51,168 Val Step[0100/1563], Loss: 2.2051 (1.2045), Acc@1: 0.3438 (0.7330), Acc@5: 0.8125 (0.9189)
2022-01-05 03:25:52,690 Val Step[0150/1563], Loss: 0.6841 (1.1256), Acc@1: 0.8438 (0.7519), Acc@5: 0.9688 (0.9253)
2022-01-05 03:25:54,182 Val Step[0200/1563], Loss: 1.1878 (1.1298), Acc@1: 0.7500 (0.7550), Acc@5: 0.9375 (0.9229)
2022-01-05 03:25:55,622 Val Step[0250/1563], Loss: 0.7688 (1.0725), Acc@1: 0.8750 (0.7693), Acc@5: 0.9688 (0.9289)
2022-01-05 03:25:57,105 Val Step[0300/1563], Loss: 1.2002 (1.1440), Acc@1: 0.7500 (0.7483), Acc@5: 0.9688 (0.9237)
2022-01-05 03:25:58,774 Val Step[0350/1563], Loss: 1.1224 (1.1478), Acc@1: 0.7500 (0.7464), Acc@5: 0.8750 (0.9251)
2022-01-05 03:26:00,149 Val Step[0400/1563], Loss: 1.3104 (1.1641), Acc@1: 0.6875 (0.7375), Acc@5: 0.9375 (0.9254)
2022-01-05 03:26:01,665 Val Step[0450/1563], Loss: 1.0814 (1.1668), Acc@1: 0.5938 (0.7347), Acc@5: 1.0000 (0.9267)
2022-01-05 03:26:03,221 Val Step[0500/1563], Loss: 0.5983 (1.1540), Acc@1: 0.9062 (0.7391), Acc@5: 1.0000 (0.9286)
2022-01-05 03:26:04,647 Val Step[0550/1563], Loss: 0.8925 (1.1285), Acc@1: 0.7812 (0.7457), Acc@5: 0.9688 (0.9310)
2022-01-05 03:26:06,211 Val Step[0600/1563], Loss: 0.8885 (1.1328), Acc@1: 0.7812 (0.7460), Acc@5: 0.9688 (0.9309)
2022-01-05 03:26:07,699 Val Step[0650/1563], Loss: 0.7162 (1.1525), Acc@1: 0.8438 (0.7417), Acc@5: 1.0000 (0.9279)
2022-01-05 03:26:09,209 Val Step[0700/1563], Loss: 1.1190 (1.1859), Acc@1: 0.7500 (0.7336), Acc@5: 0.9062 (0.9231)
2022-01-05 03:26:10,803 Val Step[0750/1563], Loss: 1.9372 (1.2222), Acc@1: 0.5312 (0.7259), Acc@5: 0.8125 (0.9176)
2022-01-05 03:26:12,249 Val Step[0800/1563], Loss: 1.0503 (1.2612), Acc@1: 0.8125 (0.7166), Acc@5: 0.9688 (0.9125)
2022-01-05 03:26:13,731 Val Step[0850/1563], Loss: 1.4347 (1.2862), Acc@1: 0.7188 (0.7103), Acc@5: 0.9375 (0.9094)
2022-01-05 03:26:15,236 Val Step[0900/1563], Loss: 0.3146 (1.2912), Acc@1: 0.9375 (0.7105), Acc@5: 1.0000 (0.9083)
2022-01-05 03:26:16,768 Val Step[0950/1563], Loss: 1.6951 (1.3135), Acc@1: 0.6875 (0.7065), Acc@5: 0.8438 (0.9051)
2022-01-05 03:26:18,268 Val Step[1000/1563], Loss: 0.6904 (1.3382), Acc@1: 0.9062 (0.7004), Acc@5: 0.9688 (0.9014)
2022-01-05 03:26:19,776 Val Step[1050/1563], Loss: 0.3907 (1.3535), Acc@1: 0.9688 (0.6969), Acc@5: 0.9688 (0.8996)
2022-01-05 03:26:21,268 Val Step[1100/1563], Loss: 1.1249 (1.3683), Acc@1: 0.7500 (0.6943), Acc@5: 0.9062 (0.8971)
2022-01-05 03:26:22,806 Val Step[1150/1563], Loss: 1.5000 (1.3855), Acc@1: 0.6875 (0.6914), Acc@5: 0.8125 (0.8946)
2022-01-05 03:26:24,294 Val Step[1200/1563], Loss: 1.3872 (1.4031), Acc@1: 0.7500 (0.6874), Acc@5: 0.8438 (0.8918)
2022-01-05 03:26:25,815 Val Step[1250/1563], Loss: 0.8434 (1.4165), Acc@1: 0.8438 (0.6854), Acc@5: 0.9375 (0.8898)
2022-01-05 03:26:27,322 Val Step[1300/1563], Loss: 1.1078 (1.4260), Acc@1: 0.8125 (0.6837), Acc@5: 0.9062 (0.8886)
2022-01-05 03:26:28,783 Val Step[1350/1563], Loss: 1.8108 (1.4451), Acc@1: 0.5000 (0.6797), Acc@5: 0.8750 (0.8856)
2022-01-05 03:26:30,261 Val Step[1400/1563], Loss: 1.2104 (1.4510), Acc@1: 0.7500 (0.6781), Acc@5: 0.9375 (0.8848)
2022-01-05 03:26:31,677 Val Step[1450/1563], Loss: 1.3180 (1.4570), Acc@1: 0.7188 (0.6768), Acc@5: 0.9375 (0.8844)
2022-01-05 03:26:33,086 Val Step[1500/1563], Loss: 1.5543 (1.4458), Acc@1: 0.7500 (0.6792), Acc@5: 0.8750 (0.8856)
2022-01-05 03:26:34,578 Val Step[1550/1563], Loss: 1.1330 (1.4451), Acc@1: 0.8750 (0.6789), Acc@5: 0.9062 (0.8857)
2022-01-05 03:26:35,441 ----- Epoch[170/310], Validation Loss: 1.4430, Validation Acc@1: 0.6794, Validation Acc@5: 0.8859, time: 101.99
2022-01-05 03:26:35,441 ----- Epoch[170/310], Train Loss: 3.9139, Train Acc: 0.3329, time: 1506.04, Best Val(epoch170) Acc@1: 0.6794
2022-01-05 03:26:35,623 Max accuracy so far: 0.6794 at epoch_170
2022-01-05 03:26:35,624 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 03:26:35,624 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 03:26:35,730 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 03:26:35,864 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-170-Loss-3.9209300800860167.pdparams
2022-01-05 03:26:35,864 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-170-Loss-3.9209300800860167.pdopt
2022-01-05 03:26:35,903 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-170-Loss-3.9209300800860167-EMA.pdparams
2022-01-05 03:26:35,903 Now training epoch 171. LR=0.000394
2022-01-05 03:27:51,992 Epoch[171/310], Step[0000/1251], Loss: 4.2087(4.2087), Acc: 0.2588(0.2588)
2022-01-05 03:28:48,344 Epoch[171/310], Step[0050/1251], Loss: 3.4358(3.8697), Acc: 0.5400(0.3349)
2022-01-05 03:29:43,527 Epoch[171/310], Step[0100/1251], Loss: 4.4251(3.9101), Acc: 0.1992(0.3380)
2022-01-05 03:30:39,529 Epoch[171/310], Step[0150/1251], Loss: 4.1227(3.9378), Acc: 0.2744(0.3290)
2022-01-05 03:31:35,744 Epoch[171/310], Step[0200/1251], Loss: 4.2322(3.9410), Acc: 0.3506(0.3295)
2022-01-05 03:32:31,178 Epoch[171/310], Step[0250/1251], Loss: 4.0638(3.9414), Acc: 0.3389(0.3282)
2022-01-05 03:33:27,152 Epoch[171/310], Step[0300/1251], Loss: 3.8616(3.9273), Acc: 0.3037(0.3290)
2022-01-05 03:34:22,307 Epoch[171/310], Step[0350/1251], Loss: 3.6770(3.9260), Acc: 0.2715(0.3254)
2022-01-05 03:35:20,716 Epoch[171/310], Step[0400/1251], Loss: 3.8468(3.9287), Acc: 0.2178(0.3243)
2022-01-05 03:36:18,273 Epoch[171/310], Step[0450/1251], Loss: 3.5564(3.9236), Acc: 0.3477(0.3236)
2022-01-05 03:37:15,227 Epoch[171/310], Step[0500/1251], Loss: 3.8277(3.9211), Acc: 0.4121(0.3235)
2022-01-05 03:38:12,988 Epoch[171/310], Step[0550/1251], Loss: 3.8272(3.9196), Acc: 0.3223(0.3228)
2022-01-05 03:39:10,005 Epoch[171/310], Step[0600/1251], Loss: 3.3488(3.9166), Acc: 0.3564(0.3241)
2022-01-05 03:40:05,996 Epoch[171/310], Step[0650/1251], Loss: 4.0451(3.9104), Acc: 0.4004(0.3255)
2022-01-05 03:41:03,143 Epoch[171/310], Step[0700/1251], Loss: 3.6632(3.9112), Acc: 0.5303(0.3260)
2022-01-05 03:41:59,992 Epoch[171/310], Step[0750/1251], Loss: 3.4990(3.9068), Acc: 0.4600(0.3258)
2022-01-05 03:42:56,718 Epoch[171/310], Step[0800/1251], Loss: 4.0919(3.9093), Acc: 0.4570(0.3251)
2022-01-05 03:43:53,351 Epoch[171/310], Step[0850/1251], Loss: 3.8521(3.9105), Acc: 0.2666(0.3263)
2022-01-05 03:44:51,055 Epoch[171/310], Step[0900/1251], Loss: 3.8615(3.9064), Acc: 0.3301(0.3270)
2022-01-05 03:45:47,493 Epoch[171/310], Step[0950/1251], Loss: 3.6436(3.9079), Acc: 0.4971(0.3271)
2022-01-05 03:46:45,278 Epoch[171/310], Step[1000/1251], Loss: 4.0363(3.9052), Acc: 0.2480(0.3281)
2022-01-05 03:47:42,985 Epoch[171/310], Step[1050/1251], Loss: 3.5312(3.9067), Acc: 0.2139(0.3283)
2022-01-05 03:48:39,701 Epoch[171/310], Step[1100/1251], Loss: 4.3475(3.9066), Acc: 0.2783(0.3281)
2022-01-05 03:49:36,719 Epoch[171/310], Step[1150/1251], Loss: 4.1785(3.9052), Acc: 0.2627(0.3279)
2022-01-05 03:50:33,228 Epoch[171/310], Step[1200/1251], Loss: 4.2855(3.9072), Acc: 0.2422(0.3280)
2022-01-05 03:51:30,515 Epoch[171/310], Step[1250/1251], Loss: 3.8880(3.9055), Acc: 0.4238(0.3286)
2022-01-05 03:51:32,385 ----- Epoch[171/310], Train Loss: 3.9055, Train Acc: 0.3286, time: 1496.48, Best Val(epoch170) Acc@1: 0.6794
2022-01-05 03:51:32,552 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 03:51:32,552 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 03:51:32,664 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 03:51:32,664 Now training epoch 172. LR=0.000389
2022-01-05 03:52:46,689 Epoch[172/310], Step[0000/1251], Loss: 3.6049(3.6049), Acc: 0.3340(0.3340)
2022-01-05 03:53:44,282 Epoch[172/310], Step[0050/1251], Loss: 3.3689(3.8858), Acc: 0.3945(0.3220)
2022-01-05 03:54:40,944 Epoch[172/310], Step[0100/1251], Loss: 4.6114(3.8909), Acc: 0.1973(0.3262)
2022-01-05 03:55:38,350 Epoch[172/310], Step[0150/1251], Loss: 3.7812(3.9031), Acc: 0.2715(0.3245)
2022-01-05 03:56:35,864 Epoch[172/310], Step[0200/1251], Loss: 3.8018(3.8839), Acc: 0.2559(0.3259)
2022-01-05 03:57:33,650 Epoch[172/310], Step[0250/1251], Loss: 4.1551(3.8955), Acc: 0.2129(0.3290)
2022-01-05 03:58:30,303 Epoch[172/310], Step[0300/1251], Loss: 4.0443(3.8982), Acc: 0.4443(0.3306)
2022-01-05 03:59:27,051 Epoch[172/310], Step[0350/1251], Loss: 3.5954(3.8963), Acc: 0.1113(0.3306)
2022-01-05 04:00:23,633 Epoch[172/310], Step[0400/1251], Loss: 3.9633(3.9003), Acc: 0.2129(0.3322)
2022-01-05 04:01:21,230 Epoch[172/310], Step[0450/1251], Loss: 3.2540(3.8993), Acc: 0.4092(0.3314)
2022-01-05 04:02:18,287 Epoch[172/310], Step[0500/1251], Loss: 3.6832(3.9030), Acc: 0.3828(0.3321)
2022-01-05 04:03:15,782 Epoch[172/310], Step[0550/1251], Loss: 3.3666(3.9022), Acc: 0.4336(0.3312)
2022-01-05 04:04:11,320 Epoch[172/310], Step[0600/1251], Loss: 3.8835(3.9017), Acc: 0.3867(0.3294)
2022-01-05 04:05:07,862 Epoch[172/310], Step[0650/1251], Loss: 3.9561(3.9020), Acc: 0.2764(0.3289)
2022-01-05 04:06:04,688 Epoch[172/310], Step[0700/1251], Loss: 4.2235(3.9052), Acc: 0.4609(0.3295)
2022-01-05 04:07:01,299 Epoch[172/310], Step[0750/1251], Loss: 4.1037(3.9069), Acc: 0.2861(0.3285)
2022-01-05 04:07:58,893 Epoch[172/310], Step[0800/1251], Loss: 3.8452(3.9079), Acc: 0.3643(0.3271)
2022-01-05 04:08:56,747 Epoch[172/310], Step[0850/1251], Loss: 3.9599(3.9094), Acc: 0.2520(0.3275)
2022-01-05 04:09:53,826 Epoch[172/310], Step[0900/1251], Loss: 4.0902(3.9114), Acc: 0.2676(0.3276)
2022-01-05 04:10:50,262 Epoch[172/310], Step[0950/1251], Loss: 3.5735(3.9068), Acc: 0.4863(0.3289)
2022-01-05 04:11:47,211 Epoch[172/310], Step[1000/1251], Loss: 4.4287(3.9083), Acc: 0.2529(0.3290)
2022-01-05 04:12:45,396 Epoch[172/310], Step[1050/1251], Loss: 3.7564(3.9063), Acc: 0.4775(0.3299)
2022-01-05 04:13:43,401 Epoch[172/310], Step[1100/1251], Loss: 4.3725(3.9049), Acc: 0.3564(0.3295)
2022-01-05 04:14:40,670 Epoch[172/310], Step[1150/1251], Loss: 3.6528(3.9073), Acc: 0.3818(0.3290)
2022-01-05 04:15:37,069 Epoch[172/310], Step[1200/1251], Loss: 4.2984(3.9097), Acc: 0.3203(0.3291)
2022-01-05 04:16:34,570 Epoch[172/310], Step[1250/1251], Loss: 3.8898(3.9068), Acc: 0.4102(0.3301)
2022-01-05 04:16:36,422 ----- Validation after Epoch: 172
2022-01-05 04:17:31,093 Val Step[0000/1563], Loss: 0.8830 (0.8830), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 04:17:32,583 Val Step[0050/1563], Loss: 2.1651 (0.8835), Acc@1: 0.5000 (0.8192), Acc@5: 0.8750 (0.9504)
2022-01-05 04:17:34,068 Val Step[0100/1563], Loss: 2.0038 (1.1974), Acc@1: 0.5312 (0.7333), Acc@5: 0.8125 (0.9186)
2022-01-05 04:17:35,557 Val Step[0150/1563], Loss: 0.7205 (1.1439), Acc@1: 0.8750 (0.7498), Acc@5: 1.0000 (0.9226)
2022-01-05 04:17:36,925 Val Step[0200/1563], Loss: 1.1115 (1.1572), Acc@1: 0.6875 (0.7508), Acc@5: 0.9688 (0.9204)
2022-01-05 04:17:38,260 Val Step[0250/1563], Loss: 0.7669 (1.0960), Acc@1: 0.8438 (0.7657), Acc@5: 1.0000 (0.9287)
2022-01-05 04:17:39,707 Val Step[0300/1563], Loss: 1.2689 (1.1593), Acc@1: 0.7188 (0.7456), Acc@5: 0.9062 (0.9241)
2022-01-05 04:17:41,085 Val Step[0350/1563], Loss: 1.2997 (1.1719), Acc@1: 0.7500 (0.7412), Acc@5: 0.9062 (0.9264)
2022-01-05 04:17:42,469 Val Step[0400/1563], Loss: 1.1344 (1.1744), Acc@1: 0.8125 (0.7372), Acc@5: 0.9688 (0.9277)
2022-01-05 04:17:44,002 Val Step[0450/1563], Loss: 1.3076 (1.1771), Acc@1: 0.5938 (0.7357), Acc@5: 0.9688 (0.9285)
2022-01-05 04:17:45,488 Val Step[0500/1563], Loss: 0.7303 (1.1693), Acc@1: 0.7812 (0.7381), Acc@5: 1.0000 (0.9301)
2022-01-05 04:17:46,917 Val Step[0550/1563], Loss: 1.0007 (1.1429), Acc@1: 0.6875 (0.7455), Acc@5: 0.9688 (0.9320)
2022-01-05 04:17:48,415 Val Step[0600/1563], Loss: 0.8570 (1.1466), Acc@1: 0.8125 (0.7449), Acc@5: 0.9688 (0.9318)
2022-01-05 04:17:49,875 Val Step[0650/1563], Loss: 0.8618 (1.1702), Acc@1: 0.8125 (0.7401), Acc@5: 1.0000 (0.9283)
2022-01-05 04:17:51,260 Val Step[0700/1563], Loss: 1.2449 (1.2041), Acc@1: 0.7188 (0.7327), Acc@5: 0.9062 (0.9235)
2022-01-05 04:17:52,700 Val Step[0750/1563], Loss: 1.7424 (1.2404), Acc@1: 0.6562 (0.7261), Acc@5: 0.7812 (0.9180)
2022-01-05 04:17:54,057 Val Step[0800/1563], Loss: 1.1810 (1.2788), Acc@1: 0.7188 (0.7164), Acc@5: 1.0000 (0.9130)
2022-01-05 04:17:55,440 Val Step[0850/1563], Loss: 1.3990 (1.3075), Acc@1: 0.6562 (0.7099), Acc@5: 0.9375 (0.9089)
2022-01-05 04:17:56,870 Val Step[0900/1563], Loss: 0.2991 (1.3087), Acc@1: 0.9688 (0.7113), Acc@5: 1.0000 (0.9078)
2022-01-05 04:17:58,358 Val Step[0950/1563], Loss: 2.0329 (1.3358), Acc@1: 0.5938 (0.7061), Acc@5: 0.8125 (0.9039)
2022-01-05 04:17:59,749 Val Step[1000/1563], Loss: 0.7762 (1.3601), Acc@1: 0.9062 (0.7008), Acc@5: 0.9688 (0.9005)
2022-01-05 04:18:01,184 Val Step[1050/1563], Loss: 0.4529 (1.3752), Acc@1: 0.9688 (0.6975), Acc@5: 0.9688 (0.8988)
2022-01-05 04:18:02,697 Val Step[1100/1563], Loss: 1.6703 (1.3922), Acc@1: 0.6875 (0.6941), Acc@5: 0.8125 (0.8963)
2022-01-05 04:18:04,182 Val Step[1150/1563], Loss: 1.3378 (1.4110), Acc@1: 0.7812 (0.6905), Acc@5: 0.8125 (0.8932)
2022-01-05 04:18:05,614 Val Step[1200/1563], Loss: 1.3900 (1.4270), Acc@1: 0.7812 (0.6876), Acc@5: 0.8438 (0.8905)
2022-01-05 04:18:06,943 Val Step[1250/1563], Loss: 0.8495 (1.4431), Acc@1: 0.9062 (0.6853), Acc@5: 0.9062 (0.8877)
2022-01-05 04:18:08,355 Val Step[1300/1563], Loss: 1.1238 (1.4551), Acc@1: 0.8438 (0.6834), Acc@5: 0.9062 (0.8863)
2022-01-05 04:18:09,816 Val Step[1350/1563], Loss: 2.1146 (1.4734), Acc@1: 0.4375 (0.6794), Acc@5: 0.7812 (0.8832)
2022-01-05 04:18:11,239 Val Step[1400/1563], Loss: 1.2625 (1.4814), Acc@1: 0.7812 (0.6782), Acc@5: 0.9375 (0.8820)
2022-01-05 04:18:12,697 Val Step[1450/1563], Loss: 1.7037 (1.4886), Acc@1: 0.6875 (0.6766), Acc@5: 0.9062 (0.8815)
2022-01-05 04:18:14,197 Val Step[1500/1563], Loss: 2.0702 (1.4782), Acc@1: 0.4688 (0.6788), Acc@5: 0.8438 (0.8829)
2022-01-05 04:18:15,655 Val Step[1550/1563], Loss: 1.0203 (1.4777), Acc@1: 0.8750 (0.6785), Acc@5: 0.9062 (0.8828)
2022-01-05 04:18:16,527 ----- Epoch[172/310], Validation Loss: 1.4758, Validation Acc@1: 0.6789, Validation Acc@5: 0.8830, time: 100.10
2022-01-05 04:18:16,527 ----- Epoch[172/310], Train Loss: 3.9068, Train Acc: 0.3301, time: 1503.75, Best Val(epoch170) Acc@1: 0.6794
2022-01-05 04:18:16,709 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 04:18:16,710 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 04:18:16,868 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 04:18:16,869 Now training epoch 173. LR=0.000384
2022-01-05 04:19:25,670 Epoch[173/310], Step[0000/1251], Loss: 3.9538(3.9538), Acc: 0.3994(0.3994)
2022-01-05 04:20:22,642 Epoch[173/310], Step[0050/1251], Loss: 3.8345(3.8467), Acc: 0.2646(0.3268)
2022-01-05 04:21:19,607 Epoch[173/310], Step[0100/1251], Loss: 3.6588(3.8900), Acc: 0.3916(0.3264)
2022-01-05 04:22:16,681 Epoch[173/310], Step[0150/1251], Loss: 3.4541(3.9037), Acc: 0.2227(0.3338)
2022-01-05 04:23:13,925 Epoch[173/310], Step[0200/1251], Loss: 3.4193(3.9001), Acc: 0.3916(0.3397)
2022-01-05 04:24:10,036 Epoch[173/310], Step[0250/1251], Loss: 3.6260(3.8944), Acc: 0.3467(0.3356)
2022-01-05 04:25:07,171 Epoch[173/310], Step[0300/1251], Loss: 3.4917(3.8923), Acc: 0.3945(0.3373)
2022-01-05 04:26:03,810 Epoch[173/310], Step[0350/1251], Loss: 3.6818(3.8947), Acc: 0.3975(0.3384)
2022-01-05 04:27:00,366 Epoch[173/310], Step[0400/1251], Loss: 3.6848(3.8833), Acc: 0.3857(0.3409)
2022-01-05 04:27:57,483 Epoch[173/310], Step[0450/1251], Loss: 3.7845(3.8803), Acc: 0.3320(0.3390)
2022-01-05 04:28:54,045 Epoch[173/310], Step[0500/1251], Loss: 3.8433(3.8710), Acc: 0.3945(0.3392)
2022-01-05 04:29:50,293 Epoch[173/310], Step[0550/1251], Loss: 3.4584(3.8677), Acc: 0.4531(0.3402)
2022-01-05 04:30:48,685 Epoch[173/310], Step[0600/1251], Loss: 3.5347(3.8766), Acc: 0.5000(0.3387)
2022-01-05 04:31:46,347 Epoch[173/310], Step[0650/1251], Loss: 3.6773(3.8782), Acc: 0.3164(0.3392)
2022-01-05 04:32:42,604 Epoch[173/310], Step[0700/1251], Loss: 4.0289(3.8872), Acc: 0.3555(0.3388)
2022-01-05 04:33:39,037 Epoch[173/310], Step[0750/1251], Loss: 3.7586(3.8970), Acc: 0.5098(0.3389)
2022-01-05 04:34:35,961 Epoch[173/310], Step[0800/1251], Loss: 4.0473(3.8985), Acc: 0.2373(0.3389)
2022-01-05 04:35:32,421 Epoch[173/310], Step[0850/1251], Loss: 4.1401(3.8990), Acc: 0.4004(0.3382)
2022-01-05 04:36:28,375 Epoch[173/310], Step[0900/1251], Loss: 3.9745(3.8984), Acc: 0.2539(0.3394)
2022-01-05 04:37:24,283 Epoch[173/310], Step[0950/1251], Loss: 3.9901(3.9011), Acc: 0.4639(0.3405)
2022-01-05 04:38:20,553 Epoch[173/310], Step[1000/1251], Loss: 3.7607(3.9009), Acc: 0.5127(0.3415)
2022-01-05 04:39:17,359 Epoch[173/310], Step[1050/1251], Loss: 3.9906(3.8991), Acc: 0.2744(0.3401)
2022-01-05 04:40:13,632 Epoch[173/310], Step[1100/1251], Loss: 4.1499(3.9000), Acc: 0.3438(0.3396)
2022-01-05 04:41:09,721 Epoch[173/310], Step[1150/1251], Loss: 3.7419(3.8976), Acc: 0.5137(0.3398)
2022-01-05 04:42:06,628 Epoch[173/310], Step[1200/1251], Loss: 4.2779(3.8995), Acc: 0.3086(0.3404)
2022-01-05 04:43:03,826 Epoch[173/310], Step[1250/1251], Loss: 4.0395(3.8996), Acc: 0.2988(0.3403)
2022-01-05 04:43:05,797 ----- Epoch[173/310], Train Loss: 3.8996, Train Acc: 0.3403, time: 1488.92, Best Val(epoch170) Acc@1: 0.6794
2022-01-05 04:43:05,987 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 04:43:05,988 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 04:43:06,093 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 04:43:06,093 Now training epoch 174. LR=0.000379
2022-01-05 04:44:19,097 Epoch[174/310], Step[0000/1251], Loss: 2.8562(2.8562), Acc: 0.3125(0.3125)
2022-01-05 04:45:16,104 Epoch[174/310], Step[0050/1251], Loss: 3.6467(3.7787), Acc: 0.2393(0.3344)
2022-01-05 04:46:12,615 Epoch[174/310], Step[0100/1251], Loss: 4.5149(3.8518), Acc: 0.3477(0.3387)
2022-01-05 04:47:08,647 Epoch[174/310], Step[0150/1251], Loss: 3.4777(3.8540), Acc: 0.2930(0.3362)
2022-01-05 04:48:05,717 Epoch[174/310], Step[0200/1251], Loss: 4.1764(3.8783), Acc: 0.4043(0.3334)
2022-01-05 04:49:03,131 Epoch[174/310], Step[0250/1251], Loss: 4.2736(3.8804), Acc: 0.1553(0.3359)
2022-01-05 04:50:00,129 Epoch[174/310], Step[0300/1251], Loss: 3.3304(3.8849), Acc: 0.5625(0.3352)
2022-01-05 04:50:57,848 Epoch[174/310], Step[0350/1251], Loss: 3.7642(3.8873), Acc: 0.3916(0.3338)
2022-01-05 04:51:54,623 Epoch[174/310], Step[0400/1251], Loss: 4.2537(3.9000), Acc: 0.2598(0.3338)
2022-01-05 04:52:52,834 Epoch[174/310], Step[0450/1251], Loss: 4.4128(3.9025), Acc: 0.2314(0.3339)
2022-01-05 04:53:49,707 Epoch[174/310], Step[0500/1251], Loss: 3.5159(3.9046), Acc: 0.3389(0.3363)
2022-01-05 04:54:46,103 Epoch[174/310], Step[0550/1251], Loss: 3.3327(3.9032), Acc: 0.0059(0.3350)
2022-01-05 04:55:44,313 Epoch[174/310], Step[0600/1251], Loss: 3.6231(3.9114), Acc: 0.2461(0.3342)
2022-01-05 04:56:42,019 Epoch[174/310], Step[0650/1251], Loss: 3.5618(3.9089), Acc: 0.4990(0.3338)
2022-01-05 04:57:38,656 Epoch[174/310], Step[0700/1251], Loss: 4.3108(3.9122), Acc: 0.1572(0.3344)
2022-01-05 04:58:35,746 Epoch[174/310], Step[0750/1251], Loss: 3.7064(3.9159), Acc: 0.4902(0.3328)
2022-01-05 04:59:32,033 Epoch[174/310], Step[0800/1251], Loss: 3.6205(3.9147), Acc: 0.5371(0.3341)
2022-01-05 05:00:29,748 Epoch[174/310], Step[0850/1251], Loss: 3.5110(3.9181), Acc: 0.3848(0.3343)
2022-01-05 05:01:26,283 Epoch[174/310], Step[0900/1251], Loss: 4.0877(3.9177), Acc: 0.2979(0.3351)
2022-01-05 05:02:22,511 Epoch[174/310], Step[0950/1251], Loss: 3.8195(3.9151), Acc: 0.3262(0.3360)
2022-01-05 05:03:20,325 Epoch[174/310], Step[1000/1251], Loss: 3.6127(3.9146), Acc: 0.4004(0.3354)
2022-01-05 05:04:15,557 Epoch[174/310], Step[1050/1251], Loss: 4.0378(3.9149), Acc: 0.1816(0.3362)
2022-01-05 05:05:12,272 Epoch[174/310], Step[1100/1251], Loss: 3.9534(3.9101), Acc: 0.4160(0.3371)
2022-01-05 05:06:09,239 Epoch[174/310], Step[1150/1251], Loss: 3.5829(3.9092), Acc: 0.4688(0.3370)
2022-01-05 05:07:07,145 Epoch[174/310], Step[1200/1251], Loss: 4.2144(3.9109), Acc: 0.2842(0.3373)
2022-01-05 05:08:04,381 Epoch[174/310], Step[1250/1251], Loss: 3.6939(3.9084), Acc: 0.5391(0.3377)
2022-01-05 05:08:06,923 ----- Validation after Epoch: 174
2022-01-05 05:09:03,890 Val Step[0000/1563], Loss: 0.9397 (0.9397), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2022-01-05 05:09:05,332 Val Step[0050/1563], Loss: 2.6927 (0.8981), Acc@1: 0.3750 (0.8094), Acc@5: 0.7812 (0.9498)
2022-01-05 05:09:06,721 Val Step[0100/1563], Loss: 1.8586 (1.1920), Acc@1: 0.5625 (0.7280), Acc@5: 0.8125 (0.9168)
2022-01-05 05:09:08,195 Val Step[0150/1563], Loss: 0.4721 (1.0985), Acc@1: 0.9062 (0.7508), Acc@5: 1.0000 (0.9259)
2022-01-05 05:09:09,803 Val Step[0200/1563], Loss: 0.8908 (1.1148), Acc@1: 0.7188 (0.7547), Acc@5: 0.9688 (0.9240)
2022-01-05 05:09:11,276 Val Step[0250/1563], Loss: 1.2151 (1.0578), Acc@1: 0.7188 (0.7678), Acc@5: 0.9375 (0.9303)
2022-01-05 05:09:12,710 Val Step[0300/1563], Loss: 0.9610 (1.1098), Acc@1: 0.7812 (0.7493), Acc@5: 0.9375 (0.9267)
2022-01-05 05:09:14,121 Val Step[0350/1563], Loss: 1.4946 (1.1245), Acc@1: 0.6875 (0.7450), Acc@5: 0.9062 (0.9283)
2022-01-05 05:09:15,570 Val Step[0400/1563], Loss: 1.1390 (1.1309), Acc@1: 0.7500 (0.7401), Acc@5: 0.9375 (0.9295)
2022-01-05 05:09:17,170 Val Step[0450/1563], Loss: 1.1847 (1.1381), Acc@1: 0.5938 (0.7372), Acc@5: 1.0000 (0.9299)
2022-01-05 05:09:18,800 Val Step[0500/1563], Loss: 0.4499 (1.1260), Acc@1: 0.9375 (0.7409), Acc@5: 1.0000 (0.9315)
2022-01-05 05:09:20,305 Val Step[0550/1563], Loss: 1.0426 (1.1005), Acc@1: 0.7188 (0.7473), Acc@5: 0.9688 (0.9336)
2022-01-05 05:09:21,798 Val Step[0600/1563], Loss: 1.1505 (1.1059), Acc@1: 0.7188 (0.7469), Acc@5: 0.9062 (0.9323)
2022-01-05 05:09:23,268 Val Step[0650/1563], Loss: 0.7270 (1.1296), Acc@1: 0.8750 (0.7421), Acc@5: 1.0000 (0.9284)
2022-01-05 05:09:24,668 Val Step[0700/1563], Loss: 1.1189 (1.1614), Acc@1: 0.7812 (0.7351), Acc@5: 0.9375 (0.9240)
2022-01-05 05:09:26,085 Val Step[0750/1563], Loss: 1.7909 (1.1981), Acc@1: 0.5938 (0.7277), Acc@5: 0.8125 (0.9191)
2022-01-05 05:09:27,458 Val Step[0800/1563], Loss: 0.7939 (1.2396), Acc@1: 0.8438 (0.7177), Acc@5: 0.9688 (0.9134)
2022-01-05 05:09:28,862 Val Step[0850/1563], Loss: 1.7050 (1.2696), Acc@1: 0.5938 (0.7109), Acc@5: 0.8438 (0.9091)
2022-01-05 05:09:30,300 Val Step[0900/1563], Loss: 0.3606 (1.2691), Acc@1: 0.9375 (0.7124), Acc@5: 1.0000 (0.9083)
2022-01-05 05:09:31,839 Val Step[0950/1563], Loss: 1.7604 (1.2902), Acc@1: 0.5625 (0.7085), Acc@5: 0.7812 (0.9055)
2022-01-05 05:09:33,251 Val Step[1000/1563], Loss: 0.8909 (1.3173), Acc@1: 0.8750 (0.7025), Acc@5: 1.0000 (0.9015)
2022-01-05 05:09:34,726 Val Step[1050/1563], Loss: 0.4587 (1.3297), Acc@1: 0.9688 (0.6999), Acc@5: 0.9688 (0.9000)
2022-01-05 05:09:36,124 Val Step[1100/1563], Loss: 0.8435 (1.3472), Acc@1: 0.8125 (0.6967), Acc@5: 0.9688 (0.8973)
2022-01-05 05:09:37,545 Val Step[1150/1563], Loss: 1.4396 (1.3674), Acc@1: 0.7500 (0.6929), Acc@5: 0.8438 (0.8945)
2022-01-05 05:09:38,942 Val Step[1200/1563], Loss: 1.3213 (1.3852), Acc@1: 0.8125 (0.6891), Acc@5: 0.8438 (0.8919)
2022-01-05 05:09:40,340 Val Step[1250/1563], Loss: 1.0137 (1.4012), Acc@1: 0.8125 (0.6869), Acc@5: 0.8750 (0.8893)
2022-01-05 05:09:41,744 Val Step[1300/1563], Loss: 1.4511 (1.4134), Acc@1: 0.6875 (0.6844), Acc@5: 0.9062 (0.8879)
2022-01-05 05:09:43,115 Val Step[1350/1563], Loss: 1.9016 (1.4320), Acc@1: 0.5312 (0.6804), Acc@5: 0.8125 (0.8851)
2022-01-05 05:09:44,551 Val Step[1400/1563], Loss: 1.2052 (1.4405), Acc@1: 0.7500 (0.6787), Acc@5: 0.9062 (0.8839)
2022-01-05 05:09:45,974 Val Step[1450/1563], Loss: 1.5951 (1.4483), Acc@1: 0.6250 (0.6768), Acc@5: 0.9062 (0.8833)
2022-01-05 05:09:47,355 Val Step[1500/1563], Loss: 1.9935 (1.4376), Acc@1: 0.5312 (0.6795), Acc@5: 0.8438 (0.8847)
2022-01-05 05:09:48,797 Val Step[1550/1563], Loss: 1.1610 (1.4366), Acc@1: 0.8750 (0.6792), Acc@5: 0.9062 (0.8850)
2022-01-05 05:09:50,345 ----- Epoch[174/310], Validation Loss: 1.4339, Validation Acc@1: 0.6799, Validation Acc@5: 0.8853, time: 103.42
2022-01-05 05:09:50,345 ----- Epoch[174/310], Train Loss: 3.9084, Train Acc: 0.3377, time: 1500.83, Best Val(epoch174) Acc@1: 0.6799
2022-01-05 05:09:50,562 Max accuracy so far: 0.6799 at epoch_174
2022-01-05 05:09:50,562 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 05:09:50,563 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 05:09:50,715 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 05:09:51,014 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 05:09:51,014 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 05:09:51,147 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 05:09:51,147 Now training epoch 175. LR=0.000374
2022-01-05 05:11:05,347 Epoch[175/310], Step[0000/1251], Loss: 3.9252(3.9252), Acc: 0.4443(0.4443)
2022-01-05 05:12:01,972 Epoch[175/310], Step[0050/1251], Loss: 4.1350(3.8498), Acc: 0.2295(0.3613)
2022-01-05 05:12:58,448 Epoch[175/310], Step[0100/1251], Loss: 3.9442(3.8795), Acc: 0.4551(0.3508)
2022-01-05 05:13:55,553 Epoch[175/310], Step[0150/1251], Loss: 3.2183(3.8929), Acc: 0.3799(0.3468)
2022-01-05 05:14:52,963 Epoch[175/310], Step[0200/1251], Loss: 3.9329(3.8987), Acc: 0.2666(0.3397)
2022-01-05 05:15:48,437 Epoch[175/310], Step[0250/1251], Loss: 4.0050(3.8897), Acc: 0.2109(0.3354)
2022-01-05 05:16:43,391 Epoch[175/310], Step[0300/1251], Loss: 3.5618(3.8928), Acc: 0.4199(0.3395)
2022-01-05 05:17:41,170 Epoch[175/310], Step[0350/1251], Loss: 4.0992(3.8976), Acc: 0.4404(0.3387)
2022-01-05 05:18:38,690 Epoch[175/310], Step[0400/1251], Loss: 3.1257(3.8940), Acc: 0.1475(0.3382)
2022-01-05 05:19:34,949 Epoch[175/310], Step[0450/1251], Loss: 4.2610(3.9026), Acc: 0.3018(0.3365)
2022-01-05 05:20:33,113 Epoch[175/310], Step[0500/1251], Loss: 3.5668(3.9097), Acc: 0.3779(0.3358)
2022-01-05 05:21:30,310 Epoch[175/310], Step[0550/1251], Loss: 3.8766(3.9051), Acc: 0.2939(0.3370)
2022-01-05 05:22:27,111 Epoch[175/310], Step[0600/1251], Loss: 3.5633(3.9036), Acc: 0.3330(0.3399)
2022-01-05 05:23:24,843 Epoch[175/310], Step[0650/1251], Loss: 3.6856(3.9060), Acc: 0.0908(0.3383)
2022-01-05 05:24:23,150 Epoch[175/310], Step[0700/1251], Loss: 4.0382(3.9083), Acc: 0.3174(0.3393)
2022-01-05 05:25:20,019 Epoch[175/310], Step[0750/1251], Loss: 3.9174(3.9021), Acc: 0.4941(0.3397)
2022-01-05 05:26:16,832 Epoch[175/310], Step[0800/1251], Loss: 3.7922(3.9009), Acc: 0.1826(0.3383)
2022-01-05 05:27:13,147 Epoch[175/310], Step[0850/1251], Loss: 4.1756(3.9004), Acc: 0.2373(0.3377)
2022-01-05 05:28:10,984 Epoch[175/310], Step[0900/1251], Loss: 4.0679(3.9025), Acc: 0.2705(0.3374)
2022-01-05 05:29:07,855 Epoch[175/310], Step[0950/1251], Loss: 3.9900(3.9024), Acc: 0.3604(0.3374)
2022-01-05 05:30:05,234 Epoch[175/310], Step[1000/1251], Loss: 3.7623(3.9022), Acc: 0.5117(0.3369)
2022-01-05 05:31:02,316 Epoch[175/310], Step[1050/1251], Loss: 3.9830(3.9035), Acc: 0.3926(0.3369)
2022-01-05 05:31:59,742 Epoch[175/310], Step[1100/1251], Loss: 4.0593(3.9044), Acc: 0.3994(0.3366)
2022-01-05 05:32:57,278 Epoch[175/310], Step[1150/1251], Loss: 4.1616(3.9043), Acc: 0.3037(0.3368)
2022-01-05 05:33:52,153 Epoch[175/310], Step[1200/1251], Loss: 4.4056(3.9065), Acc: 0.3760(0.3367)
2022-01-05 05:34:47,330 Epoch[175/310], Step[1250/1251], Loss: 3.8125(3.9080), Acc: 0.4912(0.3367)
2022-01-05 05:34:49,229 ----- Epoch[175/310], Train Loss: 3.9080, Train Acc: 0.3367, time: 1498.08, Best Val(epoch174) Acc@1: 0.6799
2022-01-05 05:34:49,403 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 05:34:49,404 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 05:34:49,516 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 05:34:49,516 Now training epoch 176. LR=0.000369
2022-01-05 05:36:05,908 Epoch[176/310], Step[0000/1251], Loss: 4.3853(4.3853), Acc: 0.1719(0.1719)
2022-01-05 05:37:03,031 Epoch[176/310], Step[0050/1251], Loss: 3.8860(3.9203), Acc: 0.3281(0.3378)
2022-01-05 05:38:00,120 Epoch[176/310], Step[0100/1251], Loss: 4.3402(3.8867), Acc: 0.4375(0.3302)
2022-01-05 05:38:57,195 Epoch[176/310], Step[0150/1251], Loss: 4.3099(3.8852), Acc: 0.3535(0.3333)
2022-01-05 05:39:54,417 Epoch[176/310], Step[0200/1251], Loss: 3.9649(3.8842), Acc: 0.3408(0.3369)
2022-01-05 05:40:51,199 Epoch[176/310], Step[0250/1251], Loss: 4.0192(3.8719), Acc: 0.4141(0.3405)
2022-01-05 05:41:47,872 Epoch[176/310], Step[0300/1251], Loss: 3.9422(3.8704), Acc: 0.2051(0.3417)
2022-01-05 05:42:46,482 Epoch[176/310], Step[0350/1251], Loss: 3.9597(3.8691), Acc: 0.2061(0.3429)
2022-01-05 05:43:42,887 Epoch[176/310], Step[0400/1251], Loss: 3.5950(3.8650), Acc: 0.2100(0.3437)
2022-01-05 05:44:40,704 Epoch[176/310], Step[0450/1251], Loss: 3.7340(3.8629), Acc: 0.2764(0.3426)
2022-01-05 05:45:38,300 Epoch[176/310], Step[0500/1251], Loss: 4.1952(3.8653), Acc: 0.3203(0.3415)
2022-01-05 05:46:36,777 Epoch[176/310], Step[0550/1251], Loss: 4.2409(3.8688), Acc: 0.2754(0.3412)
2022-01-05 05:47:33,640 Epoch[176/310], Step[0600/1251], Loss: 3.7279(3.8724), Acc: 0.3115(0.3409)
2022-01-05 05:48:31,507 Epoch[176/310], Step[0650/1251], Loss: 3.2325(3.8741), Acc: 0.0039(0.3390)
2022-01-05 05:49:28,000 Epoch[176/310], Step[0700/1251], Loss: 4.0235(3.8826), Acc: 0.3330(0.3388)
2022-01-05 05:50:25,030 Epoch[176/310], Step[0750/1251], Loss: 3.7987(3.8847), Acc: 0.0342(0.3385)
2022-01-05 05:51:22,532 Epoch[176/310], Step[0800/1251], Loss: 3.9646(3.8835), Acc: 0.3223(0.3390)
2022-01-05 05:52:19,265 Epoch[176/310], Step[0850/1251], Loss: 3.8914(3.8854), Acc: 0.4434(0.3392)
2022-01-05 05:53:15,404 Epoch[176/310], Step[0900/1251], Loss: 3.9879(3.8836), Acc: 0.4795(0.3406)
2022-01-05 05:54:11,640 Epoch[176/310], Step[0950/1251], Loss: 4.4051(3.8832), Acc: 0.4043(0.3401)
2022-01-05 05:55:06,289 Epoch[176/310], Step[1000/1251], Loss: 4.0374(3.8815), Acc: 0.3994(0.3402)
2022-01-05 05:56:02,527 Epoch[176/310], Step[1050/1251], Loss: 4.1362(3.8813), Acc: 0.4150(0.3403)
2022-01-05 05:56:58,739 Epoch[176/310], Step[1100/1251], Loss: 3.1391(3.8800), Acc: 0.4229(0.3397)
2022-01-05 05:57:54,999 Epoch[176/310], Step[1150/1251], Loss: 3.0724(3.8811), Acc: 0.3164(0.3395)
2022-01-05 05:58:52,186 Epoch[176/310], Step[1200/1251], Loss: 4.1986(3.8838), Acc: 0.1855(0.3386)
2022-01-05 05:59:48,320 Epoch[176/310], Step[1250/1251], Loss: 4.0517(3.8852), Acc: 0.3760(0.3385)
2022-01-05 05:59:50,366 ----- Validation after Epoch: 176
2022-01-05 06:00:48,407 Val Step[0000/1563], Loss: 0.8274 (0.8274), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 06:00:49,997 Val Step[0050/1563], Loss: 2.3260 (0.8507), Acc@1: 0.4688 (0.8229), Acc@5: 0.8750 (0.9547)
2022-01-05 06:00:51,522 Val Step[0100/1563], Loss: 1.7949 (1.1799), Acc@1: 0.5312 (0.7358), Acc@5: 0.8438 (0.9177)
2022-01-05 06:00:53,086 Val Step[0150/1563], Loss: 0.6174 (1.1212), Acc@1: 0.9375 (0.7554), Acc@5: 0.9688 (0.9220)
2022-01-05 06:00:54,606 Val Step[0200/1563], Loss: 1.0317 (1.1279), Acc@1: 0.7812 (0.7584), Acc@5: 0.9062 (0.9213)
2022-01-05 06:00:55,955 Val Step[0250/1563], Loss: 0.5554 (1.0655), Acc@1: 0.9375 (0.7710), Acc@5: 1.0000 (0.9294)
2022-01-05 06:00:57,383 Val Step[0300/1563], Loss: 1.1201 (1.1208), Acc@1: 0.7812 (0.7523), Acc@5: 0.9375 (0.9257)
2022-01-05 06:00:58,929 Val Step[0350/1563], Loss: 1.2388 (1.1332), Acc@1: 0.7500 (0.7477), Acc@5: 0.9062 (0.9266)
2022-01-05 06:01:00,343 Val Step[0400/1563], Loss: 1.1220 (1.1362), Acc@1: 0.8438 (0.7437), Acc@5: 0.9688 (0.9284)
2022-01-05 06:01:01,919 Val Step[0450/1563], Loss: 1.0353 (1.1435), Acc@1: 0.6875 (0.7405), Acc@5: 1.0000 (0.9293)
2022-01-05 06:01:03,417 Val Step[0500/1563], Loss: 0.5550 (1.1370), Acc@1: 0.9375 (0.7426), Acc@5: 1.0000 (0.9306)
2022-01-05 06:01:05,126 Val Step[0550/1563], Loss: 0.7323 (1.1127), Acc@1: 0.8438 (0.7491), Acc@5: 1.0000 (0.9328)
2022-01-05 06:01:06,706 Val Step[0600/1563], Loss: 0.8010 (1.1169), Acc@1: 0.8438 (0.7482), Acc@5: 0.9375 (0.9324)
2022-01-05 06:01:08,259 Val Step[0650/1563], Loss: 0.7946 (1.1419), Acc@1: 0.8438 (0.7429), Acc@5: 0.9375 (0.9291)
2022-01-05 06:01:09,773 Val Step[0700/1563], Loss: 1.4128 (1.1756), Acc@1: 0.7500 (0.7348), Acc@5: 0.8438 (0.9243)
2022-01-05 06:01:11,313 Val Step[0750/1563], Loss: 1.3499 (1.2105), Acc@1: 0.6875 (0.7282), Acc@5: 0.9062 (0.9191)
2022-01-05 06:01:12,731 Val Step[0800/1563], Loss: 0.8634 (1.2511), Acc@1: 0.7500 (0.7181), Acc@5: 1.0000 (0.9136)
2022-01-05 06:01:14,185 Val Step[0850/1563], Loss: 1.6038 (1.2785), Acc@1: 0.6250 (0.7116), Acc@5: 0.9062 (0.9100)
2022-01-05 06:01:15,673 Val Step[0900/1563], Loss: 0.2987 (1.2803), Acc@1: 0.9688 (0.7127), Acc@5: 1.0000 (0.9087)
2022-01-05 06:01:17,137 Val Step[0950/1563], Loss: 1.7109 (1.3031), Acc@1: 0.6250 (0.7081), Acc@5: 0.8750 (0.9052)
2022-01-05 06:01:18,566 Val Step[1000/1563], Loss: 0.7005 (1.3291), Acc@1: 0.9375 (0.7018), Acc@5: 0.9375 (0.9013)
2022-01-05 06:01:19,998 Val Step[1050/1563], Loss: 0.4965 (1.3402), Acc@1: 0.9688 (0.6994), Acc@5: 0.9688 (0.8999)
2022-01-05 06:01:21,394 Val Step[1100/1563], Loss: 0.8721 (1.3565), Acc@1: 0.8125 (0.6958), Acc@5: 0.9688 (0.8973)
2022-01-05 06:01:22,823 Val Step[1150/1563], Loss: 1.5707 (1.3733), Acc@1: 0.7188 (0.6927), Acc@5: 0.7500 (0.8947)
2022-01-05 06:01:24,259 Val Step[1200/1563], Loss: 1.4743 (1.3890), Acc@1: 0.7500 (0.6893), Acc@5: 0.8438 (0.8920)
2022-01-05 06:01:25,772 Val Step[1250/1563], Loss: 0.8025 (1.4022), Acc@1: 0.8438 (0.6868), Acc@5: 0.9062 (0.8896)
2022-01-05 06:01:27,162 Val Step[1300/1563], Loss: 1.0921 (1.4135), Acc@1: 0.8125 (0.6851), Acc@5: 0.9062 (0.8879)
2022-01-05 06:01:28,595 Val Step[1350/1563], Loss: 2.4112 (1.4318), Acc@1: 0.3125 (0.6814), Acc@5: 0.7500 (0.8852)
2022-01-05 06:01:30,044 Val Step[1400/1563], Loss: 1.2061 (1.4388), Acc@1: 0.6875 (0.6801), Acc@5: 0.9375 (0.8843)
2022-01-05 06:01:31,469 Val Step[1450/1563], Loss: 1.5646 (1.4463), Acc@1: 0.7188 (0.6786), Acc@5: 0.8750 (0.8835)
2022-01-05 06:01:32,809 Val Step[1500/1563], Loss: 1.9556 (1.4348), Acc@1: 0.6250 (0.6810), Acc@5: 0.7812 (0.8851)
2022-01-05 06:01:34,194 Val Step[1550/1563], Loss: 1.1477 (1.4352), Acc@1: 0.8750 (0.6809), Acc@5: 0.9062 (0.8852)
2022-01-05 06:01:35,015 ----- Epoch[176/310], Validation Loss: 1.4331, Validation Acc@1: 0.6814, Validation Acc@5: 0.8854, time: 104.65
2022-01-05 06:01:35,016 ----- Epoch[176/310], Train Loss: 3.8852, Train Acc: 0.3385, time: 1500.85, Best Val(epoch176) Acc@1: 0.6814
2022-01-05 06:01:35,199 Max accuracy so far: 0.6814 at epoch_176
2022-01-05 06:01:35,199 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 06:01:35,199 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 06:01:35,305 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 06:01:35,704 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 06:01:35,984 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 06:01:36,029 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 06:01:36,030 Now training epoch 177. LR=0.000364
2022-01-05 06:02:51,623 Epoch[177/310], Step[0000/1251], Loss: 3.7295(3.7295), Acc: 0.5020(0.5020)
2022-01-05 06:03:48,868 Epoch[177/310], Step[0050/1251], Loss: 3.8497(3.7787), Acc: 0.4629(0.3465)
2022-01-05 06:04:46,171 Epoch[177/310], Step[0100/1251], Loss: 3.9175(3.8029), Acc: 0.4258(0.3491)
2022-01-05 06:05:44,384 Epoch[177/310], Step[0150/1251], Loss: 3.9076(3.8504), Acc: 0.2041(0.3439)
2022-01-05 06:06:40,818 Epoch[177/310], Step[0200/1251], Loss: 3.3643(3.8527), Acc: 0.4658(0.3481)
2022-01-05 06:07:37,888 Epoch[177/310], Step[0250/1251], Loss: 4.5957(3.8623), Acc: 0.1904(0.3403)
2022-01-05 06:08:34,781 Epoch[177/310], Step[0300/1251], Loss: 3.7756(3.8660), Acc: 0.4199(0.3382)
2022-01-05 06:09:32,516 Epoch[177/310], Step[0350/1251], Loss: 4.0035(3.8669), Acc: 0.4053(0.3376)
2022-01-05 06:10:29,743 Epoch[177/310], Step[0400/1251], Loss: 4.0387(3.8671), Acc: 0.4180(0.3403)
2022-01-05 06:11:27,719 Epoch[177/310], Step[0450/1251], Loss: 3.7958(3.8690), Acc: 0.1758(0.3386)
2022-01-05 06:12:24,032 Epoch[177/310], Step[0500/1251], Loss: 4.2961(3.8788), Acc: 0.3926(0.3376)
2022-01-05 06:13:21,137 Epoch[177/310], Step[0550/1251], Loss: 4.0800(3.8801), Acc: 0.1748(0.3369)
2022-01-05 06:14:18,826 Epoch[177/310], Step[0600/1251], Loss: 3.8842(3.8826), Acc: 0.4941(0.3380)
2022-01-05 06:15:15,364 Epoch[177/310], Step[0650/1251], Loss: 4.1689(3.8830), Acc: 0.2832(0.3384)
2022-01-05 06:16:13,639 Epoch[177/310], Step[0700/1251], Loss: 4.0657(3.8848), Acc: 0.3984(0.3373)
2022-01-05 06:17:09,766 Epoch[177/310], Step[0750/1251], Loss: 4.2650(3.8935), Acc: 0.2383(0.3367)
2022-01-05 06:18:06,629 Epoch[177/310], Step[0800/1251], Loss: 4.2049(3.8929), Acc: 0.3486(0.3364)
2022-01-05 06:19:04,604 Epoch[177/310], Step[0850/1251], Loss: 4.3329(3.8898), Acc: 0.2129(0.3371)
2022-01-05 06:20:02,620 Epoch[177/310], Step[0900/1251], Loss: 4.3992(3.8910), Acc: 0.3145(0.3371)
2022-01-05 06:20:59,686 Epoch[177/310], Step[0950/1251], Loss: 4.2549(3.8915), Acc: 0.3936(0.3370)
2022-01-05 06:21:55,944 Epoch[177/310], Step[1000/1251], Loss: 3.6154(3.8909), Acc: 0.4668(0.3369)
2022-01-05 06:22:52,739 Epoch[177/310], Step[1050/1251], Loss: 3.6927(3.8914), Acc: 0.1670(0.3361)
2022-01-05 06:23:50,936 Epoch[177/310], Step[1100/1251], Loss: 3.7954(3.8935), Acc: 0.1748(0.3360)
2022-01-05 06:24:48,508 Epoch[177/310], Step[1150/1251], Loss: 4.2058(3.8934), Acc: 0.3896(0.3366)
2022-01-05 06:25:45,574 Epoch[177/310], Step[1200/1251], Loss: 4.4747(3.8948), Acc: 0.2432(0.3361)
2022-01-05 06:26:41,068 Epoch[177/310], Step[1250/1251], Loss: 3.4482(3.8951), Acc: 0.5156(0.3356)
2022-01-05 06:26:42,936 ----- Epoch[177/310], Train Loss: 3.8951, Train Acc: 0.3356, time: 1506.90, Best Val(epoch176) Acc@1: 0.6814
2022-01-05 06:26:43,099 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 06:26:43,099 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 06:26:43,207 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 06:26:43,207 Now training epoch 178. LR=0.000359
2022-01-05 06:27:57,547 Epoch[178/310], Step[0000/1251], Loss: 4.3134(4.3134), Acc: 0.2559(0.2559)
2022-01-05 06:28:53,396 Epoch[178/310], Step[0050/1251], Loss: 4.0257(3.9134), Acc: 0.2744(0.3437)
2022-01-05 06:29:49,186 Epoch[178/310], Step[0100/1251], Loss: 3.7565(3.8968), Acc: 0.4375(0.3426)
2022-01-05 06:30:45,054 Epoch[178/310], Step[0150/1251], Loss: 4.2235(3.9077), Acc: 0.2197(0.3412)
2022-01-05 06:31:39,160 Epoch[178/310], Step[0200/1251], Loss: 3.7980(3.9136), Acc: 0.4277(0.3423)
2022-01-05 06:32:33,615 Epoch[178/310], Step[0250/1251], Loss: 3.7319(3.9176), Acc: 0.4980(0.3357)
2022-01-05 06:33:28,903 Epoch[178/310], Step[0300/1251], Loss: 3.9033(3.9219), Acc: 0.4326(0.3375)
2022-01-05 06:34:23,665 Epoch[178/310], Step[0350/1251], Loss: 3.4327(3.9183), Acc: 0.3848(0.3341)
2022-01-05 06:35:21,360 Epoch[178/310], Step[0400/1251], Loss: 3.9714(3.9198), Acc: 0.4072(0.3322)
2022-01-05 06:36:19,743 Epoch[178/310], Step[0450/1251], Loss: 3.7699(3.9172), Acc: 0.3633(0.3321)
2022-01-05 06:37:17,469 Epoch[178/310], Step[0500/1251], Loss: 3.4475(3.9162), Acc: 0.3838(0.3334)
2022-01-05 06:38:14,506 Epoch[178/310], Step[0550/1251], Loss: 3.4748(3.9148), Acc: 0.5352(0.3333)
2022-01-05 06:39:12,720 Epoch[178/310], Step[0600/1251], Loss: 3.9529(3.9156), Acc: 0.3652(0.3337)
2022-01-05 06:40:10,817 Epoch[178/310], Step[0650/1251], Loss: 4.3788(3.9180), Acc: 0.3486(0.3336)
2022-01-05 06:41:06,034 Epoch[178/310], Step[0700/1251], Loss: 4.1710(3.9145), Acc: 0.3496(0.3345)
2022-01-05 06:42:02,804 Epoch[178/310], Step[0750/1251], Loss: 3.6272(3.9100), Acc: 0.4746(0.3343)
2022-01-05 06:43:00,877 Epoch[178/310], Step[0800/1251], Loss: 3.5745(3.9105), Acc: 0.4902(0.3323)
2022-01-05 06:43:58,173 Epoch[178/310], Step[0850/1251], Loss: 4.3864(3.9086), Acc: 0.2949(0.3324)
2022-01-05 06:44:56,026 Epoch[178/310], Step[0900/1251], Loss: 3.1116(3.9069), Acc: 0.4150(0.3319)
2022-01-05 06:45:54,477 Epoch[178/310], Step[0950/1251], Loss: 4.1289(3.9081), Acc: 0.3799(0.3327)
2022-01-05 06:46:50,422 Epoch[178/310], Step[1000/1251], Loss: 3.9170(3.9106), Acc: 0.4775(0.3329)
2022-01-05 06:47:48,706 Epoch[178/310], Step[1050/1251], Loss: 3.3945(3.9092), Acc: 0.2959(0.3325)
2022-01-05 06:48:46,379 Epoch[178/310], Step[1100/1251], Loss: 4.3834(3.9050), Acc: 0.2891(0.3324)
2022-01-05 06:49:44,380 Epoch[178/310], Step[1150/1251], Loss: 3.9566(3.9015), Acc: 0.3682(0.3328)
2022-01-05 06:50:40,829 Epoch[178/310], Step[1200/1251], Loss: 3.7570(3.9004), Acc: 0.2344(0.3333)
2022-01-05 06:51:37,958 Epoch[178/310], Step[1250/1251], Loss: 3.8782(3.8990), Acc: 0.4834(0.3331)
2022-01-05 06:51:40,181 ----- Validation after Epoch: 178
2022-01-05 06:52:33,762 Val Step[0000/1563], Loss: 0.7192 (0.7192), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-05 06:52:35,260 Val Step[0050/1563], Loss: 2.3215 (0.8924), Acc@1: 0.4375 (0.8094), Acc@5: 0.8438 (0.9461)
2022-01-05 06:52:36,673 Val Step[0100/1563], Loss: 2.1147 (1.1777), Acc@1: 0.4688 (0.7311), Acc@5: 0.8125 (0.9162)
2022-01-05 06:52:38,085 Val Step[0150/1563], Loss: 0.5135 (1.1069), Acc@1: 0.8750 (0.7508), Acc@5: 0.9688 (0.9209)
2022-01-05 06:52:39,545 Val Step[0200/1563], Loss: 1.0765 (1.1120), Acc@1: 0.8125 (0.7559), Acc@5: 0.8750 (0.9195)
2022-01-05 06:52:40,981 Val Step[0250/1563], Loss: 0.4718 (1.0571), Acc@1: 0.9375 (0.7667), Acc@5: 1.0000 (0.9260)
2022-01-05 06:52:42,349 Val Step[0300/1563], Loss: 1.1444 (1.1160), Acc@1: 0.7500 (0.7488), Acc@5: 0.9688 (0.9223)
2022-01-05 06:52:43,795 Val Step[0350/1563], Loss: 1.2658 (1.1236), Acc@1: 0.6250 (0.7448), Acc@5: 0.9062 (0.9246)
2022-01-05 06:52:45,194 Val Step[0400/1563], Loss: 0.8426 (1.1276), Acc@1: 0.8438 (0.7417), Acc@5: 0.9688 (0.9253)
2022-01-05 06:52:46,635 Val Step[0450/1563], Loss: 1.3032 (1.1334), Acc@1: 0.5938 (0.7386), Acc@5: 0.9688 (0.9260)
2022-01-05 06:52:48,149 Val Step[0500/1563], Loss: 0.5864 (1.1218), Acc@1: 0.9375 (0.7419), Acc@5: 1.0000 (0.9279)
2022-01-05 06:52:49,649 Val Step[0550/1563], Loss: 0.8121 (1.0970), Acc@1: 0.8438 (0.7482), Acc@5: 0.9375 (0.9299)
2022-01-05 06:52:51,193 Val Step[0600/1563], Loss: 0.7931 (1.1016), Acc@1: 0.8125 (0.7472), Acc@5: 0.9688 (0.9293)
2022-01-05 06:52:52,652 Val Step[0650/1563], Loss: 0.7901 (1.1188), Acc@1: 0.8125 (0.7437), Acc@5: 1.0000 (0.9267)
2022-01-05 06:52:54,082 Val Step[0700/1563], Loss: 1.2224 (1.1507), Acc@1: 0.7500 (0.7359), Acc@5: 0.9062 (0.9227)
2022-01-05 06:52:55,519 Val Step[0750/1563], Loss: 1.4067 (1.1882), Acc@1: 0.6875 (0.7287), Acc@5: 0.8750 (0.9179)
2022-01-05 06:52:56,928 Val Step[0800/1563], Loss: 1.2531 (1.2303), Acc@1: 0.7188 (0.7179), Acc@5: 0.9688 (0.9126)
2022-01-05 06:52:58,433 Val Step[0850/1563], Loss: 1.5637 (1.2574), Acc@1: 0.5938 (0.7120), Acc@5: 0.9062 (0.9088)
2022-01-05 06:52:59,907 Val Step[0900/1563], Loss: 0.4873 (1.2558), Acc@1: 0.9375 (0.7140), Acc@5: 0.9688 (0.9085)
2022-01-05 06:53:01,362 Val Step[0950/1563], Loss: 1.4806 (1.2787), Acc@1: 0.6250 (0.7094), Acc@5: 0.8750 (0.9052)
2022-01-05 06:53:02,791 Val Step[1000/1563], Loss: 0.7876 (1.3051), Acc@1: 0.8750 (0.7026), Acc@5: 1.0000 (0.9010)
2022-01-05 06:53:04,200 Val Step[1050/1563], Loss: 0.3730 (1.3206), Acc@1: 0.9688 (0.6996), Acc@5: 1.0000 (0.8988)
2022-01-05 06:53:05,600 Val Step[1100/1563], Loss: 0.9361 (1.3351), Acc@1: 0.7188 (0.6966), Acc@5: 0.9688 (0.8963)
2022-01-05 06:53:07,110 Val Step[1150/1563], Loss: 1.2350 (1.3500), Acc@1: 0.7812 (0.6936), Acc@5: 0.8438 (0.8939)
2022-01-05 06:53:08,516 Val Step[1200/1563], Loss: 1.3238 (1.3641), Acc@1: 0.7188 (0.6910), Acc@5: 0.8750 (0.8916)
2022-01-05 06:53:10,055 Val Step[1250/1563], Loss: 0.8831 (1.3767), Acc@1: 0.8125 (0.6892), Acc@5: 0.9375 (0.8897)
2022-01-05 06:53:11,453 Val Step[1300/1563], Loss: 0.9180 (1.3874), Acc@1: 0.7812 (0.6874), Acc@5: 0.9062 (0.8884)
2022-01-05 06:53:12,913 Val Step[1350/1563], Loss: 2.2676 (1.4067), Acc@1: 0.5000 (0.6835), Acc@5: 0.7500 (0.8858)
2022-01-05 06:53:14,320 Val Step[1400/1563], Loss: 1.1614 (1.4144), Acc@1: 0.7500 (0.6817), Acc@5: 0.9375 (0.8850)
2022-01-05 06:53:15,763 Val Step[1450/1563], Loss: 1.4288 (1.4203), Acc@1: 0.6250 (0.6805), Acc@5: 0.9375 (0.8847)
2022-01-05 06:53:17,168 Val Step[1500/1563], Loss: 1.8600 (1.4085), Acc@1: 0.5312 (0.6829), Acc@5: 0.7812 (0.8862)
2022-01-05 06:53:18,610 Val Step[1550/1563], Loss: 0.9531 (1.4085), Acc@1: 0.8750 (0.6827), Acc@5: 0.9062 (0.8862)
2022-01-05 06:53:19,423 ----- Epoch[178/310], Validation Loss: 1.4062, Validation Acc@1: 0.6831, Validation Acc@5: 0.8864, time: 99.24
2022-01-05 06:53:19,423 ----- Epoch[178/310], Train Loss: 3.8990, Train Acc: 0.3331, time: 1496.97, Best Val(epoch178) Acc@1: 0.6831
2022-01-05 06:53:19,610 Max accuracy so far: 0.6831 at epoch_178
2022-01-05 06:53:19,610 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 06:53:19,610 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 06:53:19,740 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 06:53:20,139 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 06:53:20,140 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 06:53:20,269 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 06:53:20,269 Now training epoch 179. LR=0.000354
2022-01-05 06:54:33,965 Epoch[179/310], Step[0000/1251], Loss: 4.3568(4.3568), Acc: 0.3252(0.3252)
2022-01-05 06:55:31,164 Epoch[179/310], Step[0050/1251], Loss: 3.9746(3.8862), Acc: 0.1768(0.3310)
2022-01-05 06:56:27,315 Epoch[179/310], Step[0100/1251], Loss: 4.1220(3.8782), Acc: 0.1523(0.3238)
2022-01-05 06:57:22,709 Epoch[179/310], Step[0150/1251], Loss: 4.2487(3.8711), Acc: 0.4062(0.3394)
2022-01-05 06:58:19,967 Epoch[179/310], Step[0200/1251], Loss: 4.0549(3.8680), Acc: 0.3125(0.3349)
2022-01-05 06:59:16,988 Epoch[179/310], Step[0250/1251], Loss: 3.9666(3.8727), Acc: 0.1709(0.3365)
2022-01-05 07:00:14,317 Epoch[179/310], Step[0300/1251], Loss: 4.3385(3.8848), Acc: 0.4326(0.3362)
2022-01-05 07:01:09,149 Epoch[179/310], Step[0350/1251], Loss: 3.7484(3.8820), Acc: 0.4883(0.3355)
2022-01-05 07:02:06,798 Epoch[179/310], Step[0400/1251], Loss: 4.5048(3.8915), Acc: 0.3018(0.3354)
2022-01-05 07:03:03,162 Epoch[179/310], Step[0450/1251], Loss: 4.1291(3.8941), Acc: 0.4004(0.3380)
2022-01-05 07:04:00,075 Epoch[179/310], Step[0500/1251], Loss: 3.9510(3.8865), Acc: 0.3340(0.3364)
2022-01-05 07:04:57,606 Epoch[179/310], Step[0550/1251], Loss: 4.3457(3.8890), Acc: 0.1787(0.3356)
2022-01-05 07:05:54,801 Epoch[179/310], Step[0600/1251], Loss: 4.3416(3.8866), Acc: 0.2998(0.3376)
2022-01-05 07:06:52,787 Epoch[179/310], Step[0650/1251], Loss: 4.3781(3.8894), Acc: 0.3223(0.3370)
2022-01-05 07:07:49,381 Epoch[179/310], Step[0700/1251], Loss: 3.7481(3.8900), Acc: 0.3447(0.3384)
2022-01-05 07:08:46,801 Epoch[179/310], Step[0750/1251], Loss: 3.7470(3.8851), Acc: 0.4561(0.3397)
2022-01-05 07:09:43,056 Epoch[179/310], Step[0800/1251], Loss: 3.3954(3.8839), Acc: 0.4326(0.3410)
2022-01-05 07:10:40,526 Epoch[179/310], Step[0850/1251], Loss: 3.8414(3.8821), Acc: 0.5020(0.3415)
2022-01-05 07:11:36,721 Epoch[179/310], Step[0900/1251], Loss: 3.5330(3.8850), Acc: 0.4375(0.3415)
2022-01-05 07:12:34,444 Epoch[179/310], Step[0950/1251], Loss: 3.7228(3.8873), Acc: 0.4863(0.3405)
2022-01-05 07:13:32,365 Epoch[179/310], Step[1000/1251], Loss: 4.0534(3.8899), Acc: 0.3594(0.3403)
2022-01-05 07:14:29,124 Epoch[179/310], Step[1050/1251], Loss: 4.0185(3.8906), Acc: 0.4492(0.3411)
2022-01-05 07:15:26,493 Epoch[179/310], Step[1100/1251], Loss: 3.6262(3.8908), Acc: 0.1719(0.3406)
2022-01-05 07:16:23,854 Epoch[179/310], Step[1150/1251], Loss: 3.4683(3.8915), Acc: 0.4648(0.3415)
2022-01-05 07:17:20,193 Epoch[179/310], Step[1200/1251], Loss: 4.3658(3.8925), Acc: 0.2891(0.3420)
2022-01-05 07:18:16,235 Epoch[179/310], Step[1250/1251], Loss: 3.8799(3.8929), Acc: 0.2842(0.3427)
2022-01-05 07:18:18,235 ----- Epoch[179/310], Train Loss: 3.8929, Train Acc: 0.3427, time: 1497.96, Best Val(epoch178) Acc@1: 0.6831
2022-01-05 07:18:18,405 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 07:18:18,406 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 07:18:18,508 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 07:18:18,508 Now training epoch 180. LR=0.000349
2022-01-05 07:19:43,514 Epoch[180/310], Step[0000/1251], Loss: 4.2250(4.2250), Acc: 0.3438(0.3438)
2022-01-05 07:20:40,249 Epoch[180/310], Step[0050/1251], Loss: 3.8259(3.8943), Acc: 0.3066(0.3284)
2022-01-05 07:21:36,903 Epoch[180/310], Step[0100/1251], Loss: 4.2410(3.8846), Acc: 0.3428(0.3426)
2022-01-05 07:22:33,306 Epoch[180/310], Step[0150/1251], Loss: 4.3260(3.8962), Acc: 0.2402(0.3353)
2022-01-05 07:23:30,861 Epoch[180/310], Step[0200/1251], Loss: 3.6150(3.8835), Acc: 0.3555(0.3369)
2022-01-05 07:24:27,475 Epoch[180/310], Step[0250/1251], Loss: 3.5743(3.8786), Acc: 0.3965(0.3392)
2022-01-05 07:25:24,260 Epoch[180/310], Step[0300/1251], Loss: 3.3269(3.8812), Acc: 0.4248(0.3401)
2022-01-05 07:26:20,752 Epoch[180/310], Step[0350/1251], Loss: 4.2444(3.8763), Acc: 0.3730(0.3410)
2022-01-05 07:27:17,169 Epoch[180/310], Step[0400/1251], Loss: 4.0977(3.8737), Acc: 0.3242(0.3415)
2022-01-05 07:28:13,957 Epoch[180/310], Step[0450/1251], Loss: 4.2363(3.8759), Acc: 0.3037(0.3397)
2022-01-05 07:29:10,380 Epoch[180/310], Step[0500/1251], Loss: 3.5279(3.8783), Acc: 0.2520(0.3388)
2022-01-05 07:30:06,410 Epoch[180/310], Step[0550/1251], Loss: 3.9580(3.8836), Acc: 0.4199(0.3395)
2022-01-05 07:31:04,631 Epoch[180/310], Step[0600/1251], Loss: 3.6831(3.8831), Acc: 0.1455(0.3398)
2022-01-05 07:32:01,821 Epoch[180/310], Step[0650/1251], Loss: 3.8958(3.8783), Acc: 0.3838(0.3396)
2022-01-05 07:32:57,712 Epoch[180/310], Step[0700/1251], Loss: 3.7679(3.8800), Acc: 0.1748(0.3381)
2022-01-05 07:33:54,539 Epoch[180/310], Step[0750/1251], Loss: 4.0358(3.8820), Acc: 0.3525(0.3384)
2022-01-05 07:34:52,400 Epoch[180/310], Step[0800/1251], Loss: 4.0753(3.8828), Acc: 0.1895(0.3391)
2022-01-05 07:35:49,614 Epoch[180/310], Step[0850/1251], Loss: 4.0853(3.8846), Acc: 0.3535(0.3389)
2022-01-05 07:36:47,494 Epoch[180/310], Step[0900/1251], Loss: 4.2095(3.8853), Acc: 0.4160(0.3394)
2022-01-05 07:37:44,214 Epoch[180/310], Step[0950/1251], Loss: 4.3073(3.8912), Acc: 0.2656(0.3378)
2022-01-05 07:38:41,930 Epoch[180/310], Step[1000/1251], Loss: 4.2422(3.8919), Acc: 0.3242(0.3379)
2022-01-05 07:39:39,873 Epoch[180/310], Step[1050/1251], Loss: 3.5962(3.8902), Acc: 0.5234(0.3373)
2022-01-05 07:40:37,649 Epoch[180/310], Step[1100/1251], Loss: 3.8452(3.8924), Acc: 0.3848(0.3367)
2022-01-05 07:41:35,876 Epoch[180/310], Step[1150/1251], Loss: 3.6135(3.8942), Acc: 0.3350(0.3368)
2022-01-05 07:42:33,167 Epoch[180/310], Step[1200/1251], Loss: 3.8617(3.8915), Acc: 0.4580(0.3369)
2022-01-05 07:43:30,153 Epoch[180/310], Step[1250/1251], Loss: 3.9807(3.8919), Acc: 0.0879(0.3363)
2022-01-05 07:43:32,037 ----- Validation after Epoch: 180
2022-01-05 07:44:23,992 Val Step[0000/1563], Loss: 0.7987 (0.7987), Acc@1: 0.9062 (0.9062), Acc@5: 0.9375 (0.9375)
2022-01-05 07:44:25,638 Val Step[0050/1563], Loss: 2.0925 (0.9040), Acc@1: 0.5000 (0.8168), Acc@5: 0.8438 (0.9510)
2022-01-05 07:44:27,075 Val Step[0100/1563], Loss: 1.8065 (1.1708), Acc@1: 0.6250 (0.7361), Acc@5: 0.8438 (0.9257)
2022-01-05 07:44:28,473 Val Step[0150/1563], Loss: 0.6147 (1.1000), Acc@1: 0.8750 (0.7539), Acc@5: 0.9688 (0.9305)
2022-01-05 07:44:29,894 Val Step[0200/1563], Loss: 1.0381 (1.1219), Acc@1: 0.7812 (0.7558), Acc@5: 0.9062 (0.9262)
2022-01-05 07:44:31,295 Val Step[0250/1563], Loss: 1.0336 (1.0659), Acc@1: 0.7500 (0.7703), Acc@5: 1.0000 (0.9328)
2022-01-05 07:44:32,781 Val Step[0300/1563], Loss: 1.2006 (1.1233), Acc@1: 0.7188 (0.7522), Acc@5: 0.9375 (0.9287)
2022-01-05 07:44:34,203 Val Step[0350/1563], Loss: 1.4031 (1.1281), Acc@1: 0.6562 (0.7490), Acc@5: 0.9062 (0.9307)
2022-01-05 07:44:35,620 Val Step[0400/1563], Loss: 1.0232 (1.1333), Acc@1: 0.8125 (0.7456), Acc@5: 0.9688 (0.9310)
2022-01-05 07:44:37,110 Val Step[0450/1563], Loss: 1.1660 (1.1373), Acc@1: 0.5625 (0.7422), Acc@5: 0.9688 (0.9317)
2022-01-05 07:44:38,597 Val Step[0500/1563], Loss: 0.5683 (1.1301), Acc@1: 0.8125 (0.7448), Acc@5: 1.0000 (0.9328)
2022-01-05 07:44:39,985 Val Step[0550/1563], Loss: 0.9701 (1.1095), Acc@1: 0.8125 (0.7503), Acc@5: 0.9375 (0.9344)
2022-01-05 07:44:41,435 Val Step[0600/1563], Loss: 1.0213 (1.1165), Acc@1: 0.7812 (0.7499), Acc@5: 0.9375 (0.9335)
2022-01-05 07:44:42,841 Val Step[0650/1563], Loss: 0.5023 (1.1374), Acc@1: 0.9062 (0.7454), Acc@5: 1.0000 (0.9300)
2022-01-05 07:44:44,363 Val Step[0700/1563], Loss: 0.9857 (1.1693), Acc@1: 0.8125 (0.7373), Acc@5: 0.9062 (0.9258)
2022-01-05 07:44:45,814 Val Step[0750/1563], Loss: 1.6166 (1.2004), Acc@1: 0.6250 (0.7314), Acc@5: 0.8438 (0.9209)
2022-01-05 07:44:47,271 Val Step[0800/1563], Loss: 0.8034 (1.2394), Acc@1: 0.7812 (0.7227), Acc@5: 1.0000 (0.9151)
2022-01-05 07:44:48,729 Val Step[0850/1563], Loss: 1.4574 (1.2677), Acc@1: 0.6250 (0.7153), Acc@5: 0.9375 (0.9112)
2022-01-05 07:44:50,201 Val Step[0900/1563], Loss: 0.3034 (1.2658), Acc@1: 0.9688 (0.7174), Acc@5: 1.0000 (0.9104)
2022-01-05 07:44:51,761 Val Step[0950/1563], Loss: 1.7541 (1.2895), Acc@1: 0.5938 (0.7127), Acc@5: 0.8438 (0.9071)
2022-01-05 07:44:53,164 Val Step[1000/1563], Loss: 0.5472 (1.3128), Acc@1: 0.9688 (0.7070), Acc@5: 1.0000 (0.9039)
2022-01-05 07:44:54,644 Val Step[1050/1563], Loss: 0.4978 (1.3284), Acc@1: 0.9688 (0.7036), Acc@5: 0.9688 (0.9021)
2022-01-05 07:44:56,084 Val Step[1100/1563], Loss: 1.0870 (1.3440), Acc@1: 0.7188 (0.7005), Acc@5: 0.9375 (0.8998)
2022-01-05 07:44:57,590 Val Step[1150/1563], Loss: 1.3870 (1.3593), Acc@1: 0.7500 (0.6974), Acc@5: 0.7812 (0.8972)
2022-01-05 07:44:59,224 Val Step[1200/1563], Loss: 1.5638 (1.3762), Acc@1: 0.6250 (0.6935), Acc@5: 0.8438 (0.8944)
2022-01-05 07:45:00,737 Val Step[1250/1563], Loss: 0.7338 (1.3894), Acc@1: 0.8438 (0.6915), Acc@5: 0.9062 (0.8922)
2022-01-05 07:45:02,224 Val Step[1300/1563], Loss: 1.0803 (1.4001), Acc@1: 0.7812 (0.6897), Acc@5: 0.9062 (0.8912)
2022-01-05 07:45:03,670 Val Step[1350/1563], Loss: 1.9054 (1.4179), Acc@1: 0.5312 (0.6860), Acc@5: 0.8125 (0.8885)
2022-01-05 07:45:05,109 Val Step[1400/1563], Loss: 1.2539 (1.4267), Acc@1: 0.7188 (0.6835), Acc@5: 0.8750 (0.8871)
2022-01-05 07:45:06,551 Val Step[1450/1563], Loss: 1.6306 (1.4356), Acc@1: 0.6562 (0.6815), Acc@5: 0.9062 (0.8863)
2022-01-05 07:45:08,043 Val Step[1500/1563], Loss: 1.8957 (1.4247), Acc@1: 0.5625 (0.6842), Acc@5: 0.8438 (0.8876)
2022-01-05 07:45:09,578 Val Step[1550/1563], Loss: 0.9786 (1.4250), Acc@1: 0.8750 (0.6841), Acc@5: 0.9062 (0.8876)
2022-01-05 07:45:10,430 ----- Epoch[180/310], Validation Loss: 1.4228, Validation Acc@1: 0.6848, Validation Acc@5: 0.8878, time: 98.39
2022-01-05 07:45:10,431 ----- Epoch[180/310], Train Loss: 3.8919, Train Acc: 0.3363, time: 1513.53, Best Val(epoch180) Acc@1: 0.6848
2022-01-05 07:45:10,665 Max accuracy so far: 0.6848 at epoch_180
2022-01-05 07:45:10,665 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 07:45:10,665 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 07:45:10,753 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 07:45:10,872 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-180-Loss-3.9017712174178505.pdparams
2022-01-05 07:45:10,872 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-180-Loss-3.9017712174178505.pdopt
2022-01-05 07:45:10,910 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-180-Loss-3.9017712174178505-EMA.pdparams
2022-01-05 07:45:10,910 Now training epoch 181. LR=0.000344
2022-01-05 07:46:24,138 Epoch[181/310], Step[0000/1251], Loss: 4.0199(4.0199), Acc: 0.4414(0.4414)
2022-01-05 07:47:20,599 Epoch[181/310], Step[0050/1251], Loss: 4.3441(3.8929), Acc: 0.3203(0.3590)
2022-01-05 07:48:16,317 Epoch[181/310], Step[0100/1251], Loss: 3.6151(3.8984), Acc: 0.3906(0.3439)
2022-01-05 07:49:12,450 Epoch[181/310], Step[0150/1251], Loss: 3.9344(3.9006), Acc: 0.1729(0.3355)
2022-01-05 07:50:08,120 Epoch[181/310], Step[0200/1251], Loss: 4.2858(3.9045), Acc: 0.3818(0.3394)
2022-01-05 07:51:03,896 Epoch[181/310], Step[0250/1251], Loss: 4.2141(3.9152), Acc: 0.2686(0.3389)
2022-01-05 07:52:01,189 Epoch[181/310], Step[0300/1251], Loss: 3.2389(3.9079), Acc: 0.4189(0.3385)
2022-01-05 07:52:57,422 Epoch[181/310], Step[0350/1251], Loss: 3.1030(3.9044), Acc: 0.4170(0.3397)
2022-01-05 07:53:54,196 Epoch[181/310], Step[0400/1251], Loss: 3.3608(3.9008), Acc: 0.3193(0.3420)
2022-01-05 07:54:51,488 Epoch[181/310], Step[0450/1251], Loss: 3.9171(3.9010), Acc: 0.3477(0.3419)
2022-01-05 07:55:49,050 Epoch[181/310], Step[0500/1251], Loss: 3.8267(3.8966), Acc: 0.4004(0.3424)
2022-01-05 07:56:46,846 Epoch[181/310], Step[0550/1251], Loss: 4.2023(3.8964), Acc: 0.3867(0.3436)
2022-01-05 07:57:44,833 Epoch[181/310], Step[0600/1251], Loss: 4.3348(3.8898), Acc: 0.1836(0.3421)
2022-01-05 07:58:41,437 Epoch[181/310], Step[0650/1251], Loss: 4.1609(3.8937), Acc: 0.3750(0.3423)
2022-01-05 07:59:39,304 Epoch[181/310], Step[0700/1251], Loss: 4.0353(3.8952), Acc: 0.3730(0.3425)
2022-01-05 08:00:35,892 Epoch[181/310], Step[0750/1251], Loss: 4.3069(3.8963), Acc: 0.1934(0.3420)
2022-01-05 08:01:32,694 Epoch[181/310], Step[0800/1251], Loss: 3.9529(3.8927), Acc: 0.2217(0.3407)
2022-01-05 08:02:30,432 Epoch[181/310], Step[0850/1251], Loss: 4.2983(3.8924), Acc: 0.2021(0.3390)
2022-01-05 08:03:27,161 Epoch[181/310], Step[0900/1251], Loss: 4.0176(3.8914), Acc: 0.2627(0.3399)
2022-01-05 08:04:24,790 Epoch[181/310], Step[0950/1251], Loss: 4.0056(3.8942), Acc: 0.2256(0.3388)
2022-01-05 08:05:22,732 Epoch[181/310], Step[1000/1251], Loss: 4.1794(3.8953), Acc: 0.3232(0.3377)
2022-01-05 08:06:19,481 Epoch[181/310], Step[1050/1251], Loss: 4.2135(3.8941), Acc: 0.3252(0.3386)
2022-01-05 08:07:16,933 Epoch[181/310], Step[1100/1251], Loss: 4.1251(3.8959), Acc: 0.3242(0.3375)
2022-01-05 08:08:14,592 Epoch[181/310], Step[1150/1251], Loss: 3.8756(3.8946), Acc: 0.2334(0.3386)
2022-01-05 08:09:12,546 Epoch[181/310], Step[1200/1251], Loss: 3.5885(3.8910), Acc: 0.3594(0.3389)
2022-01-05 08:10:08,558 Epoch[181/310], Step[1250/1251], Loss: 3.8446(3.8913), Acc: 0.4785(0.3395)
2022-01-05 08:10:10,621 ----- Epoch[181/310], Train Loss: 3.8913, Train Acc: 0.3395, time: 1499.71, Best Val(epoch180) Acc@1: 0.6848
2022-01-05 08:10:10,807 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 08:10:10,808 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 08:10:10,917 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 08:10:10,917 Now training epoch 182. LR=0.000339
2022-01-05 08:11:27,219 Epoch[182/310], Step[0000/1251], Loss: 3.6587(3.6587), Acc: 0.5059(0.5059)
2022-01-05 08:12:24,727 Epoch[182/310], Step[0050/1251], Loss: 4.1998(3.8300), Acc: 0.2764(0.3668)
2022-01-05 08:13:20,997 Epoch[182/310], Step[0100/1251], Loss: 4.2187(3.8661), Acc: 0.3721(0.3507)
2022-01-05 08:14:17,525 Epoch[182/310], Step[0150/1251], Loss: 4.2079(3.8909), Acc: 0.2529(0.3448)
2022-01-05 08:15:14,493 Epoch[182/310], Step[0200/1251], Loss: 3.8325(3.8876), Acc: 0.2324(0.3449)
2022-01-05 08:16:11,940 Epoch[182/310], Step[0250/1251], Loss: 4.0346(3.8771), Acc: 0.3330(0.3469)
2022-01-05 08:17:09,590 Epoch[182/310], Step[0300/1251], Loss: 4.6636(3.8766), Acc: 0.2393(0.3429)
2022-01-05 08:18:05,600 Epoch[182/310], Step[0350/1251], Loss: 3.8477(3.8831), Acc: 0.4531(0.3403)
2022-01-05 08:19:03,443 Epoch[182/310], Step[0400/1251], Loss: 4.0725(3.8830), Acc: 0.3975(0.3420)
2022-01-05 08:20:00,972 Epoch[182/310], Step[0450/1251], Loss: 3.8177(3.8834), Acc: 0.3828(0.3417)
2022-01-05 08:20:57,150 Epoch[182/310], Step[0500/1251], Loss: 3.7678(3.8792), Acc: 0.4688(0.3389)
2022-01-05 08:21:53,843 Epoch[182/310], Step[0550/1251], Loss: 4.2648(3.8829), Acc: 0.3740(0.3404)
2022-01-05 08:22:51,595 Epoch[182/310], Step[0600/1251], Loss: 4.0616(3.8877), Acc: 0.3691(0.3398)
2022-01-05 08:23:50,002 Epoch[182/310], Step[0650/1251], Loss: 4.0240(3.8866), Acc: 0.4326(0.3408)
2022-01-05 08:24:48,712 Epoch[182/310], Step[0700/1251], Loss: 3.9205(3.8821), Acc: 0.3574(0.3410)
2022-01-05 08:25:46,409 Epoch[182/310], Step[0750/1251], Loss: 4.4200(3.8774), Acc: 0.2441(0.3408)
2022-01-05 08:26:43,210 Epoch[182/310], Step[0800/1251], Loss: 3.7957(3.8760), Acc: 0.1680(0.3402)
2022-01-05 08:27:40,517 Epoch[182/310], Step[0850/1251], Loss: 3.6127(3.8736), Acc: 0.1982(0.3389)
2022-01-05 08:28:38,789 Epoch[182/310], Step[0900/1251], Loss: 4.2497(3.8763), Acc: 0.1973(0.3386)
2022-01-05 08:29:36,955 Epoch[182/310], Step[0950/1251], Loss: 3.9770(3.8749), Acc: 0.4648(0.3375)
2022-01-05 08:30:34,990 Epoch[182/310], Step[1000/1251], Loss: 3.9855(3.8728), Acc: 0.0918(0.3358)
2022-01-05 08:31:32,285 Epoch[182/310], Step[1050/1251], Loss: 4.0920(3.8764), Acc: 0.3945(0.3353)
2022-01-05 08:32:29,450 Epoch[182/310], Step[1100/1251], Loss: 4.1090(3.8777), Acc: 0.2363(0.3347)
2022-01-05 08:33:27,097 Epoch[182/310], Step[1150/1251], Loss: 4.1722(3.8810), Acc: 0.3633(0.3346)
2022-01-05 08:34:23,969 Epoch[182/310], Step[1200/1251], Loss: 3.9536(3.8833), Acc: 0.2559(0.3346)
2022-01-05 08:35:21,504 Epoch[182/310], Step[1250/1251], Loss: 4.1660(3.8808), Acc: 0.2891(0.3352)
2022-01-05 08:35:23,461 ----- Validation after Epoch: 182
2022-01-05 08:36:17,819 Val Step[0000/1563], Loss: 0.8402 (0.8402), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2022-01-05 08:36:19,301 Val Step[0050/1563], Loss: 2.2352 (0.9053), Acc@1: 0.4062 (0.8156), Acc@5: 0.8750 (0.9467)
2022-01-05 08:36:20,767 Val Step[0100/1563], Loss: 2.2279 (1.1974), Acc@1: 0.4375 (0.7333), Acc@5: 0.8438 (0.9183)
2022-01-05 08:36:22,174 Val Step[0150/1563], Loss: 0.6058 (1.1235), Acc@1: 0.8438 (0.7519), Acc@5: 1.0000 (0.9251)
2022-01-05 08:36:23,610 Val Step[0200/1563], Loss: 1.4021 (1.1405), Acc@1: 0.6875 (0.7517), Acc@5: 0.9062 (0.9220)
2022-01-05 08:36:25,044 Val Step[0250/1563], Loss: 0.7102 (1.0835), Acc@1: 0.9062 (0.7654), Acc@5: 0.9688 (0.9287)
2022-01-05 08:36:26,685 Val Step[0300/1563], Loss: 0.8251 (1.1420), Acc@1: 0.7500 (0.7475), Acc@5: 0.9688 (0.9244)
2022-01-05 08:36:28,101 Val Step[0350/1563], Loss: 1.2113 (1.1556), Acc@1: 0.6875 (0.7421), Acc@5: 0.9062 (0.9261)
2022-01-05 08:36:29,646 Val Step[0400/1563], Loss: 1.0666 (1.1611), Acc@1: 0.8125 (0.7389), Acc@5: 0.9688 (0.9264)
2022-01-05 08:36:31,141 Val Step[0450/1563], Loss: 1.2789 (1.1661), Acc@1: 0.6250 (0.7357), Acc@5: 1.0000 (0.9270)
2022-01-05 08:36:32,615 Val Step[0500/1563], Loss: 0.4939 (1.1539), Acc@1: 0.8750 (0.7395), Acc@5: 1.0000 (0.9290)
2022-01-05 08:36:34,189 Val Step[0550/1563], Loss: 0.7075 (1.1273), Acc@1: 0.8438 (0.7467), Acc@5: 0.9688 (0.9311)
2022-01-05 08:36:35,732 Val Step[0600/1563], Loss: 0.8345 (1.1321), Acc@1: 0.7812 (0.7458), Acc@5: 0.9688 (0.9303)
2022-01-05 08:36:37,255 Val Step[0650/1563], Loss: 0.4843 (1.1500), Acc@1: 0.9375 (0.7422), Acc@5: 1.0000 (0.9273)
2022-01-05 08:36:38,723 Val Step[0700/1563], Loss: 1.1791 (1.1836), Acc@1: 0.7500 (0.7351), Acc@5: 0.8750 (0.9227)
2022-01-05 08:36:40,172 Val Step[0750/1563], Loss: 1.4586 (1.2194), Acc@1: 0.6875 (0.7280), Acc@5: 0.8438 (0.9177)
2022-01-05 08:36:41,553 Val Step[0800/1563], Loss: 1.1765 (1.2576), Acc@1: 0.7500 (0.7182), Acc@5: 0.9688 (0.9131)
2022-01-05 08:36:43,005 Val Step[0850/1563], Loss: 1.1953 (1.2802), Acc@1: 0.6875 (0.7131), Acc@5: 0.9375 (0.9101)
2022-01-05 08:36:44,513 Val Step[0900/1563], Loss: 0.3976 (1.2816), Acc@1: 0.9375 (0.7141), Acc@5: 1.0000 (0.9092)
2022-01-05 08:36:46,033 Val Step[0950/1563], Loss: 1.3781 (1.3021), Acc@1: 0.6562 (0.7104), Acc@5: 0.8750 (0.9059)
2022-01-05 08:36:47,463 Val Step[1000/1563], Loss: 0.6125 (1.3254), Acc@1: 0.9375 (0.7047), Acc@5: 0.9688 (0.9020)
2022-01-05 08:36:48,983 Val Step[1050/1563], Loss: 0.2999 (1.3423), Acc@1: 0.9688 (0.7005), Acc@5: 1.0000 (0.8997)
2022-01-05 08:36:50,332 Val Step[1100/1563], Loss: 1.0068 (1.3584), Acc@1: 0.7812 (0.6971), Acc@5: 0.9375 (0.8971)
2022-01-05 08:36:51,744 Val Step[1150/1563], Loss: 1.2672 (1.3736), Acc@1: 0.7812 (0.6939), Acc@5: 0.8125 (0.8948)
2022-01-05 08:36:53,177 Val Step[1200/1563], Loss: 1.1974 (1.3904), Acc@1: 0.8125 (0.6903), Acc@5: 0.8438 (0.8917)
2022-01-05 08:36:54,651 Val Step[1250/1563], Loss: 0.7144 (1.4042), Acc@1: 0.8438 (0.6878), Acc@5: 0.9375 (0.8893)
2022-01-05 08:36:56,075 Val Step[1300/1563], Loss: 0.9522 (1.4136), Acc@1: 0.8750 (0.6860), Acc@5: 0.9062 (0.8884)
2022-01-05 08:36:57,522 Val Step[1350/1563], Loss: 2.1449 (1.4322), Acc@1: 0.4062 (0.6819), Acc@5: 0.7500 (0.8855)
2022-01-05 08:36:59,165 Val Step[1400/1563], Loss: 1.3589 (1.4397), Acc@1: 0.7188 (0.6802), Acc@5: 0.8750 (0.8844)
2022-01-05 08:37:00,583 Val Step[1450/1563], Loss: 1.2361 (1.4463), Acc@1: 0.7500 (0.6786), Acc@5: 0.9375 (0.8839)
2022-01-05 08:37:02,029 Val Step[1500/1563], Loss: 2.0565 (1.4354), Acc@1: 0.4688 (0.6814), Acc@5: 0.8750 (0.8856)
2022-01-05 08:37:03,392 Val Step[1550/1563], Loss: 1.0597 (1.4380), Acc@1: 0.8750 (0.6810), Acc@5: 0.9062 (0.8852)
2022-01-05 08:37:04,202 ----- Epoch[182/310], Validation Loss: 1.4356, Validation Acc@1: 0.6815, Validation Acc@5: 0.8855, time: 100.74
2022-01-05 08:37:04,202 ----- Epoch[182/310], Train Loss: 3.8808, Train Acc: 0.3352, time: 1512.54, Best Val(epoch180) Acc@1: 0.6848
2022-01-05 08:37:04,382 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 08:37:04,383 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 08:37:04,515 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 08:37:04,516 Now training epoch 183. LR=0.000334
2022-01-05 08:38:14,758 Epoch[183/310], Step[0000/1251], Loss: 3.6612(3.6612), Acc: 0.2637(0.2637)
2022-01-05 08:39:09,357 Epoch[183/310], Step[0050/1251], Loss: 3.4134(3.8528), Acc: 0.5254(0.3383)
2022-01-05 08:40:05,537 Epoch[183/310], Step[0100/1251], Loss: 4.1744(3.8199), Acc: 0.2861(0.3477)
2022-01-05 08:41:02,516 Epoch[183/310], Step[0150/1251], Loss: 4.2426(3.8430), Acc: 0.4150(0.3394)
2022-01-05 08:41:58,401 Epoch[183/310], Step[0200/1251], Loss: 3.9500(3.8339), Acc: 0.4932(0.3425)
2022-01-05 08:42:53,748 Epoch[183/310], Step[0250/1251], Loss: 3.9022(3.8485), Acc: 0.4072(0.3400)
2022-01-05 08:43:51,401 Epoch[183/310], Step[0300/1251], Loss: 4.4329(3.8577), Acc: 0.1738(0.3393)
2022-01-05 08:44:48,634 Epoch[183/310], Step[0350/1251], Loss: 4.2173(3.8597), Acc: 0.1240(0.3401)
2022-01-05 08:45:47,198 Epoch[183/310], Step[0400/1251], Loss: 4.2522(3.8607), Acc: 0.3164(0.3432)
2022-01-05 08:46:44,322 Epoch[183/310], Step[0450/1251], Loss: 3.8319(3.8701), Acc: 0.3604(0.3403)
2022-01-05 08:47:41,193 Epoch[183/310], Step[0500/1251], Loss: 3.9889(3.8671), Acc: 0.2705(0.3419)
2022-01-05 08:48:38,255 Epoch[183/310], Step[0550/1251], Loss: 3.6674(3.8719), Acc: 0.3652(0.3383)
2022-01-05 08:49:34,643 Epoch[183/310], Step[0600/1251], Loss: 3.4560(3.8639), Acc: 0.3760(0.3378)
2022-01-05 08:50:30,887 Epoch[183/310], Step[0650/1251], Loss: 4.2082(3.8676), Acc: 0.2471(0.3371)
2022-01-05 08:51:29,137 Epoch[183/310], Step[0700/1251], Loss: 3.6881(3.8699), Acc: 0.3740(0.3365)
2022-01-05 08:52:25,512 Epoch[183/310], Step[0750/1251], Loss: 3.9307(3.8677), Acc: 0.4043(0.3382)
2022-01-05 08:53:23,147 Epoch[183/310], Step[0800/1251], Loss: 4.0305(3.8715), Acc: 0.3330(0.3380)
2022-01-05 08:54:20,682 Epoch[183/310], Step[0850/1251], Loss: 4.0741(3.8740), Acc: 0.2168(0.3374)
2022-01-05 08:55:19,156 Epoch[183/310], Step[0900/1251], Loss: 4.0653(3.8732), Acc: 0.1895(0.3380)
2022-01-05 08:56:17,648 Epoch[183/310], Step[0950/1251], Loss: 3.5718(3.8707), Acc: 0.5088(0.3386)
2022-01-05 08:57:15,871 Epoch[183/310], Step[1000/1251], Loss: 3.3874(3.8726), Acc: 0.4092(0.3386)
2022-01-05 08:58:13,013 Epoch[183/310], Step[1050/1251], Loss: 3.8611(3.8728), Acc: 0.3672(0.3385)
2022-01-05 08:59:10,525 Epoch[183/310], Step[1100/1251], Loss: 3.8246(3.8713), Acc: 0.0801(0.3382)
2022-01-05 09:00:07,002 Epoch[183/310], Step[1150/1251], Loss: 3.9003(3.8749), Acc: 0.3545(0.3388)
2022-01-05 09:01:03,614 Epoch[183/310], Step[1200/1251], Loss: 4.0396(3.8717), Acc: 0.2627(0.3393)
2022-01-05 09:01:59,077 Epoch[183/310], Step[1250/1251], Loss: 4.0308(3.8692), Acc: 0.1895(0.3389)
2022-01-05 09:02:01,029 ----- Epoch[183/310], Train Loss: 3.8692, Train Acc: 0.3389, time: 1496.51, Best Val(epoch180) Acc@1: 0.6848
2022-01-05 09:02:01,199 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 09:02:01,199 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 09:02:01,308 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 09:02:01,308 Now training epoch 184. LR=0.000329
2022-01-05 09:03:16,142 Epoch[184/310], Step[0000/1251], Loss: 4.0094(4.0094), Acc: 0.3457(0.3457)
2022-01-05 09:04:10,380 Epoch[184/310], Step[0050/1251], Loss: 3.9193(3.9334), Acc: 0.3711(0.3228)
2022-01-05 09:05:05,850 Epoch[184/310], Step[0100/1251], Loss: 4.3617(3.9344), Acc: 0.3252(0.3323)
2022-01-05 09:06:02,237 Epoch[184/310], Step[0150/1251], Loss: 4.3138(3.9205), Acc: 0.3389(0.3300)
2022-01-05 09:07:00,040 Epoch[184/310], Step[0200/1251], Loss: 3.6274(3.9070), Acc: 0.3984(0.3327)
2022-01-05 09:07:58,205 Epoch[184/310], Step[0250/1251], Loss: 4.0697(3.9101), Acc: 0.2861(0.3364)
2022-01-05 09:08:55,163 Epoch[184/310], Step[0300/1251], Loss: 4.1663(3.9002), Acc: 0.3955(0.3413)
2022-01-05 09:09:52,233 Epoch[184/310], Step[0350/1251], Loss: 4.1038(3.9027), Acc: 0.1309(0.3403)
2022-01-05 09:10:49,369 Epoch[184/310], Step[0400/1251], Loss: 3.6378(3.8954), Acc: 0.2920(0.3409)
2022-01-05 09:11:46,151 Epoch[184/310], Step[0450/1251], Loss: 3.8800(3.8927), Acc: 0.3379(0.3390)
2022-01-05 09:12:43,223 Epoch[184/310], Step[0500/1251], Loss: 3.6512(3.8920), Acc: 0.4375(0.3374)
2022-01-05 09:13:40,880 Epoch[184/310], Step[0550/1251], Loss: 4.1269(3.8886), Acc: 0.1826(0.3364)
2022-01-05 09:14:39,236 Epoch[184/310], Step[0600/1251], Loss: 3.9308(3.8869), Acc: 0.3535(0.3371)
2022-01-05 09:15:36,210 Epoch[184/310], Step[0650/1251], Loss: 4.1876(3.8948), Acc: 0.4473(0.3373)
2022-01-05 09:16:33,016 Epoch[184/310], Step[0700/1251], Loss: 3.9301(3.8901), Acc: 0.4678(0.3374)
2022-01-05 09:17:28,761 Epoch[184/310], Step[0750/1251], Loss: 3.9725(3.8851), Acc: 0.3955(0.3396)
2022-01-05 09:18:24,760 Epoch[184/310], Step[0800/1251], Loss: 3.7732(3.8803), Acc: 0.4766(0.3406)
2022-01-05 09:19:21,712 Epoch[184/310], Step[0850/1251], Loss: 4.1889(3.8839), Acc: 0.2988(0.3402)
2022-01-05 09:20:19,585 Epoch[184/310], Step[0900/1251], Loss: 3.6965(3.8860), Acc: 0.2705(0.3395)
2022-01-05 09:21:16,819 Epoch[184/310], Step[0950/1251], Loss: 4.2335(3.8885), Acc: 0.4082(0.3393)
2022-01-05 09:22:15,130 Epoch[184/310], Step[1000/1251], Loss: 3.9665(3.8877), Acc: 0.3105(0.3386)
2022-01-05 09:23:13,240 Epoch[184/310], Step[1050/1251], Loss: 3.8145(3.8883), Acc: 0.2910(0.3389)
2022-01-05 09:24:11,423 Epoch[184/310], Step[1100/1251], Loss: 3.5955(3.8898), Acc: 0.4551(0.3391)
2022-01-05 09:25:08,278 Epoch[184/310], Step[1150/1251], Loss: 4.2306(3.8879), Acc: 0.3154(0.3388)
2022-01-05 09:26:05,475 Epoch[184/310], Step[1200/1251], Loss: 3.6455(3.8855), Acc: 0.4326(0.3388)
2022-01-05 09:27:01,219 Epoch[184/310], Step[1250/1251], Loss: 4.0971(3.8835), Acc: 0.2500(0.3393)
2022-01-05 09:27:03,327 ----- Validation after Epoch: 184
2022-01-05 09:27:57,061 Val Step[0000/1563], Loss: 0.7826 (0.7826), Acc@1: 0.8750 (0.8750), Acc@5: 0.9375 (0.9375)
2022-01-05 09:27:58,742 Val Step[0050/1563], Loss: 2.2581 (0.8470), Acc@1: 0.4062 (0.8254), Acc@5: 0.8750 (0.9504)
2022-01-05 09:28:00,134 Val Step[0100/1563], Loss: 2.1071 (1.1128), Acc@1: 0.5312 (0.7472), Acc@5: 0.8438 (0.9257)
2022-01-05 09:28:01,528 Val Step[0150/1563], Loss: 0.5956 (1.0598), Acc@1: 0.9062 (0.7628), Acc@5: 1.0000 (0.9298)
2022-01-05 09:28:02,990 Val Step[0200/1563], Loss: 1.4257 (1.0786), Acc@1: 0.5938 (0.7635), Acc@5: 0.9062 (0.9272)
2022-01-05 09:28:04,451 Val Step[0250/1563], Loss: 1.0298 (1.0266), Acc@1: 0.8438 (0.7759), Acc@5: 0.9688 (0.9331)
2022-01-05 09:28:05,934 Val Step[0300/1563], Loss: 1.0312 (1.0953), Acc@1: 0.7812 (0.7571), Acc@5: 0.9688 (0.9282)
2022-01-05 09:28:07,378 Val Step[0350/1563], Loss: 1.6306 (1.1149), Acc@1: 0.6250 (0.7489), Acc@5: 0.8750 (0.9292)
2022-01-05 09:28:08,807 Val Step[0400/1563], Loss: 1.0602 (1.1201), Acc@1: 0.8438 (0.7444), Acc@5: 0.9688 (0.9302)
2022-01-05 09:28:10,273 Val Step[0450/1563], Loss: 0.9203 (1.1248), Acc@1: 0.6250 (0.7422), Acc@5: 1.0000 (0.9308)
2022-01-05 09:28:11,788 Val Step[0500/1563], Loss: 0.4632 (1.1127), Acc@1: 0.9375 (0.7452), Acc@5: 1.0000 (0.9323)
2022-01-05 09:28:13,218 Val Step[0550/1563], Loss: 0.8981 (1.0895), Acc@1: 0.7812 (0.7513), Acc@5: 0.9688 (0.9346)
2022-01-05 09:28:14,741 Val Step[0600/1563], Loss: 1.0109 (1.0952), Acc@1: 0.7812 (0.7507), Acc@5: 0.9375 (0.9338)
2022-01-05 09:28:16,189 Val Step[0650/1563], Loss: 0.5641 (1.1154), Acc@1: 0.9062 (0.7468), Acc@5: 1.0000 (0.9310)
2022-01-05 09:28:17,601 Val Step[0700/1563], Loss: 1.0316 (1.1464), Acc@1: 0.7812 (0.7389), Acc@5: 0.9375 (0.9271)
2022-01-05 09:28:19,091 Val Step[0750/1563], Loss: 1.3795 (1.1824), Acc@1: 0.7188 (0.7319), Acc@5: 0.8438 (0.9215)
2022-01-05 09:28:20,489 Val Step[0800/1563], Loss: 1.3547 (1.2228), Acc@1: 0.6875 (0.7223), Acc@5: 0.9375 (0.9157)
2022-01-05 09:28:21,960 Val Step[0850/1563], Loss: 1.3309 (1.2494), Acc@1: 0.6562 (0.7160), Acc@5: 0.9062 (0.9119)
2022-01-05 09:28:23,490 Val Step[0900/1563], Loss: 0.3893 (1.2496), Acc@1: 0.9688 (0.7172), Acc@5: 1.0000 (0.9113)
2022-01-05 09:28:24,994 Val Step[0950/1563], Loss: 1.8136 (1.2744), Acc@1: 0.5625 (0.7125), Acc@5: 0.8125 (0.9077)
2022-01-05 09:28:26,377 Val Step[1000/1563], Loss: 0.5875 (1.2986), Acc@1: 0.9062 (0.7069), Acc@5: 0.9688 (0.9042)
2022-01-05 09:28:27,766 Val Step[1050/1563], Loss: 0.5626 (1.3134), Acc@1: 0.9375 (0.7037), Acc@5: 0.9688 (0.9019)
2022-01-05 09:28:29,134 Val Step[1100/1563], Loss: 1.2362 (1.3285), Acc@1: 0.7500 (0.7002), Acc@5: 0.9375 (0.8997)
2022-01-05 09:28:30,493 Val Step[1150/1563], Loss: 1.3344 (1.3438), Acc@1: 0.7188 (0.6973), Acc@5: 0.8438 (0.8975)
2022-01-05 09:28:31,914 Val Step[1200/1563], Loss: 1.3091 (1.3605), Acc@1: 0.8125 (0.6938), Acc@5: 0.8438 (0.8948)
2022-01-05 09:28:33,303 Val Step[1250/1563], Loss: 0.6845 (1.3768), Acc@1: 0.9062 (0.6908), Acc@5: 0.9375 (0.8923)
2022-01-05 09:28:34,802 Val Step[1300/1563], Loss: 1.0897 (1.3875), Acc@1: 0.7812 (0.6884), Acc@5: 0.8750 (0.8909)
2022-01-05 09:28:36,176 Val Step[1350/1563], Loss: 2.3802 (1.4064), Acc@1: 0.2500 (0.6844), Acc@5: 0.7188 (0.8877)
2022-01-05 09:28:37,631 Val Step[1400/1563], Loss: 1.2170 (1.4143), Acc@1: 0.7188 (0.6833), Acc@5: 0.9062 (0.8866)
2022-01-05 09:28:39,118 Val Step[1450/1563], Loss: 1.5528 (1.4215), Acc@1: 0.6250 (0.6817), Acc@5: 0.8750 (0.8861)
2022-01-05 09:28:40,643 Val Step[1500/1563], Loss: 1.7167 (1.4105), Acc@1: 0.5938 (0.6845), Acc@5: 0.8125 (0.8876)
2022-01-05 09:28:42,187 Val Step[1550/1563], Loss: 1.0708 (1.4113), Acc@1: 0.8750 (0.6845), Acc@5: 0.9062 (0.8871)
2022-01-05 09:28:42,998 ----- Epoch[184/310], Validation Loss: 1.4088, Validation Acc@1: 0.6848, Validation Acc@5: 0.8874, time: 99.67
2022-01-05 09:28:42,998 ----- Epoch[184/310], Train Loss: 3.8835, Train Acc: 0.3393, time: 1502.01, Best Val(epoch184) Acc@1: 0.6848
2022-01-05 09:28:43,179 Max accuracy so far: 0.6848 at epoch_184
2022-01-05 09:28:43,180 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 09:28:43,180 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 09:28:43,296 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 09:28:43,687 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 09:28:43,688 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 09:28:43,802 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 09:28:43,803 Now training epoch 185. LR=0.000324
2022-01-05 09:29:57,975 Epoch[185/310], Step[0000/1251], Loss: 4.1576(4.1576), Acc: 0.3945(0.3945)
2022-01-05 09:30:55,796 Epoch[185/310], Step[0050/1251], Loss: 4.0766(3.9132), Acc: 0.3008(0.3118)
2022-01-05 09:31:53,883 Epoch[185/310], Step[0100/1251], Loss: 4.5231(3.9031), Acc: 0.2949(0.3275)
2022-01-05 09:32:51,313 Epoch[185/310], Step[0150/1251], Loss: 4.0590(3.8586), Acc: 0.4277(0.3303)
2022-01-05 09:33:48,233 Epoch[185/310], Step[0200/1251], Loss: 3.0961(3.8463), Acc: 0.2793(0.3299)
2022-01-05 09:34:46,887 Epoch[185/310], Step[0250/1251], Loss: 3.9987(3.8456), Acc: 0.2393(0.3329)
2022-01-05 09:35:44,381 Epoch[185/310], Step[0300/1251], Loss: 3.7802(3.8518), Acc: 0.3203(0.3343)
2022-01-05 09:36:41,413 Epoch[185/310], Step[0350/1251], Loss: 3.8070(3.8632), Acc: 0.3584(0.3371)
2022-01-05 09:37:38,117 Epoch[185/310], Step[0400/1251], Loss: 3.3064(3.8583), Acc: 0.4414(0.3402)
2022-01-05 09:38:34,568 Epoch[185/310], Step[0450/1251], Loss: 3.6199(3.8534), Acc: 0.2021(0.3408)
2022-01-05 09:39:32,158 Epoch[185/310], Step[0500/1251], Loss: 4.3781(3.8565), Acc: 0.2676(0.3393)
2022-01-05 09:40:29,197 Epoch[185/310], Step[0550/1251], Loss: 3.2737(3.8522), Acc: 0.5527(0.3426)
2022-01-05 09:41:25,714 Epoch[185/310], Step[0600/1251], Loss: 3.3285(3.8508), Acc: 0.1328(0.3434)
2022-01-05 09:42:23,203 Epoch[185/310], Step[0650/1251], Loss: 4.4273(3.8510), Acc: 0.2334(0.3433)
2022-01-05 09:43:21,003 Epoch[185/310], Step[0700/1251], Loss: 3.7437(3.8549), Acc: 0.5146(0.3423)
2022-01-05 09:44:17,748 Epoch[185/310], Step[0750/1251], Loss: 3.8536(3.8561), Acc: 0.4883(0.3421)
2022-01-05 09:45:14,613 Epoch[185/310], Step[0800/1251], Loss: 3.6647(3.8600), Acc: 0.3867(0.3427)
2022-01-05 09:46:10,504 Epoch[185/310], Step[0850/1251], Loss: 4.0113(3.8660), Acc: 0.3057(0.3421)
2022-01-05 09:47:07,029 Epoch[185/310], Step[0900/1251], Loss: 3.8208(3.8700), Acc: 0.4395(0.3425)
2022-01-05 09:48:03,660 Epoch[185/310], Step[0950/1251], Loss: 4.4643(3.8656), Acc: 0.3350(0.3437)
2022-01-05 09:49:00,459 Epoch[185/310], Step[1000/1251], Loss: 3.3912(3.8698), Acc: 0.4268(0.3439)
2022-01-05 09:49:57,659 Epoch[185/310], Step[1050/1251], Loss: 4.1365(3.8721), Acc: 0.2900(0.3438)
2022-01-05 09:50:54,060 Epoch[185/310], Step[1100/1251], Loss: 3.8726(3.8730), Acc: 0.2900(0.3445)
2022-01-05 09:51:49,436 Epoch[185/310], Step[1150/1251], Loss: 4.0717(3.8731), Acc: 0.2002(0.3444)
2022-01-05 09:52:45,199 Epoch[185/310], Step[1200/1251], Loss: 4.2627(3.8732), Acc: 0.3242(0.3441)
2022-01-05 09:53:41,806 Epoch[185/310], Step[1250/1251], Loss: 3.5514(3.8729), Acc: 0.5098(0.3438)
2022-01-05 09:53:43,791 ----- Epoch[185/310], Train Loss: 3.8729, Train Acc: 0.3438, time: 1499.99, Best Val(epoch184) Acc@1: 0.6848
2022-01-05 09:53:43,963 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 09:53:43,964 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 09:53:44,068 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 09:53:44,068 Now training epoch 186. LR=0.000319
2022-01-05 09:54:55,931 Epoch[186/310], Step[0000/1251], Loss: 4.2264(4.2264), Acc: 0.3105(0.3105)
2022-01-05 09:55:52,497 Epoch[186/310], Step[0050/1251], Loss: 4.2062(3.8620), Acc: 0.4277(0.3365)
2022-01-05 09:56:48,751 Epoch[186/310], Step[0100/1251], Loss: 3.6381(3.8871), Acc: 0.1621(0.3346)
2022-01-05 09:57:45,029 Epoch[186/310], Step[0150/1251], Loss: 4.2149(3.8753), Acc: 0.0537(0.3381)
2022-01-05 09:58:42,413 Epoch[186/310], Step[0200/1251], Loss: 4.1049(3.8602), Acc: 0.3467(0.3384)
2022-01-05 09:59:38,953 Epoch[186/310], Step[0250/1251], Loss: 3.6285(3.8575), Acc: 0.4805(0.3397)
2022-01-05 10:00:36,445 Epoch[186/310], Step[0300/1251], Loss: 3.2196(3.8825), Acc: 0.4434(0.3383)
2022-01-05 10:01:33,378 Epoch[186/310], Step[0350/1251], Loss: 3.9375(3.8721), Acc: 0.4189(0.3381)
2022-01-05 10:02:30,031 Epoch[186/310], Step[0400/1251], Loss: 3.8604(3.8703), Acc: 0.3643(0.3408)
2022-01-05 10:03:25,298 Epoch[186/310], Step[0450/1251], Loss: 4.5005(3.8705), Acc: 0.3291(0.3409)
2022-01-05 10:04:20,293 Epoch[186/310], Step[0500/1251], Loss: 3.9243(3.8671), Acc: 0.4043(0.3396)
2022-01-05 10:05:16,188 Epoch[186/310], Step[0550/1251], Loss: 3.9964(3.8687), Acc: 0.3711(0.3422)
2022-01-05 10:06:13,614 Epoch[186/310], Step[0600/1251], Loss: 3.7927(3.8670), Acc: 0.3799(0.3426)
2022-01-05 10:07:11,221 Epoch[186/310], Step[0650/1251], Loss: 3.7130(3.8651), Acc: 0.4219(0.3446)
2022-01-05 10:08:09,144 Epoch[186/310], Step[0700/1251], Loss: 4.1545(3.8658), Acc: 0.1396(0.3436)
2022-01-05 10:09:07,245 Epoch[186/310], Step[0750/1251], Loss: 4.0190(3.8629), Acc: 0.4014(0.3432)
2022-01-05 10:10:05,154 Epoch[186/310], Step[0800/1251], Loss: 4.6037(3.8653), Acc: 0.1240(0.3419)
2022-01-05 10:11:02,901 Epoch[186/310], Step[0850/1251], Loss: 4.6116(3.8680), Acc: 0.2793(0.3402)
2022-01-05 10:11:59,611 Epoch[186/310], Step[0900/1251], Loss: 4.2068(3.8665), Acc: 0.4326(0.3413)
2022-01-05 10:12:57,252 Epoch[186/310], Step[0950/1251], Loss: 3.7630(3.8620), Acc: 0.4658(0.3417)
2022-01-05 10:13:55,007 Epoch[186/310], Step[1000/1251], Loss: 3.9723(3.8613), Acc: 0.3408(0.3417)
2022-01-05 10:14:52,493 Epoch[186/310], Step[1050/1251], Loss: 4.3159(3.8653), Acc: 0.3066(0.3409)
2022-01-05 10:15:50,618 Epoch[186/310], Step[1100/1251], Loss: 3.8069(3.8691), Acc: 0.4443(0.3400)
2022-01-05 10:16:47,160 Epoch[186/310], Step[1150/1251], Loss: 3.7156(3.8681), Acc: 0.2197(0.3415)
2022-01-05 10:17:44,631 Epoch[186/310], Step[1200/1251], Loss: 3.2309(3.8667), Acc: 0.5166(0.3413)
2022-01-05 10:18:41,556 Epoch[186/310], Step[1250/1251], Loss: 4.2156(3.8691), Acc: 0.2959(0.3409)
2022-01-05 10:18:43,585 ----- Validation after Epoch: 186
2022-01-05 10:19:41,284 Val Step[0000/1563], Loss: 0.7300 (0.7300), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-05 10:19:42,772 Val Step[0050/1563], Loss: 2.3666 (0.8631), Acc@1: 0.4375 (0.8284), Acc@5: 0.8125 (0.9485)
2022-01-05 10:19:44,262 Val Step[0100/1563], Loss: 1.9604 (1.1588), Acc@1: 0.5625 (0.7407), Acc@5: 0.8438 (0.9202)
2022-01-05 10:19:45,769 Val Step[0150/1563], Loss: 0.4994 (1.0986), Acc@1: 0.9062 (0.7552), Acc@5: 1.0000 (0.9253)
2022-01-05 10:19:47,183 Val Step[0200/1563], Loss: 0.9651 (1.1083), Acc@1: 0.8125 (0.7592), Acc@5: 0.9375 (0.9229)
2022-01-05 10:19:48,572 Val Step[0250/1563], Loss: 0.7077 (1.0573), Acc@1: 0.9062 (0.7723), Acc@5: 0.9688 (0.9292)
2022-01-05 10:19:50,067 Val Step[0300/1563], Loss: 1.3666 (1.1293), Acc@1: 0.6562 (0.7512), Acc@5: 0.9062 (0.9237)
2022-01-05 10:19:51,647 Val Step[0350/1563], Loss: 1.0939 (1.1393), Acc@1: 0.7812 (0.7463), Acc@5: 0.9062 (0.9253)
2022-01-05 10:19:53,043 Val Step[0400/1563], Loss: 1.2851 (1.1454), Acc@1: 0.7812 (0.7417), Acc@5: 0.9375 (0.9262)
2022-01-05 10:19:54,533 Val Step[0450/1563], Loss: 1.1557 (1.1491), Acc@1: 0.5938 (0.7387), Acc@5: 1.0000 (0.9275)
2022-01-05 10:19:56,091 Val Step[0500/1563], Loss: 0.4584 (1.1381), Acc@1: 0.9062 (0.7419), Acc@5: 1.0000 (0.9295)
2022-01-05 10:19:57,598 Val Step[0550/1563], Loss: 0.9075 (1.1106), Acc@1: 0.7500 (0.7486), Acc@5: 0.9688 (0.9322)
2022-01-05 10:19:59,214 Val Step[0600/1563], Loss: 1.0453 (1.1141), Acc@1: 0.7812 (0.7484), Acc@5: 0.9062 (0.9316)
2022-01-05 10:20:00,740 Val Step[0650/1563], Loss: 0.8808 (1.1335), Acc@1: 0.7188 (0.7439), Acc@5: 1.0000 (0.9288)
2022-01-05 10:20:02,115 Val Step[0700/1563], Loss: 1.2388 (1.1646), Acc@1: 0.7812 (0.7367), Acc@5: 0.8750 (0.9253)
2022-01-05 10:20:03,621 Val Step[0750/1563], Loss: 1.6366 (1.1985), Acc@1: 0.5938 (0.7300), Acc@5: 0.9062 (0.9209)
2022-01-05 10:20:05,012 Val Step[0800/1563], Loss: 1.0498 (1.2389), Acc@1: 0.7188 (0.7203), Acc@5: 0.9688 (0.9154)
2022-01-05 10:20:06,499 Val Step[0850/1563], Loss: 1.4027 (1.2650), Acc@1: 0.6250 (0.7147), Acc@5: 0.9375 (0.9115)
2022-01-05 10:20:07,907 Val Step[0900/1563], Loss: 0.3136 (1.2642), Acc@1: 0.9688 (0.7167), Acc@5: 1.0000 (0.9109)
2022-01-05 10:20:09,455 Val Step[0950/1563], Loss: 1.5522 (1.2841), Acc@1: 0.6250 (0.7125), Acc@5: 0.8750 (0.9080)
2022-01-05 10:20:10,893 Val Step[1000/1563], Loss: 0.5724 (1.3098), Acc@1: 0.9688 (0.7062), Acc@5: 1.0000 (0.9043)
2022-01-05 10:20:12,348 Val Step[1050/1563], Loss: 0.4542 (1.3252), Acc@1: 0.9688 (0.7031), Acc@5: 1.0000 (0.9022)
2022-01-05 10:20:13,746 Val Step[1100/1563], Loss: 1.2499 (1.3405), Acc@1: 0.7188 (0.6999), Acc@5: 0.9375 (0.8999)
2022-01-05 10:20:15,169 Val Step[1150/1563], Loss: 1.3312 (1.3580), Acc@1: 0.7812 (0.6965), Acc@5: 0.8125 (0.8972)
2022-01-05 10:20:16,595 Val Step[1200/1563], Loss: 1.1744 (1.3748), Acc@1: 0.8125 (0.6930), Acc@5: 0.8438 (0.8944)
2022-01-05 10:20:18,001 Val Step[1250/1563], Loss: 0.8874 (1.3879), Acc@1: 0.8438 (0.6908), Acc@5: 0.9062 (0.8922)
2022-01-05 10:20:19,431 Val Step[1300/1563], Loss: 1.2641 (1.3984), Acc@1: 0.7812 (0.6886), Acc@5: 0.9062 (0.8908)
2022-01-05 10:20:20,863 Val Step[1350/1563], Loss: 1.7860 (1.4150), Acc@1: 0.5625 (0.6853), Acc@5: 0.8750 (0.8882)
2022-01-05 10:20:22,339 Val Step[1400/1563], Loss: 1.2275 (1.4216), Acc@1: 0.7188 (0.6841), Acc@5: 0.9062 (0.8872)
2022-01-05 10:20:23,798 Val Step[1450/1563], Loss: 1.5088 (1.4289), Acc@1: 0.7188 (0.6823), Acc@5: 0.9375 (0.8868)
2022-01-05 10:20:25,194 Val Step[1500/1563], Loss: 1.6336 (1.4182), Acc@1: 0.5625 (0.6851), Acc@5: 0.8750 (0.8883)
2022-01-05 10:20:26,635 Val Step[1550/1563], Loss: 1.0221 (1.4184), Acc@1: 0.8750 (0.6851), Acc@5: 0.9062 (0.8883)
2022-01-05 10:20:27,576 ----- Epoch[186/310], Validation Loss: 1.4155, Validation Acc@1: 0.6856, Validation Acc@5: 0.8886, time: 103.99
2022-01-05 10:20:27,576 ----- Epoch[186/310], Train Loss: 3.8691, Train Acc: 0.3409, time: 1499.51, Best Val(epoch186) Acc@1: 0.6856
2022-01-05 10:20:27,756 Max accuracy so far: 0.6856 at epoch_186
2022-01-05 10:20:27,756 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 10:20:27,756 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 10:20:27,864 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 10:20:28,280 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 10:20:28,281 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 10:20:28,396 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 10:20:28,396 Now training epoch 187. LR=0.000315
2022-01-05 10:21:46,937 Epoch[187/310], Step[0000/1251], Loss: 3.4405(3.4405), Acc: 0.5674(0.5674)
2022-01-05 10:22:44,478 Epoch[187/310], Step[0050/1251], Loss: 3.7685(3.8492), Acc: 0.3447(0.3575)
2022-01-05 10:23:42,055 Epoch[187/310], Step[0100/1251], Loss: 3.6334(3.8510), Acc: 0.5537(0.3498)
2022-01-05 10:24:38,389 Epoch[187/310], Step[0150/1251], Loss: 3.8297(3.8233), Acc: 0.4707(0.3551)
2022-01-05 10:25:36,052 Epoch[187/310], Step[0200/1251], Loss: 2.9542(3.8203), Acc: 0.2852(0.3556)
2022-01-05 10:26:34,199 Epoch[187/310], Step[0250/1251], Loss: 3.9549(3.8183), Acc: 0.1289(0.3537)
2022-01-05 10:27:30,160 Epoch[187/310], Step[0300/1251], Loss: 3.6660(3.8207), Acc: 0.2500(0.3533)
2022-01-05 10:28:27,274 Epoch[187/310], Step[0350/1251], Loss: 3.7797(3.8344), Acc: 0.2031(0.3491)
2022-01-05 10:29:22,206 Epoch[187/310], Step[0400/1251], Loss: 4.0507(3.8366), Acc: 0.4492(0.3492)
2022-01-05 10:30:17,172 Epoch[187/310], Step[0450/1251], Loss: 3.6531(3.8425), Acc: 0.3574(0.3468)
2022-01-05 10:31:13,940 Epoch[187/310], Step[0500/1251], Loss: 3.7467(3.8426), Acc: 0.4727(0.3444)
2022-01-05 10:32:10,086 Epoch[187/310], Step[0550/1251], Loss: 3.2194(3.8460), Acc: 0.2676(0.3444)
2022-01-05 10:33:05,239 Epoch[187/310], Step[0600/1251], Loss: 3.7501(3.8476), Acc: 0.3867(0.3420)
2022-01-05 10:34:02,919 Epoch[187/310], Step[0650/1251], Loss: 4.2876(3.8494), Acc: 0.2188(0.3401)
2022-01-05 10:35:00,331 Epoch[187/310], Step[0700/1251], Loss: 3.2276(3.8474), Acc: 0.4902(0.3405)
2022-01-05 10:35:56,548 Epoch[187/310], Step[0750/1251], Loss: 3.7112(3.8481), Acc: 0.3672(0.3403)
2022-01-05 10:36:52,121 Epoch[187/310], Step[0800/1251], Loss: 3.6219(3.8487), Acc: 0.5225(0.3414)
2022-01-05 10:37:48,607 Epoch[187/310], Step[0850/1251], Loss: 3.5228(3.8493), Acc: 0.4131(0.3414)
2022-01-05 10:38:44,224 Epoch[187/310], Step[0900/1251], Loss: 3.5770(3.8502), Acc: 0.4053(0.3416)
2022-01-05 10:39:40,871 Epoch[187/310], Step[0950/1251], Loss: 4.0131(3.8538), Acc: 0.3447(0.3411)
2022-01-05 10:40:37,554 Epoch[187/310], Step[1000/1251], Loss: 3.9977(3.8525), Acc: 0.2695(0.3412)
2022-01-05 10:41:33,411 Epoch[187/310], Step[1050/1251], Loss: 3.8952(3.8550), Acc: 0.3877(0.3418)
2022-01-05 10:42:30,402 Epoch[187/310], Step[1100/1251], Loss: 3.7477(3.8549), Acc: 0.3262(0.3409)
2022-01-05 10:43:26,853 Epoch[187/310], Step[1150/1251], Loss: 3.7518(3.8530), Acc: 0.3643(0.3410)
2022-01-05 10:44:24,945 Epoch[187/310], Step[1200/1251], Loss: 3.6103(3.8528), Acc: 0.3809(0.3407)
2022-01-05 10:45:21,166 Epoch[187/310], Step[1250/1251], Loss: 3.4895(3.8488), Acc: 0.5352(0.3424)
2022-01-05 10:45:23,104 ----- Epoch[187/310], Train Loss: 3.8488, Train Acc: 0.3424, time: 1494.70, Best Val(epoch186) Acc@1: 0.6856
2022-01-05 10:45:23,283 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 10:45:23,283 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 10:45:23,408 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 10:45:23,409 Now training epoch 188. LR=0.000310
2022-01-05 10:46:38,245 Epoch[188/310], Step[0000/1251], Loss: 4.2902(4.2902), Acc: 0.2832(0.2832)
2022-01-05 10:47:34,916 Epoch[188/310], Step[0050/1251], Loss: 4.0921(3.9383), Acc: 0.1855(0.3311)
2022-01-05 10:48:32,564 Epoch[188/310], Step[0100/1251], Loss: 4.1997(3.8944), Acc: 0.3438(0.3408)
2022-01-05 10:49:28,562 Epoch[188/310], Step[0150/1251], Loss: 3.7965(3.8855), Acc: 0.3232(0.3465)
2022-01-05 10:50:26,400 Epoch[188/310], Step[0200/1251], Loss: 3.9557(3.8744), Acc: 0.4600(0.3409)
2022-01-05 10:51:23,474 Epoch[188/310], Step[0250/1251], Loss: 4.3364(3.8682), Acc: 0.3193(0.3433)
2022-01-05 10:52:20,336 Epoch[188/310], Step[0300/1251], Loss: 4.0511(3.8652), Acc: 0.4160(0.3433)
2022-01-05 10:53:17,137 Epoch[188/310], Step[0350/1251], Loss: 3.6944(3.8551), Acc: 0.2451(0.3415)
2022-01-05 10:54:14,842 Epoch[188/310], Step[0400/1251], Loss: 3.6223(3.8517), Acc: 0.3691(0.3417)
2022-01-05 10:55:12,215 Epoch[188/310], Step[0450/1251], Loss: 3.9238(3.8600), Acc: 0.2002(0.3409)
2022-01-05 10:56:08,815 Epoch[188/310], Step[0500/1251], Loss: 3.7491(3.8625), Acc: 0.5068(0.3419)
2022-01-05 10:57:05,836 Epoch[188/310], Step[0550/1251], Loss: 3.7729(3.8618), Acc: 0.4697(0.3413)
2022-01-05 10:58:02,015 Epoch[188/310], Step[0600/1251], Loss: 4.0647(3.8598), Acc: 0.4482(0.3432)
2022-01-05 10:58:58,591 Epoch[188/310], Step[0650/1251], Loss: 3.7835(3.8628), Acc: 0.4941(0.3420)
2022-01-05 10:59:55,541 Epoch[188/310], Step[0700/1251], Loss: 4.0089(3.8653), Acc: 0.2451(0.3419)
2022-01-05 11:00:53,783 Epoch[188/310], Step[0750/1251], Loss: 4.2466(3.8651), Acc: 0.4072(0.3409)
2022-01-05 11:01:50,918 Epoch[188/310], Step[0800/1251], Loss: 3.8126(3.8642), Acc: 0.3105(0.3420)
2022-01-05 11:02:48,568 Epoch[188/310], Step[0850/1251], Loss: 4.3974(3.8692), Acc: 0.3711(0.3409)
2022-01-05 11:03:45,434 Epoch[188/310], Step[0900/1251], Loss: 3.7667(3.8670), Acc: 0.4902(0.3416)
2022-01-05 11:04:42,554 Epoch[188/310], Step[0950/1251], Loss: 3.6078(3.8655), Acc: 0.4268(0.3428)
2022-01-05 11:05:39,999 Epoch[188/310], Step[1000/1251], Loss: 3.9633(3.8619), Acc: 0.1738(0.3420)
2022-01-05 11:06:37,861 Epoch[188/310], Step[1050/1251], Loss: 4.3451(3.8617), Acc: 0.1963(0.3421)
2022-01-05 11:07:35,890 Epoch[188/310], Step[1100/1251], Loss: 3.8861(3.8621), Acc: 0.1611(0.3413)
2022-01-05 11:08:33,844 Epoch[188/310], Step[1150/1251], Loss: 3.9979(3.8597), Acc: 0.3174(0.3404)
2022-01-05 11:09:31,507 Epoch[188/310], Step[1200/1251], Loss: 3.9330(3.8558), Acc: 0.3369(0.3406)
2022-01-05 11:10:28,067 Epoch[188/310], Step[1250/1251], Loss: 4.2763(3.8553), Acc: 0.3887(0.3408)
2022-01-05 11:10:30,101 ----- Validation after Epoch: 188
2022-01-05 11:11:26,370 Val Step[0000/1563], Loss: 0.7904 (0.7904), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-05 11:11:27,886 Val Step[0050/1563], Loss: 2.3536 (0.8848), Acc@1: 0.4688 (0.8254), Acc@5: 0.8438 (0.9510)
2022-01-05 11:11:29,315 Val Step[0100/1563], Loss: 2.3177 (1.1621), Acc@1: 0.3438 (0.7478), Acc@5: 0.8125 (0.9251)
2022-01-05 11:11:30,740 Val Step[0150/1563], Loss: 0.6044 (1.1008), Acc@1: 0.9375 (0.7626), Acc@5: 1.0000 (0.9288)
2022-01-05 11:11:32,168 Val Step[0200/1563], Loss: 1.2813 (1.1115), Acc@1: 0.6562 (0.7635), Acc@5: 0.9375 (0.9272)
2022-01-05 11:11:33,605 Val Step[0250/1563], Loss: 0.7112 (1.0569), Acc@1: 0.8750 (0.7764), Acc@5: 1.0000 (0.9343)
2022-01-05 11:11:35,109 Val Step[0300/1563], Loss: 1.0888 (1.1259), Acc@1: 0.7188 (0.7542), Acc@5: 0.9688 (0.9288)
2022-01-05 11:11:36,496 Val Step[0350/1563], Loss: 1.0935 (1.1328), Acc@1: 0.7188 (0.7494), Acc@5: 0.9375 (0.9307)
2022-01-05 11:11:37,860 Val Step[0400/1563], Loss: 0.9166 (1.1397), Acc@1: 0.8438 (0.7441), Acc@5: 0.9688 (0.9314)
2022-01-05 11:11:39,314 Val Step[0450/1563], Loss: 0.7178 (1.1421), Acc@1: 0.8438 (0.7417), Acc@5: 1.0000 (0.9324)
2022-01-05 11:11:40,764 Val Step[0500/1563], Loss: 0.6287 (1.1286), Acc@1: 0.8125 (0.7458), Acc@5: 1.0000 (0.9338)
2022-01-05 11:11:42,270 Val Step[0550/1563], Loss: 0.8756 (1.1035), Acc@1: 0.8125 (0.7527), Acc@5: 0.9375 (0.9359)
2022-01-05 11:11:43,842 Val Step[0600/1563], Loss: 0.8426 (1.1113), Acc@1: 0.8438 (0.7515), Acc@5: 0.9375 (0.9346)
2022-01-05 11:11:45,359 Val Step[0650/1563], Loss: 0.9492 (1.1324), Acc@1: 0.6875 (0.7473), Acc@5: 1.0000 (0.9314)
2022-01-05 11:11:46,856 Val Step[0700/1563], Loss: 1.1162 (1.1653), Acc@1: 0.7500 (0.7403), Acc@5: 0.9062 (0.9271)
2022-01-05 11:11:48,374 Val Step[0750/1563], Loss: 1.4772 (1.2005), Acc@1: 0.7188 (0.7331), Acc@5: 0.7812 (0.9214)
2022-01-05 11:11:49,837 Val Step[0800/1563], Loss: 0.9287 (1.2419), Acc@1: 0.7812 (0.7233), Acc@5: 0.9688 (0.9158)
2022-01-05 11:11:51,238 Val Step[0850/1563], Loss: 1.1609 (1.2667), Acc@1: 0.8125 (0.7172), Acc@5: 0.9375 (0.9125)
2022-01-05 11:11:52,727 Val Step[0900/1563], Loss: 0.3428 (1.2667), Acc@1: 0.9375 (0.7188), Acc@5: 0.9688 (0.9113)
2022-01-05 11:11:54,249 Val Step[0950/1563], Loss: 1.6628 (1.2894), Acc@1: 0.6250 (0.7144), Acc@5: 0.9062 (0.9078)
2022-01-05 11:11:55,717 Val Step[1000/1563], Loss: 0.6237 (1.3144), Acc@1: 0.9062 (0.7084), Acc@5: 1.0000 (0.9044)
2022-01-05 11:11:57,227 Val Step[1050/1563], Loss: 0.5426 (1.3309), Acc@1: 0.9688 (0.7042), Acc@5: 0.9688 (0.9020)
2022-01-05 11:11:58,689 Val Step[1100/1563], Loss: 1.1925 (1.3456), Acc@1: 0.7188 (0.7015), Acc@5: 0.8750 (0.9000)
2022-01-05 11:12:00,152 Val Step[1150/1563], Loss: 1.4661 (1.3631), Acc@1: 0.7500 (0.6982), Acc@5: 0.7500 (0.8973)
2022-01-05 11:12:01,635 Val Step[1200/1563], Loss: 1.2796 (1.3802), Acc@1: 0.7500 (0.6946), Acc@5: 0.8750 (0.8947)
2022-01-05 11:12:03,176 Val Step[1250/1563], Loss: 0.7808 (1.3923), Acc@1: 0.8438 (0.6927), Acc@5: 0.9062 (0.8925)
2022-01-05 11:12:04,679 Val Step[1300/1563], Loss: 1.0104 (1.4038), Acc@1: 0.7812 (0.6902), Acc@5: 0.9062 (0.8911)
2022-01-05 11:12:06,136 Val Step[1350/1563], Loss: 1.8378 (1.4210), Acc@1: 0.5000 (0.6864), Acc@5: 0.8125 (0.8888)
2022-01-05 11:12:07,581 Val Step[1400/1563], Loss: 1.3720 (1.4271), Acc@1: 0.7188 (0.6854), Acc@5: 0.8750 (0.8876)
2022-01-05 11:12:08,965 Val Step[1450/1563], Loss: 1.6710 (1.4342), Acc@1: 0.6562 (0.6835), Acc@5: 0.9062 (0.8869)
2022-01-05 11:12:10,349 Val Step[1500/1563], Loss: 1.7876 (1.4224), Acc@1: 0.5312 (0.6862), Acc@5: 0.8750 (0.8884)
2022-01-05 11:12:11,757 Val Step[1550/1563], Loss: 1.1200 (1.4240), Acc@1: 0.8750 (0.6859), Acc@5: 0.9062 (0.8882)
2022-01-05 11:12:12,578 ----- Epoch[188/310], Validation Loss: 1.4214, Validation Acc@1: 0.6862, Validation Acc@5: 0.8885, time: 102.47
2022-01-05 11:12:12,579 ----- Epoch[188/310], Train Loss: 3.8553, Train Acc: 0.3408, time: 1506.69, Best Val(epoch188) Acc@1: 0.6862
2022-01-05 11:12:12,759 Max accuracy so far: 0.6862 at epoch_188
2022-01-05 11:12:12,759 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 11:12:12,759 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 11:12:12,868 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 11:12:13,246 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 11:12:13,246 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 11:12:13,396 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 11:12:13,397 Now training epoch 189. LR=0.000305
2022-01-05 11:13:27,896 Epoch[189/310], Step[0000/1251], Loss: 3.6939(3.6939), Acc: 0.3447(0.3447)
2022-01-05 11:14:23,996 Epoch[189/310], Step[0050/1251], Loss: 3.6246(3.8555), Acc: 0.2354(0.3373)
2022-01-05 11:15:19,443 Epoch[189/310], Step[0100/1251], Loss: 4.0231(3.8591), Acc: 0.2773(0.3301)
2022-01-05 11:16:15,441 Epoch[189/310], Step[0150/1251], Loss: 4.0442(3.8503), Acc: 0.2803(0.3367)
2022-01-05 11:17:10,513 Epoch[189/310], Step[0200/1251], Loss: 3.9255(3.8548), Acc: 0.3887(0.3400)
2022-01-05 11:18:07,615 Epoch[189/310], Step[0250/1251], Loss: 3.2424(3.8418), Acc: 0.2832(0.3389)
2022-01-05 11:19:04,570 Epoch[189/310], Step[0300/1251], Loss: 3.8587(3.8272), Acc: 0.3223(0.3436)
2022-01-05 11:20:00,979 Epoch[189/310], Step[0350/1251], Loss: 4.0366(3.8213), Acc: 0.3438(0.3436)
2022-01-05 11:20:58,276 Epoch[189/310], Step[0400/1251], Loss: 3.7206(3.8306), Acc: 0.1904(0.3401)
2022-01-05 11:21:55,407 Epoch[189/310], Step[0450/1251], Loss: 4.3363(3.8365), Acc: 0.3154(0.3405)
2022-01-05 11:22:51,217 Epoch[189/310], Step[0500/1251], Loss: 4.0180(3.8455), Acc: 0.3467(0.3391)
2022-01-05 11:23:47,692 Epoch[189/310], Step[0550/1251], Loss: 3.7724(3.8454), Acc: 0.3926(0.3406)
2022-01-05 11:24:44,835 Epoch[189/310], Step[0600/1251], Loss: 3.7006(3.8473), Acc: 0.3672(0.3401)
2022-01-05 11:25:42,041 Epoch[189/310], Step[0650/1251], Loss: 4.2923(3.8453), Acc: 0.2861(0.3407)
2022-01-05 11:26:38,730 Epoch[189/310], Step[0700/1251], Loss: 3.6708(3.8503), Acc: 0.3145(0.3395)
2022-01-05 11:27:36,750 Epoch[189/310], Step[0750/1251], Loss: 3.7415(3.8496), Acc: 0.5254(0.3397)
2022-01-05 11:28:33,918 Epoch[189/310], Step[0800/1251], Loss: 4.0775(3.8458), Acc: 0.3555(0.3411)
2022-01-05 11:29:30,978 Epoch[189/310], Step[0850/1251], Loss: 3.6125(3.8425), Acc: 0.2139(0.3416)
2022-01-05 11:30:28,830 Epoch[189/310], Step[0900/1251], Loss: 4.2958(3.8484), Acc: 0.3125(0.3414)
2022-01-05 11:31:26,184 Epoch[189/310], Step[0950/1251], Loss: 3.8571(3.8506), Acc: 0.4443(0.3403)
2022-01-05 11:32:23,446 Epoch[189/310], Step[1000/1251], Loss: 3.5934(3.8496), Acc: 0.5410(0.3409)
2022-01-05 11:33:20,377 Epoch[189/310], Step[1050/1251], Loss: 4.0690(3.8509), Acc: 0.2549(0.3418)
2022-01-05 11:34:17,992 Epoch[189/310], Step[1100/1251], Loss: 3.5040(3.8490), Acc: 0.4648(0.3421)
2022-01-05 11:35:15,313 Epoch[189/310], Step[1150/1251], Loss: 3.7819(3.8529), Acc: 0.2246(0.3417)
2022-01-05 11:36:12,422 Epoch[189/310], Step[1200/1251], Loss: 4.2593(3.8543), Acc: 0.3213(0.3410)
2022-01-05 11:37:09,772 Epoch[189/310], Step[1250/1251], Loss: 3.9451(3.8589), Acc: 0.4570(0.3404)
2022-01-05 11:37:11,846 ----- Epoch[189/310], Train Loss: 3.8589, Train Acc: 0.3404, time: 1498.45, Best Val(epoch188) Acc@1: 0.6862
2022-01-05 11:37:12,021 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 11:37:12,022 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 11:37:12,128 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 11:37:12,128 Now training epoch 190. LR=0.000300
2022-01-05 11:38:34,174 Epoch[190/310], Step[0000/1251], Loss: 4.0829(4.0829), Acc: 0.3564(0.3564)
2022-01-05 11:39:31,983 Epoch[190/310], Step[0050/1251], Loss: 3.6357(3.8000), Acc: 0.3359(0.3503)
2022-01-05 11:40:28,696 Epoch[190/310], Step[0100/1251], Loss: 3.9156(3.8466), Acc: 0.3604(0.3536)
2022-01-05 11:41:25,292 Epoch[190/310], Step[0150/1251], Loss: 3.9843(3.8403), Acc: 0.2559(0.3498)
2022-01-05 11:42:21,319 Epoch[190/310], Step[0200/1251], Loss: 3.6677(3.8494), Acc: 0.4658(0.3487)
2022-01-05 11:43:17,996 Epoch[190/310], Step[0250/1251], Loss: 3.7192(3.8394), Acc: 0.3506(0.3504)
2022-01-05 11:44:14,908 Epoch[190/310], Step[0300/1251], Loss: 3.7850(3.8482), Acc: 0.4629(0.3481)
2022-01-05 11:45:11,215 Epoch[190/310], Step[0350/1251], Loss: 3.5575(3.8425), Acc: 0.5420(0.3492)
2022-01-05 11:46:08,533 Epoch[190/310], Step[0400/1251], Loss: 3.8541(3.8431), Acc: 0.0693(0.3463)
2022-01-05 11:47:06,280 Epoch[190/310], Step[0450/1251], Loss: 3.6695(3.8471), Acc: 0.2998(0.3464)
2022-01-05 11:48:03,218 Epoch[190/310], Step[0500/1251], Loss: 3.6839(3.8504), Acc: 0.4141(0.3447)
2022-01-05 11:48:59,798 Epoch[190/310], Step[0550/1251], Loss: 4.1221(3.8526), Acc: 0.3389(0.3440)
2022-01-05 11:49:55,892 Epoch[190/310], Step[0600/1251], Loss: 3.6705(3.8507), Acc: 0.2910(0.3447)
2022-01-05 11:50:52,597 Epoch[190/310], Step[0650/1251], Loss: 4.1333(3.8566), Acc: 0.2842(0.3435)
2022-01-05 11:51:47,903 Epoch[190/310], Step[0700/1251], Loss: 4.0607(3.8510), Acc: 0.2344(0.3438)
2022-01-05 11:52:44,323 Epoch[190/310], Step[0750/1251], Loss: 3.7981(3.8495), Acc: 0.3652(0.3453)
2022-01-05 11:53:40,539 Epoch[190/310], Step[0800/1251], Loss: 3.9462(3.8505), Acc: 0.1816(0.3440)
2022-01-05 11:54:38,490 Epoch[190/310], Step[0850/1251], Loss: 3.7708(3.8500), Acc: 0.3379(0.3432)
2022-01-05 11:55:35,883 Epoch[190/310], Step[0900/1251], Loss: 3.2840(3.8459), Acc: 0.4805(0.3424)
2022-01-05 11:56:32,792 Epoch[190/310], Step[0950/1251], Loss: 3.6282(3.8456), Acc: 0.3135(0.3420)
2022-01-05 11:57:29,916 Epoch[190/310], Step[1000/1251], Loss: 3.9596(3.8411), Acc: 0.3857(0.3417)
2022-01-05 11:58:26,439 Epoch[190/310], Step[1050/1251], Loss: 3.7133(3.8427), Acc: 0.2402(0.3411)
2022-01-05 11:59:23,321 Epoch[190/310], Step[1100/1251], Loss: 4.0529(3.8463), Acc: 0.3330(0.3400)
2022-01-05 12:00:19,408 Epoch[190/310], Step[1150/1251], Loss: 3.8968(3.8439), Acc: 0.5117(0.3408)
2022-01-05 12:01:15,429 Epoch[190/310], Step[1200/1251], Loss: 3.7879(3.8461), Acc: 0.3447(0.3407)
2022-01-05 12:02:11,381 Epoch[190/310], Step[1250/1251], Loss: 3.3718(3.8478), Acc: 0.5693(0.3415)
2022-01-05 12:02:13,257 ----- Validation after Epoch: 190
2022-01-05 12:03:11,389 Val Step[0000/1563], Loss: 0.7187 (0.7187), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-05 12:03:12,949 Val Step[0050/1563], Loss: 1.9072 (0.8089), Acc@1: 0.5000 (0.8401), Acc@5: 0.8750 (0.9571)
2022-01-05 12:03:14,353 Val Step[0100/1563], Loss: 1.7754 (1.1297), Acc@1: 0.5938 (0.7475), Acc@5: 0.8438 (0.9239)
2022-01-05 12:03:15,823 Val Step[0150/1563], Loss: 0.6033 (1.0642), Acc@1: 0.8438 (0.7628), Acc@5: 1.0000 (0.9294)
2022-01-05 12:03:17,257 Val Step[0200/1563], Loss: 1.1639 (1.0790), Acc@1: 0.7188 (0.7634), Acc@5: 0.9062 (0.9279)
2022-01-05 12:03:18,679 Val Step[0250/1563], Loss: 0.6195 (1.0199), Acc@1: 0.8750 (0.7771), Acc@5: 1.0000 (0.9349)
2022-01-05 12:03:20,038 Val Step[0300/1563], Loss: 1.1842 (1.0778), Acc@1: 0.7188 (0.7602), Acc@5: 0.9688 (0.9308)
2022-01-05 12:03:21,448 Val Step[0350/1563], Loss: 1.1520 (1.0851), Acc@1: 0.6875 (0.7557), Acc@5: 0.9062 (0.9326)
2022-01-05 12:03:22,873 Val Step[0400/1563], Loss: 1.2988 (1.0957), Acc@1: 0.7188 (0.7493), Acc@5: 0.9375 (0.9329)
2022-01-05 12:03:24,344 Val Step[0450/1563], Loss: 1.0519 (1.1034), Acc@1: 0.6562 (0.7452), Acc@5: 1.0000 (0.9336)
2022-01-05 12:03:25,832 Val Step[0500/1563], Loss: 0.4682 (1.0944), Acc@1: 0.9062 (0.7480), Acc@5: 1.0000 (0.9345)
2022-01-05 12:03:27,316 Val Step[0550/1563], Loss: 0.7965 (1.0704), Acc@1: 0.8125 (0.7545), Acc@5: 0.9688 (0.9366)
2022-01-05 12:03:28,866 Val Step[0600/1563], Loss: 0.7760 (1.0744), Acc@1: 0.8438 (0.7542), Acc@5: 0.9375 (0.9360)
2022-01-05 12:03:30,299 Val Step[0650/1563], Loss: 0.7041 (1.0977), Acc@1: 0.9062 (0.7488), Acc@5: 0.9688 (0.9326)
2022-01-05 12:03:31,750 Val Step[0700/1563], Loss: 1.0162 (1.1301), Acc@1: 0.7812 (0.7412), Acc@5: 0.9375 (0.9282)
2022-01-05 12:03:33,280 Val Step[0750/1563], Loss: 1.3418 (1.1688), Acc@1: 0.6875 (0.7338), Acc@5: 0.8750 (0.9222)
2022-01-05 12:03:34,749 Val Step[0800/1563], Loss: 0.9513 (1.2122), Acc@1: 0.7812 (0.7240), Acc@5: 1.0000 (0.9163)
2022-01-05 12:03:36,171 Val Step[0850/1563], Loss: 1.3982 (1.2389), Acc@1: 0.6562 (0.7185), Acc@5: 0.9375 (0.9130)
2022-01-05 12:03:37,698 Val Step[0900/1563], Loss: 0.5161 (1.2388), Acc@1: 0.9062 (0.7199), Acc@5: 0.9688 (0.9122)
2022-01-05 12:03:39,263 Val Step[0950/1563], Loss: 1.6859 (1.2580), Acc@1: 0.6250 (0.7161), Acc@5: 0.9062 (0.9089)
2022-01-05 12:03:40,757 Val Step[1000/1563], Loss: 0.6946 (1.2833), Acc@1: 0.9688 (0.7097), Acc@5: 1.0000 (0.9052)
2022-01-05 12:03:42,192 Val Step[1050/1563], Loss: 0.3669 (1.2996), Acc@1: 0.9688 (0.7060), Acc@5: 0.9688 (0.9031)
2022-01-05 12:03:43,629 Val Step[1100/1563], Loss: 1.0978 (1.3146), Acc@1: 0.7500 (0.7027), Acc@5: 0.9375 (0.9006)
2022-01-05 12:03:45,100 Val Step[1150/1563], Loss: 1.2211 (1.3285), Acc@1: 0.8125 (0.7001), Acc@5: 0.8125 (0.8986)
2022-01-05 12:03:46,517 Val Step[1200/1563], Loss: 1.3199 (1.3430), Acc@1: 0.7812 (0.6967), Acc@5: 0.8438 (0.8963)
2022-01-05 12:03:47,979 Val Step[1250/1563], Loss: 0.9207 (1.3548), Acc@1: 0.8438 (0.6951), Acc@5: 0.9062 (0.8943)
2022-01-05 12:03:49,433 Val Step[1300/1563], Loss: 1.0182 (1.3663), Acc@1: 0.7500 (0.6930), Acc@5: 0.8750 (0.8927)
2022-01-05 12:03:50,854 Val Step[1350/1563], Loss: 1.8732 (1.3848), Acc@1: 0.4375 (0.6888), Acc@5: 0.8125 (0.8898)
2022-01-05 12:03:52,301 Val Step[1400/1563], Loss: 1.3043 (1.3939), Acc@1: 0.7188 (0.6871), Acc@5: 0.9375 (0.8885)
2022-01-05 12:03:53,648 Val Step[1450/1563], Loss: 1.2565 (1.4016), Acc@1: 0.6875 (0.6850), Acc@5: 0.9688 (0.8880)
2022-01-05 12:03:55,095 Val Step[1500/1563], Loss: 1.8933 (1.3933), Acc@1: 0.5312 (0.6869), Acc@5: 0.8750 (0.8894)
2022-01-05 12:03:56,461 Val Step[1550/1563], Loss: 1.0655 (1.3957), Acc@1: 0.8750 (0.6866), Acc@5: 0.9062 (0.8889)
2022-01-05 12:03:57,294 ----- Epoch[190/310], Validation Loss: 1.3938, Validation Acc@1: 0.6871, Validation Acc@5: 0.8891, time: 104.03
2022-01-05 12:03:57,294 ----- Epoch[190/310], Train Loss: 3.8478, Train Acc: 0.3415, time: 1501.12, Best Val(epoch190) Acc@1: 0.6871
2022-01-05 12:03:57,484 Max accuracy so far: 0.6871 at epoch_190
2022-01-05 12:03:57,484 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 12:03:57,484 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 12:03:57,592 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 12:03:57,714 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-190-Loss-3.851042616186287.pdparams
2022-01-05 12:03:57,715 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-190-Loss-3.851042616186287.pdopt
2022-01-05 12:03:57,761 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-190-Loss-3.851042616186287-EMA.pdparams
2022-01-05 12:03:57,761 Now training epoch 191. LR=0.000295
2022-01-05 12:05:08,693 Epoch[191/310], Step[0000/1251], Loss: 3.5390(3.5390), Acc: 0.3789(0.3789)
2022-01-05 12:06:05,997 Epoch[191/310], Step[0050/1251], Loss: 3.9251(3.8198), Acc: 0.3438(0.3384)
2022-01-05 12:07:03,143 Epoch[191/310], Step[0100/1251], Loss: 4.1231(3.8430), Acc: 0.4277(0.3489)
2022-01-05 12:07:59,573 Epoch[191/310], Step[0150/1251], Loss: 3.8663(3.8384), Acc: 0.3799(0.3476)
2022-01-05 12:08:56,729 Epoch[191/310], Step[0200/1251], Loss: 3.7750(3.8371), Acc: 0.4756(0.3501)
2022-01-05 12:09:54,012 Epoch[191/310], Step[0250/1251], Loss: 3.3847(3.8487), Acc: 0.2803(0.3467)
2022-01-05 12:10:51,103 Epoch[191/310], Step[0300/1251], Loss: 4.0987(3.8390), Acc: 0.4004(0.3500)
2022-01-05 12:11:48,776 Epoch[191/310], Step[0350/1251], Loss: 3.3707(3.8422), Acc: 0.3232(0.3496)
2022-01-05 12:12:46,434 Epoch[191/310], Step[0400/1251], Loss: 3.7771(3.8438), Acc: 0.3926(0.3449)
2022-01-05 12:13:43,448 Epoch[191/310], Step[0450/1251], Loss: 3.9165(3.8389), Acc: 0.2432(0.3457)
2022-01-05 12:14:41,003 Epoch[191/310], Step[0500/1251], Loss: 3.9986(3.8400), Acc: 0.2803(0.3458)
2022-01-05 12:15:38,452 Epoch[191/310], Step[0550/1251], Loss: 3.3262(3.8421), Acc: 0.4434(0.3450)
2022-01-05 12:16:35,261 Epoch[191/310], Step[0600/1251], Loss: 3.8506(3.8486), Acc: 0.3926(0.3434)
2022-01-05 12:17:32,288 Epoch[191/310], Step[0650/1251], Loss: 3.5595(3.8507), Acc: 0.2363(0.3430)
2022-01-05 12:18:29,533 Epoch[191/310], Step[0700/1251], Loss: 3.5150(3.8473), Acc: 0.1621(0.3424)
2022-01-05 12:19:26,537 Epoch[191/310], Step[0750/1251], Loss: 3.2887(3.8490), Acc: 0.4883(0.3426)
2022-01-05 12:20:22,617 Epoch[191/310], Step[0800/1251], Loss: 3.6373(3.8529), Acc: 0.4639(0.3419)
2022-01-05 12:21:19,401 Epoch[191/310], Step[0850/1251], Loss: 3.9084(3.8506), Acc: 0.4102(0.3427)
2022-01-05 12:22:16,496 Epoch[191/310], Step[0900/1251], Loss: 4.1115(3.8494), Acc: 0.1641(0.3421)
2022-01-05 12:23:13,514 Epoch[191/310], Step[0950/1251], Loss: 3.6903(3.8519), Acc: 0.3711(0.3428)
2022-01-05 12:24:09,833 Epoch[191/310], Step[1000/1251], Loss: 3.6161(3.8550), Acc: 0.4092(0.3428)
2022-01-05 12:25:07,681 Epoch[191/310], Step[1050/1251], Loss: 3.4148(3.8543), Acc: 0.4053(0.3413)
2022-01-05 12:26:04,191 Epoch[191/310], Step[1100/1251], Loss: 3.5813(3.8547), Acc: 0.5049(0.3425)
2022-01-05 12:27:02,727 Epoch[191/310], Step[1150/1251], Loss: 3.6228(3.8524), Acc: 0.3066(0.3422)
2022-01-05 12:28:01,292 Epoch[191/310], Step[1200/1251], Loss: 3.9566(3.8539), Acc: 0.2637(0.3415)
2022-01-05 12:28:58,205 Epoch[191/310], Step[1250/1251], Loss: 3.8480(3.8550), Acc: 0.3418(0.3413)
2022-01-05 12:29:00,115 ----- Epoch[191/310], Train Loss: 3.8550, Train Acc: 0.3413, time: 1502.35, Best Val(epoch190) Acc@1: 0.6871
2022-01-05 12:29:00,289 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 12:29:00,289 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 12:29:00,515 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 12:29:00,690 Now training epoch 192. LR=0.000291
2022-01-05 12:30:13,974 Epoch[192/310], Step[0000/1251], Loss: 3.5966(3.5966), Acc: 0.4805(0.4805)
2022-01-05 12:31:11,005 Epoch[192/310], Step[0050/1251], Loss: 3.4914(3.8739), Acc: 0.5176(0.3151)
2022-01-05 12:32:06,294 Epoch[192/310], Step[0100/1251], Loss: 3.1277(3.7926), Acc: 0.4424(0.3351)
2022-01-05 12:33:03,397 Epoch[192/310], Step[0150/1251], Loss: 3.1579(3.7819), Acc: 0.4014(0.3465)
2022-01-05 12:34:01,085 Epoch[192/310], Step[0200/1251], Loss: 3.7831(3.7836), Acc: 0.3721(0.3426)
2022-01-05 12:34:57,526 Epoch[192/310], Step[0250/1251], Loss: 3.3895(3.7831), Acc: 0.4199(0.3464)
2022-01-05 12:35:54,856 Epoch[192/310], Step[0300/1251], Loss: 3.5625(3.7991), Acc: 0.5098(0.3444)
2022-01-05 12:36:52,762 Epoch[192/310], Step[0350/1251], Loss: 3.6094(3.7993), Acc: 0.2842(0.3428)
2022-01-05 12:37:49,928 Epoch[192/310], Step[0400/1251], Loss: 3.4125(3.8110), Acc: 0.3857(0.3455)
2022-01-05 12:38:46,663 Epoch[192/310], Step[0450/1251], Loss: 3.9573(3.8144), Acc: 0.2051(0.3474)
2022-01-05 12:39:44,341 Epoch[192/310], Step[0500/1251], Loss: 4.0161(3.8167), Acc: 0.2578(0.3454)
2022-01-05 12:40:42,140 Epoch[192/310], Step[0550/1251], Loss: 4.1355(3.8199), Acc: 0.3740(0.3437)
2022-01-05 12:41:39,674 Epoch[192/310], Step[0600/1251], Loss: 3.9242(3.8228), Acc: 0.3848(0.3442)
2022-01-05 12:42:37,020 Epoch[192/310], Step[0650/1251], Loss: 3.7962(3.8286), Acc: 0.4326(0.3434)
2022-01-05 12:43:32,606 Epoch[192/310], Step[0700/1251], Loss: 3.4998(3.8276), Acc: 0.3018(0.3443)
2022-01-05 12:44:29,500 Epoch[192/310], Step[0750/1251], Loss: 3.6496(3.8294), Acc: 0.4541(0.3433)
2022-01-05 12:45:26,045 Epoch[192/310], Step[0800/1251], Loss: 4.2074(3.8262), Acc: 0.3574(0.3434)
2022-01-05 12:46:22,299 Epoch[192/310], Step[0850/1251], Loss: 3.6270(3.8285), Acc: 0.4668(0.3437)
2022-01-05 12:47:19,288 Epoch[192/310], Step[0900/1251], Loss: 2.9841(3.8303), Acc: 0.4746(0.3425)
2022-01-05 12:48:15,885 Epoch[192/310], Step[0950/1251], Loss: 3.8913(3.8303), Acc: 0.5166(0.3420)
2022-01-05 12:49:11,860 Epoch[192/310], Step[1000/1251], Loss: 4.5151(3.8308), Acc: 0.3115(0.3430)
2022-01-05 12:50:08,602 Epoch[192/310], Step[1050/1251], Loss: 3.5771(3.8313), Acc: 0.1816(0.3434)
2022-01-05 12:51:06,230 Epoch[192/310], Step[1100/1251], Loss: 3.4156(3.8328), Acc: 0.3975(0.3426)
2022-01-05 12:52:02,004 Epoch[192/310], Step[1150/1251], Loss: 4.3245(3.8330), Acc: 0.3135(0.3436)
2022-01-05 12:52:57,565 Epoch[192/310], Step[1200/1251], Loss: 4.5162(3.8338), Acc: 0.3105(0.3432)
2022-01-05 12:53:53,906 Epoch[192/310], Step[1250/1251], Loss: 4.0738(3.8353), Acc: 0.4248(0.3433)
2022-01-05 12:53:55,958 ----- Validation after Epoch: 192
2022-01-05 12:54:51,786 Val Step[0000/1563], Loss: 0.7635 (0.7635), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2022-01-05 12:54:53,249 Val Step[0050/1563], Loss: 2.0546 (0.8308), Acc@1: 0.4375 (0.8205), Acc@5: 0.8125 (0.9504)
2022-01-05 12:54:54,690 Val Step[0100/1563], Loss: 1.8558 (1.1364), Acc@1: 0.5938 (0.7395), Acc@5: 0.8750 (0.9245)
2022-01-05 12:54:56,217 Val Step[0150/1563], Loss: 0.5366 (1.0627), Acc@1: 0.8750 (0.7568), Acc@5: 0.9688 (0.9315)
2022-01-05 12:54:57,809 Val Step[0200/1563], Loss: 1.2202 (1.0715), Acc@1: 0.6875 (0.7581), Acc@5: 0.9062 (0.9288)
2022-01-05 12:54:59,316 Val Step[0250/1563], Loss: 0.7500 (1.0132), Acc@1: 0.8125 (0.7730), Acc@5: 1.0000 (0.9350)
2022-01-05 12:55:00,788 Val Step[0300/1563], Loss: 1.0513 (1.0696), Acc@1: 0.7500 (0.7561), Acc@5: 0.9688 (0.9304)
2022-01-05 12:55:02,196 Val Step[0350/1563], Loss: 1.0635 (1.0767), Acc@1: 0.6875 (0.7532), Acc@5: 0.9375 (0.9317)
2022-01-05 12:55:03,666 Val Step[0400/1563], Loss: 1.1992 (1.0838), Acc@1: 0.7188 (0.7484), Acc@5: 0.9375 (0.9323)
2022-01-05 12:55:05,099 Val Step[0450/1563], Loss: 0.9407 (1.0866), Acc@1: 0.7188 (0.7467), Acc@5: 1.0000 (0.9338)
2022-01-05 12:55:06,548 Val Step[0500/1563], Loss: 0.4564 (1.0784), Acc@1: 0.9062 (0.7491), Acc@5: 1.0000 (0.9349)
2022-01-05 12:55:08,011 Val Step[0550/1563], Loss: 0.9052 (1.0534), Acc@1: 0.7812 (0.7552), Acc@5: 0.9688 (0.9369)
2022-01-05 12:55:09,581 Val Step[0600/1563], Loss: 0.8510 (1.0600), Acc@1: 0.7812 (0.7536), Acc@5: 0.9375 (0.9357)
2022-01-05 12:55:10,940 Val Step[0650/1563], Loss: 0.7407 (1.0831), Acc@1: 0.7500 (0.7487), Acc@5: 1.0000 (0.9323)
2022-01-05 12:55:12,398 Val Step[0700/1563], Loss: 0.8341 (1.1112), Acc@1: 0.8438 (0.7424), Acc@5: 0.9688 (0.9292)
2022-01-05 12:55:13,868 Val Step[0750/1563], Loss: 1.3713 (1.1494), Acc@1: 0.6875 (0.7347), Acc@5: 0.8438 (0.9231)
2022-01-05 12:55:15,294 Val Step[0800/1563], Loss: 0.7598 (1.1884), Acc@1: 0.7812 (0.7255), Acc@5: 1.0000 (0.9181)
2022-01-05 12:55:16,699 Val Step[0850/1563], Loss: 1.6206 (1.2151), Acc@1: 0.5938 (0.7194), Acc@5: 0.8750 (0.9143)
2022-01-05 12:55:18,184 Val Step[0900/1563], Loss: 0.3973 (1.2150), Acc@1: 0.9375 (0.7208), Acc@5: 1.0000 (0.9135)
2022-01-05 12:55:19,732 Val Step[0950/1563], Loss: 1.4941 (1.2372), Acc@1: 0.6562 (0.7169), Acc@5: 0.8750 (0.9103)
2022-01-05 12:55:21,184 Val Step[1000/1563], Loss: 0.7698 (1.2622), Acc@1: 0.8750 (0.7109), Acc@5: 0.9688 (0.9067)
2022-01-05 12:55:22,607 Val Step[1050/1563], Loss: 0.4704 (1.2778), Acc@1: 0.9688 (0.7079), Acc@5: 0.9688 (0.9047)
2022-01-05 12:55:24,028 Val Step[1100/1563], Loss: 1.2591 (1.2948), Acc@1: 0.6562 (0.7048), Acc@5: 0.9062 (0.9019)
2022-01-05 12:55:25,466 Val Step[1150/1563], Loss: 1.2816 (1.3091), Acc@1: 0.7812 (0.7023), Acc@5: 0.8125 (0.8997)
2022-01-05 12:55:26,855 Val Step[1200/1563], Loss: 1.3289 (1.3247), Acc@1: 0.7812 (0.6989), Acc@5: 0.8438 (0.8972)
2022-01-05 12:55:28,324 Val Step[1250/1563], Loss: 0.7703 (1.3390), Acc@1: 0.8750 (0.6968), Acc@5: 0.9062 (0.8948)
2022-01-05 12:55:29,753 Val Step[1300/1563], Loss: 1.1143 (1.3490), Acc@1: 0.7812 (0.6949), Acc@5: 0.8750 (0.8935)
2022-01-05 12:55:31,180 Val Step[1350/1563], Loss: 2.0697 (1.3662), Acc@1: 0.3438 (0.6909), Acc@5: 0.8125 (0.8910)
2022-01-05 12:55:32,636 Val Step[1400/1563], Loss: 0.7882 (1.3748), Acc@1: 0.9062 (0.6891), Acc@5: 0.9375 (0.8895)
2022-01-05 12:55:34,082 Val Step[1450/1563], Loss: 1.7363 (1.3816), Acc@1: 0.5625 (0.6871), Acc@5: 0.9375 (0.8891)
2022-01-05 12:55:35,538 Val Step[1500/1563], Loss: 1.9025 (1.3717), Acc@1: 0.5312 (0.6896), Acc@5: 0.8125 (0.8906)
2022-01-05 12:55:36,934 Val Step[1550/1563], Loss: 1.0079 (1.3725), Acc@1: 0.8750 (0.6894), Acc@5: 0.9062 (0.8904)
2022-01-05 12:55:37,753 ----- Epoch[192/310], Validation Loss: 1.3703, Validation Acc@1: 0.6898, Validation Acc@5: 0.8906, time: 101.79
2022-01-05 12:55:37,753 ----- Epoch[192/310], Train Loss: 3.8353, Train Acc: 0.3433, time: 1495.27, Best Val(epoch192) Acc@1: 0.6898
2022-01-05 12:55:37,941 Max accuracy so far: 0.6898 at epoch_192
2022-01-05 12:55:37,941 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 12:55:37,941 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 12:55:38,052 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 12:55:38,437 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 12:55:38,437 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 12:55:38,563 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 12:55:38,563 Now training epoch 193. LR=0.000286
2022-01-05 12:56:57,120 Epoch[193/310], Step[0000/1251], Loss: 4.0176(4.0176), Acc: 0.3574(0.3574)
2022-01-05 12:57:54,628 Epoch[193/310], Step[0050/1251], Loss: 3.8056(3.8314), Acc: 0.2764(0.3108)
2022-01-05 12:58:52,183 Epoch[193/310], Step[0100/1251], Loss: 4.0102(3.8650), Acc: 0.4355(0.3237)
2022-01-05 12:59:48,577 Epoch[193/310], Step[0150/1251], Loss: 4.1390(3.8710), Acc: 0.1611(0.3334)
2022-01-05 13:00:44,340 Epoch[193/310], Step[0200/1251], Loss: 4.0366(3.8506), Acc: 0.4150(0.3348)
2022-01-05 13:01:41,547 Epoch[193/310], Step[0250/1251], Loss: 3.6388(3.8427), Acc: 0.4482(0.3389)
2022-01-05 13:02:37,102 Epoch[193/310], Step[0300/1251], Loss: 3.0766(3.8342), Acc: 0.2900(0.3472)
2022-01-05 13:03:34,280 Epoch[193/310], Step[0350/1251], Loss: 3.6831(3.8465), Acc: 0.3330(0.3407)
2022-01-05 13:04:30,673 Epoch[193/310], Step[0400/1251], Loss: 3.9386(3.8375), Acc: 0.2695(0.3435)
2022-01-05 13:05:27,255 Epoch[193/310], Step[0450/1251], Loss: 3.5672(3.8450), Acc: 0.2383(0.3431)
2022-01-05 13:06:24,136 Epoch[193/310], Step[0500/1251], Loss: 3.6607(3.8503), Acc: 0.4307(0.3413)
2022-01-05 13:07:21,411 Epoch[193/310], Step[0550/1251], Loss: 3.6474(3.8504), Acc: 0.1963(0.3420)
2022-01-05 13:08:18,856 Epoch[193/310], Step[0600/1251], Loss: 3.5651(3.8505), Acc: 0.3887(0.3403)
2022-01-05 13:09:15,984 Epoch[193/310], Step[0650/1251], Loss: 3.4464(3.8472), Acc: 0.4863(0.3404)
2022-01-05 13:10:14,282 Epoch[193/310], Step[0700/1251], Loss: 3.9959(3.8458), Acc: 0.3506(0.3398)
2022-01-05 13:11:12,169 Epoch[193/310], Step[0750/1251], Loss: 3.6964(3.8438), Acc: 0.4365(0.3388)
2022-01-05 13:12:09,374 Epoch[193/310], Step[0800/1251], Loss: 3.2604(3.8485), Acc: 0.5371(0.3378)
2022-01-05 13:13:05,861 Epoch[193/310], Step[0850/1251], Loss: 3.9001(3.8485), Acc: 0.4922(0.3401)
2022-01-05 13:14:02,862 Epoch[193/310], Step[0900/1251], Loss: 4.1072(3.8492), Acc: 0.2402(0.3408)
2022-01-05 13:14:59,999 Epoch[193/310], Step[0950/1251], Loss: 3.9383(3.8493), Acc: 0.2988(0.3401)
2022-01-05 13:15:57,404 Epoch[193/310], Step[1000/1251], Loss: 3.7738(3.8505), Acc: 0.2549(0.3397)
2022-01-05 13:16:55,386 Epoch[193/310], Step[1050/1251], Loss: 3.7562(3.8471), Acc: 0.4072(0.3399)
2022-01-05 13:17:51,262 Epoch[193/310], Step[1100/1251], Loss: 3.8231(3.8483), Acc: 0.4072(0.3404)
2022-01-05 13:18:48,246 Epoch[193/310], Step[1150/1251], Loss: 3.9299(3.8499), Acc: 0.2119(0.3395)
2022-01-05 13:19:44,621 Epoch[193/310], Step[1200/1251], Loss: 3.7050(3.8500), Acc: 0.1045(0.3400)
2022-01-05 13:20:41,965 Epoch[193/310], Step[1250/1251], Loss: 3.9625(3.8524), Acc: 0.4404(0.3404)
2022-01-05 13:20:44,072 ----- Epoch[193/310], Train Loss: 3.8524, Train Acc: 0.3404, time: 1505.51, Best Val(epoch192) Acc@1: 0.6898
2022-01-05 13:20:44,245 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 13:20:44,245 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 13:20:44,371 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 13:20:44,372 Now training epoch 194. LR=0.000281
2022-01-05 13:21:56,830 Epoch[194/310], Step[0000/1251], Loss: 3.3480(3.3480), Acc: 0.4775(0.4775)
2022-01-05 13:22:53,913 Epoch[194/310], Step[0050/1251], Loss: 3.9687(3.7326), Acc: 0.4395(0.3695)
2022-01-05 13:23:50,348 Epoch[194/310], Step[0100/1251], Loss: 3.7628(3.7616), Acc: 0.3467(0.3550)
2022-01-05 13:24:46,088 Epoch[194/310], Step[0150/1251], Loss: 4.4794(3.7771), Acc: 0.3057(0.3592)
2022-01-05 13:25:44,366 Epoch[194/310], Step[0200/1251], Loss: 3.8385(3.7995), Acc: 0.3232(0.3482)
2022-01-05 13:26:42,648 Epoch[194/310], Step[0250/1251], Loss: 3.0429(3.7959), Acc: 0.2393(0.3418)
2022-01-05 13:27:40,753 Epoch[194/310], Step[0300/1251], Loss: 4.1486(3.8045), Acc: 0.3379(0.3451)
2022-01-05 13:28:37,191 Epoch[194/310], Step[0350/1251], Loss: 3.5751(3.8130), Acc: 0.5469(0.3473)
2022-01-05 13:29:33,639 Epoch[194/310], Step[0400/1251], Loss: 4.0014(3.8053), Acc: 0.3672(0.3516)
2022-01-05 13:30:29,994 Epoch[194/310], Step[0450/1251], Loss: 3.5387(3.8097), Acc: 0.5439(0.3540)
2022-01-05 13:31:26,778 Epoch[194/310], Step[0500/1251], Loss: 4.1577(3.8058), Acc: 0.3008(0.3543)
2022-01-05 13:32:24,272 Epoch[194/310], Step[0550/1251], Loss: 3.6933(3.8098), Acc: 0.2344(0.3533)
2022-01-05 13:33:22,160 Epoch[194/310], Step[0600/1251], Loss: 3.9414(3.8125), Acc: 0.4287(0.3503)
2022-01-05 13:34:18,965 Epoch[194/310], Step[0650/1251], Loss: 4.3519(3.8139), Acc: 0.3213(0.3502)
2022-01-05 13:35:15,425 Epoch[194/310], Step[0700/1251], Loss: 3.5319(3.8142), Acc: 0.1729(0.3505)
2022-01-05 13:36:11,279 Epoch[194/310], Step[0750/1251], Loss: 3.9449(3.8077), Acc: 0.0342(0.3518)
2022-01-05 13:37:08,284 Epoch[194/310], Step[0800/1251], Loss: 3.7059(3.8080), Acc: 0.4727(0.3512)
2022-01-05 13:38:06,044 Epoch[194/310], Step[0850/1251], Loss: 4.0246(3.8076), Acc: 0.4092(0.3506)
2022-01-05 13:39:02,414 Epoch[194/310], Step[0900/1251], Loss: 4.0756(3.8123), Acc: 0.3926(0.3487)
2022-01-05 13:40:00,008 Epoch[194/310], Step[0950/1251], Loss: 3.3159(3.8121), Acc: 0.3506(0.3478)
2022-01-05 13:40:57,854 Epoch[194/310], Step[1000/1251], Loss: 3.3558(3.8113), Acc: 0.4277(0.3477)
2022-01-05 13:41:55,713 Epoch[194/310], Step[1050/1251], Loss: 4.2176(3.8093), Acc: 0.4131(0.3487)
2022-01-05 13:42:52,195 Epoch[194/310], Step[1100/1251], Loss: 3.9017(3.8144), Acc: 0.4160(0.3485)
2022-01-05 13:43:49,435 Epoch[194/310], Step[1150/1251], Loss: 3.8927(3.8170), Acc: 0.3262(0.3474)
2022-01-05 13:44:46,644 Epoch[194/310], Step[1200/1251], Loss: 3.8329(3.8171), Acc: 0.3311(0.3484)
2022-01-05 13:45:43,782 Epoch[194/310], Step[1250/1251], Loss: 3.7398(3.8184), Acc: 0.3340(0.3471)
2022-01-05 13:45:45,657 ----- Validation after Epoch: 194
2022-01-05 13:46:43,551 Val Step[0000/1563], Loss: 0.7354 (0.7354), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 13:46:45,023 Val Step[0050/1563], Loss: 2.1926 (0.8154), Acc@1: 0.4688 (0.8407), Acc@5: 0.8750 (0.9577)
2022-01-05 13:46:46,409 Val Step[0100/1563], Loss: 2.2555 (1.0966), Acc@1: 0.3750 (0.7587), Acc@5: 0.8125 (0.9288)
2022-01-05 13:46:47,787 Val Step[0150/1563], Loss: 0.5002 (1.0344), Acc@1: 0.9062 (0.7711), Acc@5: 1.0000 (0.9327)
2022-01-05 13:46:49,278 Val Step[0200/1563], Loss: 1.3621 (1.0430), Acc@1: 0.7188 (0.7727), Acc@5: 0.9062 (0.9311)
2022-01-05 13:46:50,722 Val Step[0250/1563], Loss: 0.5544 (0.9980), Acc@1: 0.9375 (0.7832), Acc@5: 1.0000 (0.9366)
2022-01-05 13:46:52,102 Val Step[0300/1563], Loss: 1.4884 (1.0629), Acc@1: 0.6875 (0.7635), Acc@5: 0.9062 (0.9319)
2022-01-05 13:46:53,474 Val Step[0350/1563], Loss: 0.9903 (1.0728), Acc@1: 0.7812 (0.7593), Acc@5: 0.9062 (0.9331)
2022-01-05 13:46:54,811 Val Step[0400/1563], Loss: 1.0169 (1.0845), Acc@1: 0.8125 (0.7533), Acc@5: 0.9688 (0.9340)
2022-01-05 13:46:56,276 Val Step[0450/1563], Loss: 0.7403 (1.0911), Acc@1: 0.8438 (0.7499), Acc@5: 1.0000 (0.9347)
2022-01-05 13:46:57,794 Val Step[0500/1563], Loss: 0.4193 (1.0835), Acc@1: 0.9375 (0.7522), Acc@5: 1.0000 (0.9363)
2022-01-05 13:46:59,307 Val Step[0550/1563], Loss: 0.8334 (1.0594), Acc@1: 0.8125 (0.7584), Acc@5: 0.9688 (0.9384)
2022-01-05 13:47:00,744 Val Step[0600/1563], Loss: 1.0522 (1.0709), Acc@1: 0.7812 (0.7571), Acc@5: 0.9375 (0.9372)
2022-01-05 13:47:02,209 Val Step[0650/1563], Loss: 0.6356 (1.0899), Acc@1: 0.8438 (0.7533), Acc@5: 1.0000 (0.9343)
2022-01-05 13:47:03,676 Val Step[0700/1563], Loss: 1.2370 (1.1216), Acc@1: 0.7812 (0.7459), Acc@5: 0.8750 (0.9297)
2022-01-05 13:47:05,180 Val Step[0750/1563], Loss: 1.3876 (1.1541), Acc@1: 0.7188 (0.7399), Acc@5: 0.8750 (0.9250)
2022-01-05 13:47:06,633 Val Step[0800/1563], Loss: 1.1524 (1.1949), Acc@1: 0.7500 (0.7302), Acc@5: 0.9688 (0.9193)
2022-01-05 13:47:08,051 Val Step[0850/1563], Loss: 1.1930 (1.2196), Acc@1: 0.7188 (0.7245), Acc@5: 0.9375 (0.9160)
2022-01-05 13:47:09,496 Val Step[0900/1563], Loss: 0.5598 (1.2204), Acc@1: 0.9375 (0.7254), Acc@5: 0.9375 (0.9151)
2022-01-05 13:47:11,016 Val Step[0950/1563], Loss: 1.3612 (1.2441), Acc@1: 0.6562 (0.7213), Acc@5: 0.9062 (0.9114)
2022-01-05 13:47:12,423 Val Step[1000/1563], Loss: 0.4879 (1.2690), Acc@1: 1.0000 (0.7150), Acc@5: 1.0000 (0.9079)
2022-01-05 13:47:13,807 Val Step[1050/1563], Loss: 0.4630 (1.2857), Acc@1: 0.9688 (0.7112), Acc@5: 0.9688 (0.9056)
2022-01-05 13:47:15,181 Val Step[1100/1563], Loss: 1.2837 (1.3023), Acc@1: 0.6562 (0.7075), Acc@5: 0.8750 (0.9033)
2022-01-05 13:47:16,597 Val Step[1150/1563], Loss: 1.5019 (1.3183), Acc@1: 0.7812 (0.7043), Acc@5: 0.7812 (0.9008)
2022-01-05 13:47:18,056 Val Step[1200/1563], Loss: 1.1120 (1.3339), Acc@1: 0.7500 (0.7010), Acc@5: 0.8750 (0.8984)
2022-01-05 13:47:19,595 Val Step[1250/1563], Loss: 0.7195 (1.3460), Acc@1: 0.8750 (0.6992), Acc@5: 0.9062 (0.8964)
2022-01-05 13:47:21,025 Val Step[1300/1563], Loss: 1.0954 (1.3575), Acc@1: 0.8125 (0.6973), Acc@5: 0.9375 (0.8950)
2022-01-05 13:47:22,433 Val Step[1350/1563], Loss: 1.7358 (1.3762), Acc@1: 0.5000 (0.6931), Acc@5: 0.8750 (0.8921)
2022-01-05 13:47:23,834 Val Step[1400/1563], Loss: 1.1113 (1.3837), Acc@1: 0.7500 (0.6916), Acc@5: 0.9688 (0.8910)
2022-01-05 13:47:25,249 Val Step[1450/1563], Loss: 1.3908 (1.3912), Acc@1: 0.6875 (0.6898), Acc@5: 0.9062 (0.8906)
2022-01-05 13:47:26,618 Val Step[1500/1563], Loss: 1.6797 (1.3814), Acc@1: 0.5312 (0.6921), Acc@5: 0.9062 (0.8920)
2022-01-05 13:47:28,069 Val Step[1550/1563], Loss: 1.0581 (1.3829), Acc@1: 0.8750 (0.6914), Acc@5: 0.9062 (0.8920)
2022-01-05 13:47:28,937 ----- Epoch[194/310], Validation Loss: 1.3808, Validation Acc@1: 0.6919, Validation Acc@5: 0.8923, time: 103.28
2022-01-05 13:47:28,937 ----- Epoch[194/310], Train Loss: 3.8184, Train Acc: 0.3471, time: 1501.28, Best Val(epoch194) Acc@1: 0.6919
2022-01-05 13:47:29,126 Max accuracy so far: 0.6919 at epoch_194
2022-01-05 13:47:29,126 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 13:47:29,127 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 13:47:29,234 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 13:47:29,618 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 13:47:29,619 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 13:47:29,745 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 13:47:29,745 Now training epoch 195. LR=0.000277
2022-01-05 13:48:50,719 Epoch[195/310], Step[0000/1251], Loss: 3.9369(3.9369), Acc: 0.2051(0.2051)
2022-01-05 13:49:48,411 Epoch[195/310], Step[0050/1251], Loss: 3.8601(3.8838), Acc: 0.2949(0.3345)
2022-01-05 13:50:45,364 Epoch[195/310], Step[0100/1251], Loss: 4.0462(3.8347), Acc: 0.4570(0.3575)
2022-01-05 13:51:42,324 Epoch[195/310], Step[0150/1251], Loss: 4.5051(3.8440), Acc: 0.3291(0.3451)
2022-01-05 13:52:39,982 Epoch[195/310], Step[0200/1251], Loss: 3.9510(3.8435), Acc: 0.4883(0.3439)
2022-01-05 13:53:37,433 Epoch[195/310], Step[0250/1251], Loss: 4.2725(3.8446), Acc: 0.3076(0.3478)
2022-01-05 13:54:33,873 Epoch[195/310], Step[0300/1251], Loss: 3.9914(3.8447), Acc: 0.3516(0.3489)
2022-01-05 13:55:32,054 Epoch[195/310], Step[0350/1251], Loss: 4.3385(3.8508), Acc: 0.2158(0.3454)
2022-01-05 13:56:28,157 Epoch[195/310], Step[0400/1251], Loss: 3.6617(3.8554), Acc: 0.4365(0.3465)
2022-01-05 13:57:22,010 Epoch[195/310], Step[0450/1251], Loss: 4.5632(3.8562), Acc: 0.3057(0.3483)
2022-01-05 13:58:18,821 Epoch[195/310], Step[0500/1251], Loss: 3.7509(3.8575), Acc: 0.5029(0.3466)
2022-01-05 13:59:13,920 Epoch[195/310], Step[0550/1251], Loss: 4.0876(3.8492), Acc: 0.3730(0.3479)
2022-01-05 14:00:10,214 Epoch[195/310], Step[0600/1251], Loss: 3.5063(3.8435), Acc: 0.2178(0.3487)
2022-01-05 14:01:06,860 Epoch[195/310], Step[0650/1251], Loss: 4.0117(3.8417), Acc: 0.1279(0.3493)
2022-01-05 14:02:03,979 Epoch[195/310], Step[0700/1251], Loss: 3.6332(3.8361), Acc: 0.4961(0.3505)
2022-01-05 14:03:01,738 Epoch[195/310], Step[0750/1251], Loss: 3.6855(3.8325), Acc: 0.4365(0.3499)
2022-01-05 14:03:58,397 Epoch[195/310], Step[0800/1251], Loss: 4.3360(3.8282), Acc: 0.2422(0.3502)
2022-01-05 14:04:56,708 Epoch[195/310], Step[0850/1251], Loss: 4.6092(3.8298), Acc: 0.1934(0.3492)
2022-01-05 14:05:54,474 Epoch[195/310], Step[0900/1251], Loss: 3.9853(3.8314), Acc: 0.3896(0.3483)
2022-01-05 14:06:52,491 Epoch[195/310], Step[0950/1251], Loss: 3.7291(3.8298), Acc: 0.5186(0.3482)
2022-01-05 14:07:49,537 Epoch[195/310], Step[1000/1251], Loss: 4.2464(3.8343), Acc: 0.3662(0.3476)
2022-01-05 14:08:46,031 Epoch[195/310], Step[1050/1251], Loss: 3.5934(3.8328), Acc: 0.2402(0.3463)
2022-01-05 14:09:44,272 Epoch[195/310], Step[1100/1251], Loss: 4.1160(3.8362), Acc: 0.4453(0.3460)
2022-01-05 14:10:41,368 Epoch[195/310], Step[1150/1251], Loss: 4.0110(3.8342), Acc: 0.3223(0.3452)
2022-01-05 14:11:39,155 Epoch[195/310], Step[1200/1251], Loss: 3.7741(3.8339), Acc: 0.4854(0.3444)
2022-01-05 14:12:36,074 Epoch[195/310], Step[1250/1251], Loss: 4.2851(3.8319), Acc: 0.3545(0.3438)
2022-01-05 14:12:38,140 ----- Epoch[195/310], Train Loss: 3.8319, Train Acc: 0.3438, time: 1508.39, Best Val(epoch194) Acc@1: 0.6919
2022-01-05 14:12:38,315 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 14:12:38,316 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 14:12:38,422 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 14:12:38,423 Now training epoch 196. LR=0.000272
2022-01-05 14:13:54,552 Epoch[196/310], Step[0000/1251], Loss: 3.4954(3.4954), Acc: 0.5156(0.5156)
2022-01-05 14:14:51,728 Epoch[196/310], Step[0050/1251], Loss: 3.8417(3.7570), Acc: 0.4746(0.3406)
2022-01-05 14:15:48,910 Epoch[196/310], Step[0100/1251], Loss: 4.0492(3.7655), Acc: 0.3564(0.3571)
2022-01-05 14:16:45,242 Epoch[196/310], Step[0150/1251], Loss: 4.3828(3.7595), Acc: 0.3330(0.3490)
2022-01-05 14:17:42,291 Epoch[196/310], Step[0200/1251], Loss: 3.5456(3.7637), Acc: 0.2021(0.3410)
2022-01-05 14:18:39,903 Epoch[196/310], Step[0250/1251], Loss: 3.7519(3.7790), Acc: 0.3311(0.3371)
2022-01-05 14:19:37,303 Epoch[196/310], Step[0300/1251], Loss: 3.4071(3.7904), Acc: 0.5186(0.3404)
2022-01-05 14:20:35,371 Epoch[196/310], Step[0350/1251], Loss: 3.8352(3.7969), Acc: 0.2900(0.3435)
2022-01-05 14:21:31,410 Epoch[196/310], Step[0400/1251], Loss: 3.8225(3.8035), Acc: 0.2119(0.3422)
2022-01-05 14:22:28,427 Epoch[196/310], Step[0450/1251], Loss: 3.8261(3.8059), Acc: 0.3262(0.3380)
2022-01-05 14:23:26,624 Epoch[196/310], Step[0500/1251], Loss: 4.2060(3.8093), Acc: 0.2373(0.3369)
2022-01-05 14:24:24,219 Epoch[196/310], Step[0550/1251], Loss: 4.3323(3.8186), Acc: 0.3311(0.3346)
2022-01-05 14:25:21,837 Epoch[196/310], Step[0600/1251], Loss: 3.7040(3.8172), Acc: 0.4551(0.3355)
2022-01-05 14:26:19,434 Epoch[196/310], Step[0650/1251], Loss: 3.8199(3.8158), Acc: 0.3486(0.3352)
2022-01-05 14:27:16,692 Epoch[196/310], Step[0700/1251], Loss: 3.7797(3.8157), Acc: 0.2109(0.3365)
2022-01-05 14:28:14,530 Epoch[196/310], Step[0750/1251], Loss: 3.4820(3.8193), Acc: 0.1562(0.3377)
2022-01-05 14:29:11,644 Epoch[196/310], Step[0800/1251], Loss: 3.6036(3.8160), Acc: 0.3164(0.3390)
2022-01-05 14:30:09,723 Epoch[196/310], Step[0850/1251], Loss: 3.4128(3.8147), Acc: 0.5068(0.3409)
2022-01-05 14:31:08,115 Epoch[196/310], Step[0900/1251], Loss: 4.0233(3.8167), Acc: 0.4102(0.3407)
2022-01-05 14:32:05,658 Epoch[196/310], Step[0950/1251], Loss: 3.7030(3.8149), Acc: 0.2236(0.3418)
2022-01-05 14:33:02,745 Epoch[196/310], Step[1000/1251], Loss: 3.9412(3.8158), Acc: 0.4180(0.3420)
2022-01-05 14:34:00,403 Epoch[196/310], Step[1050/1251], Loss: 3.9448(3.8159), Acc: 0.3916(0.3421)
2022-01-05 14:34:57,561 Epoch[196/310], Step[1100/1251], Loss: 3.2049(3.8161), Acc: 0.5801(0.3421)
2022-01-05 14:35:55,307 Epoch[196/310], Step[1150/1251], Loss: 3.7406(3.8155), Acc: 0.4424(0.3419)
2022-01-05 14:36:53,391 Epoch[196/310], Step[1200/1251], Loss: 3.9359(3.8152), Acc: 0.3789(0.3408)
2022-01-05 14:37:49,862 Epoch[196/310], Step[1250/1251], Loss: 3.7879(3.8124), Acc: 0.4307(0.3410)
2022-01-05 14:37:51,761 ----- Validation after Epoch: 196
2022-01-05 14:38:49,770 Val Step[0000/1563], Loss: 0.7006 (0.7006), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 14:38:51,327 Val Step[0050/1563], Loss: 2.2918 (0.7830), Acc@1: 0.4688 (0.8370), Acc@5: 0.8438 (0.9522)
2022-01-05 14:38:52,782 Val Step[0100/1563], Loss: 2.1204 (1.0953), Acc@1: 0.3750 (0.7519), Acc@5: 0.8438 (0.9245)
2022-01-05 14:38:54,242 Val Step[0150/1563], Loss: 0.4427 (1.0315), Acc@1: 0.9062 (0.7670), Acc@5: 1.0000 (0.9300)
2022-01-05 14:38:55,636 Val Step[0200/1563], Loss: 1.1648 (1.0416), Acc@1: 0.7188 (0.7690), Acc@5: 0.9062 (0.9265)
2022-01-05 14:38:57,017 Val Step[0250/1563], Loss: 0.5938 (0.9880), Acc@1: 0.9062 (0.7804), Acc@5: 1.0000 (0.9333)
2022-01-05 14:38:58,557 Val Step[0300/1563], Loss: 1.1000 (1.0503), Acc@1: 0.6875 (0.7612), Acc@5: 0.9375 (0.9277)
2022-01-05 14:39:00,030 Val Step[0350/1563], Loss: 1.1521 (1.0644), Acc@1: 0.7188 (0.7553), Acc@5: 0.9062 (0.9289)
2022-01-05 14:39:01,536 Val Step[0400/1563], Loss: 1.0472 (1.0698), Acc@1: 0.7188 (0.7516), Acc@5: 0.9688 (0.9302)
2022-01-05 14:39:03,119 Val Step[0450/1563], Loss: 0.8702 (1.0767), Acc@1: 0.7500 (0.7481), Acc@5: 0.9688 (0.9309)
2022-01-05 14:39:04,654 Val Step[0500/1563], Loss: 0.4565 (1.0659), Acc@1: 0.8750 (0.7521), Acc@5: 1.0000 (0.9325)
2022-01-05 14:39:06,146 Val Step[0550/1563], Loss: 0.6581 (1.0386), Acc@1: 0.8125 (0.7590), Acc@5: 1.0000 (0.9350)
2022-01-05 14:39:07,708 Val Step[0600/1563], Loss: 0.7762 (1.0473), Acc@1: 0.7812 (0.7581), Acc@5: 0.9375 (0.9341)
2022-01-05 14:39:09,262 Val Step[0650/1563], Loss: 0.6566 (1.0695), Acc@1: 0.8438 (0.7527), Acc@5: 1.0000 (0.9312)
2022-01-05 14:39:10,700 Val Step[0700/1563], Loss: 1.2097 (1.0997), Acc@1: 0.7188 (0.7463), Acc@5: 0.9062 (0.9270)
2022-01-05 14:39:12,208 Val Step[0750/1563], Loss: 1.3956 (1.1358), Acc@1: 0.7500 (0.7396), Acc@5: 0.9062 (0.9219)
2022-01-05 14:39:13,627 Val Step[0800/1563], Loss: 0.6927 (1.1742), Acc@1: 0.8438 (0.7299), Acc@5: 1.0000 (0.9167)
2022-01-05 14:39:15,133 Val Step[0850/1563], Loss: 1.5852 (1.1998), Acc@1: 0.6875 (0.7229), Acc@5: 0.9375 (0.9139)
2022-01-05 14:39:16,623 Val Step[0900/1563], Loss: 0.3441 (1.2002), Acc@1: 0.9062 (0.7238), Acc@5: 1.0000 (0.9132)
2022-01-05 14:39:18,204 Val Step[0950/1563], Loss: 1.4622 (1.2223), Acc@1: 0.7188 (0.7194), Acc@5: 0.8438 (0.9097)
2022-01-05 14:39:19,757 Val Step[1000/1563], Loss: 0.5941 (1.2461), Acc@1: 0.9375 (0.7135), Acc@5: 0.9375 (0.9064)
2022-01-05 14:39:21,160 Val Step[1050/1563], Loss: 0.3244 (1.2610), Acc@1: 0.9688 (0.7101), Acc@5: 0.9688 (0.9046)
2022-01-05 14:39:22,644 Val Step[1100/1563], Loss: 0.8355 (1.2765), Acc@1: 0.8125 (0.7071), Acc@5: 1.0000 (0.9023)
2022-01-05 14:39:24,058 Val Step[1150/1563], Loss: 1.1995 (1.2913), Acc@1: 0.7812 (0.7044), Acc@5: 0.8125 (0.9002)
2022-01-05 14:39:25,444 Val Step[1200/1563], Loss: 1.1385 (1.3060), Acc@1: 0.8125 (0.7012), Acc@5: 0.8750 (0.8977)
2022-01-05 14:39:26,865 Val Step[1250/1563], Loss: 0.8168 (1.3188), Acc@1: 0.8750 (0.7000), Acc@5: 0.8750 (0.8957)
2022-01-05 14:39:28,349 Val Step[1300/1563], Loss: 0.7373 (1.3283), Acc@1: 0.8750 (0.6980), Acc@5: 0.9375 (0.8944)
2022-01-05 14:39:29,770 Val Step[1350/1563], Loss: 2.2879 (1.3475), Acc@1: 0.3750 (0.6934), Acc@5: 0.8125 (0.8918)
2022-01-05 14:39:31,261 Val Step[1400/1563], Loss: 0.9972 (1.3557), Acc@1: 0.8125 (0.6919), Acc@5: 0.9375 (0.8907)
2022-01-05 14:39:32,727 Val Step[1450/1563], Loss: 1.8153 (1.3618), Acc@1: 0.5625 (0.6905), Acc@5: 0.8750 (0.8903)
2022-01-05 14:39:34,282 Val Step[1500/1563], Loss: 2.0894 (1.3503), Acc@1: 0.5312 (0.6931), Acc@5: 0.7812 (0.8916)
2022-01-05 14:39:35,727 Val Step[1550/1563], Loss: 0.9834 (1.3496), Acc@1: 0.8750 (0.6929), Acc@5: 0.9062 (0.8916)
2022-01-05 14:39:36,540 ----- Epoch[196/310], Validation Loss: 1.3479, Validation Acc@1: 0.6934, Validation Acc@5: 0.8918, time: 104.78
2022-01-05 14:39:36,540 ----- Epoch[196/310], Train Loss: 3.8124, Train Acc: 0.3410, time: 1513.33, Best Val(epoch196) Acc@1: 0.6934
2022-01-05 14:39:36,721 Max accuracy so far: 0.6934 at epoch_196
2022-01-05 14:39:36,722 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 14:39:36,722 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 14:39:36,868 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 14:39:37,225 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 14:39:37,226 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 14:39:37,365 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 14:39:37,365 Now training epoch 197. LR=0.000267
2022-01-05 14:40:55,437 Epoch[197/310], Step[0000/1251], Loss: 3.6865(3.6865), Acc: 0.2490(0.2490)
2022-01-05 14:41:52,068 Epoch[197/310], Step[0050/1251], Loss: 3.4035(3.8388), Acc: 0.5127(0.3583)
2022-01-05 14:42:48,519 Epoch[197/310], Step[0100/1251], Loss: 4.2724(3.8688), Acc: 0.1631(0.3482)
2022-01-05 14:43:44,268 Epoch[197/310], Step[0150/1251], Loss: 4.0278(3.8795), Acc: 0.3203(0.3461)
2022-01-05 14:44:41,745 Epoch[197/310], Step[0200/1251], Loss: 3.5771(3.8652), Acc: 0.4443(0.3420)
2022-01-05 14:45:37,896 Epoch[197/310], Step[0250/1251], Loss: 4.2654(3.8673), Acc: 0.3154(0.3450)
2022-01-05 14:46:34,833 Epoch[197/310], Step[0300/1251], Loss: 3.6183(3.8618), Acc: 0.2900(0.3475)
2022-01-05 14:47:30,512 Epoch[197/310], Step[0350/1251], Loss: 4.0444(3.8588), Acc: 0.4355(0.3452)
2022-01-05 14:48:28,418 Epoch[197/310], Step[0400/1251], Loss: 4.3994(3.8452), Acc: 0.2451(0.3452)
2022-01-05 14:49:26,174 Epoch[197/310], Step[0450/1251], Loss: 4.0621(3.8343), Acc: 0.4424(0.3473)
2022-01-05 14:50:22,840 Epoch[197/310], Step[0500/1251], Loss: 3.3705(3.8346), Acc: 0.3525(0.3484)
2022-01-05 14:51:19,685 Epoch[197/310], Step[0550/1251], Loss: 4.1783(3.8276), Acc: 0.1113(0.3484)
2022-01-05 14:52:16,141 Epoch[197/310], Step[0600/1251], Loss: 3.8364(3.8302), Acc: 0.0303(0.3475)
2022-01-05 14:53:13,288 Epoch[197/310], Step[0650/1251], Loss: 3.5221(3.8339), Acc: 0.3857(0.3451)
2022-01-05 14:54:09,325 Epoch[197/310], Step[0700/1251], Loss: 3.8756(3.8378), Acc: 0.4385(0.3454)
2022-01-05 14:55:06,598 Epoch[197/310], Step[0750/1251], Loss: 3.4823(3.8366), Acc: 0.5127(0.3460)
2022-01-05 14:56:01,728 Epoch[197/310], Step[0800/1251], Loss: 4.0343(3.8412), Acc: 0.2500(0.3455)
2022-01-05 14:56:58,781 Epoch[197/310], Step[0850/1251], Loss: 3.6390(3.8418), Acc: 0.5352(0.3465)
2022-01-05 14:57:55,657 Epoch[197/310], Step[0900/1251], Loss: 3.6538(3.8406), Acc: 0.4990(0.3472)
2022-01-05 14:58:52,077 Epoch[197/310], Step[0950/1251], Loss: 4.2660(3.8380), Acc: 0.3574(0.3486)
2022-01-05 14:59:50,116 Epoch[197/310], Step[1000/1251], Loss: 3.7596(3.8357), Acc: 0.3389(0.3481)
2022-01-05 15:00:48,362 Epoch[197/310], Step[1050/1251], Loss: 3.5245(3.8375), Acc: 0.5391(0.3479)
2022-01-05 15:01:46,084 Epoch[197/310], Step[1100/1251], Loss: 3.7026(3.8390), Acc: 0.4922(0.3470)
2022-01-05 15:02:43,536 Epoch[197/310], Step[1150/1251], Loss: 4.0866(3.8396), Acc: 0.2354(0.3466)
2022-01-05 15:03:40,490 Epoch[197/310], Step[1200/1251], Loss: 3.9492(3.8425), Acc: 0.4170(0.3467)
2022-01-05 15:04:37,365 Epoch[197/310], Step[1250/1251], Loss: 3.6374(3.8413), Acc: 0.2305(0.3480)
2022-01-05 15:04:39,334 ----- Epoch[197/310], Train Loss: 3.8413, Train Acc: 0.3480, time: 1501.96, Best Val(epoch196) Acc@1: 0.6934
2022-01-05 15:04:39,502 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 15:04:39,503 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 15:04:39,614 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 15:04:39,614 Now training epoch 198. LR=0.000263
2022-01-05 15:05:57,022 Epoch[198/310], Step[0000/1251], Loss: 3.5789(3.5789), Acc: 0.0479(0.0479)
2022-01-05 15:06:52,379 Epoch[198/310], Step[0050/1251], Loss: 3.8301(3.7594), Acc: 0.4316(0.3547)
2022-01-05 15:07:49,491 Epoch[198/310], Step[0100/1251], Loss: 4.2215(3.7574), Acc: 0.1475(0.3517)
2022-01-05 15:08:45,248 Epoch[198/310], Step[0150/1251], Loss: 3.8245(3.7936), Acc: 0.5068(0.3544)
2022-01-05 15:09:42,741 Epoch[198/310], Step[0200/1251], Loss: 3.8251(3.8074), Acc: 0.4004(0.3443)
2022-01-05 15:10:40,737 Epoch[198/310], Step[0250/1251], Loss: 4.3888(3.8143), Acc: 0.2412(0.3418)
2022-01-05 15:11:38,618 Epoch[198/310], Step[0300/1251], Loss: 4.1502(3.8028), Acc: 0.2324(0.3462)
2022-01-05 15:12:36,682 Epoch[198/310], Step[0350/1251], Loss: 3.5746(3.8140), Acc: 0.3809(0.3437)
2022-01-05 15:13:34,170 Epoch[198/310], Step[0400/1251], Loss: 3.7086(3.8188), Acc: 0.3291(0.3396)
2022-01-05 15:14:30,914 Epoch[198/310], Step[0450/1251], Loss: 3.7680(3.8144), Acc: 0.2510(0.3416)
2022-01-05 15:15:27,094 Epoch[198/310], Step[0500/1251], Loss: 3.9236(3.8166), Acc: 0.4717(0.3423)
2022-01-05 15:16:23,182 Epoch[198/310], Step[0550/1251], Loss: 4.3511(3.8209), Acc: 0.3398(0.3440)
2022-01-05 15:17:19,512 Epoch[198/310], Step[0600/1251], Loss: 3.3683(3.8141), Acc: 0.4131(0.3418)
2022-01-05 15:18:17,007 Epoch[198/310], Step[0650/1251], Loss: 3.6636(3.8218), Acc: 0.2168(0.3425)
2022-01-05 15:19:15,403 Epoch[198/310], Step[0700/1251], Loss: 4.0475(3.8253), Acc: 0.1436(0.3415)
2022-01-05 15:20:13,269 Epoch[198/310], Step[0750/1251], Loss: 2.9771(3.8275), Acc: 0.5742(0.3419)
2022-01-05 15:21:10,693 Epoch[198/310], Step[0800/1251], Loss: 3.5529(3.8268), Acc: 0.4717(0.3425)
2022-01-05 15:22:08,350 Epoch[198/310], Step[0850/1251], Loss: 3.4067(3.8266), Acc: 0.4102(0.3431)
2022-01-05 15:23:04,369 Epoch[198/310], Step[0900/1251], Loss: 4.0371(3.8294), Acc: 0.1494(0.3431)
2022-01-05 15:24:00,112 Epoch[198/310], Step[0950/1251], Loss: 4.2898(3.8319), Acc: 0.2715(0.3445)
2022-01-05 15:24:56,813 Epoch[198/310], Step[1000/1251], Loss: 3.6363(3.8306), Acc: 0.4922(0.3450)
2022-01-05 15:25:53,513 Epoch[198/310], Step[1050/1251], Loss: 3.0650(3.8305), Acc: 0.4385(0.3462)
2022-01-05 15:26:48,843 Epoch[198/310], Step[1100/1251], Loss: 3.8650(3.8308), Acc: 0.4365(0.3458)
2022-01-05 15:27:45,472 Epoch[198/310], Step[1150/1251], Loss: 4.0910(3.8311), Acc: 0.3564(0.3450)
2022-01-05 15:28:43,219 Epoch[198/310], Step[1200/1251], Loss: 3.6572(3.8316), Acc: 0.4414(0.3440)
2022-01-05 15:29:41,316 Epoch[198/310], Step[1250/1251], Loss: 4.1043(3.8289), Acc: 0.1191(0.3452)
2022-01-05 15:29:43,211 ----- Validation after Epoch: 198
2022-01-05 15:30:40,133 Val Step[0000/1563], Loss: 0.7689 (0.7689), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2022-01-05 15:30:41,572 Val Step[0050/1563], Loss: 2.0173 (0.8396), Acc@1: 0.5000 (0.8358), Acc@5: 0.8438 (0.9522)
2022-01-05 15:30:42,955 Val Step[0100/1563], Loss: 2.0079 (1.1436), Acc@1: 0.5000 (0.7525), Acc@5: 0.8750 (0.9226)
2022-01-05 15:30:44,428 Val Step[0150/1563], Loss: 0.5264 (1.0720), Acc@1: 0.8438 (0.7678), Acc@5: 1.0000 (0.9288)
2022-01-05 15:30:45,830 Val Step[0200/1563], Loss: 1.2380 (1.0743), Acc@1: 0.7812 (0.7683), Acc@5: 0.9062 (0.9286)
2022-01-05 15:30:47,236 Val Step[0250/1563], Loss: 0.7602 (1.0225), Acc@1: 0.8750 (0.7798), Acc@5: 1.0000 (0.9351)
2022-01-05 15:30:48,727 Val Step[0300/1563], Loss: 1.2089 (1.0806), Acc@1: 0.7500 (0.7600), Acc@5: 0.9688 (0.9309)
2022-01-05 15:30:50,151 Val Step[0350/1563], Loss: 1.1468 (1.0945), Acc@1: 0.7188 (0.7545), Acc@5: 0.9062 (0.9322)
2022-01-05 15:30:51,465 Val Step[0400/1563], Loss: 0.8551 (1.0996), Acc@1: 0.8438 (0.7497), Acc@5: 0.9688 (0.9335)
2022-01-05 15:30:53,007 Val Step[0450/1563], Loss: 1.0102 (1.1052), Acc@1: 0.6875 (0.7469), Acc@5: 1.0000 (0.9345)
2022-01-05 15:30:54,464 Val Step[0500/1563], Loss: 0.4171 (1.0949), Acc@1: 0.9375 (0.7500), Acc@5: 1.0000 (0.9358)
2022-01-05 15:30:55,891 Val Step[0550/1563], Loss: 0.8194 (1.0728), Acc@1: 0.7500 (0.7556), Acc@5: 0.9375 (0.9372)
2022-01-05 15:30:57,328 Val Step[0600/1563], Loss: 0.8598 (1.0778), Acc@1: 0.8125 (0.7546), Acc@5: 0.9375 (0.9367)
2022-01-05 15:30:58,792 Val Step[0650/1563], Loss: 0.7834 (1.1004), Acc@1: 0.8125 (0.7497), Acc@5: 1.0000 (0.9335)
2022-01-05 15:31:00,320 Val Step[0700/1563], Loss: 1.0496 (1.1317), Acc@1: 0.7812 (0.7426), Acc@5: 0.9062 (0.9289)
2022-01-05 15:31:01,818 Val Step[0750/1563], Loss: 1.4953 (1.1663), Acc@1: 0.6562 (0.7354), Acc@5: 0.8750 (0.9239)
2022-01-05 15:31:03,212 Val Step[0800/1563], Loss: 1.0462 (1.2025), Acc@1: 0.7500 (0.7266), Acc@5: 1.0000 (0.9190)
2022-01-05 15:31:04,702 Val Step[0850/1563], Loss: 1.7335 (1.2298), Acc@1: 0.5938 (0.7206), Acc@5: 0.8125 (0.9152)
2022-01-05 15:31:06,216 Val Step[0900/1563], Loss: 0.3584 (1.2317), Acc@1: 0.9375 (0.7218), Acc@5: 1.0000 (0.9141)
2022-01-05 15:31:07,841 Val Step[0950/1563], Loss: 1.4963 (1.2538), Acc@1: 0.7188 (0.7177), Acc@5: 0.8750 (0.9110)
2022-01-05 15:31:09,244 Val Step[1000/1563], Loss: 0.5087 (1.2766), Acc@1: 0.9688 (0.7124), Acc@5: 0.9688 (0.9079)
2022-01-05 15:31:10,607 Val Step[1050/1563], Loss: 0.4026 (1.2909), Acc@1: 0.9688 (0.7098), Acc@5: 0.9688 (0.9062)
2022-01-05 15:31:12,004 Val Step[1100/1563], Loss: 1.0565 (1.3066), Acc@1: 0.7500 (0.7066), Acc@5: 0.9375 (0.9039)
2022-01-05 15:31:13,373 Val Step[1150/1563], Loss: 1.4752 (1.3239), Acc@1: 0.7500 (0.7040), Acc@5: 0.8125 (0.9012)
2022-01-05 15:31:14,779 Val Step[1200/1563], Loss: 1.2520 (1.3411), Acc@1: 0.7812 (0.7000), Acc@5: 0.8438 (0.8985)
2022-01-05 15:31:16,312 Val Step[1250/1563], Loss: 0.7758 (1.3523), Acc@1: 0.8750 (0.6983), Acc@5: 0.9062 (0.8963)
2022-01-05 15:31:17,737 Val Step[1300/1563], Loss: 1.0169 (1.3630), Acc@1: 0.8750 (0.6959), Acc@5: 0.9062 (0.8950)
2022-01-05 15:31:19,128 Val Step[1350/1563], Loss: 2.1724 (1.3805), Acc@1: 0.4062 (0.6918), Acc@5: 0.7812 (0.8922)
2022-01-05 15:31:20,509 Val Step[1400/1563], Loss: 1.0331 (1.3890), Acc@1: 0.7188 (0.6899), Acc@5: 0.9688 (0.8909)
2022-01-05 15:31:21,904 Val Step[1450/1563], Loss: 1.6475 (1.3959), Acc@1: 0.6562 (0.6883), Acc@5: 0.9375 (0.8904)
2022-01-05 15:31:23,352 Val Step[1500/1563], Loss: 1.9967 (1.3858), Acc@1: 0.5625 (0.6910), Acc@5: 0.7812 (0.8916)
2022-01-05 15:31:24,747 Val Step[1550/1563], Loss: 0.9592 (1.3858), Acc@1: 0.8750 (0.6909), Acc@5: 0.9062 (0.8917)
2022-01-05 15:31:25,609 ----- Epoch[198/310], Validation Loss: 1.3842, Validation Acc@1: 0.6912, Validation Acc@5: 0.8918, time: 102.39
2022-01-05 15:31:25,609 ----- Epoch[198/310], Train Loss: 3.8289, Train Acc: 0.3452, time: 1503.59, Best Val(epoch196) Acc@1: 0.6934
2022-01-05 15:31:25,787 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 15:31:25,787 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 15:31:25,967 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 15:31:26,204 Now training epoch 199. LR=0.000258
2022-01-05 15:32:39,760 Epoch[199/310], Step[0000/1251], Loss: 3.9799(3.9799), Acc: 0.1855(0.1855)
2022-01-05 15:33:36,457 Epoch[199/310], Step[0050/1251], Loss: 3.5710(3.7967), Acc: 0.4492(0.3311)
2022-01-05 15:34:32,816 Epoch[199/310], Step[0100/1251], Loss: 3.4938(3.7877), Acc: 0.2861(0.3443)
2022-01-05 15:35:30,407 Epoch[199/310], Step[0150/1251], Loss: 4.2982(3.8093), Acc: 0.2520(0.3470)
2022-01-05 15:36:27,442 Epoch[199/310], Step[0200/1251], Loss: 3.6043(3.8038), Acc: 0.3672(0.3480)
2022-01-05 15:37:23,501 Epoch[199/310], Step[0250/1251], Loss: 3.7373(3.8093), Acc: 0.3418(0.3474)
2022-01-05 15:38:19,470 Epoch[199/310], Step[0300/1251], Loss: 4.2188(3.8142), Acc: 0.3525(0.3479)
2022-01-05 15:39:15,700 Epoch[199/310], Step[0350/1251], Loss: 3.6381(3.8252), Acc: 0.2871(0.3492)
2022-01-05 15:40:12,790 Epoch[199/310], Step[0400/1251], Loss: 3.6571(3.8226), Acc: 0.3711(0.3480)
2022-01-05 15:41:10,165 Epoch[199/310], Step[0450/1251], Loss: 3.7493(3.8195), Acc: 0.3311(0.3462)
2022-01-05 15:42:07,262 Epoch[199/310], Step[0500/1251], Loss: 3.7217(3.8251), Acc: 0.4238(0.3477)
2022-01-05 15:43:05,239 Epoch[199/310], Step[0550/1251], Loss: 3.0076(3.8235), Acc: 0.1484(0.3472)
2022-01-05 15:44:01,243 Epoch[199/310], Step[0600/1251], Loss: 3.8428(3.8213), Acc: 0.3799(0.3488)
2022-01-05 15:44:57,964 Epoch[199/310], Step[0650/1251], Loss: 3.9701(3.8288), Acc: 0.4746(0.3477)
2022-01-05 15:45:56,707 Epoch[199/310], Step[0700/1251], Loss: 3.2189(3.8265), Acc: 0.2715(0.3464)
2022-01-05 15:46:54,218 Epoch[199/310], Step[0750/1251], Loss: 4.4443(3.8248), Acc: 0.3301(0.3463)
2022-01-05 15:47:52,446 Epoch[199/310], Step[0800/1251], Loss: 3.6267(3.8259), Acc: 0.2197(0.3470)
2022-01-05 15:48:49,806 Epoch[199/310], Step[0850/1251], Loss: 4.1282(3.8279), Acc: 0.2881(0.3483)
2022-01-05 15:49:47,590 Epoch[199/310], Step[0900/1251], Loss: 4.5512(3.8316), Acc: 0.3379(0.3478)
2022-01-05 15:50:45,513 Epoch[199/310], Step[0950/1251], Loss: 3.8426(3.8304), Acc: 0.4697(0.3483)
2022-01-05 15:51:43,147 Epoch[199/310], Step[1000/1251], Loss: 4.1835(3.8315), Acc: 0.3857(0.3473)
2022-01-05 15:52:40,956 Epoch[199/310], Step[1050/1251], Loss: 3.0966(3.8273), Acc: 0.4238(0.3489)
2022-01-05 15:53:38,909 Epoch[199/310], Step[1100/1251], Loss: 3.1128(3.8240), Acc: 0.4688(0.3497)
2022-01-05 15:54:37,752 Epoch[199/310], Step[1150/1251], Loss: 3.5953(3.8232), Acc: 0.1934(0.3500)
2022-01-05 15:55:35,202 Epoch[199/310], Step[1200/1251], Loss: 4.1812(3.8268), Acc: 0.4043(0.3501)
2022-01-05 15:56:33,576 Epoch[199/310], Step[1250/1251], Loss: 3.5746(3.8278), Acc: 0.5322(0.3497)
2022-01-05 15:56:35,455 ----- Epoch[199/310], Train Loss: 3.8278, Train Acc: 0.3497, time: 1509.25, Best Val(epoch196) Acc@1: 0.6934
2022-01-05 15:56:35,624 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 15:56:35,624 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 15:56:35,734 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 15:56:35,735 Now training epoch 200. LR=0.000254
2022-01-05 15:57:53,668 Epoch[200/310], Step[0000/1251], Loss: 3.4350(3.4350), Acc: 0.3535(0.3535)
2022-01-05 15:58:49,733 Epoch[200/310], Step[0050/1251], Loss: 3.4696(3.6998), Acc: 0.2471(0.3630)
2022-01-05 15:59:46,617 Epoch[200/310], Step[0100/1251], Loss: 3.5791(3.7234), Acc: 0.2178(0.3534)
2022-01-05 16:00:43,146 Epoch[200/310], Step[0150/1251], Loss: 3.6246(3.7294), Acc: 0.1416(0.3557)
2022-01-05 16:01:38,686 Epoch[200/310], Step[0200/1251], Loss: 3.2892(3.7455), Acc: 0.2471(0.3551)
2022-01-05 16:02:35,086 Epoch[200/310], Step[0250/1251], Loss: 4.1417(3.7517), Acc: 0.4053(0.3519)
2022-01-05 16:03:32,251 Epoch[200/310], Step[0300/1251], Loss: 3.5783(3.7637), Acc: 0.3916(0.3520)
2022-01-05 16:04:29,060 Epoch[200/310], Step[0350/1251], Loss: 4.0982(3.7676), Acc: 0.3643(0.3532)
2022-01-05 16:05:24,777 Epoch[200/310], Step[0400/1251], Loss: 3.6503(3.7756), Acc: 0.4189(0.3517)
2022-01-05 16:06:22,249 Epoch[200/310], Step[0450/1251], Loss: 3.9935(3.7850), Acc: 0.1934(0.3514)
2022-01-05 16:07:19,902 Epoch[200/310], Step[0500/1251], Loss: 4.3025(3.7856), Acc: 0.2480(0.3547)
2022-01-05 16:08:17,839 Epoch[200/310], Step[0550/1251], Loss: 4.4977(3.7923), Acc: 0.2510(0.3521)
2022-01-05 16:09:15,227 Epoch[200/310], Step[0600/1251], Loss: 3.5067(3.7894), Acc: 0.3623(0.3533)
2022-01-05 16:10:11,968 Epoch[200/310], Step[0650/1251], Loss: 3.6547(3.7902), Acc: 0.4736(0.3574)
2022-01-05 16:11:07,763 Epoch[200/310], Step[0700/1251], Loss: 4.1418(3.7933), Acc: 0.3721(0.3566)
2022-01-05 16:12:04,780 Epoch[200/310], Step[0750/1251], Loss: 3.7917(3.7960), Acc: 0.2568(0.3557)
2022-01-05 16:13:01,279 Epoch[200/310], Step[0800/1251], Loss: 4.2444(3.7987), Acc: 0.3369(0.3553)
2022-01-05 16:13:57,485 Epoch[200/310], Step[0850/1251], Loss: 3.7234(3.7995), Acc: 0.2607(0.3546)
2022-01-05 16:14:54,614 Epoch[200/310], Step[0900/1251], Loss: 3.8100(3.7966), Acc: 0.4365(0.3543)
2022-01-05 16:15:51,848 Epoch[200/310], Step[0950/1251], Loss: 4.2682(3.8005), Acc: 0.2666(0.3541)
2022-01-05 16:16:48,940 Epoch[200/310], Step[1000/1251], Loss: 3.5838(3.8051), Acc: 0.2373(0.3535)
2022-01-05 16:17:45,830 Epoch[200/310], Step[1050/1251], Loss: 3.5358(3.8032), Acc: 0.4619(0.3544)
2022-01-05 16:18:43,697 Epoch[200/310], Step[1100/1251], Loss: 3.8669(3.8023), Acc: 0.4092(0.3540)
2022-01-05 16:19:41,978 Epoch[200/310], Step[1150/1251], Loss: 4.2508(3.7999), Acc: 0.2510(0.3535)
2022-01-05 16:20:39,307 Epoch[200/310], Step[1200/1251], Loss: 3.3765(3.7987), Acc: 0.3447(0.3524)
2022-01-05 16:21:36,022 Epoch[200/310], Step[1250/1251], Loss: 3.8618(3.7974), Acc: 0.4189(0.3518)
2022-01-05 16:21:38,354 ----- Validation after Epoch: 200
2022-01-05 16:22:36,750 Val Step[0000/1563], Loss: 0.7852 (0.7852), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 16:22:38,258 Val Step[0050/1563], Loss: 2.5146 (0.8474), Acc@1: 0.4375 (0.8303), Acc@5: 0.7812 (0.9534)
2022-01-05 16:22:39,794 Val Step[0100/1563], Loss: 1.8643 (1.1304), Acc@1: 0.5938 (0.7481), Acc@5: 0.8125 (0.9202)
2022-01-05 16:22:41,269 Val Step[0150/1563], Loss: 0.4692 (1.0622), Acc@1: 0.9375 (0.7649), Acc@5: 1.0000 (0.9278)
2022-01-05 16:22:42,681 Val Step[0200/1563], Loss: 1.1015 (1.0843), Acc@1: 0.7188 (0.7635), Acc@5: 0.9062 (0.9254)
2022-01-05 16:22:44,112 Val Step[0250/1563], Loss: 0.6909 (1.0280), Acc@1: 0.9062 (0.7765), Acc@5: 1.0000 (0.9326)
2022-01-05 16:22:45,586 Val Step[0300/1563], Loss: 1.2375 (1.0890), Acc@1: 0.7500 (0.7601), Acc@5: 0.9062 (0.9270)
2022-01-05 16:22:47,031 Val Step[0350/1563], Loss: 1.3079 (1.1058), Acc@1: 0.6875 (0.7548), Acc@5: 0.9062 (0.9277)
2022-01-05 16:22:48,496 Val Step[0400/1563], Loss: 1.0810 (1.1132), Acc@1: 0.7812 (0.7497), Acc@5: 0.9688 (0.9288)
2022-01-05 16:22:50,032 Val Step[0450/1563], Loss: 0.8604 (1.1186), Acc@1: 0.7812 (0.7470), Acc@5: 1.0000 (0.9296)
2022-01-05 16:22:51,539 Val Step[0500/1563], Loss: 0.5349 (1.1095), Acc@1: 0.9688 (0.7493), Acc@5: 1.0000 (0.9316)
2022-01-05 16:22:53,058 Val Step[0550/1563], Loss: 1.0290 (1.0852), Acc@1: 0.7500 (0.7555), Acc@5: 0.9688 (0.9340)
2022-01-05 16:22:54,594 Val Step[0600/1563], Loss: 0.9608 (1.0897), Acc@1: 0.7812 (0.7555), Acc@5: 0.9062 (0.9329)
2022-01-05 16:22:56,054 Val Step[0650/1563], Loss: 0.8955 (1.1103), Acc@1: 0.7812 (0.7512), Acc@5: 0.9688 (0.9303)
2022-01-05 16:22:57,569 Val Step[0700/1563], Loss: 1.1693 (1.1375), Acc@1: 0.7812 (0.7446), Acc@5: 0.9062 (0.9266)
2022-01-05 16:22:59,103 Val Step[0750/1563], Loss: 1.3782 (1.1735), Acc@1: 0.7500 (0.7372), Acc@5: 0.8750 (0.9217)
2022-01-05 16:23:00,475 Val Step[0800/1563], Loss: 0.8241 (1.2130), Acc@1: 0.8125 (0.7282), Acc@5: 1.0000 (0.9169)
2022-01-05 16:23:01,923 Val Step[0850/1563], Loss: 1.3049 (1.2421), Acc@1: 0.6875 (0.7212), Acc@5: 0.9062 (0.9133)
2022-01-05 16:23:03,401 Val Step[0900/1563], Loss: 0.3398 (1.2413), Acc@1: 0.9688 (0.7231), Acc@5: 1.0000 (0.9126)
2022-01-05 16:23:04,871 Val Step[0950/1563], Loss: 1.4376 (1.2630), Acc@1: 0.7500 (0.7186), Acc@5: 0.9375 (0.9094)
2022-01-05 16:23:06,255 Val Step[1000/1563], Loss: 0.5959 (1.2874), Acc@1: 0.9062 (0.7124), Acc@5: 1.0000 (0.9062)
2022-01-05 16:23:07,703 Val Step[1050/1563], Loss: 0.4814 (1.3020), Acc@1: 0.9688 (0.7091), Acc@5: 1.0000 (0.9047)
2022-01-05 16:23:09,167 Val Step[1100/1563], Loss: 0.8851 (1.3170), Acc@1: 0.7812 (0.7058), Acc@5: 0.9688 (0.9024)
2022-01-05 16:23:10,588 Val Step[1150/1563], Loss: 1.3560 (1.3327), Acc@1: 0.7812 (0.7029), Acc@5: 0.8125 (0.9004)
2022-01-05 16:23:12,046 Val Step[1200/1563], Loss: 1.2543 (1.3475), Acc@1: 0.8125 (0.7001), Acc@5: 0.8750 (0.8981)
2022-01-05 16:23:13,455 Val Step[1250/1563], Loss: 0.8526 (1.3600), Acc@1: 0.8438 (0.6983), Acc@5: 0.8750 (0.8961)
2022-01-05 16:23:14,933 Val Step[1300/1563], Loss: 0.8721 (1.3708), Acc@1: 0.8438 (0.6963), Acc@5: 0.9062 (0.8947)
2022-01-05 16:23:16,353 Val Step[1350/1563], Loss: 1.8470 (1.3879), Acc@1: 0.5625 (0.6928), Acc@5: 0.7812 (0.8921)
2022-01-05 16:23:17,790 Val Step[1400/1563], Loss: 1.2799 (1.3964), Acc@1: 0.6875 (0.6912), Acc@5: 0.9062 (0.8907)
2022-01-05 16:23:19,201 Val Step[1450/1563], Loss: 1.6927 (1.4007), Acc@1: 0.5625 (0.6901), Acc@5: 0.9062 (0.8905)
2022-01-05 16:23:20,638 Val Step[1500/1563], Loss: 1.7478 (1.3892), Acc@1: 0.6250 (0.6927), Acc@5: 0.9062 (0.8920)
2022-01-05 16:23:22,046 Val Step[1550/1563], Loss: 1.1195 (1.3895), Acc@1: 0.8750 (0.6924), Acc@5: 0.9062 (0.8918)
2022-01-05 16:23:23,925 ----- Epoch[200/310], Validation Loss: 1.3876, Validation Acc@1: 0.6928, Validation Acc@5: 0.8921, time: 105.57
2022-01-05 16:23:23,925 ----- Epoch[200/310], Train Loss: 3.7974, Train Acc: 0.3518, time: 1502.62, Best Val(epoch196) Acc@1: 0.6934
2022-01-05 16:23:24,089 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-200-Loss-3.7964899715283313.pdparams
2022-01-05 16:23:24,090 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-200-Loss-3.7964899715283313.pdopt
2022-01-05 16:23:24,131 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-200-Loss-3.7964899715283313-EMA.pdparams
2022-01-05 16:23:24,132 Now training epoch 201. LR=0.000249
2022-01-05 16:24:43,723 Epoch[201/310], Step[0000/1251], Loss: 3.9383(3.9383), Acc: 0.2783(0.2783)
2022-01-05 16:25:42,223 Epoch[201/310], Step[0050/1251], Loss: 3.6391(3.7393), Acc: 0.5244(0.3657)
2022-01-05 16:26:38,847 Epoch[201/310], Step[0100/1251], Loss: 4.3255(3.7817), Acc: 0.2842(0.3548)
2022-01-05 16:27:35,525 Epoch[201/310], Step[0150/1251], Loss: 3.7953(3.8177), Acc: 0.4854(0.3490)
2022-01-05 16:28:32,475 Epoch[201/310], Step[0200/1251], Loss: 3.5399(3.8252), Acc: 0.3750(0.3477)
2022-01-05 16:29:29,739 Epoch[201/310], Step[0250/1251], Loss: 3.9936(3.8129), Acc: 0.3447(0.3504)
2022-01-05 16:30:27,016 Epoch[201/310], Step[0300/1251], Loss: 4.0410(3.8167), Acc: 0.4043(0.3518)
2022-01-05 16:31:23,302 Epoch[201/310], Step[0350/1251], Loss: 3.6680(3.8206), Acc: 0.2197(0.3491)
2022-01-05 16:32:19,102 Epoch[201/310], Step[0400/1251], Loss: 3.8626(3.8126), Acc: 0.4883(0.3496)
2022-01-05 16:33:17,019 Epoch[201/310], Step[0450/1251], Loss: 3.4531(3.8135), Acc: 0.3662(0.3498)
2022-01-05 16:34:14,399 Epoch[201/310], Step[0500/1251], Loss: 3.6060(3.8106), Acc: 0.4248(0.3489)
2022-01-05 16:35:12,328 Epoch[201/310], Step[0550/1251], Loss: 3.9008(3.8128), Acc: 0.3945(0.3488)
2022-01-05 16:36:08,863 Epoch[201/310], Step[0600/1251], Loss: 4.1053(3.8131), Acc: 0.3115(0.3487)
2022-01-05 16:37:06,042 Epoch[201/310], Step[0650/1251], Loss: 3.5853(3.8135), Acc: 0.4346(0.3480)
2022-01-05 16:38:02,575 Epoch[201/310], Step[0700/1251], Loss: 3.3117(3.8099), Acc: 0.4404(0.3487)
2022-01-05 16:39:00,748 Epoch[201/310], Step[0750/1251], Loss: 3.7185(3.8132), Acc: 0.5283(0.3495)
2022-01-05 16:39:59,515 Epoch[201/310], Step[0800/1251], Loss: 3.5729(3.8153), Acc: 0.4160(0.3492)
2022-01-05 16:40:56,217 Epoch[201/310], Step[0850/1251], Loss: 4.2504(3.8150), Acc: 0.2275(0.3495)
2022-01-05 16:41:53,349 Epoch[201/310], Step[0900/1251], Loss: 3.5537(3.8207), Acc: 0.3535(0.3491)
2022-01-05 16:42:50,712 Epoch[201/310], Step[0950/1251], Loss: 3.8597(3.8198), Acc: 0.2910(0.3478)
2022-01-05 16:43:47,807 Epoch[201/310], Step[1000/1251], Loss: 4.2178(3.8210), Acc: 0.4023(0.3470)
2022-01-05 16:44:44,589 Epoch[201/310], Step[1050/1251], Loss: 4.2189(3.8242), Acc: 0.3096(0.3474)
2022-01-05 16:45:41,205 Epoch[201/310], Step[1100/1251], Loss: 3.3289(3.8218), Acc: 0.3994(0.3469)
2022-01-05 16:46:38,428 Epoch[201/310], Step[1150/1251], Loss: 3.6325(3.8188), Acc: 0.3281(0.3487)
2022-01-05 16:47:35,918 Epoch[201/310], Step[1200/1251], Loss: 3.6727(3.8158), Acc: 0.1250(0.3488)
2022-01-05 16:48:32,558 Epoch[201/310], Step[1250/1251], Loss: 3.5111(3.8156), Acc: 0.3574(0.3483)
2022-01-05 16:48:34,438 ----- Epoch[201/310], Train Loss: 3.8156, Train Acc: 0.3483, time: 1510.30, Best Val(epoch196) Acc@1: 0.6934
2022-01-05 16:48:34,606 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 16:48:34,607 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 16:48:34,713 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 16:48:34,713 Now training epoch 202. LR=0.000245
2022-01-05 16:49:46,344 Epoch[202/310], Step[0000/1251], Loss: 3.8659(3.8659), Acc: 0.3477(0.3477)
2022-01-05 16:50:43,145 Epoch[202/310], Step[0050/1251], Loss: 4.0460(3.8325), Acc: 0.4307(0.3433)
2022-01-05 16:51:38,906 Epoch[202/310], Step[0100/1251], Loss: 3.8476(3.8366), Acc: 0.4004(0.3509)
2022-01-05 16:52:35,111 Epoch[202/310], Step[0150/1251], Loss: 3.7258(3.8243), Acc: 0.1865(0.3471)
2022-01-05 16:53:30,458 Epoch[202/310], Step[0200/1251], Loss: 4.0547(3.8268), Acc: 0.3281(0.3389)
2022-01-05 16:54:26,235 Epoch[202/310], Step[0250/1251], Loss: 3.5696(3.8240), Acc: 0.2373(0.3382)
2022-01-05 16:55:23,181 Epoch[202/310], Step[0300/1251], Loss: 3.9523(3.8232), Acc: 0.2705(0.3397)
2022-01-05 16:56:18,859 Epoch[202/310], Step[0350/1251], Loss: 3.7003(3.8213), Acc: 0.4883(0.3437)
2022-01-05 16:57:15,836 Epoch[202/310], Step[0400/1251], Loss: 3.7867(3.8107), Acc: 0.3652(0.3452)
2022-01-05 16:58:12,968 Epoch[202/310], Step[0450/1251], Loss: 4.0067(3.8114), Acc: 0.3799(0.3440)
2022-01-05 16:59:09,882 Epoch[202/310], Step[0500/1251], Loss: 3.2425(3.8141), Acc: 0.4316(0.3457)
2022-01-05 17:00:07,836 Epoch[202/310], Step[0550/1251], Loss: 4.0706(3.8161), Acc: 0.3174(0.3456)
2022-01-05 17:01:05,022 Epoch[202/310], Step[0600/1251], Loss: 3.9715(3.8154), Acc: 0.3232(0.3466)
2022-01-05 17:02:01,020 Epoch[202/310], Step[0650/1251], Loss: 3.7115(3.8132), Acc: 0.4678(0.3464)
2022-01-05 17:02:56,729 Epoch[202/310], Step[0700/1251], Loss: 4.0013(3.8119), Acc: 0.2256(0.3469)
2022-01-05 17:03:54,359 Epoch[202/310], Step[0750/1251], Loss: 3.8563(3.8139), Acc: 0.2266(0.3436)
2022-01-05 17:04:50,885 Epoch[202/310], Step[0800/1251], Loss: 3.6417(3.8153), Acc: 0.4639(0.3421)
2022-01-05 17:05:48,434 Epoch[202/310], Step[0850/1251], Loss: 3.6486(3.8127), Acc: 0.5244(0.3431)
2022-01-05 17:06:46,004 Epoch[202/310], Step[0900/1251], Loss: 3.7640(3.8106), Acc: 0.3760(0.3438)
2022-01-05 17:07:42,856 Epoch[202/310], Step[0950/1251], Loss: 3.9614(3.8145), Acc: 0.3613(0.3443)
2022-01-05 17:08:37,522 Epoch[202/310], Step[1000/1251], Loss: 3.7364(3.8136), Acc: 0.4443(0.3446)
2022-01-05 17:09:33,353 Epoch[202/310], Step[1050/1251], Loss: 3.9628(3.8143), Acc: 0.3643(0.3450)
2022-01-05 17:10:29,882 Epoch[202/310], Step[1100/1251], Loss: 3.7140(3.8174), Acc: 0.2988(0.3443)
2022-01-05 17:11:25,975 Epoch[202/310], Step[1150/1251], Loss: 3.9856(3.8188), Acc: 0.4492(0.3444)
2022-01-05 17:12:22,907 Epoch[202/310], Step[1200/1251], Loss: 4.1601(3.8178), Acc: 0.4834(0.3449)
2022-01-05 17:13:18,242 Epoch[202/310], Step[1250/1251], Loss: 4.1715(3.8164), Acc: 0.2510(0.3466)
2022-01-05 17:13:20,138 ----- Validation after Epoch: 202
2022-01-05 17:14:17,813 Val Step[0000/1563], Loss: 0.6342 (0.6342), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-05 17:14:19,389 Val Step[0050/1563], Loss: 2.5053 (0.8179), Acc@1: 0.3750 (0.8382), Acc@5: 0.7812 (0.9504)
2022-01-05 17:14:20,851 Val Step[0100/1563], Loss: 1.7253 (1.1073), Acc@1: 0.6250 (0.7568), Acc@5: 0.8438 (0.9226)
2022-01-05 17:14:22,335 Val Step[0150/1563], Loss: 0.5833 (1.0355), Acc@1: 0.9062 (0.7740), Acc@5: 0.9688 (0.9294)
2022-01-05 17:14:23,773 Val Step[0200/1563], Loss: 1.3076 (1.0604), Acc@1: 0.7188 (0.7705), Acc@5: 0.9062 (0.9266)
2022-01-05 17:14:25,231 Val Step[0250/1563], Loss: 0.8785 (1.0114), Acc@1: 0.8125 (0.7831), Acc@5: 1.0000 (0.9333)
2022-01-05 17:14:26,726 Val Step[0300/1563], Loss: 1.2796 (1.0807), Acc@1: 0.7188 (0.7626), Acc@5: 0.9062 (0.9274)
2022-01-05 17:14:28,352 Val Step[0350/1563], Loss: 1.2442 (1.0851), Acc@1: 0.7188 (0.7595), Acc@5: 0.8750 (0.9301)
2022-01-05 17:14:29,756 Val Step[0400/1563], Loss: 1.2293 (1.0909), Acc@1: 0.6562 (0.7538), Acc@5: 0.9375 (0.9313)
2022-01-05 17:14:31,236 Val Step[0450/1563], Loss: 0.8762 (1.0939), Acc@1: 0.6875 (0.7502), Acc@5: 1.0000 (0.9324)
2022-01-05 17:14:32,779 Val Step[0500/1563], Loss: 0.3599 (1.0841), Acc@1: 0.9688 (0.7532), Acc@5: 1.0000 (0.9341)
2022-01-05 17:14:34,211 Val Step[0550/1563], Loss: 0.7010 (1.0639), Acc@1: 0.8438 (0.7592), Acc@5: 0.9375 (0.9358)
2022-01-05 17:14:35,710 Val Step[0600/1563], Loss: 1.0419 (1.0715), Acc@1: 0.7812 (0.7579), Acc@5: 0.9375 (0.9349)
2022-01-05 17:14:37,218 Val Step[0650/1563], Loss: 0.7697 (1.0913), Acc@1: 0.7812 (0.7537), Acc@5: 1.0000 (0.9318)
2022-01-05 17:14:38,645 Val Step[0700/1563], Loss: 1.0451 (1.1219), Acc@1: 0.7812 (0.7456), Acc@5: 0.9375 (0.9275)
2022-01-05 17:14:40,114 Val Step[0750/1563], Loss: 1.4593 (1.1586), Acc@1: 0.6875 (0.7383), Acc@5: 0.8750 (0.9225)
2022-01-05 17:14:41,568 Val Step[0800/1563], Loss: 1.0478 (1.1970), Acc@1: 0.7500 (0.7290), Acc@5: 1.0000 (0.9173)
2022-01-05 17:14:42,997 Val Step[0850/1563], Loss: 1.4196 (1.2230), Acc@1: 0.6250 (0.7230), Acc@5: 0.9062 (0.9141)
2022-01-05 17:14:44,543 Val Step[0900/1563], Loss: 0.4601 (1.2220), Acc@1: 0.9062 (0.7248), Acc@5: 1.0000 (0.9136)
2022-01-05 17:14:46,103 Val Step[0950/1563], Loss: 1.4520 (1.2440), Acc@1: 0.7188 (0.7211), Acc@5: 0.8438 (0.9101)
2022-01-05 17:14:47,541 Val Step[1000/1563], Loss: 0.6062 (1.2677), Acc@1: 0.9375 (0.7157), Acc@5: 1.0000 (0.9070)
2022-01-05 17:14:48,973 Val Step[1050/1563], Loss: 0.3783 (1.2833), Acc@1: 0.9688 (0.7128), Acc@5: 0.9688 (0.9051)
2022-01-05 17:14:50,412 Val Step[1100/1563], Loss: 0.9865 (1.2993), Acc@1: 0.8125 (0.7092), Acc@5: 0.9688 (0.9028)
2022-01-05 17:14:51,831 Val Step[1150/1563], Loss: 1.4534 (1.3141), Acc@1: 0.7500 (0.7063), Acc@5: 0.7812 (0.9008)
2022-01-05 17:14:53,326 Val Step[1200/1563], Loss: 1.1258 (1.3290), Acc@1: 0.7500 (0.7030), Acc@5: 0.8438 (0.8985)
2022-01-05 17:14:54,763 Val Step[1250/1563], Loss: 0.6324 (1.3400), Acc@1: 0.9062 (0.7012), Acc@5: 0.9375 (0.8966)
2022-01-05 17:14:56,172 Val Step[1300/1563], Loss: 0.9159 (1.3517), Acc@1: 0.8438 (0.6987), Acc@5: 0.9062 (0.8954)
2022-01-05 17:14:57,670 Val Step[1350/1563], Loss: 1.7330 (1.3684), Acc@1: 0.5312 (0.6949), Acc@5: 0.8125 (0.8931)
2022-01-05 17:14:59,207 Val Step[1400/1563], Loss: 1.1824 (1.3766), Acc@1: 0.7188 (0.6931), Acc@5: 0.9375 (0.8921)
2022-01-05 17:15:00,621 Val Step[1450/1563], Loss: 1.3638 (1.3835), Acc@1: 0.7500 (0.6912), Acc@5: 0.9688 (0.8918)
2022-01-05 17:15:02,122 Val Step[1500/1563], Loss: 1.6825 (1.3718), Acc@1: 0.5938 (0.6940), Acc@5: 0.9062 (0.8933)
2022-01-05 17:15:03,612 Val Step[1550/1563], Loss: 1.0373 (1.3728), Acc@1: 0.8750 (0.6935), Acc@5: 0.9062 (0.8929)
2022-01-05 17:15:04,415 ----- Epoch[202/310], Validation Loss: 1.3712, Validation Acc@1: 0.6938, Validation Acc@5: 0.8931, time: 104.27
2022-01-05 17:15:04,415 ----- Epoch[202/310], Train Loss: 3.8164, Train Acc: 0.3466, time: 1485.42, Best Val(epoch202) Acc@1: 0.6938
2022-01-05 17:15:04,594 Max accuracy so far: 0.6938 at epoch_202
2022-01-05 17:15:04,594 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 17:15:04,594 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 17:15:04,705 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 17:15:05,091 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 17:15:05,091 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 17:15:05,221 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 17:15:05,221 Now training epoch 203. LR=0.000240
2022-01-05 17:16:26,599 Epoch[203/310], Step[0000/1251], Loss: 3.3687(3.3687), Acc: 0.5225(0.5225)
2022-01-05 17:17:24,086 Epoch[203/310], Step[0050/1251], Loss: 4.1375(3.7575), Acc: 0.3945(0.3549)
2022-01-05 17:18:20,966 Epoch[203/310], Step[0100/1251], Loss: 4.0360(3.7670), Acc: 0.4287(0.3325)
2022-01-05 17:19:18,616 Epoch[203/310], Step[0150/1251], Loss: 3.7073(3.7789), Acc: 0.4180(0.3306)
2022-01-05 17:20:16,388 Epoch[203/310], Step[0200/1251], Loss: 4.1186(3.8074), Acc: 0.2842(0.3278)
2022-01-05 17:21:13,552 Epoch[203/310], Step[0250/1251], Loss: 3.7543(3.8003), Acc: 0.3506(0.3325)
2022-01-05 17:22:09,840 Epoch[203/310], Step[0300/1251], Loss: 4.5833(3.7953), Acc: 0.3428(0.3345)
2022-01-05 17:23:07,059 Epoch[203/310], Step[0350/1251], Loss: 4.0591(3.7951), Acc: 0.2578(0.3367)
2022-01-05 17:24:03,286 Epoch[203/310], Step[0400/1251], Loss: 3.5908(3.8041), Acc: 0.4229(0.3364)
2022-01-05 17:24:59,654 Epoch[203/310], Step[0450/1251], Loss: 3.5997(3.8091), Acc: 0.4102(0.3387)
2022-01-05 17:25:56,470 Epoch[203/310], Step[0500/1251], Loss: 4.0975(3.8044), Acc: 0.2842(0.3391)
2022-01-05 17:26:54,859 Epoch[203/310], Step[0550/1251], Loss: 3.3752(3.8064), Acc: 0.3926(0.3402)
2022-01-05 17:27:53,749 Epoch[203/310], Step[0600/1251], Loss: 4.2098(3.8063), Acc: 0.3555(0.3397)
2022-01-05 17:28:51,518 Epoch[203/310], Step[0650/1251], Loss: 3.5729(3.8070), Acc: 0.2910(0.3393)
2022-01-05 17:29:48,578 Epoch[203/310], Step[0700/1251], Loss: 3.8114(3.8070), Acc: 0.4014(0.3401)
2022-01-05 17:30:47,185 Epoch[203/310], Step[0750/1251], Loss: 4.0648(3.8056), Acc: 0.1162(0.3416)
2022-01-05 17:31:45,994 Epoch[203/310], Step[0800/1251], Loss: 4.0502(3.8068), Acc: 0.2754(0.3431)
2022-01-05 17:32:44,009 Epoch[203/310], Step[0850/1251], Loss: 3.5101(3.8080), Acc: 0.3359(0.3439)
2022-01-05 17:33:41,177 Epoch[203/310], Step[0900/1251], Loss: 3.7437(3.8048), Acc: 0.4238(0.3450)
2022-01-05 17:34:39,696 Epoch[203/310], Step[0950/1251], Loss: 3.4057(3.7996), Acc: 0.3604(0.3455)
2022-01-05 17:35:37,174 Epoch[203/310], Step[1000/1251], Loss: 3.6612(3.8002), Acc: 0.3486(0.3455)
2022-01-05 17:36:33,998 Epoch[203/310], Step[1050/1251], Loss: 4.3670(3.8015), Acc: 0.2617(0.3457)
2022-01-05 17:37:30,044 Epoch[203/310], Step[1100/1251], Loss: 3.6609(3.8011), Acc: 0.2949(0.3448)
2022-01-05 17:38:24,715 Epoch[203/310], Step[1150/1251], Loss: 3.9374(3.8015), Acc: 0.3857(0.3460)
2022-01-05 17:39:23,671 Epoch[203/310], Step[1200/1251], Loss: 3.7909(3.8012), Acc: 0.2861(0.3466)
2022-01-05 17:40:20,940 Epoch[203/310], Step[1250/1251], Loss: 4.0321(3.8014), Acc: 0.4189(0.3467)
2022-01-05 17:40:22,919 ----- Epoch[203/310], Train Loss: 3.8014, Train Acc: 0.3467, time: 1517.69, Best Val(epoch202) Acc@1: 0.6938
2022-01-05 17:40:23,093 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 17:40:23,094 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 17:40:23,215 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 17:40:23,215 Now training epoch 204. LR=0.000236
2022-01-05 17:41:48,184 Epoch[204/310], Step[0000/1251], Loss: 3.9050(3.9050), Acc: 0.2578(0.2578)
2022-01-05 17:42:44,407 Epoch[204/310], Step[0050/1251], Loss: 3.2657(3.7879), Acc: 0.4404(0.3511)
2022-01-05 17:43:42,520 Epoch[204/310], Step[0100/1251], Loss: 4.0360(3.7730), Acc: 0.3477(0.3591)
2022-01-05 17:44:39,296 Epoch[204/310], Step[0150/1251], Loss: 3.7309(3.7652), Acc: 0.4111(0.3555)
2022-01-05 17:45:37,059 Epoch[204/310], Step[0200/1251], Loss: 3.5311(3.7675), Acc: 0.2832(0.3540)
2022-01-05 17:46:34,201 Epoch[204/310], Step[0250/1251], Loss: 3.6841(3.7733), Acc: 0.3438(0.3521)
2022-01-05 17:47:31,059 Epoch[204/310], Step[0300/1251], Loss: 4.4591(3.7802), Acc: 0.2168(0.3564)
2022-01-05 17:48:29,199 Epoch[204/310], Step[0350/1251], Loss: 3.5165(3.7834), Acc: 0.2852(0.3551)
2022-01-05 17:49:25,196 Epoch[204/310], Step[0400/1251], Loss: 4.6233(3.7884), Acc: 0.2715(0.3556)
2022-01-05 17:50:21,325 Epoch[204/310], Step[0450/1251], Loss: 4.1419(3.7924), Acc: 0.2588(0.3539)
2022-01-05 17:51:17,132 Epoch[204/310], Step[0500/1251], Loss: 3.6347(3.7955), Acc: 0.0742(0.3524)
2022-01-05 17:52:12,094 Epoch[204/310], Step[0550/1251], Loss: 4.2942(3.8005), Acc: 0.3721(0.3510)
2022-01-05 17:53:07,753 Epoch[204/310], Step[0600/1251], Loss: 3.5816(3.8009), Acc: 0.4844(0.3536)
2022-01-05 17:54:05,037 Epoch[204/310], Step[0650/1251], Loss: 4.1549(3.8053), Acc: 0.3086(0.3533)
2022-01-05 17:55:00,510 Epoch[204/310], Step[0700/1251], Loss: 3.7510(3.8025), Acc: 0.2676(0.3553)
2022-01-05 17:55:55,531 Epoch[204/310], Step[0750/1251], Loss: 3.9021(3.8035), Acc: 0.3760(0.3568)
2022-01-05 17:56:51,459 Epoch[204/310], Step[0800/1251], Loss: 3.8033(3.8045), Acc: 0.1484(0.3560)
2022-01-05 17:57:47,976 Epoch[204/310], Step[0850/1251], Loss: 3.7692(3.8031), Acc: 0.4023(0.3552)
2022-01-05 17:58:42,998 Epoch[204/310], Step[0900/1251], Loss: 3.6945(3.8039), Acc: 0.0605(0.3545)
2022-01-05 17:59:39,778 Epoch[204/310], Step[0950/1251], Loss: 3.2924(3.8065), Acc: 0.4482(0.3529)
2022-01-05 18:00:36,480 Epoch[204/310], Step[1000/1251], Loss: 3.8705(3.8072), Acc: 0.2959(0.3550)
2022-01-05 18:01:33,520 Epoch[204/310], Step[1050/1251], Loss: 3.8683(3.8073), Acc: 0.3359(0.3541)
2022-01-05 18:02:31,738 Epoch[204/310], Step[1100/1251], Loss: 3.7301(3.8052), Acc: 0.4619(0.3558)
2022-01-05 18:03:28,486 Epoch[204/310], Step[1150/1251], Loss: 3.4780(3.8048), Acc: 0.4473(0.3551)
2022-01-05 18:04:26,400 Epoch[204/310], Step[1200/1251], Loss: 3.3887(3.8040), Acc: 0.0186(0.3556)
2022-01-05 18:05:22,901 Epoch[204/310], Step[1250/1251], Loss: 3.7509(3.8031), Acc: 0.4385(0.3556)
2022-01-05 18:05:24,968 ----- Validation after Epoch: 204
2022-01-05 18:06:23,458 Val Step[0000/1563], Loss: 0.6668 (0.6668), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-05 18:06:25,082 Val Step[0050/1563], Loss: 2.3183 (0.8001), Acc@1: 0.3750 (0.8333), Acc@5: 0.8438 (0.9583)
2022-01-05 18:06:26,508 Val Step[0100/1563], Loss: 1.9961 (1.0944), Acc@1: 0.5312 (0.7584), Acc@5: 0.8438 (0.9295)
2022-01-05 18:06:28,067 Val Step[0150/1563], Loss: 0.4226 (1.0311), Acc@1: 0.9062 (0.7719), Acc@5: 1.0000 (0.9336)
2022-01-05 18:06:29,625 Val Step[0200/1563], Loss: 0.8954 (1.0467), Acc@1: 0.7500 (0.7708), Acc@5: 0.9375 (0.9293)
2022-01-05 18:06:31,103 Val Step[0250/1563], Loss: 0.5805 (0.9954), Acc@1: 0.9375 (0.7816), Acc@5: 1.0000 (0.9350)
2022-01-05 18:06:32,560 Val Step[0300/1563], Loss: 1.1331 (1.0598), Acc@1: 0.6562 (0.7640), Acc@5: 0.9688 (0.9295)
2022-01-05 18:06:33,955 Val Step[0350/1563], Loss: 1.1973 (1.0659), Acc@1: 0.6875 (0.7588), Acc@5: 0.9375 (0.9317)
2022-01-05 18:06:35,405 Val Step[0400/1563], Loss: 1.0319 (1.0769), Acc@1: 0.8438 (0.7535), Acc@5: 0.9688 (0.9317)
2022-01-05 18:06:36,918 Val Step[0450/1563], Loss: 1.0053 (1.0838), Acc@1: 0.6250 (0.7507), Acc@5: 1.0000 (0.9323)
2022-01-05 18:06:38,490 Val Step[0500/1563], Loss: 0.4441 (1.0738), Acc@1: 0.9375 (0.7535), Acc@5: 1.0000 (0.9337)
2022-01-05 18:06:39,998 Val Step[0550/1563], Loss: 0.9993 (1.0521), Acc@1: 0.7500 (0.7592), Acc@5: 0.9375 (0.9359)
2022-01-05 18:06:41,536 Val Step[0600/1563], Loss: 0.8981 (1.0591), Acc@1: 0.8125 (0.7583), Acc@5: 0.9062 (0.9345)
2022-01-05 18:06:42,935 Val Step[0650/1563], Loss: 0.5840 (1.0798), Acc@1: 0.8125 (0.7540), Acc@5: 1.0000 (0.9315)
2022-01-05 18:06:44,322 Val Step[0700/1563], Loss: 0.9646 (1.1138), Acc@1: 0.8125 (0.7461), Acc@5: 0.9688 (0.9272)
2022-01-05 18:06:45,822 Val Step[0750/1563], Loss: 1.4764 (1.1477), Acc@1: 0.7500 (0.7396), Acc@5: 0.9062 (0.9229)
2022-01-05 18:06:47,324 Val Step[0800/1563], Loss: 0.9761 (1.1886), Acc@1: 0.7500 (0.7300), Acc@5: 1.0000 (0.9176)
2022-01-05 18:06:48,708 Val Step[0850/1563], Loss: 1.2424 (1.2167), Acc@1: 0.6875 (0.7236), Acc@5: 0.9375 (0.9136)
2022-01-05 18:06:50,082 Val Step[0900/1563], Loss: 0.3089 (1.2162), Acc@1: 0.9375 (0.7252), Acc@5: 1.0000 (0.9133)
2022-01-05 18:06:51,577 Val Step[0950/1563], Loss: 1.1485 (1.2341), Acc@1: 0.7500 (0.7213), Acc@5: 0.9375 (0.9106)
2022-01-05 18:06:52,932 Val Step[1000/1563], Loss: 0.6692 (1.2592), Acc@1: 0.9062 (0.7155), Acc@5: 0.9688 (0.9072)
2022-01-05 18:06:54,429 Val Step[1050/1563], Loss: 0.4005 (1.2738), Acc@1: 0.9688 (0.7125), Acc@5: 0.9688 (0.9051)
2022-01-05 18:06:55,895 Val Step[1100/1563], Loss: 1.0102 (1.2917), Acc@1: 0.7500 (0.7084), Acc@5: 0.9062 (0.9023)
2022-01-05 18:06:57,416 Val Step[1150/1563], Loss: 1.3263 (1.3054), Acc@1: 0.7812 (0.7057), Acc@5: 0.7812 (0.9003)
2022-01-05 18:06:58,922 Val Step[1200/1563], Loss: 1.4521 (1.3237), Acc@1: 0.7812 (0.7016), Acc@5: 0.8438 (0.8976)
2022-01-05 18:07:00,429 Val Step[1250/1563], Loss: 0.7541 (1.3373), Acc@1: 0.8750 (0.6990), Acc@5: 0.9375 (0.8954)
2022-01-05 18:07:01,870 Val Step[1300/1563], Loss: 0.7636 (1.3462), Acc@1: 0.8750 (0.6974), Acc@5: 0.9375 (0.8942)
2022-01-05 18:07:03,318 Val Step[1350/1563], Loss: 2.3926 (1.3637), Acc@1: 0.3750 (0.6935), Acc@5: 0.7812 (0.8916)
2022-01-05 18:07:04,756 Val Step[1400/1563], Loss: 1.3097 (1.3733), Acc@1: 0.6875 (0.6914), Acc@5: 0.9375 (0.8904)
2022-01-05 18:07:06,283 Val Step[1450/1563], Loss: 1.7003 (1.3800), Acc@1: 0.6562 (0.6898), Acc@5: 0.8750 (0.8900)
2022-01-05 18:07:07,803 Val Step[1500/1563], Loss: 1.8438 (1.3691), Acc@1: 0.5312 (0.6922), Acc@5: 0.8438 (0.8917)
2022-01-05 18:07:09,258 Val Step[1550/1563], Loss: 0.9715 (1.3694), Acc@1: 0.8750 (0.6921), Acc@5: 0.9062 (0.8915)
2022-01-05 18:07:10,084 ----- Epoch[204/310], Validation Loss: 1.3679, Validation Acc@1: 0.6923, Validation Acc@5: 0.8916, time: 105.11
2022-01-05 18:07:10,084 ----- Epoch[204/310], Train Loss: 3.8031, Train Acc: 0.3556, time: 1501.75, Best Val(epoch202) Acc@1: 0.6938
2022-01-05 18:07:10,271 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 18:07:10,272 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 18:07:10,420 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 18:07:10,421 Now training epoch 205. LR=0.000232
2022-01-05 18:08:26,413 Epoch[205/310], Step[0000/1251], Loss: 3.5243(3.5243), Acc: 0.4062(0.4062)
2022-01-05 18:09:22,911 Epoch[205/310], Step[0050/1251], Loss: 3.9064(3.8392), Acc: 0.1943(0.3393)
2022-01-05 18:10:19,413 Epoch[205/310], Step[0100/1251], Loss: 3.5666(3.8125), Acc: 0.3408(0.3550)
2022-01-05 18:11:16,027 Epoch[205/310], Step[0150/1251], Loss: 4.1569(3.8073), Acc: 0.3506(0.3574)
2022-01-05 18:12:13,719 Epoch[205/310], Step[0200/1251], Loss: 4.0288(3.8166), Acc: 0.2012(0.3515)
2022-01-05 18:13:10,399 Epoch[205/310], Step[0250/1251], Loss: 4.0333(3.8144), Acc: 0.4668(0.3548)
2022-01-05 18:14:07,252 Epoch[205/310], Step[0300/1251], Loss: 3.3067(3.8160), Acc: 0.2832(0.3530)
2022-01-05 18:15:03,150 Epoch[205/310], Step[0350/1251], Loss: 4.6289(3.8112), Acc: 0.1943(0.3546)
2022-01-05 18:16:00,388 Epoch[205/310], Step[0400/1251], Loss: 4.2523(3.8055), Acc: 0.2695(0.3542)
2022-01-05 18:16:57,574 Epoch[205/310], Step[0450/1251], Loss: 3.9537(3.7963), Acc: 0.3291(0.3526)
2022-01-05 18:17:55,875 Epoch[205/310], Step[0500/1251], Loss: 4.0662(3.7956), Acc: 0.3115(0.3518)
2022-01-05 18:18:51,928 Epoch[205/310], Step[0550/1251], Loss: 3.8875(3.7969), Acc: 0.3398(0.3506)
2022-01-05 18:19:48,433 Epoch[205/310], Step[0600/1251], Loss: 3.6911(3.7942), Acc: 0.2383(0.3512)
2022-01-05 18:20:46,477 Epoch[205/310], Step[0650/1251], Loss: 4.0875(3.7911), Acc: 0.3184(0.3517)
2022-01-05 18:21:44,567 Epoch[205/310], Step[0700/1251], Loss: 3.8074(3.7881), Acc: 0.2432(0.3505)
2022-01-05 18:22:41,988 Epoch[205/310], Step[0750/1251], Loss: 3.6229(3.7872), Acc: 0.3203(0.3516)
2022-01-05 18:23:39,467 Epoch[205/310], Step[0800/1251], Loss: 3.9269(3.7906), Acc: 0.4111(0.3505)
2022-01-05 18:24:35,766 Epoch[205/310], Step[0850/1251], Loss: 3.8603(3.7886), Acc: 0.3809(0.3513)
2022-01-05 18:25:34,110 Epoch[205/310], Step[0900/1251], Loss: 4.0238(3.7872), Acc: 0.3721(0.3505)
2022-01-05 18:26:31,709 Epoch[205/310], Step[0950/1251], Loss: 3.9332(3.7863), Acc: 0.4590(0.3509)
2022-01-05 18:27:29,317 Epoch[205/310], Step[1000/1251], Loss: 3.9635(3.7874), Acc: 0.4316(0.3515)
2022-01-05 18:28:26,745 Epoch[205/310], Step[1050/1251], Loss: 3.1831(3.7916), Acc: 0.5508(0.3515)
2022-01-05 18:29:23,574 Epoch[205/310], Step[1100/1251], Loss: 4.0157(3.7902), Acc: 0.2900(0.3519)
2022-01-05 18:30:21,570 Epoch[205/310], Step[1150/1251], Loss: 4.2847(3.7904), Acc: 0.3574(0.3506)
2022-01-05 18:31:18,181 Epoch[205/310], Step[1200/1251], Loss: 3.9054(3.7918), Acc: 0.4736(0.3501)
2022-01-05 18:32:15,446 Epoch[205/310], Step[1250/1251], Loss: 3.7351(3.7921), Acc: 0.4863(0.3501)
2022-01-05 18:32:17,469 ----- Epoch[205/310], Train Loss: 3.7921, Train Acc: 0.3501, time: 1507.04, Best Val(epoch202) Acc@1: 0.6938
2022-01-05 18:32:17,659 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 18:32:17,659 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 18:32:17,787 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 18:32:17,787 Now training epoch 206. LR=0.000227
2022-01-05 18:33:36,655 Epoch[206/310], Step[0000/1251], Loss: 4.3506(4.3506), Acc: 0.3652(0.3652)
2022-01-05 18:34:33,521 Epoch[206/310], Step[0050/1251], Loss: 3.6072(3.8304), Acc: 0.4082(0.3494)
2022-01-05 18:35:31,179 Epoch[206/310], Step[0100/1251], Loss: 4.2129(3.7864), Acc: 0.3350(0.3606)
2022-01-05 18:36:28,165 Epoch[206/310], Step[0150/1251], Loss: 4.0165(3.7812), Acc: 0.1260(0.3563)
2022-01-05 18:37:24,635 Epoch[206/310], Step[0200/1251], Loss: 3.4167(3.7817), Acc: 0.4668(0.3602)
2022-01-05 18:38:21,217 Epoch[206/310], Step[0250/1251], Loss: 4.0266(3.7797), Acc: 0.3691(0.3599)
2022-01-05 18:39:19,269 Epoch[206/310], Step[0300/1251], Loss: 3.4961(3.7783), Acc: 0.5381(0.3589)
2022-01-05 18:40:16,832 Epoch[206/310], Step[0350/1251], Loss: 4.1785(3.7781), Acc: 0.3447(0.3588)
2022-01-05 18:41:14,982 Epoch[206/310], Step[0400/1251], Loss: 4.0380(3.7843), Acc: 0.4502(0.3584)
2022-01-05 18:42:13,277 Epoch[206/310], Step[0450/1251], Loss: 3.4745(3.7849), Acc: 0.2275(0.3582)
2022-01-05 18:43:10,196 Epoch[206/310], Step[0500/1251], Loss: 3.8047(3.7867), Acc: 0.4131(0.3596)
2022-01-05 18:44:07,781 Epoch[206/310], Step[0550/1251], Loss: 3.6512(3.7932), Acc: 0.5010(0.3570)
2022-01-05 18:45:04,872 Epoch[206/310], Step[0600/1251], Loss: 3.5769(3.7889), Acc: 0.4639(0.3560)
2022-01-05 18:46:01,997 Epoch[206/310], Step[0650/1251], Loss: 3.5747(3.7968), Acc: 0.3086(0.3550)
2022-01-05 18:47:00,384 Epoch[206/310], Step[0700/1251], Loss: 4.4762(3.7972), Acc: 0.3311(0.3541)
2022-01-05 18:47:58,659 Epoch[206/310], Step[0750/1251], Loss: 3.6616(3.7945), Acc: 0.5098(0.3524)
2022-01-05 18:48:55,238 Epoch[206/310], Step[0800/1251], Loss: 4.0544(3.7922), Acc: 0.3242(0.3524)
2022-01-05 18:49:53,107 Epoch[206/310], Step[0850/1251], Loss: 3.7742(3.7898), Acc: 0.3691(0.3523)
2022-01-05 18:50:50,947 Epoch[206/310], Step[0900/1251], Loss: 2.9933(3.7895), Acc: 0.3135(0.3513)
2022-01-05 18:51:48,239 Epoch[206/310], Step[0950/1251], Loss: 4.0130(3.7903), Acc: 0.3945(0.3516)
2022-01-05 18:52:45,763 Epoch[206/310], Step[1000/1251], Loss: 3.8880(3.7876), Acc: 0.2090(0.3512)
2022-01-05 18:53:42,687 Epoch[206/310], Step[1050/1251], Loss: 3.5429(3.7868), Acc: 0.5625(0.3502)
2022-01-05 18:54:39,819 Epoch[206/310], Step[1100/1251], Loss: 3.7954(3.7878), Acc: 0.3115(0.3499)
2022-01-05 18:55:36,919 Epoch[206/310], Step[1150/1251], Loss: 3.9496(3.7905), Acc: 0.2549(0.3493)
2022-01-05 18:56:34,262 Epoch[206/310], Step[1200/1251], Loss: 3.5519(3.7904), Acc: 0.4639(0.3485)
2022-01-05 18:57:31,276 Epoch[206/310], Step[1250/1251], Loss: 3.6611(3.7897), Acc: 0.3145(0.3485)
2022-01-05 18:57:33,068 ----- Validation after Epoch: 206
2022-01-05 18:58:34,647 Val Step[0000/1563], Loss: 0.6543 (0.6543), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 18:58:36,111 Val Step[0050/1563], Loss: 2.0323 (0.7717), Acc@1: 0.4688 (0.8272), Acc@5: 0.8750 (0.9614)
2022-01-05 18:58:37,503 Val Step[0100/1563], Loss: 2.1844 (1.0726), Acc@1: 0.4062 (0.7488), Acc@5: 0.8438 (0.9261)
2022-01-05 18:58:38,934 Val Step[0150/1563], Loss: 0.5162 (1.0176), Acc@1: 0.9062 (0.7651), Acc@5: 1.0000 (0.9313)
2022-01-05 18:58:40,376 Val Step[0200/1563], Loss: 1.0268 (1.0169), Acc@1: 0.7812 (0.7696), Acc@5: 0.9375 (0.9316)
2022-01-05 18:58:41,756 Val Step[0250/1563], Loss: 0.6806 (0.9691), Acc@1: 0.8750 (0.7816), Acc@5: 1.0000 (0.9369)
2022-01-05 18:58:43,159 Val Step[0300/1563], Loss: 1.2558 (1.0238), Acc@1: 0.6875 (0.7653), Acc@5: 0.9062 (0.9340)
2022-01-05 18:58:44,526 Val Step[0350/1563], Loss: 1.0712 (1.0316), Acc@1: 0.7812 (0.7618), Acc@5: 0.9375 (0.9357)
2022-01-05 18:58:45,901 Val Step[0400/1563], Loss: 0.9566 (1.0441), Acc@1: 0.7812 (0.7565), Acc@5: 0.9688 (0.9352)
2022-01-05 18:58:47,434 Val Step[0450/1563], Loss: 0.9765 (1.0510), Acc@1: 0.7500 (0.7541), Acc@5: 1.0000 (0.9352)
2022-01-05 18:58:48,885 Val Step[0500/1563], Loss: 0.5716 (1.0381), Acc@1: 0.8125 (0.7576), Acc@5: 1.0000 (0.9368)
2022-01-05 18:58:50,367 Val Step[0550/1563], Loss: 0.6577 (1.0156), Acc@1: 0.8438 (0.7635), Acc@5: 1.0000 (0.9389)
2022-01-05 18:58:51,867 Val Step[0600/1563], Loss: 0.8030 (1.0218), Acc@1: 0.8125 (0.7621), Acc@5: 0.9375 (0.9381)
2022-01-05 18:58:53,334 Val Step[0650/1563], Loss: 0.7563 (1.0411), Acc@1: 0.6875 (0.7571), Acc@5: 1.0000 (0.9355)
2022-01-05 18:58:54,815 Val Step[0700/1563], Loss: 1.1109 (1.0717), Acc@1: 0.8125 (0.7507), Acc@5: 0.9375 (0.9318)
2022-01-05 18:58:56,274 Val Step[0750/1563], Loss: 1.4003 (1.1088), Acc@1: 0.6875 (0.7438), Acc@5: 0.8750 (0.9267)
2022-01-05 18:58:57,690 Val Step[0800/1563], Loss: 1.0802 (1.1470), Acc@1: 0.8125 (0.7350), Acc@5: 1.0000 (0.9219)
2022-01-05 18:58:59,129 Val Step[0850/1563], Loss: 1.3015 (1.1731), Acc@1: 0.6562 (0.7289), Acc@5: 0.9375 (0.9185)
2022-01-05 18:59:00,686 Val Step[0900/1563], Loss: 0.3458 (1.1757), Acc@1: 0.9375 (0.7296), Acc@5: 1.0000 (0.9175)
2022-01-05 18:59:02,178 Val Step[0950/1563], Loss: 1.3318 (1.1967), Acc@1: 0.7188 (0.7260), Acc@5: 0.8750 (0.9143)
2022-01-05 18:59:03,582 Val Step[1000/1563], Loss: 0.4767 (1.2221), Acc@1: 0.9375 (0.7198), Acc@5: 1.0000 (0.9105)
2022-01-05 18:59:04,964 Val Step[1050/1563], Loss: 0.4960 (1.2359), Acc@1: 0.9688 (0.7164), Acc@5: 0.9688 (0.9091)
2022-01-05 18:59:06,378 Val Step[1100/1563], Loss: 0.9284 (1.2542), Acc@1: 0.7500 (0.7129), Acc@5: 1.0000 (0.9063)
2022-01-05 18:59:07,776 Val Step[1150/1563], Loss: 1.3341 (1.2683), Acc@1: 0.7812 (0.7100), Acc@5: 0.7812 (0.9041)
2022-01-05 18:59:09,165 Val Step[1200/1563], Loss: 1.3781 (1.2834), Acc@1: 0.7188 (0.7071), Acc@5: 0.8438 (0.9019)
2022-01-05 18:59:10,620 Val Step[1250/1563], Loss: 0.6129 (1.2950), Acc@1: 0.8750 (0.7055), Acc@5: 0.9375 (0.8998)
2022-01-05 18:59:12,264 Val Step[1300/1563], Loss: 1.0627 (1.3056), Acc@1: 0.8125 (0.7031), Acc@5: 0.9375 (0.8986)
2022-01-05 18:59:13,743 Val Step[1350/1563], Loss: 1.7720 (1.3248), Acc@1: 0.5625 (0.6989), Acc@5: 0.8750 (0.8956)
2022-01-05 18:59:15,220 Val Step[1400/1563], Loss: 1.0998 (1.3309), Acc@1: 0.6875 (0.6975), Acc@5: 0.8750 (0.8945)
2022-01-05 18:59:16,640 Val Step[1450/1563], Loss: 1.5524 (1.3370), Acc@1: 0.7188 (0.6960), Acc@5: 0.9062 (0.8941)
2022-01-05 18:59:18,183 Val Step[1500/1563], Loss: 1.9747 (1.3268), Acc@1: 0.5938 (0.6985), Acc@5: 0.7812 (0.8956)
2022-01-05 18:59:19,630 Val Step[1550/1563], Loss: 1.0227 (1.3275), Acc@1: 0.8750 (0.6981), Acc@5: 0.9062 (0.8954)
2022-01-05 18:59:20,476 ----- Epoch[206/310], Validation Loss: 1.3258, Validation Acc@1: 0.6984, Validation Acc@5: 0.8956, time: 107.41
2022-01-05 18:59:20,476 ----- Epoch[206/310], Train Loss: 3.7897, Train Acc: 0.3485, time: 1515.28, Best Val(epoch206) Acc@1: 0.6984
2022-01-05 18:59:20,662 Max accuracy so far: 0.6984 at epoch_206
2022-01-05 18:59:20,662 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 18:59:20,662 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 18:59:20,767 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 18:59:21,156 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 18:59:21,156 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 18:59:21,319 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 18:59:21,320 Now training epoch 207. LR=0.000223
2022-01-05 19:00:34,875 Epoch[207/310], Step[0000/1251], Loss: 3.8521(3.8521), Acc: 0.3398(0.3398)
2022-01-05 19:01:30,831 Epoch[207/310], Step[0050/1251], Loss: 3.6846(3.6937), Acc: 0.3398(0.3552)
2022-01-05 19:02:25,850 Epoch[207/310], Step[0100/1251], Loss: 3.9123(3.7253), Acc: 0.3330(0.3676)
2022-01-05 19:03:21,681 Epoch[207/310], Step[0150/1251], Loss: 4.0438(3.7307), Acc: 0.4746(0.3582)
2022-01-05 19:04:18,368 Epoch[207/310], Step[0200/1251], Loss: 4.0248(3.7539), Acc: 0.4072(0.3537)
2022-01-05 19:05:15,805 Epoch[207/310], Step[0250/1251], Loss: 3.9068(3.7680), Acc: 0.4219(0.3563)
2022-01-05 19:06:13,722 Epoch[207/310], Step[0300/1251], Loss: 3.6212(3.7740), Acc: 0.4443(0.3583)
2022-01-05 19:07:10,434 Epoch[207/310], Step[0350/1251], Loss: 3.6867(3.7852), Acc: 0.3643(0.3559)
2022-01-05 19:08:07,422 Epoch[207/310], Step[0400/1251], Loss: 3.4892(3.7831), Acc: 0.3838(0.3576)
2022-01-05 19:09:05,005 Epoch[207/310], Step[0450/1251], Loss: 3.9750(3.7825), Acc: 0.3389(0.3543)
2022-01-05 19:10:02,693 Epoch[207/310], Step[0500/1251], Loss: 4.0319(3.7863), Acc: 0.4180(0.3515)
2022-01-05 19:10:59,582 Epoch[207/310], Step[0550/1251], Loss: 4.3500(3.7905), Acc: 0.3750(0.3526)
2022-01-05 19:11:56,410 Epoch[207/310], Step[0600/1251], Loss: 4.6927(3.7924), Acc: 0.2178(0.3518)
2022-01-05 19:12:53,012 Epoch[207/310], Step[0650/1251], Loss: 3.4904(3.7906), Acc: 0.5439(0.3513)
2022-01-05 19:13:49,322 Epoch[207/310], Step[0700/1251], Loss: 3.9023(3.7907), Acc: 0.4307(0.3536)
2022-01-05 19:14:46,138 Epoch[207/310], Step[0750/1251], Loss: 3.9888(3.7940), Acc: 0.3750(0.3526)
2022-01-05 19:15:44,118 Epoch[207/310], Step[0800/1251], Loss: 4.1766(3.7962), Acc: 0.2939(0.3515)
2022-01-05 19:16:42,132 Epoch[207/310], Step[0850/1251], Loss: 3.7638(3.8006), Acc: 0.1855(0.3505)
2022-01-05 19:17:40,312 Epoch[207/310], Step[0900/1251], Loss: 3.6774(3.8022), Acc: 0.2012(0.3497)
2022-01-05 19:18:38,294 Epoch[207/310], Step[0950/1251], Loss: 3.9012(3.8062), Acc: 0.4023(0.3487)
2022-01-05 19:19:35,700 Epoch[207/310], Step[1000/1251], Loss: 3.6716(3.8068), Acc: 0.2422(0.3477)
2022-01-05 19:20:33,433 Epoch[207/310], Step[1050/1251], Loss: 3.1269(3.8059), Acc: 0.4316(0.3475)
2022-01-05 19:21:31,795 Epoch[207/310], Step[1100/1251], Loss: 3.4457(3.8065), Acc: 0.4131(0.3479)
2022-01-05 19:22:29,899 Epoch[207/310], Step[1150/1251], Loss: 3.5984(3.8065), Acc: 0.3867(0.3487)
2022-01-05 19:23:28,304 Epoch[207/310], Step[1200/1251], Loss: 4.0439(3.8037), Acc: 0.3711(0.3488)
2022-01-05 19:24:26,427 Epoch[207/310], Step[1250/1251], Loss: 3.9904(3.8029), Acc: 0.4092(0.3483)
2022-01-05 19:24:28,600 ----- Epoch[207/310], Train Loss: 3.8029, Train Acc: 0.3483, time: 1507.28, Best Val(epoch206) Acc@1: 0.6984
2022-01-05 19:24:28,768 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 19:24:28,768 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 19:24:28,880 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 19:24:28,880 Now training epoch 208. LR=0.000219
2022-01-05 19:25:43,380 Epoch[208/310], Step[0000/1251], Loss: 3.8736(3.8736), Acc: 0.3584(0.3584)
2022-01-05 19:26:41,035 Epoch[208/310], Step[0050/1251], Loss: 3.6928(3.8457), Acc: 0.4990(0.3518)
2022-01-05 19:27:38,001 Epoch[208/310], Step[0100/1251], Loss: 4.3284(3.8131), Acc: 0.4111(0.3518)
2022-01-05 19:28:34,799 Epoch[208/310], Step[0150/1251], Loss: 4.0154(3.7896), Acc: 0.2734(0.3577)
2022-01-05 19:29:32,049 Epoch[208/310], Step[0200/1251], Loss: 3.8661(3.7783), Acc: 0.2490(0.3597)
2022-01-05 19:30:29,009 Epoch[208/310], Step[0250/1251], Loss: 3.6644(3.7862), Acc: 0.2842(0.3634)
2022-01-05 19:31:27,203 Epoch[208/310], Step[0300/1251], Loss: 4.0827(3.7963), Acc: 0.3418(0.3625)
2022-01-05 19:32:23,956 Epoch[208/310], Step[0350/1251], Loss: 3.6962(3.7986), Acc: 0.4551(0.3610)
2022-01-05 19:33:21,028 Epoch[208/310], Step[0400/1251], Loss: 3.5848(3.7976), Acc: 0.5098(0.3593)
2022-01-05 19:34:19,198 Epoch[208/310], Step[0450/1251], Loss: 4.1428(3.7967), Acc: 0.3896(0.3603)
2022-01-05 19:35:16,219 Epoch[208/310], Step[0500/1251], Loss: 3.1862(3.8003), Acc: 0.4150(0.3611)
2022-01-05 19:36:14,076 Epoch[208/310], Step[0550/1251], Loss: 3.8658(3.7943), Acc: 0.3857(0.3614)
2022-01-05 19:37:12,249 Epoch[208/310], Step[0600/1251], Loss: 4.1786(3.7977), Acc: 0.2598(0.3608)
2022-01-05 19:38:10,127 Epoch[208/310], Step[0650/1251], Loss: 3.9792(3.7942), Acc: 0.3584(0.3615)
2022-01-05 19:39:08,085 Epoch[208/310], Step[0700/1251], Loss: 3.8015(3.7956), Acc: 0.2471(0.3613)
2022-01-05 19:40:05,635 Epoch[208/310], Step[0750/1251], Loss: 3.5457(3.7968), Acc: 0.2139(0.3614)
2022-01-05 19:41:03,434 Epoch[208/310], Step[0800/1251], Loss: 3.7177(3.7997), Acc: 0.3340(0.3609)
2022-01-05 19:42:00,151 Epoch[208/310], Step[0850/1251], Loss: 3.6866(3.8002), Acc: 0.3486(0.3607)
2022-01-05 19:42:58,262 Epoch[208/310], Step[0900/1251], Loss: 3.7734(3.8019), Acc: 0.4658(0.3596)
2022-01-05 19:43:56,313 Epoch[208/310], Step[0950/1251], Loss: 4.0857(3.8000), Acc: 0.3887(0.3594)
2022-01-05 19:44:54,301 Epoch[208/310], Step[1000/1251], Loss: 3.3955(3.7998), Acc: 0.3164(0.3574)
2022-01-05 19:45:52,447 Epoch[208/310], Step[1050/1251], Loss: 3.8933(3.8008), Acc: 0.3154(0.3576)
2022-01-05 19:46:49,681 Epoch[208/310], Step[1100/1251], Loss: 3.6344(3.8012), Acc: 0.5039(0.3576)
2022-01-05 19:47:46,491 Epoch[208/310], Step[1150/1251], Loss: 3.8682(3.7982), Acc: 0.4473(0.3590)
2022-01-05 19:48:43,366 Epoch[208/310], Step[1200/1251], Loss: 3.4571(3.7983), Acc: 0.5469(0.3596)
2022-01-05 19:49:40,107 Epoch[208/310], Step[1250/1251], Loss: 4.1732(3.7991), Acc: 0.1484(0.3598)
2022-01-05 19:49:42,155 ----- Validation after Epoch: 208
2022-01-05 19:50:41,793 Val Step[0000/1563], Loss: 0.7599 (0.7599), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 19:50:43,349 Val Step[0050/1563], Loss: 2.2929 (0.7985), Acc@1: 0.4062 (0.8395), Acc@5: 0.8438 (0.9516)
2022-01-05 19:50:44,840 Val Step[0100/1563], Loss: 1.8248 (1.0770), Acc@1: 0.5625 (0.7556), Acc@5: 0.8125 (0.9288)
2022-01-05 19:50:46,302 Val Step[0150/1563], Loss: 0.5516 (1.0153), Acc@1: 0.9062 (0.7717), Acc@5: 0.9688 (0.9348)
2022-01-05 19:50:47,730 Val Step[0200/1563], Loss: 1.0151 (1.0259), Acc@1: 0.8438 (0.7729), Acc@5: 0.9375 (0.9338)
2022-01-05 19:50:49,146 Val Step[0250/1563], Loss: 0.8991 (0.9723), Acc@1: 0.8125 (0.7857), Acc@5: 0.9688 (0.9402)
2022-01-05 19:50:50,556 Val Step[0300/1563], Loss: 1.0387 (1.0356), Acc@1: 0.7812 (0.7698), Acc@5: 0.9062 (0.9344)
2022-01-05 19:50:51,936 Val Step[0350/1563], Loss: 1.2008 (1.0466), Acc@1: 0.7812 (0.7658), Acc@5: 0.9375 (0.9361)
2022-01-05 19:50:53,304 Val Step[0400/1563], Loss: 0.9872 (1.0614), Acc@1: 0.8438 (0.7586), Acc@5: 0.9688 (0.9357)
2022-01-05 19:50:54,801 Val Step[0450/1563], Loss: 0.9285 (1.0671), Acc@1: 0.7188 (0.7553), Acc@5: 1.0000 (0.9366)
2022-01-05 19:50:56,255 Val Step[0500/1563], Loss: 0.6076 (1.0583), Acc@1: 0.8750 (0.7584), Acc@5: 1.0000 (0.9381)
2022-01-05 19:50:57,751 Val Step[0550/1563], Loss: 0.7665 (1.0366), Acc@1: 0.7812 (0.7638), Acc@5: 1.0000 (0.9403)
2022-01-05 19:50:59,317 Val Step[0600/1563], Loss: 1.0687 (1.0432), Acc@1: 0.7812 (0.7637), Acc@5: 0.9062 (0.9391)
2022-01-05 19:51:00,865 Val Step[0650/1563], Loss: 0.6058 (1.0639), Acc@1: 0.9688 (0.7591), Acc@5: 1.0000 (0.9358)
2022-01-05 19:51:02,341 Val Step[0700/1563], Loss: 0.8607 (1.0969), Acc@1: 0.8438 (0.7507), Acc@5: 0.9688 (0.9317)
2022-01-05 19:51:03,786 Val Step[0750/1563], Loss: 1.1635 (1.1330), Acc@1: 0.7500 (0.7433), Acc@5: 0.9375 (0.9269)
2022-01-05 19:51:05,300 Val Step[0800/1563], Loss: 0.9506 (1.1715), Acc@1: 0.8438 (0.7342), Acc@5: 1.0000 (0.9219)
2022-01-05 19:51:06,785 Val Step[0850/1563], Loss: 1.4247 (1.1954), Acc@1: 0.6562 (0.7293), Acc@5: 0.9062 (0.9189)
2022-01-05 19:51:08,271 Val Step[0900/1563], Loss: 0.3756 (1.1972), Acc@1: 0.9688 (0.7304), Acc@5: 1.0000 (0.9179)
2022-01-05 19:51:09,793 Val Step[0950/1563], Loss: 1.3258 (1.2182), Acc@1: 0.7500 (0.7266), Acc@5: 0.9062 (0.9146)
2022-01-05 19:51:11,219 Val Step[1000/1563], Loss: 0.4894 (1.2432), Acc@1: 0.9688 (0.7204), Acc@5: 1.0000 (0.9111)
2022-01-05 19:51:12,707 Val Step[1050/1563], Loss: 0.4307 (1.2591), Acc@1: 0.9375 (0.7165), Acc@5: 0.9688 (0.9090)
2022-01-05 19:51:14,178 Val Step[1100/1563], Loss: 1.0374 (1.2741), Acc@1: 0.7812 (0.7134), Acc@5: 0.9062 (0.9066)
2022-01-05 19:51:15,601 Val Step[1150/1563], Loss: 1.1776 (1.2878), Acc@1: 0.8125 (0.7111), Acc@5: 0.8438 (0.9046)
2022-01-05 19:51:17,034 Val Step[1200/1563], Loss: 1.1584 (1.3026), Acc@1: 0.7812 (0.7087), Acc@5: 0.8438 (0.9022)
2022-01-05 19:51:18,461 Val Step[1250/1563], Loss: 0.8269 (1.3153), Acc@1: 0.8438 (0.7067), Acc@5: 0.9375 (0.9000)
2022-01-05 19:51:19,954 Val Step[1300/1563], Loss: 0.8219 (1.3266), Acc@1: 0.8750 (0.7044), Acc@5: 0.9375 (0.8986)
2022-01-05 19:51:21,394 Val Step[1350/1563], Loss: 1.8624 (1.3439), Acc@1: 0.4688 (0.7006), Acc@5: 0.8438 (0.8960)
2022-01-05 19:51:22,889 Val Step[1400/1563], Loss: 1.2589 (1.3520), Acc@1: 0.7188 (0.6988), Acc@5: 0.9062 (0.8949)
2022-01-05 19:51:24,353 Val Step[1450/1563], Loss: 1.3270 (1.3587), Acc@1: 0.7500 (0.6972), Acc@5: 0.9062 (0.8944)
2022-01-05 19:51:25,858 Val Step[1500/1563], Loss: 1.9362 (1.3477), Acc@1: 0.5625 (0.6997), Acc@5: 0.8125 (0.8958)
2022-01-05 19:51:27,307 Val Step[1550/1563], Loss: 1.1003 (1.3507), Acc@1: 0.8438 (0.6988), Acc@5: 0.9062 (0.8954)
2022-01-05 19:51:28,201 ----- Epoch[208/310], Validation Loss: 1.3484, Validation Acc@1: 0.6993, Validation Acc@5: 0.8957, time: 106.04
2022-01-05 19:51:28,201 ----- Epoch[208/310], Train Loss: 3.7991, Train Acc: 0.3598, time: 1513.27, Best Val(epoch208) Acc@1: 0.6993
2022-01-05 19:51:28,432 Max accuracy so far: 0.6993 at epoch_208
2022-01-05 19:51:28,432 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 19:51:28,432 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 19:51:28,525 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 19:51:28,889 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 19:51:28,890 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 19:51:29,012 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 19:51:29,013 Now training epoch 209. LR=0.000214
2022-01-05 19:52:48,563 Epoch[209/310], Step[0000/1251], Loss: 3.5287(3.5287), Acc: 0.3750(0.3750)
2022-01-05 19:53:44,904 Epoch[209/310], Step[0050/1251], Loss: 4.1122(3.8492), Acc: 0.4150(0.3480)
2022-01-05 19:54:41,570 Epoch[209/310], Step[0100/1251], Loss: 4.0154(3.8300), Acc: 0.4014(0.3495)
2022-01-05 19:55:38,099 Epoch[209/310], Step[0150/1251], Loss: 3.5350(3.8122), Acc: 0.5088(0.3526)
2022-01-05 19:56:36,251 Epoch[209/310], Step[0200/1251], Loss: 3.9338(3.8298), Acc: 0.2754(0.3479)
2022-01-05 19:57:32,365 Epoch[209/310], Step[0250/1251], Loss: 3.5658(3.8234), Acc: 0.5410(0.3509)
2022-01-05 19:58:28,463 Epoch[209/310], Step[0300/1251], Loss: 3.8025(3.8271), Acc: 0.5117(0.3524)
2022-01-05 19:59:24,941 Epoch[209/310], Step[0350/1251], Loss: 3.3778(3.8159), Acc: 0.5352(0.3519)
2022-01-05 20:00:20,729 Epoch[209/310], Step[0400/1251], Loss: 3.8882(3.8106), Acc: 0.2900(0.3521)
2022-01-05 20:01:17,349 Epoch[209/310], Step[0450/1251], Loss: 3.5522(3.8038), Acc: 0.3379(0.3523)
2022-01-05 20:02:13,761 Epoch[209/310], Step[0500/1251], Loss: 4.1219(3.7948), Acc: 0.3389(0.3519)
2022-01-05 20:03:10,585 Epoch[209/310], Step[0550/1251], Loss: 3.6641(3.7951), Acc: 0.2822(0.3524)
2022-01-05 20:04:07,137 Epoch[209/310], Step[0600/1251], Loss: 4.0352(3.8018), Acc: 0.4082(0.3515)
2022-01-05 20:05:04,775 Epoch[209/310], Step[0650/1251], Loss: 3.8560(3.8027), Acc: 0.3760(0.3499)
2022-01-05 20:06:00,070 Epoch[209/310], Step[0700/1251], Loss: 3.5489(3.8054), Acc: 0.4062(0.3497)
2022-01-05 20:06:58,152 Epoch[209/310], Step[0750/1251], Loss: 3.5591(3.8070), Acc: 0.5488(0.3509)
2022-01-05 20:07:54,613 Epoch[209/310], Step[0800/1251], Loss: 4.0275(3.8043), Acc: 0.4678(0.3532)
2022-01-05 20:08:51,546 Epoch[209/310], Step[0850/1251], Loss: 4.1589(3.7997), Acc: 0.2598(0.3510)
2022-01-05 20:09:47,852 Epoch[209/310], Step[0900/1251], Loss: 3.4287(3.8004), Acc: 0.3535(0.3505)
2022-01-05 20:10:46,458 Epoch[209/310], Step[0950/1251], Loss: 3.8259(3.7983), Acc: 0.3906(0.3512)
2022-01-05 20:11:43,497 Epoch[209/310], Step[1000/1251], Loss: 3.3029(3.8005), Acc: 0.5029(0.3516)
2022-01-05 20:12:41,551 Epoch[209/310], Step[1050/1251], Loss: 3.8897(3.8010), Acc: 0.4072(0.3513)
2022-01-05 20:13:36,339 Epoch[209/310], Step[1100/1251], Loss: 3.8876(3.8003), Acc: 0.4424(0.3518)
2022-01-05 20:14:34,586 Epoch[209/310], Step[1150/1251], Loss: 3.8397(3.8007), Acc: 0.4248(0.3518)
2022-01-05 20:15:32,539 Epoch[209/310], Step[1200/1251], Loss: 3.9275(3.7985), Acc: 0.3389(0.3515)
2022-01-05 20:16:29,667 Epoch[209/310], Step[1250/1251], Loss: 3.9099(3.7994), Acc: 0.2881(0.3514)
2022-01-05 20:16:31,857 ----- Epoch[209/310], Train Loss: 3.7994, Train Acc: 0.3514, time: 1502.84, Best Val(epoch208) Acc@1: 0.6993
2022-01-05 20:16:32,032 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 20:16:32,032 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 20:16:32,140 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 20:16:32,141 Now training epoch 210. LR=0.000210
2022-01-05 20:17:51,288 Epoch[210/310], Step[0000/1251], Loss: 3.9740(3.9740), Acc: 0.2969(0.2969)
2022-01-05 20:18:48,210 Epoch[210/310], Step[0050/1251], Loss: 4.1434(3.7916), Acc: 0.2764(0.3733)
2022-01-05 20:19:44,677 Epoch[210/310], Step[0100/1251], Loss: 3.5780(3.7711), Acc: 0.3633(0.3624)
2022-01-05 20:20:41,663 Epoch[210/310], Step[0150/1251], Loss: 3.9238(3.7789), Acc: 0.4258(0.3626)
2022-01-05 20:21:38,383 Epoch[210/310], Step[0200/1251], Loss: 4.0770(3.7880), Acc: 0.2646(0.3591)
2022-01-05 20:22:35,335 Epoch[210/310], Step[0250/1251], Loss: 3.8417(3.7727), Acc: 0.4062(0.3610)
2022-01-05 20:23:31,685 Epoch[210/310], Step[0300/1251], Loss: 3.6705(3.7851), Acc: 0.2100(0.3606)
2022-01-05 20:24:28,666 Epoch[210/310], Step[0350/1251], Loss: 4.2913(3.7934), Acc: 0.2910(0.3551)
2022-01-05 20:25:25,800 Epoch[210/310], Step[0400/1251], Loss: 3.3311(3.7889), Acc: 0.5625(0.3541)
2022-01-05 20:26:24,059 Epoch[210/310], Step[0450/1251], Loss: 3.7444(3.7917), Acc: 0.3262(0.3519)
2022-01-05 20:27:21,250 Epoch[210/310], Step[0500/1251], Loss: 3.6901(3.7902), Acc: 0.3154(0.3486)
2022-01-05 20:28:16,976 Epoch[210/310], Step[0550/1251], Loss: 3.9506(3.7958), Acc: 0.3447(0.3481)
2022-01-05 20:29:12,022 Epoch[210/310], Step[0600/1251], Loss: 4.1299(3.7889), Acc: 0.3623(0.3484)
2022-01-05 20:30:07,015 Epoch[210/310], Step[0650/1251], Loss: 3.2742(3.7879), Acc: 0.3916(0.3491)
2022-01-05 20:31:03,069 Epoch[210/310], Step[0700/1251], Loss: 4.1527(3.7870), Acc: 0.3066(0.3494)
2022-01-05 20:31:59,057 Epoch[210/310], Step[0750/1251], Loss: 3.9585(3.7857), Acc: 0.3281(0.3493)
2022-01-05 20:32:56,269 Epoch[210/310], Step[0800/1251], Loss: 3.8534(3.7904), Acc: 0.2900(0.3471)
2022-01-05 20:33:53,517 Epoch[210/310], Step[0850/1251], Loss: 3.4386(3.7924), Acc: 0.4307(0.3467)
2022-01-05 20:34:50,316 Epoch[210/310], Step[0900/1251], Loss: 3.3057(3.7963), Acc: 0.4316(0.3457)
2022-01-05 20:35:47,237 Epoch[210/310], Step[0950/1251], Loss: 3.5432(3.7949), Acc: 0.5215(0.3458)
2022-01-05 20:36:43,178 Epoch[210/310], Step[1000/1251], Loss: 3.6404(3.7932), Acc: 0.0889(0.3464)
2022-01-05 20:37:39,281 Epoch[210/310], Step[1050/1251], Loss: 3.4481(3.7939), Acc: 0.2119(0.3464)
2022-01-05 20:38:36,756 Epoch[210/310], Step[1100/1251], Loss: 3.6678(3.7932), Acc: 0.4824(0.3468)
2022-01-05 20:39:33,346 Epoch[210/310], Step[1150/1251], Loss: 3.9439(3.7970), Acc: 0.4004(0.3466)
2022-01-05 20:40:31,003 Epoch[210/310], Step[1200/1251], Loss: 4.0894(3.7957), Acc: 0.3398(0.3465)
2022-01-05 20:41:27,185 Epoch[210/310], Step[1250/1251], Loss: 3.5222(3.7943), Acc: 0.2451(0.3465)
2022-01-05 20:41:29,097 ----- Validation after Epoch: 210
2022-01-05 20:42:31,814 Val Step[0000/1563], Loss: 0.7414 (0.7414), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-05 20:42:33,296 Val Step[0050/1563], Loss: 2.2778 (0.8152), Acc@1: 0.3438 (0.8321), Acc@5: 0.8125 (0.9504)
2022-01-05 20:42:34,831 Val Step[0100/1563], Loss: 1.8056 (1.0713), Acc@1: 0.5625 (0.7587), Acc@5: 0.8750 (0.9264)
2022-01-05 20:42:36,241 Val Step[0150/1563], Loss: 0.6121 (1.0221), Acc@1: 0.8750 (0.7738), Acc@5: 0.9688 (0.9286)
2022-01-05 20:42:37,700 Val Step[0200/1563], Loss: 1.0292 (1.0361), Acc@1: 0.7188 (0.7722), Acc@5: 0.9375 (0.9280)
2022-01-05 20:42:39,130 Val Step[0250/1563], Loss: 0.3892 (0.9824), Acc@1: 1.0000 (0.7857), Acc@5: 1.0000 (0.9355)
2022-01-05 20:42:40,661 Val Step[0300/1563], Loss: 1.0567 (1.0402), Acc@1: 0.7500 (0.7678), Acc@5: 0.9062 (0.9311)
2022-01-05 20:42:42,168 Val Step[0350/1563], Loss: 1.1260 (1.0592), Acc@1: 0.7188 (0.7600), Acc@5: 0.9062 (0.9316)
2022-01-05 20:42:43,650 Val Step[0400/1563], Loss: 1.1940 (1.0636), Acc@1: 0.7500 (0.7551), Acc@5: 0.9688 (0.9327)
2022-01-05 20:42:45,196 Val Step[0450/1563], Loss: 1.1703 (1.0715), Acc@1: 0.5938 (0.7531), Acc@5: 1.0000 (0.9336)
2022-01-05 20:42:46,748 Val Step[0500/1563], Loss: 0.3984 (1.0619), Acc@1: 0.9688 (0.7561), Acc@5: 1.0000 (0.9352)
2022-01-05 20:42:48,307 Val Step[0550/1563], Loss: 0.7257 (1.0383), Acc@1: 0.8125 (0.7626), Acc@5: 0.9688 (0.9378)
2022-01-05 20:42:49,880 Val Step[0600/1563], Loss: 0.8393 (1.0441), Acc@1: 0.8125 (0.7622), Acc@5: 0.9375 (0.9370)
2022-01-05 20:42:51,353 Val Step[0650/1563], Loss: 0.6053 (1.0672), Acc@1: 0.9062 (0.7574), Acc@5: 1.0000 (0.9342)
2022-01-05 20:42:52,842 Val Step[0700/1563], Loss: 1.2385 (1.0990), Acc@1: 0.7812 (0.7507), Acc@5: 0.8750 (0.9306)
2022-01-05 20:42:54,304 Val Step[0750/1563], Loss: 1.3083 (1.1305), Acc@1: 0.7812 (0.7446), Acc@5: 0.8750 (0.9262)
2022-01-05 20:42:55,770 Val Step[0800/1563], Loss: 0.6695 (1.1699), Acc@1: 0.9062 (0.7351), Acc@5: 1.0000 (0.9213)
2022-01-05 20:42:57,195 Val Step[0850/1563], Loss: 1.4713 (1.1940), Acc@1: 0.6562 (0.7294), Acc@5: 0.9062 (0.9180)
2022-01-05 20:42:58,708 Val Step[0900/1563], Loss: 0.4357 (1.1949), Acc@1: 0.9375 (0.7308), Acc@5: 1.0000 (0.9171)
2022-01-05 20:43:00,261 Val Step[0950/1563], Loss: 1.4166 (1.2166), Acc@1: 0.7188 (0.7266), Acc@5: 0.9062 (0.9141)
2022-01-05 20:43:01,667 Val Step[1000/1563], Loss: 0.5078 (1.2385), Acc@1: 0.9375 (0.7212), Acc@5: 1.0000 (0.9108)
2022-01-05 20:43:02,991 Val Step[1050/1563], Loss: 0.3208 (1.2531), Acc@1: 0.9688 (0.7177), Acc@5: 1.0000 (0.9092)
2022-01-05 20:43:04,431 Val Step[1100/1563], Loss: 0.8171 (1.2683), Acc@1: 0.8438 (0.7150), Acc@5: 0.9688 (0.9068)
2022-01-05 20:43:05,943 Val Step[1150/1563], Loss: 1.2745 (1.2845), Acc@1: 0.7812 (0.7120), Acc@5: 0.8125 (0.9041)
2022-01-05 20:43:07,339 Val Step[1200/1563], Loss: 1.2019 (1.3005), Acc@1: 0.8125 (0.7086), Acc@5: 0.8438 (0.9020)
2022-01-05 20:43:08,733 Val Step[1250/1563], Loss: 0.7030 (1.3129), Acc@1: 0.9062 (0.7070), Acc@5: 0.9062 (0.8999)
2022-01-05 20:43:10,201 Val Step[1300/1563], Loss: 1.0446 (1.3235), Acc@1: 0.8125 (0.7046), Acc@5: 0.9375 (0.8984)
2022-01-05 20:43:11,694 Val Step[1350/1563], Loss: 2.2116 (1.3417), Acc@1: 0.4688 (0.7000), Acc@5: 0.8125 (0.8962)
2022-01-05 20:43:13,192 Val Step[1400/1563], Loss: 1.1478 (1.3483), Acc@1: 0.7188 (0.6989), Acc@5: 0.9062 (0.8952)
2022-01-05 20:43:14,592 Val Step[1450/1563], Loss: 1.6287 (1.3550), Acc@1: 0.5625 (0.6970), Acc@5: 0.9688 (0.8949)
2022-01-05 20:43:15,949 Val Step[1500/1563], Loss: 1.5951 (1.3437), Acc@1: 0.5938 (0.6994), Acc@5: 0.8750 (0.8960)
2022-01-05 20:43:17,389 Val Step[1550/1563], Loss: 1.0335 (1.3454), Acc@1: 0.8750 (0.6988), Acc@5: 0.9062 (0.8958)
2022-01-05 20:43:18,522 ----- Epoch[210/310], Validation Loss: 1.3443, Validation Acc@1: 0.6991, Validation Acc@5: 0.8958, time: 109.42
2022-01-05 20:43:18,523 ----- Epoch[210/310], Train Loss: 3.7943, Train Acc: 0.3465, time: 1496.95, Best Val(epoch208) Acc@1: 0.6993
2022-01-05 20:43:18,678 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-210-Loss-3.7954531541164163.pdparams
2022-01-05 20:43:18,679 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-210-Loss-3.7954531541164163.pdopt
2022-01-05 20:43:18,717 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-210-Loss-3.7954531541164163-EMA.pdparams
2022-01-05 20:43:18,718 Now training epoch 211. LR=0.000206
2022-01-05 20:44:41,874 Epoch[211/310], Step[0000/1251], Loss: 3.6672(3.6672), Acc: 0.3691(0.3691)
2022-01-05 20:45:39,695 Epoch[211/310], Step[0050/1251], Loss: 4.2900(3.8113), Acc: 0.2598(0.3643)
2022-01-05 20:46:37,702 Epoch[211/310], Step[0100/1251], Loss: 2.9979(3.7764), Acc: 0.4434(0.3552)
2022-01-05 20:47:34,180 Epoch[211/310], Step[0150/1251], Loss: 3.8232(3.7678), Acc: 0.2080(0.3540)
2022-01-05 20:48:31,383 Epoch[211/310], Step[0200/1251], Loss: 3.8562(3.7823), Acc: 0.4404(0.3479)
2022-01-05 20:49:29,018 Epoch[211/310], Step[0250/1251], Loss: 3.3020(3.7737), Acc: 0.4209(0.3489)
2022-01-05 20:50:25,045 Epoch[211/310], Step[0300/1251], Loss: 3.9495(3.7747), Acc: 0.2588(0.3489)
2022-01-05 20:51:22,668 Epoch[211/310], Step[0350/1251], Loss: 4.3644(3.7709), Acc: 0.2207(0.3482)
2022-01-05 20:52:20,024 Epoch[211/310], Step[0400/1251], Loss: 4.2684(3.7740), Acc: 0.3086(0.3497)
2022-01-05 20:53:16,849 Epoch[211/310], Step[0450/1251], Loss: 3.4742(3.7630), Acc: 0.2422(0.3535)
2022-01-05 20:54:15,167 Epoch[211/310], Step[0500/1251], Loss: 3.9716(3.7706), Acc: 0.3359(0.3529)
2022-01-05 20:55:12,564 Epoch[211/310], Step[0550/1251], Loss: 3.6283(3.7717), Acc: 0.2324(0.3516)
2022-01-05 20:56:10,484 Epoch[211/310], Step[0600/1251], Loss: 3.5442(3.7735), Acc: 0.4580(0.3525)
2022-01-05 20:57:08,239 Epoch[211/310], Step[0650/1251], Loss: 4.0659(3.7788), Acc: 0.3174(0.3522)
2022-01-05 20:58:04,861 Epoch[211/310], Step[0700/1251], Loss: 4.0898(3.7815), Acc: 0.2227(0.3518)
2022-01-05 20:59:01,997 Epoch[211/310], Step[0750/1251], Loss: 3.9047(3.7780), Acc: 0.3105(0.3517)
2022-01-05 20:59:59,269 Epoch[211/310], Step[0800/1251], Loss: 3.4323(3.7767), Acc: 0.5342(0.3512)
2022-01-05 21:00:56,902 Epoch[211/310], Step[0850/1251], Loss: 3.7999(3.7720), Acc: 0.4648(0.3516)
2022-01-05 21:01:54,647 Epoch[211/310], Step[0900/1251], Loss: 3.5736(3.7709), Acc: 0.4941(0.3515)
2022-01-05 21:02:51,327 Epoch[211/310], Step[0950/1251], Loss: 3.9444(3.7753), Acc: 0.4287(0.3511)
2022-01-05 21:03:48,930 Epoch[211/310], Step[1000/1251], Loss: 4.3610(3.7744), Acc: 0.3906(0.3510)
2022-01-05 21:04:47,126 Epoch[211/310], Step[1050/1251], Loss: 3.7331(3.7745), Acc: 0.2490(0.3513)
2022-01-05 21:05:43,933 Epoch[211/310], Step[1100/1251], Loss: 3.4706(3.7733), Acc: 0.5234(0.3517)
2022-01-05 21:06:40,531 Epoch[211/310], Step[1150/1251], Loss: 4.2798(3.7770), Acc: 0.2822(0.3511)
2022-01-05 21:07:37,261 Epoch[211/310], Step[1200/1251], Loss: 3.6515(3.7760), Acc: 0.3320(0.3514)
2022-01-05 21:08:34,160 Epoch[211/310], Step[1250/1251], Loss: 3.5466(3.7766), Acc: 0.3848(0.3517)
2022-01-05 21:08:36,052 ----- Epoch[211/310], Train Loss: 3.7766, Train Acc: 0.3517, time: 1517.33, Best Val(epoch208) Acc@1: 0.6993
2022-01-05 21:08:36,227 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 21:08:36,227 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 21:08:36,336 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 21:08:36,336 Now training epoch 212. LR=0.000202
2022-01-05 21:10:05,122 Epoch[212/310], Step[0000/1251], Loss: 4.0390(4.0390), Acc: 0.4023(0.4023)
2022-01-05 21:11:03,613 Epoch[212/310], Step[0050/1251], Loss: 4.2378(3.7875), Acc: 0.3789(0.3444)
2022-01-05 21:12:00,692 Epoch[212/310], Step[0100/1251], Loss: 3.7247(3.7705), Acc: 0.3574(0.3582)
2022-01-05 21:12:59,001 Epoch[212/310], Step[0150/1251], Loss: 3.9643(3.7818), Acc: 0.1465(0.3516)
2022-01-05 21:13:55,908 Epoch[212/310], Step[0200/1251], Loss: 4.0260(3.7694), Acc: 0.2998(0.3504)
2022-01-05 21:14:52,538 Epoch[212/310], Step[0250/1251], Loss: 3.9628(3.7721), Acc: 0.3135(0.3510)
2022-01-05 21:15:49,631 Epoch[212/310], Step[0300/1251], Loss: 4.1746(3.7828), Acc: 0.2197(0.3496)
2022-01-05 21:16:46,046 Epoch[212/310], Step[0350/1251], Loss: 3.8479(3.7805), Acc: 0.4600(0.3459)
2022-01-05 21:17:43,043 Epoch[212/310], Step[0400/1251], Loss: 4.0193(3.7737), Acc: 0.4307(0.3451)
2022-01-05 21:18:39,984 Epoch[212/310], Step[0450/1251], Loss: 3.5865(3.7732), Acc: 0.4883(0.3449)
2022-01-05 21:19:36,263 Epoch[212/310], Step[0500/1251], Loss: 3.9116(3.7682), Acc: 0.1514(0.3476)
2022-01-05 21:20:34,481 Epoch[212/310], Step[0550/1251], Loss: 3.3700(3.7735), Acc: 0.5049(0.3470)
2022-01-05 21:21:32,912 Epoch[212/310], Step[0600/1251], Loss: 4.0772(3.7780), Acc: 0.2988(0.3496)
2022-01-05 21:22:30,894 Epoch[212/310], Step[0650/1251], Loss: 3.6181(3.7788), Acc: 0.3906(0.3497)
2022-01-05 21:23:27,913 Epoch[212/310], Step[0700/1251], Loss: 4.1824(3.7838), Acc: 0.3115(0.3483)
2022-01-05 21:24:25,010 Epoch[212/310], Step[0750/1251], Loss: 3.5215(3.7835), Acc: 0.5166(0.3487)
2022-01-05 21:25:21,345 Epoch[212/310], Step[0800/1251], Loss: 3.8241(3.7833), Acc: 0.4912(0.3482)
2022-01-05 21:26:18,569 Epoch[212/310], Step[0850/1251], Loss: 3.8865(3.7845), Acc: 0.2725(0.3460)
2022-01-05 21:27:16,555 Epoch[212/310], Step[0900/1251], Loss: 3.8809(3.7856), Acc: 0.4277(0.3451)
2022-01-05 21:28:13,628 Epoch[212/310], Step[0950/1251], Loss: 3.6362(3.7852), Acc: 0.4873(0.3464)
2022-01-05 21:29:11,319 Epoch[212/310], Step[1000/1251], Loss: 3.4986(3.7859), Acc: 0.2295(0.3480)
2022-01-05 21:30:09,090 Epoch[212/310], Step[1050/1251], Loss: 4.5017(3.7897), Acc: 0.3125(0.3482)
2022-01-05 21:31:06,117 Epoch[212/310], Step[1100/1251], Loss: 3.7281(3.7899), Acc: 0.2949(0.3492)
2022-01-05 21:32:03,901 Epoch[212/310], Step[1150/1251], Loss: 3.6350(3.7877), Acc: 0.2803(0.3505)
2022-01-05 21:33:01,784 Epoch[212/310], Step[1200/1251], Loss: 3.3018(3.7846), Acc: 0.5586(0.3506)
2022-01-05 21:33:59,688 Epoch[212/310], Step[1250/1251], Loss: 3.8694(3.7865), Acc: 0.4121(0.3494)
2022-01-05 21:34:01,680 ----- Validation after Epoch: 212
2022-01-05 21:35:01,261 Val Step[0000/1563], Loss: 0.7453 (0.7453), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 21:35:02,882 Val Step[0050/1563], Loss: 2.3082 (0.8473), Acc@1: 0.4375 (0.8413), Acc@5: 0.8438 (0.9559)
2022-01-05 21:35:04,381 Val Step[0100/1563], Loss: 1.8939 (1.1338), Acc@1: 0.5625 (0.7587), Acc@5: 0.8750 (0.9291)
2022-01-05 21:35:05,825 Val Step[0150/1563], Loss: 0.5759 (1.0682), Acc@1: 0.8750 (0.7744), Acc@5: 1.0000 (0.9334)
2022-01-05 21:35:07,268 Val Step[0200/1563], Loss: 1.1126 (1.0833), Acc@1: 0.7812 (0.7755), Acc@5: 0.9062 (0.9303)
2022-01-05 21:35:08,698 Val Step[0250/1563], Loss: 0.7710 (1.0331), Acc@1: 0.8438 (0.7886), Acc@5: 1.0000 (0.9370)
2022-01-05 21:35:10,111 Val Step[0300/1563], Loss: 1.1170 (1.1009), Acc@1: 0.7500 (0.7678), Acc@5: 0.9688 (0.9305)
2022-01-05 21:35:11,595 Val Step[0350/1563], Loss: 1.2997 (1.1092), Acc@1: 0.6875 (0.7635), Acc@5: 0.9062 (0.9324)
2022-01-05 21:35:12,940 Val Step[0400/1563], Loss: 1.0437 (1.1161), Acc@1: 0.8125 (0.7590), Acc@5: 0.9688 (0.9330)
2022-01-05 21:35:14,404 Val Step[0450/1563], Loss: 1.1644 (1.1206), Acc@1: 0.6250 (0.7565), Acc@5: 1.0000 (0.9340)
2022-01-05 21:35:15,938 Val Step[0500/1563], Loss: 0.5618 (1.1084), Acc@1: 0.8750 (0.7596), Acc@5: 1.0000 (0.9353)
2022-01-05 21:35:17,540 Val Step[0550/1563], Loss: 0.8282 (1.0845), Acc@1: 0.8438 (0.7658), Acc@5: 1.0000 (0.9381)
2022-01-05 21:35:19,128 Val Step[0600/1563], Loss: 0.7912 (1.0869), Acc@1: 0.8438 (0.7651), Acc@5: 0.9375 (0.9376)
2022-01-05 21:35:20,639 Val Step[0650/1563], Loss: 0.8355 (1.1111), Acc@1: 0.7812 (0.7588), Acc@5: 0.9688 (0.9338)
2022-01-05 21:35:22,173 Val Step[0700/1563], Loss: 0.9151 (1.1424), Acc@1: 0.8438 (0.7515), Acc@5: 0.9688 (0.9296)
2022-01-05 21:35:23,635 Val Step[0750/1563], Loss: 1.6132 (1.1755), Acc@1: 0.6562 (0.7449), Acc@5: 0.9062 (0.9241)
2022-01-05 21:35:25,042 Val Step[0800/1563], Loss: 0.7373 (1.2120), Acc@1: 0.8438 (0.7360), Acc@5: 1.0000 (0.9198)
2022-01-05 21:35:26,521 Val Step[0850/1563], Loss: 1.7916 (1.2385), Acc@1: 0.5625 (0.7296), Acc@5: 0.8438 (0.9163)
2022-01-05 21:35:28,017 Val Step[0900/1563], Loss: 0.3725 (1.2372), Acc@1: 0.9688 (0.7312), Acc@5: 1.0000 (0.9158)
2022-01-05 21:35:29,517 Val Step[0950/1563], Loss: 1.3772 (1.2573), Acc@1: 0.7812 (0.7279), Acc@5: 0.9062 (0.9126)
2022-01-05 21:35:30,977 Val Step[1000/1563], Loss: 0.8294 (1.2790), Acc@1: 0.9375 (0.7224), Acc@5: 0.9375 (0.9095)
2022-01-05 21:35:32,500 Val Step[1050/1563], Loss: 0.4700 (1.2923), Acc@1: 0.9688 (0.7188), Acc@5: 1.0000 (0.9079)
2022-01-05 21:35:33,943 Val Step[1100/1563], Loss: 1.1756 (1.3091), Acc@1: 0.6875 (0.7151), Acc@5: 0.9062 (0.9052)
2022-01-05 21:35:35,398 Val Step[1150/1563], Loss: 1.2243 (1.3250), Acc@1: 0.7812 (0.7122), Acc@5: 0.8438 (0.9024)
2022-01-05 21:35:36,893 Val Step[1200/1563], Loss: 1.3471 (1.3402), Acc@1: 0.7812 (0.7087), Acc@5: 0.8438 (0.9001)
2022-01-05 21:35:38,340 Val Step[1250/1563], Loss: 0.7710 (1.3561), Acc@1: 0.8750 (0.7060), Acc@5: 0.9375 (0.8975)
2022-01-05 21:35:39,847 Val Step[1300/1563], Loss: 0.9415 (1.3641), Acc@1: 0.8750 (0.7040), Acc@5: 0.9375 (0.8966)
2022-01-05 21:35:41,273 Val Step[1350/1563], Loss: 2.0555 (1.3809), Acc@1: 0.4062 (0.7000), Acc@5: 0.8438 (0.8944)
2022-01-05 21:35:42,627 Val Step[1400/1563], Loss: 1.0308 (1.3878), Acc@1: 0.7812 (0.6985), Acc@5: 0.9688 (0.8937)
2022-01-05 21:35:44,019 Val Step[1450/1563], Loss: 1.3870 (1.3945), Acc@1: 0.7188 (0.6969), Acc@5: 0.9375 (0.8935)
2022-01-05 21:35:45,409 Val Step[1500/1563], Loss: 1.5117 (1.3830), Acc@1: 0.6562 (0.6992), Acc@5: 0.8438 (0.8949)
2022-01-05 21:35:46,797 Val Step[1550/1563], Loss: 1.0668 (1.3828), Acc@1: 0.8750 (0.6991), Acc@5: 0.9062 (0.8950)
2022-01-05 21:35:47,772 ----- Epoch[212/310], Validation Loss: 1.3809, Validation Acc@1: 0.6995, Validation Acc@5: 0.8952, time: 106.09
2022-01-05 21:35:47,772 ----- Epoch[212/310], Train Loss: 3.7865, Train Acc: 0.3494, time: 1525.34, Best Val(epoch212) Acc@1: 0.6995
2022-01-05 21:35:47,980 Max accuracy so far: 0.6995 at epoch_212
2022-01-05 21:35:47,980 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 21:35:47,981 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 21:35:48,189 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 21:35:48,457 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 21:35:48,458 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 21:35:48,604 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 21:35:48,604 Now training epoch 213. LR=0.000198
2022-01-05 21:37:09,976 Epoch[213/310], Step[0000/1251], Loss: 4.1376(4.1376), Acc: 0.3955(0.3955)
2022-01-05 21:38:07,148 Epoch[213/310], Step[0050/1251], Loss: 3.5546(3.8126), Acc: 0.5391(0.3595)
2022-01-05 21:39:02,870 Epoch[213/310], Step[0100/1251], Loss: 3.5710(3.7780), Acc: 0.4590(0.3684)
2022-01-05 21:40:00,829 Epoch[213/310], Step[0150/1251], Loss: 4.1271(3.7842), Acc: 0.2500(0.3694)
2022-01-05 21:40:58,346 Epoch[213/310], Step[0200/1251], Loss: 3.5090(3.7861), Acc: 0.5615(0.3648)
2022-01-05 21:41:55,282 Epoch[213/310], Step[0250/1251], Loss: 3.7296(3.7925), Acc: 0.3389(0.3596)
2022-01-05 21:42:52,040 Epoch[213/310], Step[0300/1251], Loss: 3.8586(3.7827), Acc: 0.3984(0.3607)
2022-01-05 21:43:48,281 Epoch[213/310], Step[0350/1251], Loss: 3.6747(3.7839), Acc: 0.3037(0.3613)
2022-01-05 21:44:45,132 Epoch[213/310], Step[0400/1251], Loss: 3.0102(3.7777), Acc: 0.2441(0.3600)
2022-01-05 21:45:41,418 Epoch[213/310], Step[0450/1251], Loss: 3.5091(3.7703), Acc: 0.3955(0.3575)
2022-01-05 21:46:37,019 Epoch[213/310], Step[0500/1251], Loss: 3.9111(3.7721), Acc: 0.2734(0.3562)
2022-01-05 21:47:32,909 Epoch[213/310], Step[0550/1251], Loss: 3.4052(3.7684), Acc: 0.5195(0.3558)
2022-01-05 21:48:30,466 Epoch[213/310], Step[0600/1251], Loss: 3.8261(3.7658), Acc: 0.3037(0.3541)
2022-01-05 21:49:28,208 Epoch[213/310], Step[0650/1251], Loss: 3.8040(3.7639), Acc: 0.3584(0.3543)
2022-01-05 21:50:26,172 Epoch[213/310], Step[0700/1251], Loss: 3.5315(3.7646), Acc: 0.5127(0.3552)
2022-01-05 21:51:24,028 Epoch[213/310], Step[0750/1251], Loss: 4.1535(3.7617), Acc: 0.3545(0.3559)
2022-01-05 21:52:20,343 Epoch[213/310], Step[0800/1251], Loss: 4.2931(3.7662), Acc: 0.2451(0.3554)
2022-01-05 21:53:16,058 Epoch[213/310], Step[0850/1251], Loss: 3.5424(3.7663), Acc: 0.4062(0.3553)
2022-01-05 21:54:13,368 Epoch[213/310], Step[0900/1251], Loss: 3.6793(3.7678), Acc: 0.1641(0.3552)
2022-01-05 21:55:10,601 Epoch[213/310], Step[0950/1251], Loss: 3.6689(3.7655), Acc: 0.2598(0.3559)
2022-01-05 21:56:07,205 Epoch[213/310], Step[1000/1251], Loss: 3.9840(3.7681), Acc: 0.3906(0.3564)
2022-01-05 21:57:04,432 Epoch[213/310], Step[1050/1251], Loss: 3.6577(3.7697), Acc: 0.4434(0.3552)
2022-01-05 21:58:01,139 Epoch[213/310], Step[1100/1251], Loss: 4.7262(3.7738), Acc: 0.2061(0.3543)
2022-01-05 21:58:58,158 Epoch[213/310], Step[1150/1251], Loss: 3.5266(3.7765), Acc: 0.4102(0.3537)
2022-01-05 21:59:55,318 Epoch[213/310], Step[1200/1251], Loss: 3.8188(3.7750), Acc: 0.3301(0.3552)
2022-01-05 22:00:52,166 Epoch[213/310], Step[1250/1251], Loss: 3.7735(3.7718), Acc: 0.3545(0.3559)
2022-01-05 22:00:54,331 ----- Epoch[213/310], Train Loss: 3.7718, Train Acc: 0.3559, time: 1505.72, Best Val(epoch212) Acc@1: 0.6995
2022-01-05 22:00:54,505 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 22:00:54,505 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 22:00:54,614 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 22:00:54,614 Now training epoch 214. LR=0.000193
2022-01-05 22:02:18,035 Epoch[214/310], Step[0000/1251], Loss: 3.0328(3.0328), Acc: 0.6104(0.6104)
2022-01-05 22:03:13,858 Epoch[214/310], Step[0050/1251], Loss: 4.0536(3.7753), Acc: 0.3584(0.3507)
2022-01-05 22:04:11,008 Epoch[214/310], Step[0100/1251], Loss: 3.9719(3.7651), Acc: 0.1895(0.3447)
2022-01-05 22:05:07,466 Epoch[214/310], Step[0150/1251], Loss: 4.2403(3.7809), Acc: 0.2871(0.3464)
2022-01-05 22:06:04,420 Epoch[214/310], Step[0200/1251], Loss: 3.7207(3.7636), Acc: 0.4912(0.3513)
2022-01-05 22:07:01,175 Epoch[214/310], Step[0250/1251], Loss: 4.1342(3.7745), Acc: 0.2734(0.3452)
2022-01-05 22:07:58,107 Epoch[214/310], Step[0300/1251], Loss: 3.5910(3.7708), Acc: 0.4932(0.3477)
2022-01-05 22:08:55,423 Epoch[214/310], Step[0350/1251], Loss: 3.6775(3.7681), Acc: 0.3760(0.3444)
2022-01-05 22:09:52,139 Epoch[214/310], Step[0400/1251], Loss: 3.7941(3.7593), Acc: 0.3887(0.3430)
2022-01-05 22:10:50,545 Epoch[214/310], Step[0450/1251], Loss: 3.7883(3.7523), Acc: 0.3389(0.3423)
2022-01-05 22:11:47,927 Epoch[214/310], Step[0500/1251], Loss: 3.4331(3.7559), Acc: 0.4746(0.3443)
2022-01-05 22:12:45,619 Epoch[214/310], Step[0550/1251], Loss: 3.6073(3.7544), Acc: 0.2285(0.3439)
2022-01-05 22:13:42,544 Epoch[214/310], Step[0600/1251], Loss: 3.7378(3.7533), Acc: 0.4590(0.3447)
2022-01-05 22:14:39,163 Epoch[214/310], Step[0650/1251], Loss: 3.8617(3.7573), Acc: 0.3115(0.3463)
2022-01-05 22:15:36,457 Epoch[214/310], Step[0700/1251], Loss: 3.1358(3.7580), Acc: 0.5752(0.3484)
2022-01-05 22:16:33,683 Epoch[214/310], Step[0750/1251], Loss: 4.4634(3.7570), Acc: 0.2100(0.3491)
2022-01-05 22:17:29,551 Epoch[214/310], Step[0800/1251], Loss: 3.5536(3.7619), Acc: 0.4707(0.3495)
2022-01-05 22:18:25,521 Epoch[214/310], Step[0850/1251], Loss: 4.1180(3.7610), Acc: 0.4346(0.3493)
2022-01-05 22:19:21,779 Epoch[214/310], Step[0900/1251], Loss: 3.4681(3.7579), Acc: 0.1729(0.3490)
2022-01-05 22:20:18,076 Epoch[214/310], Step[0950/1251], Loss: 3.4277(3.7585), Acc: 0.3623(0.3502)
2022-01-05 22:21:14,814 Epoch[214/310], Step[1000/1251], Loss: 3.6017(3.7651), Acc: 0.4434(0.3496)
2022-01-05 22:22:11,539 Epoch[214/310], Step[1050/1251], Loss: 3.8159(3.7633), Acc: 0.4307(0.3505)
2022-01-05 22:23:09,495 Epoch[214/310], Step[1100/1251], Loss: 4.0318(3.7657), Acc: 0.3320(0.3515)
2022-01-05 22:24:07,304 Epoch[214/310], Step[1150/1251], Loss: 3.7996(3.7672), Acc: 0.4453(0.3521)
2022-01-05 22:25:04,736 Epoch[214/310], Step[1200/1251], Loss: 3.9981(3.7679), Acc: 0.3350(0.3524)
2022-01-05 22:26:00,860 Epoch[214/310], Step[1250/1251], Loss: 3.5587(3.7666), Acc: 0.4053(0.3530)
2022-01-05 22:26:02,873 ----- Validation after Epoch: 214
2022-01-05 22:27:01,063 Val Step[0000/1563], Loss: 0.6864 (0.6864), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-05 22:27:02,676 Val Step[0050/1563], Loss: 2.0145 (0.7890), Acc@1: 0.4688 (0.8419), Acc@5: 0.8750 (0.9553)
2022-01-05 22:27:04,183 Val Step[0100/1563], Loss: 1.6417 (1.0738), Acc@1: 0.6250 (0.7611), Acc@5: 0.8750 (0.9316)
2022-01-05 22:27:05,667 Val Step[0150/1563], Loss: 0.4999 (1.0213), Acc@1: 0.9375 (0.7763), Acc@5: 1.0000 (0.9350)
2022-01-05 22:27:07,152 Val Step[0200/1563], Loss: 1.1410 (1.0368), Acc@1: 0.7500 (0.7769), Acc@5: 0.9062 (0.9313)
2022-01-05 22:27:08,603 Val Step[0250/1563], Loss: 0.5896 (0.9860), Acc@1: 0.9062 (0.7888), Acc@5: 1.0000 (0.9377)
2022-01-05 22:27:10,031 Val Step[0300/1563], Loss: 1.2287 (1.0495), Acc@1: 0.6562 (0.7675), Acc@5: 0.9688 (0.9326)
2022-01-05 22:27:11,414 Val Step[0350/1563], Loss: 1.1150 (1.0568), Acc@1: 0.7188 (0.7628), Acc@5: 0.9375 (0.9338)
2022-01-05 22:27:12,891 Val Step[0400/1563], Loss: 1.0371 (1.0625), Acc@1: 0.7188 (0.7578), Acc@5: 0.9688 (0.9345)
2022-01-05 22:27:14,395 Val Step[0450/1563], Loss: 0.8183 (1.0650), Acc@1: 0.8125 (0.7551), Acc@5: 1.0000 (0.9353)
2022-01-05 22:27:15,923 Val Step[0500/1563], Loss: 0.4989 (1.0567), Acc@1: 0.9375 (0.7588), Acc@5: 1.0000 (0.9371)
2022-01-05 22:27:17,425 Val Step[0550/1563], Loss: 0.9789 (1.0342), Acc@1: 0.8125 (0.7650), Acc@5: 0.9375 (0.9391)
2022-01-05 22:27:19,013 Val Step[0600/1563], Loss: 0.9066 (1.0401), Acc@1: 0.8125 (0.7645), Acc@5: 0.9062 (0.9382)
2022-01-05 22:27:20,612 Val Step[0650/1563], Loss: 0.5620 (1.0603), Acc@1: 0.9062 (0.7604), Acc@5: 1.0000 (0.9349)
2022-01-05 22:27:22,061 Val Step[0700/1563], Loss: 1.1248 (1.0912), Acc@1: 0.7500 (0.7525), Acc@5: 0.9688 (0.9315)
2022-01-05 22:27:23,513 Val Step[0750/1563], Loss: 1.5378 (1.1258), Acc@1: 0.7188 (0.7449), Acc@5: 0.8750 (0.9272)
2022-01-05 22:27:24,879 Val Step[0800/1563], Loss: 0.9359 (1.1624), Acc@1: 0.7812 (0.7363), Acc@5: 1.0000 (0.9223)
2022-01-05 22:27:26,372 Val Step[0850/1563], Loss: 1.3468 (1.1886), Acc@1: 0.6250 (0.7298), Acc@5: 0.9375 (0.9192)
2022-01-05 22:27:27,871 Val Step[0900/1563], Loss: 0.3015 (1.1884), Acc@1: 0.9375 (0.7317), Acc@5: 1.0000 (0.9186)
2022-01-05 22:27:29,398 Val Step[0950/1563], Loss: 1.5150 (1.2105), Acc@1: 0.7188 (0.7275), Acc@5: 0.8750 (0.9153)
2022-01-05 22:27:30,813 Val Step[1000/1563], Loss: 0.8009 (1.2336), Acc@1: 0.8438 (0.7217), Acc@5: 0.9688 (0.9122)
2022-01-05 22:27:32,205 Val Step[1050/1563], Loss: 0.3464 (1.2494), Acc@1: 0.9688 (0.7181), Acc@5: 0.9688 (0.9104)
2022-01-05 22:27:33,595 Val Step[1100/1563], Loss: 1.0400 (1.2654), Acc@1: 0.7500 (0.7149), Acc@5: 0.9688 (0.9080)
2022-01-05 22:27:35,071 Val Step[1150/1563], Loss: 1.3184 (1.2792), Acc@1: 0.7188 (0.7121), Acc@5: 0.8125 (0.9057)
2022-01-05 22:27:36,610 Val Step[1200/1563], Loss: 1.2166 (1.2942), Acc@1: 0.7812 (0.7086), Acc@5: 0.8438 (0.9034)
2022-01-05 22:27:38,069 Val Step[1250/1563], Loss: 0.7232 (1.3063), Acc@1: 0.8750 (0.7071), Acc@5: 0.9375 (0.9013)
2022-01-05 22:27:39,502 Val Step[1300/1563], Loss: 0.9832 (1.3187), Acc@1: 0.8125 (0.7044), Acc@5: 0.8750 (0.8996)
2022-01-05 22:27:40,924 Val Step[1350/1563], Loss: 1.8620 (1.3353), Acc@1: 0.5312 (0.7005), Acc@5: 0.8125 (0.8975)
2022-01-05 22:27:42,354 Val Step[1400/1563], Loss: 1.0028 (1.3415), Acc@1: 0.8125 (0.6995), Acc@5: 0.9688 (0.8967)
2022-01-05 22:27:43,851 Val Step[1450/1563], Loss: 1.1099 (1.3475), Acc@1: 0.8125 (0.6980), Acc@5: 0.9688 (0.8963)
2022-01-05 22:27:45,295 Val Step[1500/1563], Loss: 1.8098 (1.3363), Acc@1: 0.5938 (0.7006), Acc@5: 0.8438 (0.8979)
2022-01-05 22:27:46,722 Val Step[1550/1563], Loss: 1.0522 (1.3383), Acc@1: 0.8750 (0.7001), Acc@5: 0.9062 (0.8975)
2022-01-05 22:27:47,518 ----- Epoch[214/310], Validation Loss: 1.3366, Validation Acc@1: 0.7004, Validation Acc@5: 0.8977, time: 104.64
2022-01-05 22:27:47,519 ----- Epoch[214/310], Train Loss: 3.7666, Train Acc: 0.3530, time: 1508.25, Best Val(epoch214) Acc@1: 0.7004
2022-01-05 22:27:47,702 Max accuracy so far: 0.7004 at epoch_214
2022-01-05 22:27:47,702 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 22:27:47,703 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 22:27:47,815 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 22:27:48,194 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 22:27:48,194 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 22:27:48,327 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 22:27:48,327 Now training epoch 215. LR=0.000189
2022-01-05 22:29:07,287 Epoch[215/310], Step[0000/1251], Loss: 3.5609(3.5609), Acc: 0.2627(0.2627)
2022-01-05 22:30:04,497 Epoch[215/310], Step[0050/1251], Loss: 3.6833(3.7536), Acc: 0.3701(0.3730)
2022-01-05 22:31:02,100 Epoch[215/310], Step[0100/1251], Loss: 3.3238(3.7581), Acc: 0.3330(0.3678)
2022-01-05 22:31:59,056 Epoch[215/310], Step[0150/1251], Loss: 3.1504(3.7604), Acc: 0.3145(0.3718)
2022-01-05 22:32:56,493 Epoch[215/310], Step[0200/1251], Loss: 4.2607(3.7546), Acc: 0.3086(0.3800)
2022-01-05 22:33:53,593 Epoch[215/310], Step[0250/1251], Loss: 3.5951(3.7447), Acc: 0.3301(0.3772)
2022-01-05 22:34:50,175 Epoch[215/310], Step[0300/1251], Loss: 4.0837(3.7397), Acc: 0.2783(0.3741)
2022-01-05 22:35:49,015 Epoch[215/310], Step[0350/1251], Loss: 4.0419(3.7510), Acc: 0.3408(0.3733)
2022-01-05 22:36:47,199 Epoch[215/310], Step[0400/1251], Loss: 3.7117(3.7482), Acc: 0.3389(0.3724)
2022-01-05 22:37:44,991 Epoch[215/310], Step[0450/1251], Loss: 3.8140(3.7472), Acc: 0.0732(0.3728)
2022-01-05 22:38:43,053 Epoch[215/310], Step[0500/1251], Loss: 3.2429(3.7488), Acc: 0.5488(0.3717)
2022-01-05 22:39:40,787 Epoch[215/310], Step[0550/1251], Loss: 3.8757(3.7489), Acc: 0.3008(0.3705)
2022-01-05 22:40:37,753 Epoch[215/310], Step[0600/1251], Loss: 4.2238(3.7473), Acc: 0.2031(0.3701)
2022-01-05 22:41:36,222 Epoch[215/310], Step[0650/1251], Loss: 3.9077(3.7534), Acc: 0.4932(0.3701)
2022-01-05 22:42:33,131 Epoch[215/310], Step[0700/1251], Loss: 4.2633(3.7540), Acc: 0.3174(0.3698)
2022-01-05 22:43:30,004 Epoch[215/310], Step[0750/1251], Loss: 4.0631(3.7573), Acc: 0.2227(0.3676)
2022-01-05 22:44:26,687 Epoch[215/310], Step[0800/1251], Loss: 3.8187(3.7586), Acc: 0.3652(0.3657)
2022-01-05 22:45:25,460 Epoch[215/310], Step[0850/1251], Loss: 3.3842(3.7613), Acc: 0.4395(0.3640)
2022-01-05 22:46:23,229 Epoch[215/310], Step[0900/1251], Loss: 3.3167(3.7630), Acc: 0.4102(0.3632)
2022-01-05 22:47:19,267 Epoch[215/310], Step[0950/1251], Loss: 3.8464(3.7600), Acc: 0.3135(0.3638)
2022-01-05 22:48:17,474 Epoch[215/310], Step[1000/1251], Loss: 3.2508(3.7600), Acc: 0.4131(0.3629)
2022-01-05 22:49:12,649 Epoch[215/310], Step[1050/1251], Loss: 3.0952(3.7593), Acc: 0.3740(0.3633)
2022-01-05 22:50:08,879 Epoch[215/310], Step[1100/1251], Loss: 4.0827(3.7610), Acc: 0.1689(0.3618)
2022-01-05 22:51:05,472 Epoch[215/310], Step[1150/1251], Loss: 3.6027(3.7612), Acc: 0.3584(0.3609)
2022-01-05 22:52:01,968 Epoch[215/310], Step[1200/1251], Loss: 3.9638(3.7647), Acc: 0.2520(0.3607)
2022-01-05 22:52:56,273 Epoch[215/310], Step[1250/1251], Loss: 4.0860(3.7674), Acc: 0.3457(0.3606)
2022-01-05 22:52:58,388 ----- Epoch[215/310], Train Loss: 3.7674, Train Acc: 0.3606, time: 1510.06, Best Val(epoch214) Acc@1: 0.7004
2022-01-05 22:52:58,562 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 22:52:58,562 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 22:52:58,695 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 22:52:58,695 Now training epoch 216. LR=0.000185
2022-01-05 22:54:13,301 Epoch[216/310], Step[0000/1251], Loss: 3.7763(3.7763), Acc: 0.2012(0.2012)
2022-01-05 22:55:10,028 Epoch[216/310], Step[0050/1251], Loss: 3.7952(3.6975), Acc: 0.1152(0.3675)
2022-01-05 22:56:06,917 Epoch[216/310], Step[0100/1251], Loss: 3.9510(3.7264), Acc: 0.3750(0.3663)
2022-01-05 22:57:03,750 Epoch[216/310], Step[0150/1251], Loss: 3.7262(3.7313), Acc: 0.3291(0.3557)
2022-01-05 22:58:00,048 Epoch[216/310], Step[0200/1251], Loss: 3.1226(3.7318), Acc: 0.3281(0.3545)
2022-01-05 22:58:56,175 Epoch[216/310], Step[0250/1251], Loss: 3.7038(3.7352), Acc: 0.2412(0.3537)
2022-01-05 22:59:53,724 Epoch[216/310], Step[0300/1251], Loss: 4.2834(3.7282), Acc: 0.3662(0.3544)
2022-01-05 23:00:51,902 Epoch[216/310], Step[0350/1251], Loss: 4.1531(3.7336), Acc: 0.3115(0.3552)
2022-01-05 23:01:49,233 Epoch[216/310], Step[0400/1251], Loss: 3.6005(3.7301), Acc: 0.5645(0.3547)
2022-01-05 23:02:46,399 Epoch[216/310], Step[0450/1251], Loss: 3.6305(3.7233), Acc: 0.3457(0.3552)
2022-01-05 23:03:43,010 Epoch[216/310], Step[0500/1251], Loss: 4.1389(3.7268), Acc: 0.2236(0.3541)
2022-01-05 23:04:40,120 Epoch[216/310], Step[0550/1251], Loss: 3.8543(3.7265), Acc: 0.3545(0.3542)
2022-01-05 23:05:38,922 Epoch[216/310], Step[0600/1251], Loss: 4.0347(3.7321), Acc: 0.3926(0.3546)
2022-01-05 23:06:35,831 Epoch[216/310], Step[0650/1251], Loss: 3.8395(3.7339), Acc: 0.0576(0.3543)
2022-01-05 23:07:31,856 Epoch[216/310], Step[0700/1251], Loss: 3.8095(3.7352), Acc: 0.4287(0.3539)
2022-01-05 23:08:28,986 Epoch[216/310], Step[0750/1251], Loss: 3.0493(3.7326), Acc: 0.3252(0.3538)
2022-01-05 23:09:25,530 Epoch[216/310], Step[0800/1251], Loss: 3.3834(3.7348), Acc: 0.5410(0.3544)
2022-01-05 23:10:23,010 Epoch[216/310], Step[0850/1251], Loss: 4.0102(3.7381), Acc: 0.3232(0.3552)
2022-01-05 23:11:19,871 Epoch[216/310], Step[0900/1251], Loss: 3.7500(3.7403), Acc: 0.4736(0.3539)
2022-01-05 23:12:14,636 Epoch[216/310], Step[0950/1251], Loss: 3.7337(3.7415), Acc: 0.4424(0.3540)
2022-01-05 23:13:10,084 Epoch[216/310], Step[1000/1251], Loss: 4.1810(3.7407), Acc: 0.1777(0.3548)
2022-01-05 23:14:07,817 Epoch[216/310], Step[1050/1251], Loss: 4.2869(3.7397), Acc: 0.1348(0.3562)
2022-01-05 23:15:03,186 Epoch[216/310], Step[1100/1251], Loss: 3.5968(3.7411), Acc: 0.1475(0.3573)
2022-01-05 23:15:59,460 Epoch[216/310], Step[1150/1251], Loss: 3.8798(3.7420), Acc: 0.3086(0.3572)
2022-01-05 23:16:56,616 Epoch[216/310], Step[1200/1251], Loss: 3.6009(3.7419), Acc: 0.2705(0.3572)
2022-01-05 23:17:54,435 Epoch[216/310], Step[1250/1251], Loss: 3.7482(3.7436), Acc: 0.3887(0.3560)
2022-01-05 23:17:56,632 ----- Validation after Epoch: 216
2022-01-05 23:18:54,726 Val Step[0000/1563], Loss: 0.7712 (0.7712), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-05 23:18:56,210 Val Step[0050/1563], Loss: 2.0852 (0.7905), Acc@1: 0.4375 (0.8370), Acc@5: 0.8750 (0.9540)
2022-01-05 23:18:57,592 Val Step[0100/1563], Loss: 2.0244 (1.0743), Acc@1: 0.5312 (0.7553), Acc@5: 0.8438 (0.9270)
2022-01-05 23:18:59,096 Val Step[0150/1563], Loss: 0.4284 (1.0139), Acc@1: 0.9062 (0.7705), Acc@5: 1.0000 (0.9329)
2022-01-05 23:19:00,555 Val Step[0200/1563], Loss: 1.0670 (1.0264), Acc@1: 0.7500 (0.7724), Acc@5: 0.9062 (0.9314)
2022-01-05 23:19:01,937 Val Step[0250/1563], Loss: 0.6199 (0.9775), Acc@1: 0.9375 (0.7849), Acc@5: 1.0000 (0.9379)
2022-01-05 23:19:03,365 Val Step[0300/1563], Loss: 1.2192 (1.0401), Acc@1: 0.7188 (0.7674), Acc@5: 0.8750 (0.9329)
2022-01-05 23:19:04,774 Val Step[0350/1563], Loss: 1.0772 (1.0465), Acc@1: 0.7500 (0.7641), Acc@5: 0.9375 (0.9353)
2022-01-05 23:19:06,174 Val Step[0400/1563], Loss: 0.9290 (1.0516), Acc@1: 0.8125 (0.7595), Acc@5: 0.9688 (0.9363)
2022-01-05 23:19:07,694 Val Step[0450/1563], Loss: 0.8818 (1.0549), Acc@1: 0.7500 (0.7571), Acc@5: 1.0000 (0.9369)
2022-01-05 23:19:09,221 Val Step[0500/1563], Loss: 0.3919 (1.0454), Acc@1: 0.9375 (0.7612), Acc@5: 1.0000 (0.9376)
2022-01-05 23:19:10,745 Val Step[0550/1563], Loss: 0.6871 (1.0225), Acc@1: 0.8750 (0.7672), Acc@5: 0.9688 (0.9394)
2022-01-05 23:19:12,275 Val Step[0600/1563], Loss: 0.9603 (1.0292), Acc@1: 0.7500 (0.7659), Acc@5: 0.9062 (0.9390)
2022-01-05 23:19:13,716 Val Step[0650/1563], Loss: 0.7150 (1.0514), Acc@1: 0.8750 (0.7612), Acc@5: 1.0000 (0.9358)
2022-01-05 23:19:15,201 Val Step[0700/1563], Loss: 1.0364 (1.0859), Acc@1: 0.8438 (0.7530), Acc@5: 0.9062 (0.9315)
2022-01-05 23:19:16,740 Val Step[0750/1563], Loss: 1.5086 (1.1182), Acc@1: 0.7500 (0.7469), Acc@5: 0.8438 (0.9272)
2022-01-05 23:19:18,250 Val Step[0800/1563], Loss: 0.6992 (1.1568), Acc@1: 0.8750 (0.7376), Acc@5: 1.0000 (0.9222)
2022-01-05 23:19:19,804 Val Step[0850/1563], Loss: 1.4830 (1.1838), Acc@1: 0.5625 (0.7311), Acc@5: 0.9062 (0.9187)
2022-01-05 23:19:21,300 Val Step[0900/1563], Loss: 0.2699 (1.1828), Acc@1: 0.9688 (0.7325), Acc@5: 1.0000 (0.9182)
2022-01-05 23:19:22,877 Val Step[0950/1563], Loss: 1.2416 (1.2047), Acc@1: 0.7188 (0.7282), Acc@5: 0.9375 (0.9153)
2022-01-05 23:19:24,273 Val Step[1000/1563], Loss: 0.5387 (1.2290), Acc@1: 0.9375 (0.7223), Acc@5: 1.0000 (0.9119)
2022-01-05 23:19:25,637 Val Step[1050/1563], Loss: 0.4208 (1.2445), Acc@1: 0.9688 (0.7185), Acc@5: 0.9688 (0.9101)
2022-01-05 23:19:27,146 Val Step[1100/1563], Loss: 1.0127 (1.2573), Acc@1: 0.7812 (0.7160), Acc@5: 0.9688 (0.9082)
2022-01-05 23:19:28,663 Val Step[1150/1563], Loss: 1.2363 (1.2721), Acc@1: 0.7812 (0.7127), Acc@5: 0.8438 (0.9062)
2022-01-05 23:19:30,105 Val Step[1200/1563], Loss: 1.1128 (1.2867), Acc@1: 0.7812 (0.7097), Acc@5: 0.8438 (0.9037)
2022-01-05 23:19:31,555 Val Step[1250/1563], Loss: 0.6805 (1.2991), Acc@1: 0.8750 (0.7079), Acc@5: 0.9062 (0.9016)
2022-01-05 23:19:32,991 Val Step[1300/1563], Loss: 0.9375 (1.3079), Acc@1: 0.8438 (0.7064), Acc@5: 0.9062 (0.9005)
2022-01-05 23:19:34,429 Val Step[1350/1563], Loss: 1.9230 (1.3261), Acc@1: 0.4062 (0.7021), Acc@5: 0.8438 (0.8975)
2022-01-05 23:19:35,937 Val Step[1400/1563], Loss: 1.0655 (1.3322), Acc@1: 0.6875 (0.7010), Acc@5: 0.9062 (0.8965)
2022-01-05 23:19:37,292 Val Step[1450/1563], Loss: 1.5307 (1.3397), Acc@1: 0.6250 (0.6995), Acc@5: 0.8750 (0.8959)
2022-01-05 23:19:38,704 Val Step[1500/1563], Loss: 1.6696 (1.3292), Acc@1: 0.5000 (0.7022), Acc@5: 0.8750 (0.8970)
2022-01-05 23:19:40,108 Val Step[1550/1563], Loss: 0.8478 (1.3301), Acc@1: 0.8750 (0.7016), Acc@5: 0.9062 (0.8970)
2022-01-05 23:19:40,946 ----- Epoch[216/310], Validation Loss: 1.3277, Validation Acc@1: 0.7021, Validation Acc@5: 0.8974, time: 104.31
2022-01-05 23:19:40,947 ----- Epoch[216/310], Train Loss: 3.7436, Train Acc: 0.3560, time: 1497.93, Best Val(epoch216) Acc@1: 0.7021
2022-01-05 23:19:41,128 Max accuracy so far: 0.7021 at epoch_216
2022-01-05 23:19:41,128 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-05 23:19:41,128 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-05 23:19:41,241 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-05 23:19:41,724 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 23:19:41,724 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 23:19:41,827 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 23:19:41,828 Now training epoch 217. LR=0.000181
2022-01-05 23:20:57,860 Epoch[217/310], Step[0000/1251], Loss: 3.6236(3.6236), Acc: 0.3955(0.3955)
2022-01-05 23:21:55,297 Epoch[217/310], Step[0050/1251], Loss: 3.7814(3.7324), Acc: 0.4268(0.3611)
2022-01-05 23:22:53,310 Epoch[217/310], Step[0100/1251], Loss: 4.1732(3.7861), Acc: 0.1445(0.3411)
2022-01-05 23:23:49,452 Epoch[217/310], Step[0150/1251], Loss: 3.9956(3.7850), Acc: 0.2354(0.3480)
2022-01-05 23:24:47,436 Epoch[217/310], Step[0200/1251], Loss: 3.3729(3.7794), Acc: 0.3750(0.3503)
2022-01-05 23:25:45,065 Epoch[217/310], Step[0250/1251], Loss: 3.6646(3.7800), Acc: 0.3975(0.3434)
2022-01-05 23:26:41,611 Epoch[217/310], Step[0300/1251], Loss: 4.0434(3.7744), Acc: 0.4355(0.3449)
2022-01-05 23:27:38,763 Epoch[217/310], Step[0350/1251], Loss: 3.8403(3.7734), Acc: 0.3057(0.3470)
2022-01-05 23:28:35,112 Epoch[217/310], Step[0400/1251], Loss: 4.1150(3.7807), Acc: 0.4072(0.3477)
2022-01-05 23:29:31,803 Epoch[217/310], Step[0450/1251], Loss: 3.5894(3.7767), Acc: 0.5039(0.3485)
2022-01-05 23:30:29,068 Epoch[217/310], Step[0500/1251], Loss: 4.1038(3.7698), Acc: 0.2832(0.3508)
2022-01-05 23:31:26,746 Epoch[217/310], Step[0550/1251], Loss: 3.8254(3.7756), Acc: 0.4834(0.3503)
2022-01-05 23:32:23,306 Epoch[217/310], Step[0600/1251], Loss: 3.8334(3.7712), Acc: 0.2451(0.3494)
2022-01-05 23:33:21,024 Epoch[217/310], Step[0650/1251], Loss: 3.8719(3.7638), Acc: 0.3438(0.3519)
2022-01-05 23:34:18,020 Epoch[217/310], Step[0700/1251], Loss: 4.3101(3.7684), Acc: 0.3057(0.3517)
2022-01-05 23:35:15,896 Epoch[217/310], Step[0750/1251], Loss: 3.6571(3.7651), Acc: 0.4805(0.3515)
2022-01-05 23:36:13,137 Epoch[217/310], Step[0800/1251], Loss: 4.1300(3.7648), Acc: 0.2832(0.3515)
2022-01-05 23:37:10,057 Epoch[217/310], Step[0850/1251], Loss: 3.9600(3.7675), Acc: 0.3594(0.3514)
2022-01-05 23:38:07,667 Epoch[217/310], Step[0900/1251], Loss: 4.1342(3.7704), Acc: 0.3320(0.3518)
2022-01-05 23:39:05,278 Epoch[217/310], Step[0950/1251], Loss: 4.0422(3.7733), Acc: 0.3438(0.3513)
2022-01-05 23:40:02,786 Epoch[217/310], Step[1000/1251], Loss: 3.9826(3.7722), Acc: 0.0547(0.3523)
2022-01-05 23:41:00,504 Epoch[217/310], Step[1050/1251], Loss: 3.8477(3.7687), Acc: 0.1641(0.3527)
2022-01-05 23:41:58,801 Epoch[217/310], Step[1100/1251], Loss: 3.6117(3.7666), Acc: 0.5439(0.3536)
2022-01-05 23:42:54,096 Epoch[217/310], Step[1150/1251], Loss: 3.9413(3.7660), Acc: 0.3545(0.3536)
2022-01-05 23:43:50,585 Epoch[217/310], Step[1200/1251], Loss: 3.4583(3.7652), Acc: 0.4893(0.3539)
2022-01-05 23:44:47,656 Epoch[217/310], Step[1250/1251], Loss: 3.9683(3.7625), Acc: 0.2725(0.3545)
2022-01-05 23:44:49,678 ----- Epoch[217/310], Train Loss: 3.7625, Train Acc: 0.3545, time: 1507.85, Best Val(epoch216) Acc@1: 0.7021
2022-01-05 23:44:49,855 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-05 23:44:49,855 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-05 23:44:49,966 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-05 23:44:49,966 Now training epoch 218. LR=0.000177
2022-01-05 23:46:07,664 Epoch[218/310], Step[0000/1251], Loss: 3.5968(3.5968), Acc: 0.4414(0.4414)
2022-01-05 23:47:04,992 Epoch[218/310], Step[0050/1251], Loss: 3.4592(3.7493), Acc: 0.3818(0.3432)
2022-01-05 23:48:02,289 Epoch[218/310], Step[0100/1251], Loss: 3.9295(3.7542), Acc: 0.4912(0.3525)
2022-01-05 23:48:58,450 Epoch[218/310], Step[0150/1251], Loss: 3.3655(3.7597), Acc: 0.4453(0.3466)
2022-01-05 23:49:54,701 Epoch[218/310], Step[0200/1251], Loss: 4.1656(3.7499), Acc: 0.3750(0.3573)
2022-01-05 23:50:52,340 Epoch[218/310], Step[0250/1251], Loss: 3.2492(3.7521), Acc: 0.4121(0.3561)
2022-01-05 23:51:48,427 Epoch[218/310], Step[0300/1251], Loss: 3.4144(3.7509), Acc: 0.3359(0.3587)
2022-01-05 23:52:45,365 Epoch[218/310], Step[0350/1251], Loss: 3.3342(3.7516), Acc: 0.5518(0.3566)
2022-01-05 23:53:40,627 Epoch[218/310], Step[0400/1251], Loss: 3.7467(3.7665), Acc: 0.2217(0.3548)
2022-01-05 23:54:37,499 Epoch[218/310], Step[0450/1251], Loss: 3.3334(3.7642), Acc: 0.4404(0.3541)
2022-01-05 23:55:34,701 Epoch[218/310], Step[0500/1251], Loss: 3.4731(3.7648), Acc: 0.5420(0.3553)
2022-01-05 23:56:32,015 Epoch[218/310], Step[0550/1251], Loss: 3.2545(3.7633), Acc: 0.4590(0.3546)
2022-01-05 23:57:29,125 Epoch[218/310], Step[0600/1251], Loss: 3.5140(3.7610), Acc: 0.3711(0.3570)
2022-01-05 23:58:25,102 Epoch[218/310], Step[0650/1251], Loss: 3.4424(3.7644), Acc: 0.4336(0.3572)
2022-01-05 23:59:23,028 Epoch[218/310], Step[0700/1251], Loss: 3.5337(3.7629), Acc: 0.5762(0.3551)
2022-01-06 00:00:20,140 Epoch[218/310], Step[0750/1251], Loss: 3.6691(3.7605), Acc: 0.1289(0.3562)
2022-01-06 00:01:17,991 Epoch[218/310], Step[0800/1251], Loss: 2.9791(3.7604), Acc: 0.4424(0.3561)
2022-01-06 00:02:14,593 Epoch[218/310], Step[0850/1251], Loss: 3.8836(3.7626), Acc: 0.2764(0.3567)
2022-01-06 00:03:11,224 Epoch[218/310], Step[0900/1251], Loss: 3.8364(3.7604), Acc: 0.1416(0.3567)
2022-01-06 00:04:06,289 Epoch[218/310], Step[0950/1251], Loss: 3.3602(3.7575), Acc: 0.2998(0.3584)
2022-01-06 00:05:02,415 Epoch[218/310], Step[1000/1251], Loss: 3.2136(3.7602), Acc: 0.3408(0.3570)
2022-01-06 00:05:58,929 Epoch[218/310], Step[1050/1251], Loss: 4.3854(3.7645), Acc: 0.3301(0.3570)
2022-01-06 00:06:56,091 Epoch[218/310], Step[1100/1251], Loss: 3.7202(3.7622), Acc: 0.4648(0.3571)
2022-01-06 00:07:54,421 Epoch[218/310], Step[1150/1251], Loss: 3.6663(3.7605), Acc: 0.1592(0.3565)
2022-01-06 00:08:51,623 Epoch[218/310], Step[1200/1251], Loss: 3.5162(3.7588), Acc: 0.4395(0.3558)
2022-01-06 00:09:49,658 Epoch[218/310], Step[1250/1251], Loss: 4.1899(3.7565), Acc: 0.3516(0.3562)
2022-01-06 00:09:52,548 ----- Validation after Epoch: 218
2022-01-06 00:10:51,101 Val Step[0000/1563], Loss: 0.7536 (0.7536), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-06 00:10:52,785 Val Step[0050/1563], Loss: 2.2837 (0.8186), Acc@1: 0.4375 (0.8364), Acc@5: 0.8125 (0.9516)
2022-01-06 00:10:54,323 Val Step[0100/1563], Loss: 1.6822 (1.0990), Acc@1: 0.5938 (0.7565), Acc@5: 0.8438 (0.9261)
2022-01-06 00:10:55,941 Val Step[0150/1563], Loss: 0.5504 (1.0285), Acc@1: 0.9375 (0.7742), Acc@5: 0.9688 (0.9340)
2022-01-06 00:10:57,535 Val Step[0200/1563], Loss: 1.0321 (1.0489), Acc@1: 0.7188 (0.7749), Acc@5: 0.9062 (0.9317)
2022-01-06 00:10:59,144 Val Step[0250/1563], Loss: 0.6989 (0.9956), Acc@1: 0.8750 (0.7875), Acc@5: 1.0000 (0.9382)
2022-01-06 00:11:00,677 Val Step[0300/1563], Loss: 1.2980 (1.0566), Acc@1: 0.6875 (0.7705), Acc@5: 0.9062 (0.9332)
2022-01-06 00:11:02,213 Val Step[0350/1563], Loss: 1.2787 (1.0567), Acc@1: 0.6250 (0.7658), Acc@5: 0.9062 (0.9361)
2022-01-06 00:11:03,766 Val Step[0400/1563], Loss: 1.0520 (1.0630), Acc@1: 0.7188 (0.7618), Acc@5: 0.9688 (0.9366)
2022-01-06 00:11:05,333 Val Step[0450/1563], Loss: 0.8715 (1.0660), Acc@1: 0.8125 (0.7598), Acc@5: 1.0000 (0.9372)
2022-01-06 00:11:06,875 Val Step[0500/1563], Loss: 0.5273 (1.0562), Acc@1: 0.9375 (0.7636), Acc@5: 1.0000 (0.9387)
2022-01-06 00:11:08,439 Val Step[0550/1563], Loss: 0.8227 (1.0343), Acc@1: 0.7188 (0.7688), Acc@5: 1.0000 (0.9408)
2022-01-06 00:11:10,063 Val Step[0600/1563], Loss: 0.8409 (1.0390), Acc@1: 0.8125 (0.7683), Acc@5: 0.9375 (0.9398)
2022-01-06 00:11:11,584 Val Step[0650/1563], Loss: 0.6574 (1.0603), Acc@1: 0.8750 (0.7636), Acc@5: 1.0000 (0.9363)
2022-01-06 00:11:13,158 Val Step[0700/1563], Loss: 1.2670 (1.0910), Acc@1: 0.7188 (0.7564), Acc@5: 0.8750 (0.9320)
2022-01-06 00:11:14,757 Val Step[0750/1563], Loss: 1.2852 (1.1252), Acc@1: 0.7500 (0.7498), Acc@5: 0.9062 (0.9272)
2022-01-06 00:11:16,327 Val Step[0800/1563], Loss: 0.7957 (1.1638), Acc@1: 0.8125 (0.7401), Acc@5: 1.0000 (0.9221)
2022-01-06 00:11:17,826 Val Step[0850/1563], Loss: 1.1926 (1.1862), Acc@1: 0.6875 (0.7341), Acc@5: 0.9688 (0.9193)
2022-01-06 00:11:19,266 Val Step[0900/1563], Loss: 0.3162 (1.1862), Acc@1: 0.9688 (0.7353), Acc@5: 1.0000 (0.9185)
2022-01-06 00:11:20,749 Val Step[0950/1563], Loss: 1.4719 (1.2080), Acc@1: 0.7812 (0.7312), Acc@5: 0.9062 (0.9153)
2022-01-06 00:11:22,231 Val Step[1000/1563], Loss: 0.7296 (1.2331), Acc@1: 0.9062 (0.7247), Acc@5: 0.9688 (0.9122)
2022-01-06 00:11:23,651 Val Step[1050/1563], Loss: 0.4834 (1.2472), Acc@1: 0.9688 (0.7218), Acc@5: 0.9688 (0.9101)
2022-01-06 00:11:25,023 Val Step[1100/1563], Loss: 1.0818 (1.2616), Acc@1: 0.7812 (0.7187), Acc@5: 0.9062 (0.9078)
2022-01-06 00:11:26,495 Val Step[1150/1563], Loss: 1.1854 (1.2787), Acc@1: 0.7812 (0.7154), Acc@5: 0.8125 (0.9053)
2022-01-06 00:11:27,893 Val Step[1200/1563], Loss: 1.4553 (1.2940), Acc@1: 0.7812 (0.7126), Acc@5: 0.8438 (0.9028)
2022-01-06 00:11:29,404 Val Step[1250/1563], Loss: 0.7362 (1.3077), Acc@1: 0.8750 (0.7105), Acc@5: 0.9375 (0.9004)
2022-01-06 00:11:30,927 Val Step[1300/1563], Loss: 0.9609 (1.3176), Acc@1: 0.8438 (0.7085), Acc@5: 0.9062 (0.8993)
2022-01-06 00:11:32,439 Val Step[1350/1563], Loss: 1.9907 (1.3335), Acc@1: 0.4062 (0.7047), Acc@5: 0.8438 (0.8970)
2022-01-06 00:11:34,023 Val Step[1400/1563], Loss: 1.0473 (1.3411), Acc@1: 0.7188 (0.7030), Acc@5: 1.0000 (0.8961)
2022-01-06 00:11:35,396 Val Step[1450/1563], Loss: 1.7389 (1.3476), Acc@1: 0.5938 (0.7015), Acc@5: 0.8438 (0.8955)
2022-01-06 00:11:36,755 Val Step[1500/1563], Loss: 1.5030 (1.3376), Acc@1: 0.6250 (0.7039), Acc@5: 0.8750 (0.8968)
2022-01-06 00:11:38,168 Val Step[1550/1563], Loss: 0.9957 (1.3392), Acc@1: 0.8750 (0.7034), Acc@5: 0.9062 (0.8965)
2022-01-06 00:11:39,026 ----- Epoch[218/310], Validation Loss: 1.3375, Validation Acc@1: 0.7037, Validation Acc@5: 0.8967, time: 106.48
2022-01-06 00:11:39,027 ----- Epoch[218/310], Train Loss: 3.7565, Train Acc: 0.3562, time: 1502.58, Best Val(epoch218) Acc@1: 0.7037
2022-01-06 00:11:39,225 Max accuracy so far: 0.7037 at epoch_218
2022-01-06 00:11:39,226 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-06 00:11:39,226 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-06 00:11:39,349 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-06 00:11:39,725 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-06 00:11:39,725 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-06 00:11:39,871 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-06 00:11:39,871 Now training epoch 219. LR=0.000173
2022-01-06 00:12:53,309 Epoch[219/310], Step[0000/1251], Loss: 3.6984(3.6984), Acc: 0.4365(0.4365)
2022-01-06 00:13:50,646 Epoch[219/310], Step[0050/1251], Loss: 3.7607(3.7799), Acc: 0.2266(0.3683)
2022-01-06 00:14:48,074 Epoch[219/310], Step[0100/1251], Loss: 3.6606(3.7697), Acc: 0.5107(0.3744)
2022-01-06 00:15:44,986 Epoch[219/310], Step[0150/1251], Loss: 3.7102(3.7632), Acc: 0.3760(0.3634)
2022-01-06 00:16:42,460 Epoch[219/310], Step[0200/1251], Loss: 3.3196(3.7482), Acc: 0.4600(0.3645)
2022-01-06 00:17:38,126 Epoch[219/310], Step[0250/1251], Loss: 3.4313(3.7466), Acc: 0.5332(0.3632)
2022-01-06 00:18:35,334 Epoch[219/310], Step[0300/1251], Loss: 3.6462(3.7476), Acc: 0.3682(0.3612)
2022-01-06 00:19:31,915 Epoch[219/310], Step[0350/1251], Loss: 3.7131(3.7552), Acc: 0.4746(0.3587)
2022-01-06 00:20:29,367 Epoch[219/310], Step[0400/1251], Loss: 3.6155(3.7478), Acc: 0.3359(0.3592)
2022-01-06 00:21:26,547 Epoch[219/310], Step[0450/1251], Loss: 3.8004(3.7539), Acc: 0.4561(0.3593)
2022-01-06 00:22:24,234 Epoch[219/310], Step[0500/1251], Loss: 3.2000(3.7536), Acc: 0.5400(0.3617)
2022-01-06 00:23:22,945 Epoch[219/310], Step[0550/1251], Loss: 3.4594(3.7543), Acc: 0.3311(0.3599)
2022-01-06 00:24:20,640 Epoch[219/310], Step[0600/1251], Loss: 3.4145(3.7524), Acc: 0.4697(0.3615)
2022-01-06 00:25:17,331 Epoch[219/310], Step[0650/1251], Loss: 3.9647(3.7491), Acc: 0.3721(0.3615)
2022-01-06 00:26:15,158 Epoch[219/310], Step[0700/1251], Loss: 4.1643(3.7520), Acc: 0.3955(0.3608)
2022-01-06 00:27:12,497 Epoch[219/310], Step[0750/1251], Loss: 3.7531(3.7524), Acc: 0.4336(0.3631)
2022-01-06 00:28:08,726 Epoch[219/310], Step[0800/1251], Loss: 3.8629(3.7493), Acc: 0.3213(0.3635)
2022-01-06 00:29:05,595 Epoch[219/310], Step[0850/1251], Loss: 3.4833(3.7503), Acc: 0.4609(0.3630)
2022-01-06 00:30:02,582 Epoch[219/310], Step[0900/1251], Loss: 4.1126(3.7504), Acc: 0.2910(0.3619)
2022-01-06 00:31:01,416 Epoch[219/310], Step[0950/1251], Loss: 3.6528(3.7478), Acc: 0.2754(0.3629)
2022-01-06 00:31:58,755 Epoch[219/310], Step[1000/1251], Loss: 3.8172(3.7515), Acc: 0.2520(0.3619)
2022-01-06 00:32:56,113 Epoch[219/310], Step[1050/1251], Loss: 4.1505(3.7508), Acc: 0.3369(0.3612)
2022-01-06 00:33:52,862 Epoch[219/310], Step[1100/1251], Loss: 3.9425(3.7512), Acc: 0.4248(0.3608)
2022-01-06 00:34:49,867 Epoch[219/310], Step[1150/1251], Loss: 3.5798(3.7502), Acc: 0.3398(0.3601)
2022-01-06 00:35:47,172 Epoch[219/310], Step[1200/1251], Loss: 3.3437(3.7496), Acc: 0.3037(0.3598)
2022-01-06 00:36:43,333 Epoch[219/310], Step[1250/1251], Loss: 3.5635(3.7467), Acc: 0.4932(0.3603)
2022-01-06 00:36:46,032 ----- Epoch[219/310], Train Loss: 3.7467, Train Acc: 0.3603, time: 1506.16, Best Val(epoch218) Acc@1: 0.7037
2022-01-06 00:36:46,215 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-06 00:36:46,216 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-06 00:36:46,487 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-06 00:36:46,487 Now training epoch 220. LR=0.000170
2022-01-06 00:38:00,911 Epoch[220/310], Step[0000/1251], Loss: 3.3233(3.3233), Acc: 0.2803(0.2803)
2022-01-06 00:38:58,042 Epoch[220/310], Step[0050/1251], Loss: 3.7668(3.7721), Acc: 0.4795(0.3366)
2022-01-06 00:39:55,931 Epoch[220/310], Step[0100/1251], Loss: 3.5849(3.7761), Acc: 0.2480(0.3349)
2022-01-06 00:40:53,284 Epoch[220/310], Step[0150/1251], Loss: 3.5605(3.7826), Acc: 0.4746(0.3418)
2022-01-06 00:41:50,739 Epoch[220/310], Step[0200/1251], Loss: 3.3251(3.7810), Acc: 0.4688(0.3497)
2022-01-06 00:42:46,830 Epoch[220/310], Step[0250/1251], Loss: 3.6036(3.7755), Acc: 0.1475(0.3513)
2022-01-06 00:43:44,889 Epoch[220/310], Step[0300/1251], Loss: 3.7093(3.7688), Acc: 0.2412(0.3548)
2022-01-06 00:44:40,753 Epoch[220/310], Step[0350/1251], Loss: 3.8134(3.7685), Acc: 0.1953(0.3554)
2022-01-06 00:45:36,985 Epoch[220/310], Step[0400/1251], Loss: 3.8263(3.7645), Acc: 0.2744(0.3585)
2022-01-06 00:46:32,550 Epoch[220/310], Step[0450/1251], Loss: 3.6367(3.7640), Acc: 0.2744(0.3595)
2022-01-06 00:47:29,261 Epoch[220/310], Step[0500/1251], Loss: 3.0002(3.7602), Acc: 0.2441(0.3589)
2022-01-06 00:48:24,898 Epoch[220/310], Step[0550/1251], Loss: 4.3786(3.7551), Acc: 0.3613(0.3599)
2022-01-06 00:49:19,913 Epoch[220/310], Step[0600/1251], Loss: 3.5753(3.7569), Acc: 0.2510(0.3611)
2022-01-06 00:50:15,370 Epoch[220/310], Step[0650/1251], Loss: 3.4483(3.7566), Acc: 0.5889(0.3614)
2022-01-06 00:51:11,684 Epoch[220/310], Step[0700/1251], Loss: 4.1359(3.7557), Acc: 0.2646(0.3609)
2022-01-06 00:52:08,086 Epoch[220/310], Step[0750/1251], Loss: 3.4137(3.7595), Acc: 0.2002(0.3583)
2022-01-06 00:53:04,136 Epoch[220/310], Step[0800/1251], Loss: 2.9066(3.7629), Acc: 0.6006(0.3578)
2022-01-06 00:54:00,598 Epoch[220/310], Step[0850/1251], Loss: 4.1747(3.7629), Acc: 0.3145(0.3569)
2022-01-06 00:54:56,749 Epoch[220/310], Step[0900/1251], Loss: 3.4246(3.7604), Acc: 0.2109(0.3570)
2022-01-06 00:55:52,465 Epoch[220/310], Step[0950/1251], Loss: 4.1582(3.7593), Acc: 0.4199(0.3572)
2022-01-06 00:56:48,501 Epoch[220/310], Step[1000/1251], Loss: 3.2873(3.7583), Acc: 0.3545(0.3589)
2022-01-06 00:57:46,383 Epoch[220/310], Step[1050/1251], Loss: 3.8081(3.7614), Acc: 0.3857(0.3571)
2022-01-06 00:58:44,061 Epoch[220/310], Step[1100/1251], Loss: 4.0003(3.7632), Acc: 0.3066(0.3561)
2022-01-06 00:59:40,307 Epoch[220/310], Step[1150/1251], Loss: 3.7326(3.7627), Acc: 0.4355(0.3575)
2022-01-06 01:00:37,604 Epoch[220/310], Step[1200/1251], Loss: 3.7777(3.7589), Acc: 0.2461(0.3580)
2022-01-06 01:01:33,200 Epoch[220/310], Step[1250/1251], Loss: 4.0592(3.7600), Acc: 0.4414(0.3582)
2022-01-06 01:01:35,249 ----- Validation after Epoch: 220
2022-01-06 01:02:36,045 Val Step[0000/1563], Loss: 0.6019 (0.6019), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-06 01:02:37,673 Val Step[0050/1563], Loss: 2.2088 (0.7926), Acc@1: 0.5000 (0.8438), Acc@5: 0.8438 (0.9602)
2022-01-06 01:02:39,188 Val Step[0100/1563], Loss: 1.9822 (1.0887), Acc@1: 0.5000 (0.7633), Acc@5: 0.8125 (0.9245)
2022-01-06 01:02:40,654 Val Step[0150/1563], Loss: 0.5348 (1.0275), Acc@1: 0.9375 (0.7775), Acc@5: 0.9688 (0.9313)
2022-01-06 01:02:42,106 Val Step[0200/1563], Loss: 0.9762 (1.0296), Acc@1: 0.7812 (0.7812), Acc@5: 0.9375 (0.9302)
2022-01-06 01:02:43,508 Val Step[0250/1563], Loss: 0.5790 (0.9813), Acc@1: 0.9062 (0.7923), Acc@5: 1.0000 (0.9361)
2022-01-06 01:02:44,961 Val Step[0300/1563], Loss: 1.0492 (1.0419), Acc@1: 0.7188 (0.7737), Acc@5: 0.9688 (0.9310)
2022-01-06 01:02:46,376 Val Step[0350/1563], Loss: 1.0403 (1.0445), Acc@1: 0.7188 (0.7692), Acc@5: 0.9062 (0.9338)
2022-01-06 01:02:47,830 Val Step[0400/1563], Loss: 0.8770 (1.0515), Acc@1: 0.8750 (0.7644), Acc@5: 0.9688 (0.9345)
2022-01-06 01:02:49,301 Val Step[0450/1563], Loss: 1.0072 (1.0553), Acc@1: 0.7188 (0.7623), Acc@5: 1.0000 (0.9354)
2022-01-06 01:02:50,842 Val Step[0500/1563], Loss: 0.5723 (1.0474), Acc@1: 0.8750 (0.7652), Acc@5: 1.0000 (0.9369)
2022-01-06 01:02:52,333 Val Step[0550/1563], Loss: 0.9332 (1.0242), Acc@1: 0.7188 (0.7714), Acc@5: 0.9688 (0.9394)
2022-01-06 01:02:53,866 Val Step[0600/1563], Loss: 0.7355 (1.0315), Acc@1: 0.8750 (0.7704), Acc@5: 0.9375 (0.9386)
2022-01-06 01:02:55,274 Val Step[0650/1563], Loss: 0.6452 (1.0508), Acc@1: 0.9062 (0.7659), Acc@5: 1.0000 (0.9356)
2022-01-06 01:02:56,737 Val Step[0700/1563], Loss: 1.2562 (1.0778), Acc@1: 0.7812 (0.7594), Acc@5: 0.9062 (0.9322)
2022-01-06 01:02:58,237 Val Step[0750/1563], Loss: 1.1024 (1.1117), Acc@1: 0.8125 (0.7522), Acc@5: 0.9062 (0.9282)
2022-01-06 01:02:59,648 Val Step[0800/1563], Loss: 0.8965 (1.1491), Acc@1: 0.7500 (0.7434), Acc@5: 1.0000 (0.9233)
2022-01-06 01:03:01,115 Val Step[0850/1563], Loss: 1.3537 (1.1768), Acc@1: 0.6562 (0.7364), Acc@5: 0.9375 (0.9198)
2022-01-06 01:03:02,591 Val Step[0900/1563], Loss: 0.2940 (1.1779), Acc@1: 0.9688 (0.7378), Acc@5: 1.0000 (0.9190)
2022-01-06 01:03:04,128 Val Step[0950/1563], Loss: 1.5562 (1.1981), Acc@1: 0.6875 (0.7342), Acc@5: 0.9062 (0.9160)
2022-01-06 01:03:05,508 Val Step[1000/1563], Loss: 0.5213 (1.2215), Acc@1: 0.9688 (0.7282), Acc@5: 1.0000 (0.9128)
2022-01-06 01:03:06,923 Val Step[1050/1563], Loss: 0.3672 (1.2355), Acc@1: 0.9688 (0.7251), Acc@5: 0.9688 (0.9110)
2022-01-06 01:03:08,385 Val Step[1100/1563], Loss: 0.7944 (1.2496), Acc@1: 0.8750 (0.7216), Acc@5: 0.9375 (0.9090)
2022-01-06 01:03:10,066 Val Step[1150/1563], Loss: 1.4505 (1.2635), Acc@1: 0.7812 (0.7189), Acc@5: 0.8438 (0.9068)
2022-01-06 01:03:11,427 Val Step[1200/1563], Loss: 1.1755 (1.2795), Acc@1: 0.7500 (0.7151), Acc@5: 0.8750 (0.9046)
2022-01-06 01:03:12,858 Val Step[1250/1563], Loss: 0.7777 (1.2922), Acc@1: 0.8750 (0.7126), Acc@5: 0.9375 (0.9028)
2022-01-06 01:03:14,455 Val Step[1300/1563], Loss: 0.8840 (1.3028), Acc@1: 0.8750 (0.7104), Acc@5: 0.9062 (0.9013)
2022-01-06 01:03:15,858 Val Step[1350/1563], Loss: 2.1073 (1.3210), Acc@1: 0.4375 (0.7058), Acc@5: 0.7500 (0.8988)
2022-01-06 01:03:17,283 Val Step[1400/1563], Loss: 1.1805 (1.3273), Acc@1: 0.7188 (0.7046), Acc@5: 0.9062 (0.8981)
2022-01-06 01:03:18,708 Val Step[1450/1563], Loss: 1.5089 (1.3339), Acc@1: 0.6562 (0.7028), Acc@5: 0.9375 (0.8980)
2022-01-06 01:03:20,230 Val Step[1500/1563], Loss: 1.9851 (1.3237), Acc@1: 0.5000 (0.7053), Acc@5: 0.8125 (0.8993)
2022-01-06 01:03:21,625 Val Step[1550/1563], Loss: 0.9985 (1.3262), Acc@1: 0.8750 (0.7046), Acc@5: 0.9062 (0.8989)
2022-01-06 01:03:22,400 ----- Epoch[220/310], Validation Loss: 1.3243, Validation Acc@1: 0.7051, Validation Acc@5: 0.8992, time: 107.15
2022-01-06 01:03:22,400 ----- Epoch[220/310], Train Loss: 3.7600, Train Acc: 0.3582, time: 1488.76, Best Val(epoch220) Acc@1: 0.7051
2022-01-06 01:03:22,577 Max accuracy so far: 0.7051 at epoch_220
2022-01-06 01:03:22,577 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-06 01:03:22,577 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-06 01:03:22,683 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-06 01:03:22,798 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-220-Loss-3.7659336281813784.pdparams
2022-01-06 01:03:22,798 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-220-Loss-3.7659336281813784.pdopt
2022-01-06 01:03:22,835 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Epoch-220-Loss-3.7659336281813784-EMA.pdparams
2022-01-06 01:03:22,836 Now training epoch 221. LR=0.000166
2022-01-06 01:04:46,549 Epoch[221/310], Step[0000/1251], Loss: 4.1778(4.1778), Acc: 0.3037(0.3037)
2022-01-06 01:05:43,182 Epoch[221/310], Step[0050/1251], Loss: 3.4005(3.7294), Acc: 0.2764(0.3516)
2022-01-06 01:06:39,212 Epoch[221/310], Step[0100/1251], Loss: 3.9759(3.7176), Acc: 0.1641(0.3454)
2022-01-06 01:07:33,772 Epoch[221/310], Step[0150/1251], Loss: 3.2414(3.6866), Acc: 0.5596(0.3538)
2022-01-06 01:08:29,676 Epoch[221/310], Step[0200/1251], Loss: 3.7911(3.6998), Acc: 0.3457(0.3626)
2022-01-06 01:09:25,247 Epoch[221/310], Step[0250/1251], Loss: 3.5936(3.6946), Acc: 0.4268(0.3618)
2022-01-06 01:10:21,346 Epoch[221/310], Step[0300/1251], Loss: 3.6833(3.7013), Acc: 0.4316(0.3589)
2022-01-06 01:11:17,042 Epoch[221/310], Step[0350/1251], Loss: 3.4779(3.6976), Acc: 0.4248(0.3598)
2022-01-06 01:12:13,366 Epoch[221/310], Step[0400/1251], Loss: 4.3120(3.7066), Acc: 0.2754(0.3605)
2022-01-06 01:13:11,143 Epoch[221/310], Step[0450/1251], Loss: 3.9024(3.7105), Acc: 0.2705(0.3589)
2022-01-06 01:14:07,134 Epoch[221/310], Step[0500/1251], Loss: 4.1589(3.7120), Acc: 0.2920(0.3600)
2022-01-06 01:15:05,602 Epoch[221/310], Step[0550/1251], Loss: 4.1297(3.7181), Acc: 0.2803(0.3598)
2022-01-06 01:16:04,284 Epoch[221/310], Step[0600/1251], Loss: 3.2622(3.7168), Acc: 0.2695(0.3612)
2022-01-06 01:17:02,726 Epoch[221/310], Step[0650/1251], Loss: 3.7477(3.7150), Acc: 0.3926(0.3622)
2022-01-06 01:18:00,806 Epoch[221/310], Step[0700/1251], Loss: 3.6527(3.7183), Acc: 0.3447(0.3610)
2022-01-06 01:18:57,383 Epoch[221/310], Step[0750/1251], Loss: 3.8077(3.7233), Acc: 0.4648(0.3613)
2022-01-06 01:19:53,936 Epoch[221/310], Step[0800/1251], Loss: 3.3358(3.7246), Acc: 0.5566(0.3620)
2022-01-06 01:20:51,701 Epoch[221/310], Step[0850/1251], Loss: 3.8867(3.7239), Acc: 0.5078(0.3611)
2022-01-06 01:21:49,177 Epoch[221/310], Step[0900/1251], Loss: 3.9847(3.7261), Acc: 0.4053(0.3600)
2022-01-06 01:22:47,413 Epoch[221/310], Step[0950/1251], Loss: 3.4007(3.7297), Acc: 0.5674(0.3597)
2022-01-06 01:23:46,255 Epoch[221/310], Step[1000/1251], Loss: 3.3706(3.7297), Acc: 0.1787(0.3602)
2022-01-06 01:24:44,025 Epoch[221/310], Step[1050/1251], Loss: 3.7285(3.7328), Acc: 0.3369(0.3594)
2022-01-06 01:25:41,591 Epoch[221/310], Step[1100/1251], Loss: 3.5394(3.7316), Acc: 0.4756(0.3604)
2022-01-06 01:26:38,818 Epoch[221/310], Step[1150/1251], Loss: 3.9399(3.7335), Acc: 0.1836(0.3599)
2022-01-06 01:27:35,655 Epoch[221/310], Step[1200/1251], Loss: 4.2794(3.7342), Acc: 0.2900(0.3584)
2022-01-06 01:28:30,950 Epoch[221/310], Step[1250/1251], Loss: 4.1398(3.7332), Acc: 0.3750(0.3584)
2022-01-06 01:28:34,168 ----- Epoch[221/310], Train Loss: 3.7332, Train Acc: 0.3584, time: 1511.33, Best Val(epoch220) Acc@1: 0.7051
2022-01-06 01:28:34,347 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-06 01:28:34,347 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-06 01:28:34,459 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-06 01:28:34,460 Now training epoch 222. LR=0.000162
2022-01-06 01:29:50,494 Epoch[222/310], Step[0000/1251], Loss: 3.7350(3.7350), Acc: 0.5059(0.5059)
2022-01-06 01:30:47,384 Epoch[222/310], Step[0050/1251], Loss: 3.0421(3.7032), Acc: 0.6123(0.3778)
2022-01-06 01:31:44,098 Epoch[222/310], Step[0100/1251], Loss: 3.7080(3.6879), Acc: 0.4434(0.3849)
2022-01-06 01:32:40,267 Epoch[222/310], Step[0150/1251], Loss: 3.8461(3.6760), Acc: 0.2402(0.3766)
2022-01-06 01:33:36,726 Epoch[222/310], Step[0200/1251], Loss: 3.7498(3.7064), Acc: 0.4170(0.3671)
2022-01-06 01:34:33,711 Epoch[222/310], Step[0250/1251], Loss: 3.3161(3.6919), Acc: 0.2471(0.3669)
2022-01-06 01:35:30,595 Epoch[222/310], Step[0300/1251], Loss: 4.0923(3.6978), Acc: 0.2930(0.3671)
2022-01-06 01:36:27,792 Epoch[222/310], Step[0350/1251], Loss: 3.5960(3.7028), Acc: 0.3818(0.3677)
2022-01-06 01:37:24,457 Epoch[222/310], Step[0400/1251], Loss: 3.5434(3.7086), Acc: 0.2139(0.3665)
2022-01-06 01:38:20,655 Epoch[222/310], Step[0450/1251], Loss: 3.7790(3.7218), Acc: 0.4570(0.3651)
2022-01-06 01:39:16,915 Epoch[222/310], Step[0500/1251], Loss: 3.4801(3.7304), Acc: 0.5645(0.3635)
2022-01-06 01:40:13,434 Epoch[222/310], Step[0550/1251], Loss: 3.4715(3.7326), Acc: 0.3242(0.3646)
2022-01-06 01:41:09,945 Epoch[222/310], Step[0600/1251], Loss: 3.3196(3.7383), Acc: 0.4365(0.3667)
2022-01-06 01:42:07,343 Epoch[222/310], Step[0650/1251], Loss: 3.7261(3.7399), Acc: 0.3213(0.3638)
2022-01-06 01:43:04,968 Epoch[222/310], Step[0700/1251], Loss: 3.3670(3.7394), Acc: 0.3730(0.3636)
2022-01-06 01:44:01,592 Epoch[222/310], Step[0750/1251], Loss: 3.4829(3.7376), Acc: 0.3701(0.3634)
2022-01-06 01:44:58,726 Epoch[222/310], Step[0800/1251], Loss: 3.6404(3.7353), Acc: 0.3164(0.3631)
2022-01-06 01:45:55,768 Epoch[222/310], Step[0850/1251], Loss: 4.1354(3.7416), Acc: 0.2314(0.3618)
2022-01-06 01:46:52,484 Epoch[222/310], Step[0900/1251], Loss: 3.3730(3.7426), Acc: 0.4521(0.3621)
2022-01-06 01:47:50,141 Epoch[222/310], Step[0950/1251], Loss: 3.4987(3.7384), Acc: 0.4971(0.3627)
2022-01-06 01:48:47,523 Epoch[222/310], Step[1000/1251], Loss: 3.6545(3.7432), Acc: 0.4902(0.3614)
2022-01-06 01:49:44,498 Epoch[222/310], Step[1050/1251], Loss: 3.7318(3.7381), Acc: 0.1738(0.3617)
2022-01-06 01:50:42,008 Epoch[222/310], Step[1100/1251], Loss: 4.2962(3.7399), Acc: 0.1943(0.3603)
2022-01-06 01:51:38,505 Epoch[222/310], Step[1150/1251], Loss: 3.6661(3.7383), Acc: 0.1738(0.3601)
2022-01-06 01:52:35,135 Epoch[222/310], Step[1200/1251], Loss: 3.3537(3.7365), Acc: 0.5400(0.3608)
2022-01-06 01:53:30,308 Epoch[222/310], Step[1250/1251], Loss: 3.9612(3.7364), Acc: 0.3936(0.3603)
2022-01-06 01:53:32,414 ----- Validation after Epoch: 222
2022-01-06 01:54:29,758 Val Step[0000/1563], Loss: 0.7337 (0.7337), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 01:54:31,288 Val Step[0050/1563], Loss: 2.0865 (0.7618), Acc@1: 0.4375 (0.8419), Acc@5: 0.8438 (0.9589)
2022-01-06 01:54:32,677 Val Step[0100/1563], Loss: 2.0704 (1.0546), Acc@1: 0.5000 (0.7577), Acc@5: 0.8125 (0.9304)
2022-01-06 01:54:34,161 Val Step[0150/1563], Loss: 0.4094 (0.9934), Acc@1: 0.9375 (0.7724), Acc@5: 1.0000 (0.9358)
2022-01-06 01:54:35,631 Val Step[0200/1563], Loss: 0.9625 (0.9983), Acc@1: 0.7812 (0.7750), Acc@5: 0.9375 (0.9347)
2022-01-06 01:54:37,043 Val Step[0250/1563], Loss: 0.6199 (0.9492), Acc@1: 0.9062 (0.7870), Acc@5: 1.0000 (0.9406)
2022-01-06 01:54:38,451 Val Step[0300/1563], Loss: 1.2992 (1.0066), Acc@1: 0.6562 (0.7725), Acc@5: 0.8750 (0.9353)
2022-01-06 01:54:39,854 Val Step[0350/1563], Loss: 1.1178 (1.0150), Acc@1: 0.7188 (0.7678), Acc@5: 0.9375 (0.9366)
2022-01-06 01:54:41,295 Val Step[0400/1563], Loss: 1.0390 (1.0210), Acc@1: 0.7812 (0.7622), Acc@5: 0.9688 (0.9366)
2022-01-06 01:54:42,723 Val Step[0450/1563], Loss: 0.9848 (1.0282), Acc@1: 0.7500 (0.7592), Acc@5: 1.0000 (0.9372)
2022-01-06 01:54:44,207 Val Step[0500/1563], Loss: 0.4793 (1.0186), Acc@1: 0.8438 (0.7624), Acc@5: 1.0000 (0.9389)
2022-01-06 01:54:45,711 Val Step[0550/1563], Loss: 0.8795 (0.9964), Acc@1: 0.7188 (0.7691), Acc@5: 1.0000 (0.9413)
2022-01-06 01:54:47,283 Val Step[0600/1563], Loss: 0.9932 (1.0022), Acc@1: 0.8438 (0.7687), Acc@5: 0.8750 (0.9406)
2022-01-06 01:54:48,801 Val Step[0650/1563], Loss: 0.6519 (1.0235), Acc@1: 0.8750 (0.7641), Acc@5: 1.0000 (0.9371)
2022-01-06 01:54:50,215 Val Step[0700/1563], Loss: 1.0065 (1.0536), Acc@1: 0.8750 (0.7568), Acc@5: 0.9688 (0.9333)
2022-01-06 01:54:51,762 Val Step[0750/1563], Loss: 1.4956 (1.0883), Acc@1: 0.7500 (0.7500), Acc@5: 0.8750 (0.9286)
2022-01-06 01:54:53,241 Val Step[0800/1563], Loss: 0.8353 (1.1295), Acc@1: 0.7812 (0.7401), Acc@5: 1.0000 (0.9235)
2022-01-06 01:54:54,703 Val Step[0850/1563], Loss: 1.2719 (1.1566), Acc@1: 0.6562 (0.7338), Acc@5: 0.9375 (0.9197)
2022-01-06 01:54:56,129 Val Step[0900/1563], Loss: 0.3631 (1.1565), Acc@1: 0.9375 (0.7356), Acc@5: 1.0000 (0.9193)
2022-01-06 01:54:57,599 Val Step[0950/1563], Loss: 1.6014 (1.1782), Acc@1: 0.5625 (0.7317), Acc@5: 0.9062 (0.9161)
2022-01-06 01:54:59,098 Val Step[1000/1563], Loss: 0.5530 (1.2034), Acc@1: 0.9375 (0.7258), Acc@5: 1.0000 (0.9126)
2022-01-06 01:55:00,597 Val Step[1050/1563], Loss: 0.3812 (1.2155), Acc@1: 0.9375 (0.7230), Acc@5: 0.9688 (0.9111)
2022-01-06 01:55:02,122 Val Step[1100/1563], Loss: 0.9519 (1.2304), Acc@1: 0.7500 (0.7199), Acc@5: 0.9375 (0.9090)
2022-01-06 01:55:03,585 Val Step[1150/1563], Loss: 1.3267 (1.2459), Acc@1: 0.7812 (0.7170), Acc@5: 0.8125 (0.9067)
2022-01-06 01:55:05,031 Val Step[1200/1563], Loss: 1.4091 (1.2632), Acc@1: 0.7188 (0.7133), Acc@5: 0.8438 (0.9044)
2022-01-06 01:55:06,647 Val Step[1250/1563], Loss: 0.7565 (1.2757), Acc@1: 0.8750 (0.7113), Acc@5: 0.9062 (0.9023)
2022-01-06 01:55:08,098 Val Step[1300/1563], Loss: 0.9414 (1.2847), Acc@1: 0.8125 (0.7095), Acc@5: 0.9062 (0.9013)
2022-01-06 01:55:09,519 Val Step[1350/1563], Loss: 1.6717 (1.3010), Acc@1: 0.5312 (0.7055), Acc@5: 0.8438 (0.8991)
2022-01-06 01:55:10,984 Val Step[1400/1563], Loss: 1.1054 (1.3085), Acc@1: 0.7188 (0.7038), Acc@5: 0.9375 (0.8980)
2022-01-06 01:55:12,369 Val Step[1450/1563], Loss: 1.1171 (1.3155), Acc@1: 0.8438 (0.7021), Acc@5: 0.9375 (0.8974)
2022-01-06 01:55:13,841 Val Step[1500/1563], Loss: 1.7542 (1.3045), Acc@1: 0.5938 (0.7047), Acc@5: 0.8438 (0.8990)
2022-01-06 01:55:15,312 Val Step[1550/1563], Loss: 0.9175 (1.3061), Acc@1: 0.8750 (0.7040), Acc@5: 0.9062 (0.8987)
2022-01-06 01:55:16,219 ----- Epoch[222/310], Validation Loss: 1.3042, Validation Acc@1: 0.7044, Validation Acc@5: 0.8989, time: 103.80
2022-01-06 01:55:16,219 ----- Epoch[222/310], Train Loss: 3.7364, Train Acc: 0.3603, time: 1497.95, Best Val(epoch220) Acc@1: 0.7051
2022-01-06 01:55:16,402 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-06 01:55:16,402 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-06 01:55:16,509 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-06 01:55:16,509 Now training epoch 223. LR=0.000158
2022-01-06 01:56:32,464 Epoch[223/310], Step[0000/1251], Loss: 3.6196(3.6196), Acc: 0.4092(0.4092)
2022-01-06 01:57:30,220 Epoch[223/310], Step[0050/1251], Loss: 3.5626(3.6757), Acc: 0.3799(0.3749)
2022-01-06 01:58:27,563 Epoch[223/310], Step[0100/1251], Loss: 3.9085(3.7180), Acc: 0.3848(0.3668)
2022-01-06 01:59:23,869 Epoch[223/310], Step[0150/1251], Loss: 3.8193(3.7143), Acc: 0.4170(0.3682)
2022-01-06 02:00:19,180 Epoch[223/310], Step[0200/1251], Loss: 4.1825(3.7264), Acc: 0.2070(0.3626)
2022-01-06 02:01:15,482 Epoch[223/310], Step[0250/1251], Loss: 3.8598(3.7161), Acc: 0.4766(0.3699)
2022-01-06 02:02:11,386 Epoch[223/310], Step[0300/1251], Loss: 3.4942(3.7114), Acc: 0.5352(0.3751)
2022-01-06 02:03:06,769 Epoch[223/310], Step[0350/1251], Loss: 3.9234(3.7044), Acc: 0.1875(0.3756)
2022-01-06 02:04:04,375 Epoch[223/310], Step[0400/1251], Loss: 3.5799(3.7019), Acc: 0.3320(0.3709)
2022-01-06 02:05:01,069 Epoch[223/310], Step[0450/1251], Loss: 3.5162(3.7007), Acc: 0.5166(0.3682)
2022-01-06 02:05:57,910 Epoch[223/310], Step[0500/1251], Loss: 3.9693(3.7084), Acc: 0.4873(0.3671)
2022-01-06 02:06:54,598 Epoch[223/310], Step[0550/1251], Loss: 3.8698(3.7088), Acc: 0.4629(0.3673)
2022-01-06 02:07:51,359 Epoch[223/310], Step[0600/1251], Loss: 3.8265(3.7106), Acc: 0.3223(0.3682)
2022-01-06 02:08:49,488 Epoch[223/310], Step[0650/1251], Loss: 3.1928(3.7075), Acc: 0.4590(0.3685)
2022-01-06 02:09:46,738 Epoch[223/310], Step[0700/1251], Loss: 3.3791(3.7097), Acc: 0.4453(0.3672)
2022-01-06 02:10:43,803 Epoch[223/310], Step[0750/1251], Loss: 3.5167(3.7121), Acc: 0.4111(0.3674)
2022-01-06 02:11:41,322 Epoch[223/310], Step[0800/1251], Loss: 3.3193(3.7171), Acc: 0.5322(0.3659)
2022-01-06 02:12:38,089 Epoch[223/310], Step[0850/1251], Loss: 3.4846(3.7161), Acc: 0.5654(0.3645)
2022-01-06 02:13:35,364 Epoch[223/310], Step[0900/1251], Loss: 3.1366(3.7145), Acc: 0.4521(0.3641)
2022-01-06 02:14:32,698 Epoch[223/310], Step[0950/1251], Loss: 3.5178(3.7156), Acc: 0.5537(0.3643)
2022-01-06 02:15:27,848 Epoch[223/310], Step[1000/1251], Loss: 3.8948(3.7154), Acc: 0.3643(0.3651)
2022-01-06 02:16:25,288 Epoch[223/310], Step[1050/1251], Loss: 3.9314(3.7109), Acc: 0.3955(0.3645)
2022-01-06 02:17:23,363 Epoch[223/310], Step[1100/1251], Loss: 3.8064(3.7154), Acc: 0.1982(0.3639)
2022-01-06 02:18:21,646 Epoch[223/310], Step[1150/1251], Loss: 3.4275(3.7143), Acc: 0.3896(0.3637)
2022-01-06 02:19:19,134 Epoch[223/310], Step[1200/1251], Loss: 4.0489(3.7153), Acc: 0.2041(0.3648)
2022-01-06 02:20:16,513 Epoch[223/310], Step[1250/1251], Loss: 3.8140(3.7209), Acc: 0.4971(0.3644)
2022-01-06 02:20:18,496 ----- Epoch[223/310], Train Loss: 3.7209, Train Acc: 0.3644, time: 1501.98, Best Val(epoch220) Acc@1: 0.7051
2022-01-06 02:20:18,680 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-06 02:20:18,681 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-06 02:20:18,778 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-06 02:20:18,778 Now training epoch 224. LR=0.000154
2022-01-06 02:21:34,319 Epoch[224/310], Step[0000/1251], Loss: 3.5467(3.5467), Acc: 0.2715(0.2715)
2022-01-06 02:22:31,292 Epoch[224/310], Step[0050/1251], Loss: 4.1779(3.6744), Acc: 0.3076(0.3648)
2022-01-06 02:23:28,032 Epoch[224/310], Step[0100/1251], Loss: 3.9937(3.7245), Acc: 0.3584(0.3610)
2022-01-06 02:24:24,446 Epoch[224/310], Step[0150/1251], Loss: 3.5697(3.7220), Acc: 0.4014(0.3593)
2022-01-06 02:25:21,133 Epoch[224/310], Step[0200/1251], Loss: 3.3650(3.7209), Acc: 0.5615(0.3601)
2022-01-06 02:26:18,622 Epoch[224/310], Step[0250/1251], Loss: 3.7296(3.7157), Acc: 0.4307(0.3566)
2022-01-06 02:27:14,158 Epoch[224/310], Step[0300/1251], Loss: 3.9835(3.7136), Acc: 0.1797(0.3590)
2022-01-06 02:28:10,289 Epoch[224/310], Step[0350/1251], Loss: 3.5259(3.7130), Acc: 0.4043(0.3597)
2022-01-06 02:29:07,571 Epoch[224/310], Step[0400/1251], Loss: 4.1691(3.7229), Acc: 0.4014(0.3577)
2022-01-06 02:30:04,466 Epoch[224/310], Step[0450/1251], Loss: 4.0084(3.7295), Acc: 0.4160(0.3589)
2022-01-06 02:31:02,454 Epoch[224/310], Step[0500/1251], Loss: 3.8317(3.7245), Acc: 0.4561(0.3567)
2022-01-06 02:32:00,045 Epoch[224/310], Step[0550/1251], Loss: 3.6814(3.7224), Acc: 0.1846(0.3577)
2022-01-06 02:32:57,857 Epoch[224/310], Step[0600/1251], Loss: 3.9168(3.7218), Acc: 0.3574(0.3568)
2022-01-06 02:33:52,939 Epoch[224/310], Step[0650/1251], Loss: 3.8666(3.7263), Acc: 0.2158(0.3555)
2022-01-06 02:34:49,681 Epoch[224/310], Step[0700/1251], Loss: 3.8342(3.7242), Acc: 0.3477(0.3562)
2022-01-06 02:35:46,530 Epoch[224/310], Step[0750/1251], Loss: 3.3427(3.7201), Acc: 0.4775(0.3578)
2022-01-06 02:36:43,646 Epoch[224/310], Step[0800/1251], Loss: 4.3229(3.7187), Acc: 0.1602(0.3569)
2022-01-06 02:37:40,216 Epoch[224/310], Step[0850/1251], Loss: 4.1219(3.7250), Acc: 0.2939(0.3558)
2022-01-06 02:38:38,061 Epoch[224/310], Step[0900/1251], Loss: 3.9394(3.7272), Acc: 0.2646(0.3566)
2022-01-06 02:39:36,580 Epoch[224/310], Step[0950/1251], Loss: 3.6955(3.7257), Acc: 0.4795(0.3566)
2022-01-06 02:40:33,895 Epoch[224/310], Step[1000/1251], Loss: 3.0030(3.7239), Acc: 0.2822(0.3574)
2022-01-06 02:41:31,104 Epoch[224/310], Step[1050/1251], Loss: 4.1795(3.7247), Acc: 0.3281(0.3584)
2022-01-06 02:42:28,386 Epoch[224/310], Step[1100/1251], Loss: 3.7505(3.7223), Acc: 0.4209(0.3588)
2022-01-06 02:43:25,751 Epoch[224/310], Step[1150/1251], Loss: 3.5866(3.7220), Acc: 0.3730(0.3575)
2022-01-06 02:44:22,192 Epoch[224/310], Step[1200/1251], Loss: 3.2874(3.7217), Acc: 0.5068(0.3591)
2022-01-06 02:45:18,201 Epoch[224/310], Step[1250/1251], Loss: 4.2170(3.7193), Acc: 0.3125(0.3596)
2022-01-06 02:45:20,401 ----- Validation after Epoch: 224
2022-01-06 02:46:15,595 Val Step[0000/1563], Loss: 0.7230 (0.7230), Acc@1: 0.9062 (0.9062), Acc@5: 0.9375 (0.9375)
2022-01-06 02:46:17,127 Val Step[0050/1563], Loss: 2.1633 (0.8141), Acc@1: 0.5000 (0.8388), Acc@5: 0.8125 (0.9510)
2022-01-06 02:46:18,580 Val Step[0100/1563], Loss: 1.8418 (1.0617), Acc@1: 0.5625 (0.7584), Acc@5: 0.8438 (0.9319)
2022-01-06 02:46:20,087 Val Step[0150/1563], Loss: 0.5376 (0.9982), Acc@1: 0.9062 (0.7759), Acc@5: 1.0000 (0.9363)
2022-01-06 02:46:21,646 Val Step[0200/1563], Loss: 1.1471 (1.0125), Acc@1: 0.7188 (0.7778), Acc@5: 0.9375 (0.9344)
2022-01-06 02:46:23,221 Val Step[0250/1563], Loss: 0.4409 (0.9602), Acc@1: 0.9375 (0.7920), Acc@5: 1.0000 (0.9401)
2022-01-06 02:46:24,663 Val Step[0300/1563], Loss: 1.0206 (1.0174), Acc@1: 0.7812 (0.7762), Acc@5: 0.9688 (0.9353)
2022-01-06 02:46:26,143 Val Step[0350/1563], Loss: 1.1537 (1.0275), Acc@1: 0.7188 (0.7716), Acc@5: 0.9062 (0.9370)
2022-01-06 02:46:27,595 Val Step[0400/1563], Loss: 1.1341 (1.0413), Acc@1: 0.7812 (0.7653), Acc@5: 0.9688 (0.9366)
2022-01-06 02:46:29,181 Val Step[0450/1563], Loss: 0.8684 (1.0475), Acc@1: 0.7188 (0.7633), Acc@5: 1.0000 (0.9376)
2022-01-06 02:46:30,749 Val Step[0500/1563], Loss: 0.5120 (1.0375), Acc@1: 0.8750 (0.7659), Acc@5: 1.0000 (0.9390)
2022-01-06 02:46:32,283 Val Step[0550/1563], Loss: 0.5821 (1.0150), Acc@1: 0.8750 (0.7717), Acc@5: 0.9688 (0.9411)
2022-01-06 02:46:33,864 Val Step[0600/1563], Loss: 1.0109 (1.0231), Acc@1: 0.7812 (0.7706), Acc@5: 0.9375 (0.9403)
2022-01-06 02:46:35,367 Val Step[0650/1563], Loss: 0.5320 (1.0448), Acc@1: 0.8750 (0.7660), Acc@5: 1.0000 (0.9372)
2022-01-06 02:46:36,871 Val Step[0700/1563], Loss: 0.9839 (1.0757), Acc@1: 0.7812 (0.7582), Acc@5: 0.9375 (0.9332)
2022-01-06 02:46:38,449 Val Step[0750/1563], Loss: 1.1540 (1.1062), Acc@1: 0.8125 (0.7527), Acc@5: 0.9062 (0.9290)
2022-01-06 02:46:39,908 Val Step[0800/1563], Loss: 0.9569 (1.1467), Acc@1: 0.7500 (0.7427), Acc@5: 1.0000 (0.9236)
2022-01-06 02:46:41,350 Val Step[0850/1563], Loss: 1.2463 (1.1723), Acc@1: 0.6562 (0.7372), Acc@5: 0.9375 (0.9202)
2022-01-06 02:46:42,848 Val Step[0900/1563], Loss: 0.3691 (1.1725), Acc@1: 0.9688 (0.7387), Acc@5: 1.0000 (0.9195)
2022-01-06 02:46:44,337 Val Step[0950/1563], Loss: 1.2577 (1.1916), Acc@1: 0.7188 (0.7351), Acc@5: 0.9062 (0.9169)
2022-01-06 02:46:45,749 Val Step[1000/1563], Loss: 0.5216 (1.2152), Acc@1: 0.9688 (0.7297), Acc@5: 1.0000 (0.9137)
2022-01-06 02:46:47,272 Val Step[1050/1563], Loss: 0.3713 (1.2297), Acc@1: 0.9688 (0.7269), Acc@5: 0.9688 (0.9120)
2022-01-06 02:46:48,804 Val Step[1100/1563], Loss: 0.9983 (1.2421), Acc@1: 0.7188 (0.7239), Acc@5: 0.9688 (0.9101)
2022-01-06 02:46:50,242 Val Step[1150/1563], Loss: 1.3595 (1.2588), Acc@1: 0.7812 (0.7208), Acc@5: 0.7812 (0.9079)
2022-01-06 02:46:51,612 Val Step[1200/1563], Loss: 1.4300 (1.2742), Acc@1: 0.7188 (0.7171), Acc@5: 0.8438 (0.9055)
2022-01-06 02:46:53,101 Val Step[1250/1563], Loss: 0.7930 (1.2865), Acc@1: 0.8750 (0.7146), Acc@5: 0.9062 (0.9036)
2022-01-06 02:46:54,499 Val Step[1300/1563], Loss: 1.1215 (1.2949), Acc@1: 0.8125 (0.7132), Acc@5: 0.9062 (0.9023)
2022-01-06 02:46:55,908 Val Step[1350/1563], Loss: 2.3370 (1.3125), Acc@1: 0.3750 (0.7092), Acc@5: 0.7188 (0.8996)
2022-01-06 02:46:57,413 Val Step[1400/1563], Loss: 0.8921 (1.3209), Acc@1: 0.8125 (0.7073), Acc@5: 0.9688 (0.8984)
2022-01-06 02:46:58,864 Val Step[1450/1563], Loss: 1.2897 (1.3263), Acc@1: 0.7188 (0.7057), Acc@5: 0.9062 (0.8981)
2022-01-06 02:47:00,312 Val Step[1500/1563], Loss: 1.8972 (1.3164), Acc@1: 0.5625 (0.7081), Acc@5: 0.8438 (0.8995)
2022-01-06 02:47:01,686 Val Step[1550/1563], Loss: 0.8955 (1.3167), Acc@1: 0.8750 (0.7076), Acc@5: 0.9062 (0.8995)
2022-01-06 02:47:02,525 ----- Epoch[224/310], Validation Loss: 1.3148, Validation Acc@1: 0.7079, Validation Acc@5: 0.8997, time: 102.12
2022-01-06 02:47:02,526 ----- Epoch[224/310], Train Loss: 3.7193, Train Acc: 0.3596, time: 1501.62, Best Val(epoch224) Acc@1: 0.7079
2022-01-06 02:47:02,744 Max accuracy so far: 0.7079 at epoch_224
2022-01-06 02:47:02,745 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdparams
2022-01-06 02:47:02,745 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT.pdopt
2022-01-06 02:47:02,836 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/Best_PiT-EMA.pdparams
2022-01-06 02:47:03,262 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-06 02:47:03,262 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-06 02:47:03,370 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-06 02:47:03,371 Now training epoch 225. LR=0.000151
2022-01-06 02:48:20,324 Epoch[225/310], Step[0000/1251], Loss: 3.4440(3.4440), Acc: 0.5254(0.5254)
2022-01-06 02:49:17,391 Epoch[225/310], Step[0050/1251], Loss: 3.8816(3.7402), Acc: 0.2832(0.3610)
2022-01-06 02:50:12,701 Epoch[225/310], Step[0100/1251], Loss: 3.6848(3.7150), Acc: 0.3652(0.3521)
2022-01-06 02:51:10,085 Epoch[225/310], Step[0150/1251], Loss: 3.8769(3.7317), Acc: 0.3242(0.3478)
2022-01-06 02:52:08,099 Epoch[225/310], Step[0200/1251], Loss: 3.2261(3.7361), Acc: 0.5918(0.3531)
2022-01-06 02:53:03,592 Epoch[225/310], Step[0250/1251], Loss: 3.5503(3.7234), Acc: 0.5439(0.3562)
2022-01-06 02:54:00,336 Epoch[225/310], Step[0300/1251], Loss: 3.9544(3.7287), Acc: 0.2295(0.3565)
2022-01-06 02:54:56,404 Epoch[225/310], Step[0350/1251], Loss: 4.3607(3.7330), Acc: 0.3652(0.3601)
2022-01-06 02:55:53,363 Epoch[225/310], Step[0400/1251], Loss: 3.3495(3.7371), Acc: 0.0420(0.3556)
2022-01-06 02:56:50,073 Epoch[225/310], Step[0450/1251], Loss: 3.9895(3.7407), Acc: 0.3154(0.3563)
2022-01-06 02:57:46,850 Epoch[225/310], Step[0500/1251], Loss: 3.5847(3.7428), Acc: 0.4785(0.3573)
2022-01-06 02:58:42,978 Epoch[225/310], Step[0550/1251], Loss: 3.4296(3.7448), Acc: 0.4033(0.3555)
2022-01-06 02:59:40,137 Epoch[225/310], Step[0600/1251], Loss: 3.9485(3.7399), Acc: 0.3682(0.3570)
2022-01-06 03:00:35,813 Epoch[225/310], Step[0650/1251], Loss: 4.1301(3.7395), Acc: 0.3594(0.3580)
2022-01-06 03:01:34,034 Epoch[225/310], Step[0700/1251], Loss: 4.1263(3.7391), Acc: 0.0879(0.3577)
2022-01-06 03:02:32,115 Epoch[225/310], Step[0750/1251], Loss: 3.9143(3.7372), Acc: 0.2959(0.3564)
2022-01-06 03:03:29,372 Epoch[225/310], Step[0800/1251], Loss: 3.3045(3.7378), Acc: 0.3926(0.3570)
2022-01-06 03:04:26,645 Epoch[225/310], Step[0850/1251], Loss: 4.1164(3.7388), Acc: 0.3896(0.3571)
2022-01-06 03:05:24,134 Epoch[225/310], Step[0900/1251], Loss: 4.0127(3.7355), Acc: 0.3916(0.3574)
2022-01-06 03:06:21,962 Epoch[225/310], Step[0950/1251], Loss: 3.3475(3.7379), Acc: 0.4229(0.3570)
2022-01-06 03:07:19,644 Epoch[225/310], Step[1000/1251], Loss: 3.2182(3.7391), Acc: 0.2188(0.3569)
2022-01-06 03:08:16,599 Epoch[225/310], Step[1050/1251], Loss: 3.4828(3.7382), Acc: 0.3389(0.3577)
2022-01-06 03:09:13,349 Epoch[225/310], Step[1100/1251], Loss: 3.4065(3.7385), Acc: 0.3330(0.3578)
2022-01-06 03:10:08,326 Epoch[225/310], Step[1150/1251], Loss: 3.0534(3.7367), Acc: 0.6055(0.3580)
2022-01-06 03:11:05,365 Epoch[225/310], Step[1200/1251], Loss: 4.1563(3.7359), Acc: 0.3301(0.3579)
2022-01-06 03:12:03,166 Epoch[225/310], Step[1250/1251], Loss: 3.8773(3.7332), Acc: 0.2363(0.3582)
2022-01-06 03:12:05,156 ----- Epoch[225/310], Train Loss: 3.7332, Train Acc: 0.3582, time: 1501.78, Best Val(epoch224) Acc@1: 0.7079
2022-01-06 03:12:05,367 ----- Save model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdparams
2022-01-06 03:12:05,367 ----- Save optim: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest.pdopt
2022-01-06 03:12:05,471 ----- Save ema model: /root/paddlejob/workspace/output/train-20220104-15-19-59/PiT-Latest-EMA.pdparams
2022-01-06 03:12:05,471 Now training epoch 226. LR=0.000147
