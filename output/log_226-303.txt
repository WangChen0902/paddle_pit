2022-01-06 10:33:42,007 
AMP: False
AUG:
  AUTO_AUGMENT: None
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: ./Light_ILSVRC2012
  IMAGENET_MEAN: [0.485, 0.456, 0.406]
  IMAGENET_STD: [0.229, 0.224, 0.225]
  IMAGE_SIZE: 224
  NUM_WORKERS: 8
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.0
  DISTILL: False
  DROPOUT: 0.0
  DROP_PATH: 0.1
  NAME: pit_ti
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: ./resume/PiT-Latest
  TRANS:
    BASE_DIMS: [32, 32, 32]
    DEPTH: [2, 6, 4]
    HEADS: [2, 4, 8]
    PATCH_SIZE: 16
    STRIDE: 8
  TYPE: PiT
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output/train-20220106-10-32-46
SAVE_FREQ: 10
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  COOLDOWN_EPOCHS: 10
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  DISTILLATION_ALPHA: 0.5
  DISTILLATION_TAU: 1.0
  DISTILLATION_TYPE: none
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 225
  LINEAR_SCALED_LR: None
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  MODEL_EMA: True
  MODEL_EMA_DECAY: 0.99996
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  TEACHER_MODEL: ./regnety_160
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
2022-01-06 10:33:42,007 ----- world_size = 4, local_rank = 0
2022-01-06 10:33:42,135 ----- Total # of train batch (single gpu): 1251
2022-01-06 10:33:42,136 ----- Total # of val batch (single gpu): 1563
2022-01-06 10:33:42,605 ----- Resume Training: Load model and optmizer from ./resume/PiT-Latest
2022-01-06 10:33:43,132 ----- Load model ema from ./resume/PiT-Latest-EMA.pdparams
2022-01-06 10:33:43,132 Start training from epoch 226.
2022-01-06 10:33:43,132 Now training epoch 226. LR=0.000147
2022-01-06 10:34:52,708 Epoch[226/310], Step[0000/1251], Loss: 3.7120(3.7120), Acc: 0.2529(0.2529)
2022-01-06 10:35:51,766 Epoch[226/310], Step[0050/1251], Loss: 3.5832(3.7700), Acc: 0.3936(0.3412)
2022-01-06 10:36:49,885 Epoch[226/310], Step[0100/1251], Loss: 3.8470(3.7630), Acc: 0.3076(0.3522)
2022-01-06 10:37:47,817 Epoch[226/310], Step[0150/1251], Loss: 3.7675(3.7483), Acc: 0.3770(0.3495)
2022-01-06 10:38:46,343 Epoch[226/310], Step[0200/1251], Loss: 3.3323(3.7232), Acc: 0.4277(0.3511)
2022-01-06 10:39:45,992 Epoch[226/310], Step[0250/1251], Loss: 3.5189(3.7084), Acc: 0.5566(0.3538)
2022-01-06 10:40:43,396 Epoch[226/310], Step[0300/1251], Loss: 3.6704(3.6927), Acc: 0.4023(0.3595)
2022-01-06 10:41:41,213 Epoch[226/310], Step[0350/1251], Loss: 4.1815(3.7115), Acc: 0.3252(0.3610)
2022-01-06 10:42:38,754 Epoch[226/310], Step[0400/1251], Loss: 3.9777(3.7105), Acc: 0.3545(0.3595)
2022-01-06 10:43:35,815 Epoch[226/310], Step[0450/1251], Loss: 3.9934(3.7174), Acc: 0.1768(0.3573)
2022-01-06 10:44:33,066 Epoch[226/310], Step[0500/1251], Loss: 3.2375(3.7159), Acc: 0.5146(0.3583)
2022-01-06 10:45:29,164 Epoch[226/310], Step[0550/1251], Loss: 3.6772(3.7090), Acc: 0.4619(0.3596)
2022-01-06 10:46:28,131 Epoch[226/310], Step[0600/1251], Loss: 3.5616(3.7122), Acc: 0.4951(0.3605)
2022-01-06 10:47:27,046 Epoch[226/310], Step[0650/1251], Loss: 4.5376(3.7154), Acc: 0.1621(0.3588)
2022-01-06 10:48:25,920 Epoch[226/310], Step[0700/1251], Loss: 4.1866(3.7148), Acc: 0.3730(0.3600)
2022-01-06 10:49:25,733 Epoch[226/310], Step[0750/1251], Loss: 3.5842(3.7156), Acc: 0.5186(0.3615)
2022-01-06 10:50:24,245 Epoch[226/310], Step[0800/1251], Loss: 4.0989(3.7166), Acc: 0.3271(0.3611)
2022-01-06 10:51:23,681 Epoch[226/310], Step[0850/1251], Loss: 4.1069(3.7120), Acc: 0.1670(0.3605)
2022-01-06 10:52:23,057 Epoch[226/310], Step[0900/1251], Loss: 4.1012(3.7118), Acc: 0.2910(0.3612)
2022-01-06 10:53:22,744 Epoch[226/310], Step[0950/1251], Loss: 3.6107(3.7105), Acc: 0.3467(0.3619)
2022-01-06 10:54:22,387 Epoch[226/310], Step[1000/1251], Loss: 3.6852(3.7110), Acc: 0.1514(0.3609)
2022-01-06 10:55:21,410 Epoch[226/310], Step[1050/1251], Loss: 3.4582(3.7116), Acc: 0.3213(0.3590)
2022-01-06 10:56:20,082 Epoch[226/310], Step[1100/1251], Loss: 3.9104(3.7119), Acc: 0.2969(0.3583)
2022-01-06 10:57:18,380 Epoch[226/310], Step[1150/1251], Loss: 4.0694(3.7131), Acc: 0.2480(0.3577)
2022-01-06 10:58:18,283 Epoch[226/310], Step[1200/1251], Loss: 4.1183(3.7142), Acc: 0.3740(0.3574)
2022-01-06 10:59:17,267 Epoch[226/310], Step[1250/1251], Loss: 3.6784(3.7148), Acc: 0.3369(0.3569)
2022-01-06 10:59:18,573 ----- Validation after Epoch: 226
2022-01-06 11:00:17,462 Val Step[0000/1563], Loss: 0.6577 (0.6577), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 11:00:18,837 Val Step[0050/1563], Loss: 2.3577 (0.7505), Acc@1: 0.4062 (0.8456), Acc@5: 0.8125 (0.9547)
2022-01-06 11:00:20,099 Val Step[0100/1563], Loss: 1.7187 (1.0355), Acc@1: 0.5938 (0.7636), Acc@5: 0.8125 (0.9319)
2022-01-06 11:00:21,451 Val Step[0150/1563], Loss: 0.3998 (0.9753), Acc@1: 0.9062 (0.7802), Acc@5: 1.0000 (0.9365)
2022-01-06 11:00:22,739 Val Step[0200/1563], Loss: 1.0635 (0.9799), Acc@1: 0.7188 (0.7833), Acc@5: 0.9062 (0.9363)
2022-01-06 11:00:24,025 Val Step[0250/1563], Loss: 0.5105 (0.9285), Acc@1: 0.9062 (0.7964), Acc@5: 1.0000 (0.9421)
2022-01-06 11:00:25,291 Val Step[0300/1563], Loss: 1.1031 (0.9829), Acc@1: 0.7188 (0.7802), Acc@5: 0.9062 (0.9373)
2022-01-06 11:00:26,558 Val Step[0350/1563], Loss: 1.0182 (0.9934), Acc@1: 0.7188 (0.7751), Acc@5: 0.9375 (0.9380)
2022-01-06 11:00:27,811 Val Step[0400/1563], Loss: 1.0726 (1.0020), Acc@1: 0.7500 (0.7687), Acc@5: 0.9688 (0.9385)
2022-01-06 11:00:29,082 Val Step[0450/1563], Loss: 1.0880 (1.0073), Acc@1: 0.6875 (0.7656), Acc@5: 1.0000 (0.9393)
2022-01-06 11:00:30,494 Val Step[0500/1563], Loss: 0.4871 (0.9974), Acc@1: 0.8750 (0.7695), Acc@5: 1.0000 (0.9406)
2022-01-06 11:00:31,894 Val Step[0550/1563], Loss: 0.6654 (0.9758), Acc@1: 0.8125 (0.7747), Acc@5: 0.9688 (0.9427)
2022-01-06 11:00:33,223 Val Step[0600/1563], Loss: 0.9438 (0.9837), Acc@1: 0.8125 (0.7740), Acc@5: 0.9375 (0.9420)
2022-01-06 11:00:34,535 Val Step[0650/1563], Loss: 0.6221 (1.0051), Acc@1: 0.8750 (0.7692), Acc@5: 1.0000 (0.9394)
2022-01-06 11:00:35,839 Val Step[0700/1563], Loss: 1.0480 (1.0345), Acc@1: 0.8438 (0.7625), Acc@5: 0.9062 (0.9359)
2022-01-06 11:00:37,220 Val Step[0750/1563], Loss: 1.2481 (1.0690), Acc@1: 0.7188 (0.7559), Acc@5: 0.8750 (0.9314)
2022-01-06 11:00:38,512 Val Step[0800/1563], Loss: 0.8993 (1.1078), Acc@1: 0.7188 (0.7462), Acc@5: 1.0000 (0.9267)
2022-01-06 11:00:39,773 Val Step[0850/1563], Loss: 1.1907 (1.1302), Acc@1: 0.6875 (0.7410), Acc@5: 0.9375 (0.9239)
2022-01-06 11:00:41,029 Val Step[0900/1563], Loss: 0.2572 (1.1304), Acc@1: 0.9688 (0.7426), Acc@5: 1.0000 (0.9231)
2022-01-06 11:00:42,381 Val Step[0950/1563], Loss: 1.4975 (1.1525), Acc@1: 0.6875 (0.7386), Acc@5: 0.8750 (0.9200)
2022-01-06 11:00:43,640 Val Step[1000/1563], Loss: 0.5434 (1.1761), Acc@1: 0.9375 (0.7327), Acc@5: 1.0000 (0.9165)
2022-01-06 11:00:45,038 Val Step[1050/1563], Loss: 0.3707 (1.1920), Acc@1: 0.9688 (0.7290), Acc@5: 0.9688 (0.9147)
2022-01-06 11:00:46,462 Val Step[1100/1563], Loss: 0.9924 (1.2086), Acc@1: 0.7500 (0.7251), Acc@5: 0.9375 (0.9124)
2022-01-06 11:00:47,911 Val Step[1150/1563], Loss: 1.3556 (1.2234), Acc@1: 0.7812 (0.7227), Acc@5: 0.8125 (0.9102)
2022-01-06 11:00:49,360 Val Step[1200/1563], Loss: 1.2810 (1.2386), Acc@1: 0.7188 (0.7193), Acc@5: 0.8438 (0.9080)
2022-01-06 11:00:50,766 Val Step[1250/1563], Loss: 0.6883 (1.2522), Acc@1: 0.8750 (0.7169), Acc@5: 0.9375 (0.9057)
2022-01-06 11:00:52,085 Val Step[1300/1563], Loss: 0.8255 (1.2641), Acc@1: 0.8750 (0.7144), Acc@5: 0.9375 (0.9041)
2022-01-06 11:00:53,390 Val Step[1350/1563], Loss: 1.5736 (1.2817), Acc@1: 0.6250 (0.7104), Acc@5: 0.8750 (0.9016)
2022-01-06 11:00:54,688 Val Step[1400/1563], Loss: 1.1523 (1.2893), Acc@1: 0.6562 (0.7090), Acc@5: 0.9375 (0.9006)
2022-01-06 11:00:56,013 Val Step[1450/1563], Loss: 1.3455 (1.2943), Acc@1: 0.7188 (0.7075), Acc@5: 0.9375 (0.9004)
2022-01-06 11:00:57,251 Val Step[1500/1563], Loss: 1.8010 (1.2836), Acc@1: 0.5625 (0.7100), Acc@5: 0.8125 (0.9017)
2022-01-06 11:00:58,492 Val Step[1550/1563], Loss: 1.0883 (1.2846), Acc@1: 0.8750 (0.7098), Acc@5: 0.9062 (0.9015)
2022-01-06 11:00:59,242 ----- Epoch[226/310], Validation Loss: 1.2834, Validation Acc@1: 0.7099, Validation Acc@5: 0.9017, time: 100.67
2022-01-06 11:00:59,242 ----- Epoch[226/310], Train Loss: 3.7148, Train Acc: 0.3569, time: 1535.44, Best Val(epoch226) Acc@1: 0.7099
2022-01-06 11:00:59,399 Max accuracy so far: 0.7099 at epoch_226
2022-01-06 11:00:59,399 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-06 11:00:59,400 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-06 11:00:59,440 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-06 11:00:59,563 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 11:00:59,564 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 11:00:59,609 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 11:00:59,609 Now training epoch 227. LR=0.000143
2022-01-06 11:02:13,898 Epoch[227/310], Step[0000/1251], Loss: 3.5676(3.5676), Acc: 0.1650(0.1650)
2022-01-06 11:03:09,946 Epoch[227/310], Step[0050/1251], Loss: 3.1616(3.7744), Acc: 0.2852(0.3523)
2022-01-06 11:04:04,543 Epoch[227/310], Step[0100/1251], Loss: 3.8986(3.7298), Acc: 0.0840(0.3495)
2022-01-06 11:05:00,099 Epoch[227/310], Step[0150/1251], Loss: 3.5200(3.7412), Acc: 0.3838(0.3537)
2022-01-06 11:05:56,854 Epoch[227/310], Step[0200/1251], Loss: 3.8840(3.7375), Acc: 0.3613(0.3566)
2022-01-06 11:06:54,154 Epoch[227/310], Step[0250/1251], Loss: 4.0382(3.7282), Acc: 0.2939(0.3597)
2022-01-06 11:07:49,389 Epoch[227/310], Step[0300/1251], Loss: 3.5164(3.7290), Acc: 0.2061(0.3625)
2022-01-06 11:08:45,414 Epoch[227/310], Step[0350/1251], Loss: 4.2310(3.7324), Acc: 0.3164(0.3613)
2022-01-06 11:09:42,000 Epoch[227/310], Step[0400/1251], Loss: 3.3755(3.7279), Acc: 0.4238(0.3609)
2022-01-06 11:10:39,464 Epoch[227/310], Step[0450/1251], Loss: 3.2835(3.7270), Acc: 0.5146(0.3599)
2022-01-06 11:11:36,758 Epoch[227/310], Step[0500/1251], Loss: 3.7545(3.7235), Acc: 0.1758(0.3582)
2022-01-06 11:12:33,205 Epoch[227/310], Step[0550/1251], Loss: 3.4543(3.7243), Acc: 0.3066(0.3586)
2022-01-06 11:13:29,429 Epoch[227/310], Step[0600/1251], Loss: 3.8823(3.7184), Acc: 0.1797(0.3613)
2022-01-06 11:14:27,501 Epoch[227/310], Step[0650/1251], Loss: 3.8237(3.7121), Acc: 0.3203(0.3595)
2022-01-06 11:15:23,463 Epoch[227/310], Step[0700/1251], Loss: 3.4672(3.7024), Acc: 0.5430(0.3610)
2022-01-06 11:16:20,841 Epoch[227/310], Step[0750/1251], Loss: 3.8444(3.7011), Acc: 0.2900(0.3621)
2022-01-06 11:17:17,572 Epoch[227/310], Step[0800/1251], Loss: 3.7461(3.7040), Acc: 0.4092(0.3605)
2022-01-06 11:18:15,518 Epoch[227/310], Step[0850/1251], Loss: 3.8935(3.7048), Acc: 0.3984(0.3590)
2022-01-06 11:19:12,927 Epoch[227/310], Step[0900/1251], Loss: 3.5360(3.7080), Acc: 0.3369(0.3590)
2022-01-06 11:20:09,461 Epoch[227/310], Step[0950/1251], Loss: 3.5067(3.7110), Acc: 0.5039(0.3588)
2022-01-06 11:21:07,120 Epoch[227/310], Step[1000/1251], Loss: 4.2266(3.7124), Acc: 0.2266(0.3579)
2022-01-06 11:22:05,859 Epoch[227/310], Step[1050/1251], Loss: 3.6943(3.7111), Acc: 0.1982(0.3566)
2022-01-06 11:23:03,775 Epoch[227/310], Step[1100/1251], Loss: 3.8434(3.7140), Acc: 0.1885(0.3568)
2022-01-06 11:24:00,967 Epoch[227/310], Step[1150/1251], Loss: 3.7858(3.7140), Acc: 0.4111(0.3564)
2022-01-06 11:24:58,000 Epoch[227/310], Step[1200/1251], Loss: 3.8426(3.7146), Acc: 0.4189(0.3576)
2022-01-06 11:25:54,588 Epoch[227/310], Step[1250/1251], Loss: 3.0886(3.7155), Acc: 0.5674(0.3584)
2022-01-06 11:25:56,869 ----- Epoch[227/310], Train Loss: 3.7155, Train Acc: 0.3584, time: 1497.26, Best Val(epoch226) Acc@1: 0.7099
2022-01-06 11:25:57,042 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 11:25:57,042 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 11:25:57,148 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 11:25:57,149 Now training epoch 228. LR=0.000140
2022-01-06 11:27:10,924 Epoch[228/310], Step[0000/1251], Loss: 3.5730(3.5730), Acc: 0.3994(0.3994)
2022-01-06 11:28:07,690 Epoch[228/310], Step[0050/1251], Loss: 3.4775(3.6575), Acc: 0.3936(0.3854)
2022-01-06 11:29:05,115 Epoch[228/310], Step[0100/1251], Loss: 3.4399(3.6743), Acc: 0.4297(0.3834)
2022-01-06 11:30:02,702 Epoch[228/310], Step[0150/1251], Loss: 3.8785(3.6798), Acc: 0.4580(0.3700)
2022-01-06 11:31:01,249 Epoch[228/310], Step[0200/1251], Loss: 3.5007(3.6936), Acc: 0.4658(0.3719)
2022-01-06 11:31:59,680 Epoch[228/310], Step[0250/1251], Loss: 3.3282(3.6947), Acc: 0.5703(0.3725)
2022-01-06 11:32:59,000 Epoch[228/310], Step[0300/1251], Loss: 3.6568(3.6911), Acc: 0.4443(0.3708)
2022-01-06 11:33:57,282 Epoch[228/310], Step[0350/1251], Loss: 4.2089(3.6950), Acc: 0.3955(0.3724)
2022-01-06 11:34:53,817 Epoch[228/310], Step[0400/1251], Loss: 3.7251(3.6960), Acc: 0.4717(0.3718)
2022-01-06 11:35:52,677 Epoch[228/310], Step[0450/1251], Loss: 3.8382(3.6939), Acc: 0.4023(0.3688)
2022-01-06 11:36:54,801 Epoch[228/310], Step[0500/1251], Loss: 3.8882(3.6934), Acc: 0.1826(0.3664)
2022-01-06 11:37:55,244 Epoch[228/310], Step[0550/1251], Loss: 3.1775(3.6994), Acc: 0.4395(0.3652)
2022-01-06 11:38:55,710 Epoch[228/310], Step[0600/1251], Loss: 3.6947(3.7003), Acc: 0.5088(0.3668)
2022-01-06 11:39:55,803 Epoch[228/310], Step[0650/1251], Loss: 4.3182(3.7005), Acc: 0.3574(0.3661)
2022-01-06 11:40:55,073 Epoch[228/310], Step[0700/1251], Loss: 3.8478(3.7033), Acc: 0.1445(0.3644)
2022-01-06 11:41:55,603 Epoch[228/310], Step[0750/1251], Loss: 3.6563(3.7081), Acc: 0.2002(0.3620)
2022-01-06 11:42:55,776 Epoch[228/310], Step[0800/1251], Loss: 3.8897(3.7110), Acc: 0.3584(0.3631)
2022-01-06 11:43:55,172 Epoch[228/310], Step[0850/1251], Loss: 3.9163(3.7121), Acc: 0.2764(0.3621)
2022-01-06 11:44:55,346 Epoch[228/310], Step[0900/1251], Loss: 3.7567(3.7124), Acc: 0.3164(0.3597)
2022-01-06 11:45:54,834 Epoch[228/310], Step[0950/1251], Loss: 3.5953(3.7095), Acc: 0.1465(0.3603)
2022-01-06 11:46:55,294 Epoch[228/310], Step[1000/1251], Loss: 4.0476(3.7096), Acc: 0.3721(0.3607)
2022-01-06 11:47:56,813 Epoch[228/310], Step[1050/1251], Loss: 3.7780(3.7084), Acc: 0.2510(0.3600)
2022-01-06 11:48:58,058 Epoch[228/310], Step[1100/1251], Loss: 3.6874(3.7085), Acc: 0.2588(0.3596)
2022-01-06 11:49:58,097 Epoch[228/310], Step[1150/1251], Loss: 4.0452(3.7133), Acc: 0.3721(0.3595)
2022-01-06 11:50:58,763 Epoch[228/310], Step[1200/1251], Loss: 3.5544(3.7099), Acc: 0.4326(0.3594)
2022-01-06 11:51:59,605 Epoch[228/310], Step[1250/1251], Loss: 3.8285(3.7134), Acc: 0.4014(0.3585)
2022-01-06 11:52:01,254 ----- Validation after Epoch: 228
2022-01-06 11:53:09,001 Val Step[0000/1563], Loss: 0.8044 (0.8044), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2022-01-06 11:53:10,478 Val Step[0050/1563], Loss: 2.1948 (0.7865), Acc@1: 0.4375 (0.8413), Acc@5: 0.8438 (0.9565)
2022-01-06 11:53:11,896 Val Step[0100/1563], Loss: 2.0540 (1.0420), Acc@1: 0.4062 (0.7723), Acc@5: 0.8438 (0.9316)
2022-01-06 11:53:13,302 Val Step[0150/1563], Loss: 0.4567 (0.9912), Acc@1: 0.8750 (0.7823), Acc@5: 1.0000 (0.9356)
2022-01-06 11:53:14,723 Val Step[0200/1563], Loss: 1.2145 (0.9996), Acc@1: 0.7500 (0.7845), Acc@5: 0.8750 (0.9347)
2022-01-06 11:53:16,149 Val Step[0250/1563], Loss: 0.5743 (0.9501), Acc@1: 0.9062 (0.7973), Acc@5: 1.0000 (0.9404)
2022-01-06 11:53:17,544 Val Step[0300/1563], Loss: 1.0788 (1.0044), Acc@1: 0.7188 (0.7819), Acc@5: 0.9062 (0.9359)
2022-01-06 11:53:18,906 Val Step[0350/1563], Loss: 1.1793 (1.0148), Acc@1: 0.7188 (0.7765), Acc@5: 0.9062 (0.9379)
2022-01-06 11:53:20,487 Val Step[0400/1563], Loss: 0.9133 (1.0227), Acc@1: 0.8438 (0.7708), Acc@5: 0.9688 (0.9384)
2022-01-06 11:53:21,933 Val Step[0450/1563], Loss: 1.1801 (1.0290), Acc@1: 0.6875 (0.7675), Acc@5: 1.0000 (0.9387)
2022-01-06 11:53:23,390 Val Step[0500/1563], Loss: 0.4805 (1.0244), Acc@1: 0.9062 (0.7696), Acc@5: 1.0000 (0.9391)
2022-01-06 11:53:24,781 Val Step[0550/1563], Loss: 0.7709 (1.0058), Acc@1: 0.7812 (0.7744), Acc@5: 0.9375 (0.9409)
2022-01-06 11:53:26,377 Val Step[0600/1563], Loss: 0.7896 (1.0137), Acc@1: 0.8438 (0.7743), Acc@5: 0.9375 (0.9400)
2022-01-06 11:53:27,877 Val Step[0650/1563], Loss: 0.7468 (1.0343), Acc@1: 0.8125 (0.7693), Acc@5: 1.0000 (0.9373)
2022-01-06 11:53:29,353 Val Step[0700/1563], Loss: 1.1041 (1.0637), Acc@1: 0.7500 (0.7619), Acc@5: 0.9375 (0.9341)
2022-01-06 11:53:30,783 Val Step[0750/1563], Loss: 1.3748 (1.0955), Acc@1: 0.7500 (0.7557), Acc@5: 0.9062 (0.9300)
2022-01-06 11:53:32,324 Val Step[0800/1563], Loss: 0.9012 (1.1337), Acc@1: 0.7188 (0.7467), Acc@5: 1.0000 (0.9249)
2022-01-06 11:53:33,756 Val Step[0850/1563], Loss: 1.2978 (1.1576), Acc@1: 0.6875 (0.7408), Acc@5: 0.9375 (0.9222)
2022-01-06 11:53:35,110 Val Step[0900/1563], Loss: 0.2390 (1.1575), Acc@1: 0.9688 (0.7426), Acc@5: 1.0000 (0.9215)
2022-01-06 11:53:36,537 Val Step[0950/1563], Loss: 1.4016 (1.1795), Acc@1: 0.7500 (0.7383), Acc@5: 0.9062 (0.9180)
2022-01-06 11:53:37,898 Val Step[1000/1563], Loss: 0.6692 (1.2034), Acc@1: 0.9062 (0.7322), Acc@5: 0.9688 (0.9145)
2022-01-06 11:53:39,274 Val Step[1050/1563], Loss: 0.4140 (1.2157), Acc@1: 0.9375 (0.7293), Acc@5: 0.9688 (0.9132)
2022-01-06 11:53:40,694 Val Step[1100/1563], Loss: 1.1042 (1.2323), Acc@1: 0.7500 (0.7255), Acc@5: 0.9688 (0.9108)
2022-01-06 11:53:42,028 Val Step[1150/1563], Loss: 1.2456 (1.2475), Acc@1: 0.7812 (0.7229), Acc@5: 0.8438 (0.9085)
2022-01-06 11:53:43,375 Val Step[1200/1563], Loss: 1.2914 (1.2633), Acc@1: 0.7500 (0.7193), Acc@5: 0.8438 (0.9060)
2022-01-06 11:53:44,682 Val Step[1250/1563], Loss: 0.7775 (1.2776), Acc@1: 0.8750 (0.7168), Acc@5: 0.9375 (0.9037)
2022-01-06 11:53:46,158 Val Step[1300/1563], Loss: 0.9712 (1.2865), Acc@1: 0.8125 (0.7153), Acc@5: 0.9062 (0.9025)
2022-01-06 11:53:47,704 Val Step[1350/1563], Loss: 2.1013 (1.3055), Acc@1: 0.3750 (0.7107), Acc@5: 0.8125 (0.8996)
2022-01-06 11:53:49,221 Val Step[1400/1563], Loss: 1.0420 (1.3130), Acc@1: 0.7188 (0.7089), Acc@5: 0.9375 (0.8988)
2022-01-06 11:53:50,672 Val Step[1450/1563], Loss: 1.3446 (1.3200), Acc@1: 0.6875 (0.7065), Acc@5: 0.9375 (0.8984)
2022-01-06 11:53:52,054 Val Step[1500/1563], Loss: 1.5589 (1.3093), Acc@1: 0.6250 (0.7089), Acc@5: 0.8438 (0.8998)
2022-01-06 11:53:53,449 Val Step[1550/1563], Loss: 0.9312 (1.3098), Acc@1: 0.8750 (0.7086), Acc@5: 0.9062 (0.8999)
2022-01-06 11:53:54,242 ----- Epoch[228/310], Validation Loss: 1.3078, Validation Acc@1: 0.7090, Validation Acc@5: 0.9002, time: 112.99
2022-01-06 11:53:54,242 ----- Epoch[228/310], Train Loss: 3.7134, Train Acc: 0.3585, time: 1564.10, Best Val(epoch226) Acc@1: 0.7099
2022-01-06 11:53:54,431 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 11:53:54,431 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 11:53:54,538 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 11:53:54,538 Now training epoch 229. LR=0.000136
2022-01-06 11:55:18,165 Epoch[229/310], Step[0000/1251], Loss: 3.8263(3.8263), Acc: 0.0420(0.0420)
2022-01-06 11:56:16,380 Epoch[229/310], Step[0050/1251], Loss: 3.6725(3.7337), Acc: 0.4736(0.3622)
2022-01-06 11:57:14,612 Epoch[229/310], Step[0100/1251], Loss: 3.6421(3.7406), Acc: 0.3496(0.3640)
2022-01-06 11:58:13,822 Epoch[229/310], Step[0150/1251], Loss: 3.6956(3.7171), Acc: 0.5234(0.3723)
2022-01-06 11:59:13,469 Epoch[229/310], Step[0200/1251], Loss: 4.0839(3.7044), Acc: 0.2969(0.3681)
2022-01-06 12:00:12,204 Epoch[229/310], Step[0250/1251], Loss: 3.6829(3.7073), Acc: 0.2578(0.3602)
2022-01-06 12:01:12,299 Epoch[229/310], Step[0300/1251], Loss: 3.6639(3.7076), Acc: 0.1748(0.3641)
2022-01-06 12:02:11,890 Epoch[229/310], Step[0350/1251], Loss: 3.4943(3.6999), Acc: 0.2949(0.3660)
2022-01-06 12:03:10,814 Epoch[229/310], Step[0400/1251], Loss: 3.6454(3.6975), Acc: 0.2275(0.3666)
2022-01-06 12:04:10,576 Epoch[229/310], Step[0450/1251], Loss: 3.2767(3.6989), Acc: 0.6084(0.3653)
2022-01-06 12:05:09,549 Epoch[229/310], Step[0500/1251], Loss: 3.8724(3.7059), Acc: 0.3350(0.3644)
2022-01-06 12:06:08,637 Epoch[229/310], Step[0550/1251], Loss: 3.7745(3.7040), Acc: 0.3086(0.3655)
2022-01-06 12:07:09,485 Epoch[229/310], Step[0600/1251], Loss: 3.6170(3.7032), Acc: 0.3701(0.3661)
2022-01-06 12:08:09,610 Epoch[229/310], Step[0650/1251], Loss: 3.5906(3.7059), Acc: 0.3369(0.3648)
2022-01-06 12:09:12,417 Epoch[229/310], Step[0700/1251], Loss: 3.7482(3.7101), Acc: 0.4639(0.3634)
2022-01-06 12:10:14,125 Epoch[229/310], Step[0750/1251], Loss: 3.4202(3.7126), Acc: 0.3975(0.3622)
2022-01-06 12:11:15,169 Epoch[229/310], Step[0800/1251], Loss: 3.4292(3.7182), Acc: 0.4092(0.3599)
2022-01-06 12:12:15,061 Epoch[229/310], Step[0850/1251], Loss: 3.8944(3.7232), Acc: 0.4424(0.3582)
2022-01-06 12:13:15,828 Epoch[229/310], Step[0900/1251], Loss: 3.7588(3.7221), Acc: 0.2178(0.3593)
2022-01-06 12:14:16,796 Epoch[229/310], Step[0950/1251], Loss: 3.7127(3.7235), Acc: 0.2217(0.3587)
2022-01-06 12:15:16,467 Epoch[229/310], Step[1000/1251], Loss: 3.6172(3.7207), Acc: 0.4805(0.3587)
2022-01-06 12:16:15,799 Epoch[229/310], Step[1050/1251], Loss: 4.0889(3.7208), Acc: 0.3057(0.3599)
2022-01-06 12:17:15,126 Epoch[229/310], Step[1100/1251], Loss: 4.3034(3.7200), Acc: 0.2188(0.3601)
2022-01-06 12:18:13,648 Epoch[229/310], Step[1150/1251], Loss: 2.8677(3.7157), Acc: 0.6143(0.3612)
2022-01-06 12:19:14,051 Epoch[229/310], Step[1200/1251], Loss: 3.7288(3.7174), Acc: 0.4424(0.3615)
2022-01-06 12:20:14,399 Epoch[229/310], Step[1250/1251], Loss: 4.1727(3.7185), Acc: 0.4111(0.3609)
2022-01-06 12:20:16,635 ----- Epoch[229/310], Train Loss: 3.7185, Train Acc: 0.3609, time: 1582.09, Best Val(epoch226) Acc@1: 0.7099
2022-01-06 12:20:16,809 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 12:20:16,809 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 12:20:16,914 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 12:20:16,914 Now training epoch 230. LR=0.000133
2022-01-06 12:21:34,185 Epoch[230/310], Step[0000/1251], Loss: 3.6189(3.6189), Acc: 0.3867(0.3867)
2022-01-06 12:22:33,008 Epoch[230/310], Step[0050/1251], Loss: 4.0666(3.7181), Acc: 0.2842(0.3418)
2022-01-06 12:23:29,821 Epoch[230/310], Step[0100/1251], Loss: 4.0733(3.7094), Acc: 0.3516(0.3741)
2022-01-06 12:24:29,728 Epoch[230/310], Step[0150/1251], Loss: 3.5026(3.6744), Acc: 0.5635(0.3827)
2022-01-06 12:25:28,580 Epoch[230/310], Step[0200/1251], Loss: 3.8732(3.6819), Acc: 0.3535(0.3715)
2022-01-06 12:26:28,430 Epoch[230/310], Step[0250/1251], Loss: 4.0804(3.6979), Acc: 0.4238(0.3704)
2022-01-06 12:27:28,745 Epoch[230/310], Step[0300/1251], Loss: 3.8422(3.6981), Acc: 0.3135(0.3691)
2022-01-06 12:28:28,315 Epoch[230/310], Step[0350/1251], Loss: 3.8093(3.6972), Acc: 0.3369(0.3678)
2022-01-06 12:29:28,611 Epoch[230/310], Step[0400/1251], Loss: 3.8778(3.7007), Acc: 0.3662(0.3685)
2022-01-06 12:30:28,531 Epoch[230/310], Step[0450/1251], Loss: 3.2502(3.7058), Acc: 0.6025(0.3674)
2022-01-06 12:31:26,905 Epoch[230/310], Step[0500/1251], Loss: 3.9124(3.7009), Acc: 0.4883(0.3670)
2022-01-06 12:32:24,995 Epoch[230/310], Step[0550/1251], Loss: 3.5719(3.6979), Acc: 0.1816(0.3673)
2022-01-06 12:33:25,694 Epoch[230/310], Step[0600/1251], Loss: 3.2569(3.7007), Acc: 0.2236(0.3651)
2022-01-06 12:34:23,626 Epoch[230/310], Step[0650/1251], Loss: 3.9328(3.7023), Acc: 0.4639(0.3626)
2022-01-06 12:35:23,637 Epoch[230/310], Step[0700/1251], Loss: 3.2614(3.7058), Acc: 0.5674(0.3613)
2022-01-06 12:36:22,904 Epoch[230/310], Step[0750/1251], Loss: 3.9272(3.7017), Acc: 0.3369(0.3622)
2022-01-06 12:37:22,045 Epoch[230/310], Step[0800/1251], Loss: 3.7535(3.6980), Acc: 0.4893(0.3625)
2022-01-06 12:38:22,379 Epoch[230/310], Step[0850/1251], Loss: 4.1558(3.7005), Acc: 0.3203(0.3620)
2022-01-06 12:39:21,398 Epoch[230/310], Step[0900/1251], Loss: 3.4101(3.7030), Acc: 0.2402(0.3616)
2022-01-06 12:40:21,222 Epoch[230/310], Step[0950/1251], Loss: 2.9657(3.7044), Acc: 0.4561(0.3628)
2022-01-06 12:41:20,103 Epoch[230/310], Step[1000/1251], Loss: 4.1441(3.7067), Acc: 0.2988(0.3621)
2022-01-06 12:42:19,718 Epoch[230/310], Step[1050/1251], Loss: 3.8081(3.7039), Acc: 0.3271(0.3626)
2022-01-06 12:43:20,048 Epoch[230/310], Step[1100/1251], Loss: 3.4326(3.7044), Acc: 0.2158(0.3628)
2022-01-06 12:44:19,614 Epoch[230/310], Step[1150/1251], Loss: 3.9349(3.7037), Acc: 0.4004(0.3629)
2022-01-06 12:45:19,664 Epoch[230/310], Step[1200/1251], Loss: 3.5239(3.7033), Acc: 0.5459(0.3629)
2022-01-06 12:46:19,720 Epoch[230/310], Step[1250/1251], Loss: 4.0867(3.7054), Acc: 0.3848(0.3617)
2022-01-06 12:46:21,321 ----- Validation after Epoch: 230
2022-01-06 12:47:22,944 Val Step[0000/1563], Loss: 0.7862 (0.7862), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 12:47:24,222 Val Step[0050/1563], Loss: 2.1841 (0.8150), Acc@1: 0.4688 (0.8401), Acc@5: 0.8125 (0.9553)
2022-01-06 12:47:25,488 Val Step[0100/1563], Loss: 2.1561 (1.0739), Acc@1: 0.4688 (0.7729), Acc@5: 0.8438 (0.9341)
2022-01-06 12:47:26,767 Val Step[0150/1563], Loss: 0.5193 (1.0204), Acc@1: 0.9062 (0.7841), Acc@5: 1.0000 (0.9383)
2022-01-06 12:47:28,142 Val Step[0200/1563], Loss: 1.3682 (1.0441), Acc@1: 0.7500 (0.7839), Acc@5: 0.8750 (0.9341)
2022-01-06 12:47:29,449 Val Step[0250/1563], Loss: 0.5905 (0.9958), Acc@1: 0.8750 (0.7966), Acc@5: 1.0000 (0.9391)
2022-01-06 12:47:30,783 Val Step[0300/1563], Loss: 1.3962 (1.0569), Acc@1: 0.6875 (0.7783), Acc@5: 0.9062 (0.9342)
2022-01-06 12:47:32,113 Val Step[0350/1563], Loss: 1.0402 (1.0611), Acc@1: 0.7500 (0.7737), Acc@5: 0.9062 (0.9368)
2022-01-06 12:47:33,390 Val Step[0400/1563], Loss: 0.9008 (1.0701), Acc@1: 0.8125 (0.7675), Acc@5: 0.9688 (0.9365)
2022-01-06 12:47:34,658 Val Step[0450/1563], Loss: 0.9289 (1.0757), Acc@1: 0.8125 (0.7651), Acc@5: 1.0000 (0.9371)
2022-01-06 12:47:36,002 Val Step[0500/1563], Loss: 0.3993 (1.0652), Acc@1: 0.9688 (0.7682), Acc@5: 1.0000 (0.9390)
2022-01-06 12:47:37,348 Val Step[0550/1563], Loss: 1.1452 (1.0466), Acc@1: 0.7188 (0.7730), Acc@5: 0.9375 (0.9408)
2022-01-06 12:47:38,672 Val Step[0600/1563], Loss: 0.9505 (1.0535), Acc@1: 0.7812 (0.7728), Acc@5: 0.9375 (0.9399)
2022-01-06 12:47:39,996 Val Step[0650/1563], Loss: 0.7284 (1.0735), Acc@1: 0.8438 (0.7688), Acc@5: 1.0000 (0.9370)
2022-01-06 12:47:41,321 Val Step[0700/1563], Loss: 1.0550 (1.1006), Acc@1: 0.7812 (0.7621), Acc@5: 0.9375 (0.9338)
2022-01-06 12:47:42,662 Val Step[0750/1563], Loss: 1.4332 (1.1328), Acc@1: 0.7500 (0.7558), Acc@5: 0.9062 (0.9295)
2022-01-06 12:47:44,080 Val Step[0800/1563], Loss: 0.7620 (1.1696), Acc@1: 0.8438 (0.7463), Acc@5: 1.0000 (0.9247)
2022-01-06 12:47:45,331 Val Step[0850/1563], Loss: 1.3478 (1.1921), Acc@1: 0.6562 (0.7405), Acc@5: 0.9375 (0.9214)
2022-01-06 12:47:46,577 Val Step[0900/1563], Loss: 0.3243 (1.1927), Acc@1: 0.9688 (0.7421), Acc@5: 1.0000 (0.9208)
2022-01-06 12:47:48,020 Val Step[0950/1563], Loss: 1.2086 (1.2119), Acc@1: 0.8125 (0.7384), Acc@5: 0.9375 (0.9177)
2022-01-06 12:47:49,433 Val Step[1000/1563], Loss: 0.7300 (1.2338), Acc@1: 0.9062 (0.7325), Acc@5: 1.0000 (0.9145)
2022-01-06 12:47:50,739 Val Step[1050/1563], Loss: 0.3492 (1.2474), Acc@1: 0.9375 (0.7294), Acc@5: 1.0000 (0.9129)
2022-01-06 12:47:51,990 Val Step[1100/1563], Loss: 1.0282 (1.2629), Acc@1: 0.7500 (0.7263), Acc@5: 0.9062 (0.9103)
2022-01-06 12:47:53,272 Val Step[1150/1563], Loss: 1.3078 (1.2776), Acc@1: 0.7812 (0.7232), Acc@5: 0.8125 (0.9082)
2022-01-06 12:47:54,534 Val Step[1200/1563], Loss: 1.4831 (1.2911), Acc@1: 0.6562 (0.7199), Acc@5: 0.8438 (0.9061)
2022-01-06 12:47:55,791 Val Step[1250/1563], Loss: 0.8906 (1.3018), Acc@1: 0.8750 (0.7182), Acc@5: 0.9375 (0.9044)
2022-01-06 12:47:57,042 Val Step[1300/1563], Loss: 0.9679 (1.3103), Acc@1: 0.8438 (0.7165), Acc@5: 0.9062 (0.9032)
2022-01-06 12:47:58,302 Val Step[1350/1563], Loss: 1.7382 (1.3257), Acc@1: 0.5000 (0.7127), Acc@5: 0.8438 (0.9006)
2022-01-06 12:47:59,567 Val Step[1400/1563], Loss: 1.0835 (1.3314), Acc@1: 0.7500 (0.7113), Acc@5: 0.9688 (0.8999)
2022-01-06 12:48:00,851 Val Step[1450/1563], Loss: 1.5490 (1.3387), Acc@1: 0.6250 (0.7095), Acc@5: 0.9375 (0.8993)
2022-01-06 12:48:02,132 Val Step[1500/1563], Loss: 1.6516 (1.3285), Acc@1: 0.6250 (0.7119), Acc@5: 0.8750 (0.9007)
2022-01-06 12:48:03,452 Val Step[1550/1563], Loss: 0.9564 (1.3298), Acc@1: 0.8750 (0.7112), Acc@5: 0.9062 (0.9005)
2022-01-06 12:48:04,231 ----- Epoch[230/310], Validation Loss: 1.3277, Validation Acc@1: 0.7116, Validation Acc@5: 0.9008, time: 102.91
2022-01-06 12:48:04,231 ----- Epoch[230/310], Train Loss: 3.7054, Train Acc: 0.3617, time: 1564.40, Best Val(epoch230) Acc@1: 0.7116
2022-01-06 12:48:04,412 Max accuracy so far: 0.7116 at epoch_230
2022-01-06 12:48:04,413 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-06 12:48:04,413 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-06 12:48:04,513 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-06 12:48:04,649 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-230-Loss-3.7231072628621957.pdparams
2022-01-06 12:48:04,650 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-230-Loss-3.7231072628621957.pdopt
2022-01-06 12:48:04,691 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-230-Loss-3.7231072628621957-EMA.pdparams
2022-01-06 12:48:04,691 Now training epoch 231. LR=0.000129
2022-01-06 12:49:21,318 Epoch[231/310], Step[0000/1251], Loss: 3.8753(3.8753), Acc: 0.2598(0.2598)
2022-01-06 12:50:20,843 Epoch[231/310], Step[0050/1251], Loss: 4.0828(3.7778), Acc: 0.3086(0.3612)
2022-01-06 12:51:20,234 Epoch[231/310], Step[0100/1251], Loss: 2.9499(3.7535), Acc: 0.4482(0.3658)
2022-01-06 12:52:20,002 Epoch[231/310], Step[0150/1251], Loss: 4.1320(3.7752), Acc: 0.2559(0.3627)
2022-01-06 12:53:19,217 Epoch[231/310], Step[0200/1251], Loss: 3.4623(3.7572), Acc: 0.5107(0.3574)
2022-01-06 12:54:18,971 Epoch[231/310], Step[0250/1251], Loss: 3.8460(3.7437), Acc: 0.3066(0.3566)
2022-01-06 12:55:18,434 Epoch[231/310], Step[0300/1251], Loss: 3.5765(3.7478), Acc: 0.3721(0.3557)
2022-01-06 12:56:18,367 Epoch[231/310], Step[0350/1251], Loss: 3.3503(3.7406), Acc: 0.1484(0.3569)
2022-01-06 12:57:16,225 Epoch[231/310], Step[0400/1251], Loss: 3.9828(3.7367), Acc: 0.4189(0.3596)
2022-01-06 12:58:15,417 Epoch[231/310], Step[0450/1251], Loss: 3.3191(3.7230), Acc: 0.2422(0.3631)
2022-01-06 12:59:15,629 Epoch[231/310], Step[0500/1251], Loss: 4.1525(3.7211), Acc: 0.2168(0.3623)
2022-01-06 13:00:15,801 Epoch[231/310], Step[0550/1251], Loss: 3.7880(3.7218), Acc: 0.3887(0.3612)
2022-01-06 13:01:15,282 Epoch[231/310], Step[0600/1251], Loss: 3.6747(3.7131), Acc: 0.3496(0.3606)
2022-01-06 13:02:14,687 Epoch[231/310], Step[0650/1251], Loss: 3.3212(3.7127), Acc: 0.3213(0.3602)
2022-01-06 13:03:14,845 Epoch[231/310], Step[0700/1251], Loss: 3.6083(3.7142), Acc: 0.2920(0.3601)
2022-01-06 13:04:14,220 Epoch[231/310], Step[0750/1251], Loss: 3.7431(3.7095), Acc: 0.5010(0.3608)
2022-01-06 13:05:09,933 Epoch[231/310], Step[0800/1251], Loss: 4.1040(3.7088), Acc: 0.3613(0.3627)
2022-01-06 13:06:07,892 Epoch[231/310], Step[0850/1251], Loss: 3.8112(3.7104), Acc: 0.2764(0.3631)
2022-01-06 13:07:07,128 Epoch[231/310], Step[0900/1251], Loss: 3.5383(3.7099), Acc: 0.3125(0.3628)
2022-01-06 13:08:07,217 Epoch[231/310], Step[0950/1251], Loss: 3.4632(3.7080), Acc: 0.4648(0.3620)
2022-01-06 13:09:06,499 Epoch[231/310], Step[1000/1251], Loss: 3.7279(3.7074), Acc: 0.4639(0.3615)
2022-01-06 13:10:06,923 Epoch[231/310], Step[1050/1251], Loss: 3.5869(3.7073), Acc: 0.5654(0.3596)
2022-01-06 13:11:07,537 Epoch[231/310], Step[1100/1251], Loss: 3.8224(3.7057), Acc: 0.3564(0.3596)
2022-01-06 13:12:06,954 Epoch[231/310], Step[1150/1251], Loss: 3.7859(3.7033), Acc: 0.5137(0.3603)
2022-01-06 13:13:07,509 Epoch[231/310], Step[1200/1251], Loss: 4.3405(3.7049), Acc: 0.2754(0.3599)
2022-01-06 13:14:07,972 Epoch[231/310], Step[1250/1251], Loss: 3.6426(3.7071), Acc: 0.2764(0.3611)
2022-01-06 13:14:09,470 ----- Epoch[231/310], Train Loss: 3.7071, Train Acc: 0.3611, time: 1564.78, Best Val(epoch230) Acc@1: 0.7116
2022-01-06 13:14:09,651 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 13:14:09,651 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 13:14:09,757 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 13:14:09,757 Now training epoch 232. LR=0.000126
2022-01-06 13:15:30,070 Epoch[232/310], Step[0000/1251], Loss: 3.3341(3.3341), Acc: 0.3750(0.3750)
2022-01-06 13:16:27,522 Epoch[232/310], Step[0050/1251], Loss: 3.8818(3.7056), Acc: 0.2520(0.3735)
2022-01-06 13:17:24,999 Epoch[232/310], Step[0100/1251], Loss: 3.7841(3.6963), Acc: 0.4424(0.3632)
2022-01-06 13:18:22,853 Epoch[232/310], Step[0150/1251], Loss: 3.5994(3.6959), Acc: 0.5234(0.3564)
2022-01-06 13:19:21,478 Epoch[232/310], Step[0200/1251], Loss: 3.6671(3.6971), Acc: 0.3770(0.3625)
2022-01-06 13:20:19,713 Epoch[232/310], Step[0250/1251], Loss: 3.9614(3.6906), Acc: 0.2881(0.3656)
2022-01-06 13:21:19,093 Epoch[232/310], Step[0300/1251], Loss: 3.6809(3.6879), Acc: 0.3623(0.3630)
2022-01-06 13:22:17,625 Epoch[232/310], Step[0350/1251], Loss: 3.9362(3.6856), Acc: 0.2812(0.3655)
2022-01-06 13:23:15,413 Epoch[232/310], Step[0400/1251], Loss: 3.5709(3.6994), Acc: 0.2549(0.3632)
2022-01-06 13:24:14,204 Epoch[232/310], Step[0450/1251], Loss: 3.5057(3.6974), Acc: 0.2686(0.3634)
2022-01-06 13:25:13,451 Epoch[232/310], Step[0500/1251], Loss: 3.9059(3.6959), Acc: 0.3652(0.3646)
2022-01-06 13:26:11,833 Epoch[232/310], Step[0550/1251], Loss: 3.3903(3.6936), Acc: 0.3818(0.3643)
2022-01-06 13:27:10,049 Epoch[232/310], Step[0600/1251], Loss: 4.0679(3.6957), Acc: 0.2539(0.3651)
2022-01-06 13:28:09,708 Epoch[232/310], Step[0650/1251], Loss: 4.1953(3.6932), Acc: 0.3076(0.3662)
2022-01-06 13:29:08,689 Epoch[232/310], Step[0700/1251], Loss: 4.0919(3.6925), Acc: 0.1885(0.3662)
2022-01-06 13:30:08,820 Epoch[232/310], Step[0750/1251], Loss: 4.3925(3.6939), Acc: 0.3428(0.3662)
2022-01-06 13:31:06,799 Epoch[232/310], Step[0800/1251], Loss: 3.3108(3.6959), Acc: 0.2646(0.3660)
2022-01-06 13:32:05,649 Epoch[232/310], Step[0850/1251], Loss: 4.0361(3.6974), Acc: 0.3857(0.3664)
2022-01-06 13:33:03,804 Epoch[232/310], Step[0900/1251], Loss: 3.6141(3.6964), Acc: 0.2715(0.3660)
2022-01-06 13:34:02,365 Epoch[232/310], Step[0950/1251], Loss: 3.7057(3.6954), Acc: 0.5195(0.3652)
2022-01-06 13:34:58,866 Epoch[232/310], Step[1000/1251], Loss: 3.4816(3.6968), Acc: 0.3271(0.3660)
2022-01-06 13:35:57,195 Epoch[232/310], Step[1050/1251], Loss: 3.7329(3.6977), Acc: 0.3789(0.3661)
2022-01-06 13:36:54,992 Epoch[232/310], Step[1100/1251], Loss: 4.1850(3.6991), Acc: 0.1895(0.3660)
2022-01-06 13:37:54,297 Epoch[232/310], Step[1150/1251], Loss: 3.7483(3.7027), Acc: 0.4629(0.3653)
2022-01-06 13:38:52,728 Epoch[232/310], Step[1200/1251], Loss: 3.6918(3.7008), Acc: 0.3057(0.3656)
2022-01-06 13:39:51,931 Epoch[232/310], Step[1250/1251], Loss: 4.0107(3.7010), Acc: 0.3711(0.3662)
2022-01-06 13:39:53,429 ----- Validation after Epoch: 232
2022-01-06 13:40:53,827 Val Step[0000/1563], Loss: 0.7236 (0.7236), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-06 13:40:55,248 Val Step[0050/1563], Loss: 2.2616 (0.8010), Acc@1: 0.4062 (0.8456), Acc@5: 0.9062 (0.9571)
2022-01-06 13:40:56,529 Val Step[0100/1563], Loss: 2.0228 (1.0710), Acc@1: 0.5312 (0.7673), Acc@5: 0.8438 (0.9310)
2022-01-06 13:40:57,813 Val Step[0150/1563], Loss: 0.4904 (1.0178), Acc@1: 0.9062 (0.7792), Acc@5: 1.0000 (0.9365)
2022-01-06 13:40:59,116 Val Step[0200/1563], Loss: 1.0757 (1.0271), Acc@1: 0.7500 (0.7802), Acc@5: 0.9375 (0.9341)
2022-01-06 13:41:00,415 Val Step[0250/1563], Loss: 0.6238 (0.9766), Acc@1: 0.8750 (0.7912), Acc@5: 1.0000 (0.9400)
2022-01-06 13:41:01,712 Val Step[0300/1563], Loss: 1.1493 (1.0365), Acc@1: 0.6562 (0.7733), Acc@5: 1.0000 (0.9348)
2022-01-06 13:41:03,098 Val Step[0350/1563], Loss: 1.2276 (1.0459), Acc@1: 0.7500 (0.7671), Acc@5: 0.9062 (0.9368)
2022-01-06 13:41:04,396 Val Step[0400/1563], Loss: 0.9235 (1.0520), Acc@1: 0.8125 (0.7638), Acc@5: 0.9688 (0.9373)
2022-01-06 13:41:05,747 Val Step[0450/1563], Loss: 1.0258 (1.0559), Acc@1: 0.7188 (0.7619), Acc@5: 1.0000 (0.9385)
2022-01-06 13:41:07,062 Val Step[0500/1563], Loss: 0.4767 (1.0457), Acc@1: 0.9375 (0.7660), Acc@5: 1.0000 (0.9397)
2022-01-06 13:41:08,451 Val Step[0550/1563], Loss: 0.8376 (1.0255), Acc@1: 0.7812 (0.7713), Acc@5: 0.9688 (0.9419)
2022-01-06 13:41:09,753 Val Step[0600/1563], Loss: 0.7838 (1.0321), Acc@1: 0.8438 (0.7703), Acc@5: 0.9375 (0.9413)
2022-01-06 13:41:11,019 Val Step[0650/1563], Loss: 0.7947 (1.0522), Acc@1: 0.8750 (0.7655), Acc@5: 0.9688 (0.9387)
2022-01-06 13:41:12,283 Val Step[0700/1563], Loss: 1.2290 (1.0832), Acc@1: 0.6875 (0.7580), Acc@5: 0.9062 (0.9347)
2022-01-06 13:41:13,543 Val Step[0750/1563], Loss: 1.3869 (1.1150), Acc@1: 0.7500 (0.7517), Acc@5: 0.8750 (0.9301)
2022-01-06 13:41:14,876 Val Step[0800/1563], Loss: 0.8675 (1.1549), Acc@1: 0.8125 (0.7421), Acc@5: 0.9688 (0.9248)
2022-01-06 13:41:16,174 Val Step[0850/1563], Loss: 1.3102 (1.1797), Acc@1: 0.6562 (0.7365), Acc@5: 0.9375 (0.9219)
2022-01-06 13:41:17,511 Val Step[0900/1563], Loss: 0.2342 (1.1777), Acc@1: 1.0000 (0.7387), Acc@5: 1.0000 (0.9216)
2022-01-06 13:41:18,887 Val Step[0950/1563], Loss: 1.1784 (1.1956), Acc@1: 0.7500 (0.7359), Acc@5: 0.9375 (0.9188)
2022-01-06 13:41:20,150 Val Step[1000/1563], Loss: 0.6070 (1.2169), Acc@1: 0.9062 (0.7301), Acc@5: 1.0000 (0.9159)
2022-01-06 13:41:21,415 Val Step[1050/1563], Loss: 0.4385 (1.2317), Acc@1: 0.9688 (0.7270), Acc@5: 0.9688 (0.9140)
2022-01-06 13:41:22,670 Val Step[1100/1563], Loss: 0.8877 (1.2445), Acc@1: 0.7812 (0.7241), Acc@5: 0.9375 (0.9122)
2022-01-06 13:41:23,928 Val Step[1150/1563], Loss: 1.4044 (1.2596), Acc@1: 0.7812 (0.7212), Acc@5: 0.8125 (0.9098)
2022-01-06 13:41:25,193 Val Step[1200/1563], Loss: 1.3299 (1.2726), Acc@1: 0.8125 (0.7185), Acc@5: 0.8438 (0.9077)
2022-01-06 13:41:26,452 Val Step[1250/1563], Loss: 0.7632 (1.2843), Acc@1: 0.8750 (0.7166), Acc@5: 0.9375 (0.9060)
2022-01-06 13:41:27,711 Val Step[1300/1563], Loss: 0.9280 (1.2925), Acc@1: 0.8750 (0.7151), Acc@5: 0.9062 (0.9050)
2022-01-06 13:41:29,083 Val Step[1350/1563], Loss: 1.9018 (1.3108), Acc@1: 0.5000 (0.7109), Acc@5: 0.8750 (0.9022)
2022-01-06 13:41:30,531 Val Step[1400/1563], Loss: 1.0605 (1.3179), Acc@1: 0.7188 (0.7091), Acc@5: 0.9375 (0.9012)
2022-01-06 13:41:31,936 Val Step[1450/1563], Loss: 1.5938 (1.3242), Acc@1: 0.6250 (0.7073), Acc@5: 0.8750 (0.9008)
2022-01-06 13:41:33,304 Val Step[1500/1563], Loss: 1.7587 (1.3136), Acc@1: 0.5938 (0.7097), Acc@5: 0.8438 (0.9023)
2022-01-06 13:41:34,573 Val Step[1550/1563], Loss: 0.9525 (1.3148), Acc@1: 0.8750 (0.7090), Acc@5: 0.9062 (0.9022)
2022-01-06 13:41:35,319 ----- Epoch[232/310], Validation Loss: 1.3132, Validation Acc@1: 0.7093, Validation Acc@5: 0.9025, time: 101.89
2022-01-06 13:41:35,319 ----- Epoch[232/310], Train Loss: 3.7010, Train Acc: 0.3662, time: 1543.67, Best Val(epoch230) Acc@1: 0.7116
2022-01-06 13:41:35,500 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 13:41:35,500 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 13:41:35,605 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 13:41:35,605 Now training epoch 233. LR=0.000123
2022-01-06 13:42:52,902 Epoch[233/310], Step[0000/1251], Loss: 3.7571(3.7571), Acc: 0.3770(0.3770)
2022-01-06 13:43:50,705 Epoch[233/310], Step[0050/1251], Loss: 3.5787(3.6424), Acc: 0.5176(0.4112)
2022-01-06 13:44:50,429 Epoch[233/310], Step[0100/1251], Loss: 3.1316(3.6860), Acc: 0.3301(0.3784)
2022-01-06 13:45:49,945 Epoch[233/310], Step[0150/1251], Loss: 4.0760(3.6807), Acc: 0.4414(0.3704)
2022-01-06 13:46:48,506 Epoch[233/310], Step[0200/1251], Loss: 4.0362(3.6853), Acc: 0.2480(0.3667)
2022-01-06 13:47:47,893 Epoch[233/310], Step[0250/1251], Loss: 3.2386(3.6730), Acc: 0.2773(0.3651)
2022-01-06 13:48:47,241 Epoch[233/310], Step[0300/1251], Loss: 3.7633(3.6631), Acc: 0.5039(0.3687)
2022-01-06 13:49:46,867 Epoch[233/310], Step[0350/1251], Loss: 3.7098(3.6688), Acc: 0.4854(0.3693)
2022-01-06 13:50:46,068 Epoch[233/310], Step[0400/1251], Loss: 4.1198(3.6722), Acc: 0.3057(0.3703)
2022-01-06 13:51:45,421 Epoch[233/310], Step[0450/1251], Loss: 3.7188(3.6730), Acc: 0.5156(0.3737)
2022-01-06 13:52:42,977 Epoch[233/310], Step[0500/1251], Loss: 3.6564(3.6702), Acc: 0.5107(0.3731)
2022-01-06 13:53:41,140 Epoch[233/310], Step[0550/1251], Loss: 3.7731(3.6734), Acc: 0.2520(0.3685)
2022-01-06 13:54:37,824 Epoch[233/310], Step[0600/1251], Loss: 3.9012(3.6739), Acc: 0.4727(0.3703)
2022-01-06 13:55:36,288 Epoch[233/310], Step[0650/1251], Loss: 3.4759(3.6794), Acc: 0.3291(0.3717)
2022-01-06 13:56:36,322 Epoch[233/310], Step[0700/1251], Loss: 3.9523(3.6855), Acc: 0.4619(0.3712)
2022-01-06 13:57:35,525 Epoch[233/310], Step[0750/1251], Loss: 3.3597(3.6865), Acc: 0.3555(0.3703)
2022-01-06 13:58:35,735 Epoch[233/310], Step[0800/1251], Loss: 4.0234(3.6839), Acc: 0.3105(0.3715)
2022-01-06 13:59:35,561 Epoch[233/310], Step[0850/1251], Loss: 3.5785(3.6813), Acc: 0.4326(0.3716)
2022-01-06 14:00:35,762 Epoch[233/310], Step[0900/1251], Loss: 3.9430(3.6833), Acc: 0.2910(0.3707)
2022-01-06 14:01:35,999 Epoch[233/310], Step[0950/1251], Loss: 3.9507(3.6856), Acc: 0.4375(0.3702)
2022-01-06 14:02:34,570 Epoch[233/310], Step[1000/1251], Loss: 4.1076(3.6877), Acc: 0.3047(0.3689)
2022-01-06 14:03:34,122 Epoch[233/310], Step[1050/1251], Loss: 3.0085(3.6908), Acc: 0.4805(0.3676)
2022-01-06 14:04:33,678 Epoch[233/310], Step[1100/1251], Loss: 3.7588(3.6889), Acc: 0.4219(0.3687)
2022-01-06 14:05:33,672 Epoch[233/310], Step[1150/1251], Loss: 3.5272(3.6880), Acc: 0.4941(0.3686)
2022-01-06 14:06:33,521 Epoch[233/310], Step[1200/1251], Loss: 3.7693(3.6877), Acc: 0.1807(0.3675)
2022-01-06 14:07:33,721 Epoch[233/310], Step[1250/1251], Loss: 3.9424(3.6837), Acc: 0.4492(0.3685)
2022-01-06 14:07:35,179 ----- Epoch[233/310], Train Loss: 3.6837, Train Acc: 0.3685, time: 1559.57, Best Val(epoch230) Acc@1: 0.7116
2022-01-06 14:07:35,352 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 14:07:35,353 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 14:07:35,459 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 14:07:35,459 Now training epoch 234. LR=0.000119
2022-01-06 14:08:52,809 Epoch[234/310], Step[0000/1251], Loss: 3.8056(3.8056), Acc: 0.4521(0.4521)
2022-01-06 14:09:52,157 Epoch[234/310], Step[0050/1251], Loss: 4.3042(3.7399), Acc: 0.2930(0.3613)
2022-01-06 14:10:50,547 Epoch[234/310], Step[0100/1251], Loss: 3.6220(3.7205), Acc: 0.3574(0.3717)
2022-01-06 14:11:49,957 Epoch[234/310], Step[0150/1251], Loss: 3.6980(3.7075), Acc: 0.4346(0.3696)
2022-01-06 14:12:47,351 Epoch[234/310], Step[0200/1251], Loss: 3.6240(3.6997), Acc: 0.4678(0.3703)
2022-01-06 14:13:45,650 Epoch[234/310], Step[0250/1251], Loss: 3.5150(3.6919), Acc: 0.4463(0.3660)
2022-01-06 14:14:42,886 Epoch[234/310], Step[0300/1251], Loss: 3.4602(3.6934), Acc: 0.4355(0.3702)
2022-01-06 14:15:43,081 Epoch[234/310], Step[0350/1251], Loss: 3.5265(3.6970), Acc: 0.1992(0.3699)
2022-01-06 14:16:43,020 Epoch[234/310], Step[0400/1251], Loss: 3.6423(3.7022), Acc: 0.4570(0.3666)
2022-01-06 14:17:42,253 Epoch[234/310], Step[0450/1251], Loss: 3.2086(3.6963), Acc: 0.5645(0.3669)
2022-01-06 14:18:41,337 Epoch[234/310], Step[0500/1251], Loss: 3.7051(3.6915), Acc: 0.4590(0.3658)
2022-01-06 14:19:40,471 Epoch[234/310], Step[0550/1251], Loss: 3.4134(3.6894), Acc: 0.4170(0.3669)
2022-01-06 14:20:39,276 Epoch[234/310], Step[0600/1251], Loss: 3.7148(3.6893), Acc: 0.4248(0.3684)
2022-01-06 14:21:38,499 Epoch[234/310], Step[0650/1251], Loss: 4.0128(3.6913), Acc: 0.2305(0.3663)
2022-01-06 14:22:38,032 Epoch[234/310], Step[0700/1251], Loss: 3.2348(3.6943), Acc: 0.5586(0.3641)
2022-01-06 14:23:37,134 Epoch[234/310], Step[0750/1251], Loss: 3.4781(3.6956), Acc: 0.4980(0.3658)
2022-01-06 14:24:35,399 Epoch[234/310], Step[0800/1251], Loss: 3.6013(3.6957), Acc: 0.4951(0.3679)
2022-01-06 14:25:34,212 Epoch[234/310], Step[0850/1251], Loss: 3.5343(3.6982), Acc: 0.2666(0.3687)
2022-01-06 14:26:33,525 Epoch[234/310], Step[0900/1251], Loss: 3.5556(3.6962), Acc: 0.4688(0.3699)
2022-01-06 14:27:33,518 Epoch[234/310], Step[0950/1251], Loss: 3.7036(3.6936), Acc: 0.2080(0.3707)
2022-01-06 14:28:33,519 Epoch[234/310], Step[1000/1251], Loss: 3.4540(3.6955), Acc: 0.2842(0.3697)
2022-01-06 14:29:33,298 Epoch[234/310], Step[1050/1251], Loss: 3.7165(3.6947), Acc: 0.2734(0.3704)
2022-01-06 14:30:33,281 Epoch[234/310], Step[1100/1251], Loss: 3.5806(3.6932), Acc: 0.4551(0.3703)
2022-01-06 14:31:33,095 Epoch[234/310], Step[1150/1251], Loss: 3.4557(3.6931), Acc: 0.1416(0.3698)
2022-01-06 14:32:31,872 Epoch[234/310], Step[1200/1251], Loss: 3.9752(3.6911), Acc: 0.2275(0.3694)
2022-01-06 14:33:31,773 Epoch[234/310], Step[1250/1251], Loss: 4.0229(3.6896), Acc: 0.1914(0.3701)
2022-01-06 14:33:33,243 ----- Validation after Epoch: 234
2022-01-06 14:34:39,679 Val Step[0000/1563], Loss: 0.6339 (0.6339), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 14:34:41,014 Val Step[0050/1563], Loss: 2.4015 (0.7783), Acc@1: 0.3750 (0.8401), Acc@5: 0.8750 (0.9577)
2022-01-06 14:34:42,291 Val Step[0100/1563], Loss: 1.8522 (1.0421), Acc@1: 0.5312 (0.7667), Acc@5: 0.8438 (0.9366)
2022-01-06 14:34:43,682 Val Step[0150/1563], Loss: 0.4051 (0.9902), Acc@1: 0.9062 (0.7794), Acc@5: 1.0000 (0.9394)
2022-01-06 14:34:45,046 Val Step[0200/1563], Loss: 0.9033 (0.9969), Acc@1: 0.7812 (0.7809), Acc@5: 0.9375 (0.9367)
2022-01-06 14:34:46,327 Val Step[0250/1563], Loss: 0.5965 (0.9458), Acc@1: 0.9375 (0.7935), Acc@5: 1.0000 (0.9431)
2022-01-06 14:34:47,594 Val Step[0300/1563], Loss: 1.1477 (1.0063), Acc@1: 0.6875 (0.7762), Acc@5: 0.9688 (0.9380)
2022-01-06 14:34:48,937 Val Step[0350/1563], Loss: 1.2732 (1.0169), Acc@1: 0.6875 (0.7709), Acc@5: 0.9062 (0.9400)
2022-01-06 14:34:50,215 Val Step[0400/1563], Loss: 0.9785 (1.0260), Acc@1: 0.8125 (0.7645), Acc@5: 0.9688 (0.9405)
2022-01-06 14:34:51,625 Val Step[0450/1563], Loss: 0.8310 (1.0302), Acc@1: 0.8125 (0.7629), Acc@5: 1.0000 (0.9414)
2022-01-06 14:34:52,905 Val Step[0500/1563], Loss: 0.4267 (1.0191), Acc@1: 0.9688 (0.7661), Acc@5: 1.0000 (0.9428)
2022-01-06 14:34:54,348 Val Step[0550/1563], Loss: 0.8227 (0.9997), Acc@1: 0.8125 (0.7719), Acc@5: 0.9375 (0.9445)
2022-01-06 14:34:55,799 Val Step[0600/1563], Loss: 0.7719 (1.0060), Acc@1: 0.8125 (0.7707), Acc@5: 0.9375 (0.9435)
2022-01-06 14:34:57,197 Val Step[0650/1563], Loss: 0.8480 (1.0249), Acc@1: 0.7812 (0.7663), Acc@5: 0.9688 (0.9410)
2022-01-06 14:34:58,582 Val Step[0700/1563], Loss: 1.1316 (1.0559), Acc@1: 0.7500 (0.7590), Acc@5: 0.8750 (0.9373)
2022-01-06 14:34:59,896 Val Step[0750/1563], Loss: 1.4192 (1.0877), Acc@1: 0.7500 (0.7526), Acc@5: 0.8750 (0.9325)
2022-01-06 14:35:01,220 Val Step[0800/1563], Loss: 0.8223 (1.1249), Acc@1: 0.7500 (0.7444), Acc@5: 1.0000 (0.9277)
2022-01-06 14:35:02,513 Val Step[0850/1563], Loss: 1.3072 (1.1540), Acc@1: 0.6250 (0.7372), Acc@5: 0.9375 (0.9238)
2022-01-06 14:35:03,772 Val Step[0900/1563], Loss: 0.2707 (1.1530), Acc@1: 0.9688 (0.7387), Acc@5: 1.0000 (0.9231)
2022-01-06 14:35:05,218 Val Step[0950/1563], Loss: 1.1796 (1.1735), Acc@1: 0.8438 (0.7350), Acc@5: 0.9062 (0.9201)
2022-01-06 14:35:06,487 Val Step[1000/1563], Loss: 0.6078 (1.1957), Acc@1: 0.8750 (0.7298), Acc@5: 1.0000 (0.9169)
2022-01-06 14:35:07,746 Val Step[1050/1563], Loss: 0.4061 (1.2091), Acc@1: 0.9688 (0.7268), Acc@5: 0.9688 (0.9153)
2022-01-06 14:35:09,004 Val Step[1100/1563], Loss: 0.7372 (1.2226), Acc@1: 0.8438 (0.7241), Acc@5: 1.0000 (0.9134)
2022-01-06 14:35:10,278 Val Step[1150/1563], Loss: 1.4586 (1.2364), Acc@1: 0.7812 (0.7216), Acc@5: 0.8125 (0.9113)
2022-01-06 14:35:11,592 Val Step[1200/1563], Loss: 1.1155 (1.2499), Acc@1: 0.7188 (0.7186), Acc@5: 0.8750 (0.9092)
2022-01-06 14:35:12,913 Val Step[1250/1563], Loss: 0.8346 (1.2611), Acc@1: 0.8750 (0.7171), Acc@5: 0.9375 (0.9075)
2022-01-06 14:35:14,236 Val Step[1300/1563], Loss: 0.8346 (1.2688), Acc@1: 0.8438 (0.7158), Acc@5: 0.9375 (0.9063)
2022-01-06 14:35:15,532 Val Step[1350/1563], Loss: 2.0688 (1.2874), Acc@1: 0.4062 (0.7114), Acc@5: 0.8125 (0.9036)
2022-01-06 14:35:16,821 Val Step[1400/1563], Loss: 1.2397 (1.2932), Acc@1: 0.7188 (0.7099), Acc@5: 0.9062 (0.9027)
2022-01-06 14:35:18,237 Val Step[1450/1563], Loss: 1.2295 (1.3003), Acc@1: 0.7188 (0.7081), Acc@5: 0.9062 (0.9022)
2022-01-06 14:35:19,503 Val Step[1500/1563], Loss: 1.7536 (1.2903), Acc@1: 0.5625 (0.7102), Acc@5: 0.8750 (0.9035)
2022-01-06 14:35:20,767 Val Step[1550/1563], Loss: 0.8949 (1.2913), Acc@1: 0.8750 (0.7100), Acc@5: 0.9062 (0.9033)
2022-01-06 14:35:21,548 ----- Epoch[234/310], Validation Loss: 1.2894, Validation Acc@1: 0.7103, Validation Acc@5: 0.9036, time: 108.30
2022-01-06 14:35:21,548 ----- Epoch[234/310], Train Loss: 3.6896, Train Acc: 0.3701, time: 1557.78, Best Val(epoch230) Acc@1: 0.7116
2022-01-06 14:35:21,731 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 14:35:21,731 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 14:35:21,836 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 14:35:21,836 Now training epoch 235. LR=0.000116
2022-01-06 14:36:42,069 Epoch[235/310], Step[0000/1251], Loss: 3.4927(3.4927), Acc: 0.4238(0.4238)
2022-01-06 14:37:39,078 Epoch[235/310], Step[0050/1251], Loss: 3.4198(3.7491), Acc: 0.2637(0.3545)
2022-01-06 14:38:37,508 Epoch[235/310], Step[0100/1251], Loss: 4.0892(3.7348), Acc: 0.3975(0.3608)
2022-01-06 14:39:37,343 Epoch[235/310], Step[0150/1251], Loss: 3.0544(3.7147), Acc: 0.4668(0.3662)
2022-01-06 14:40:37,345 Epoch[235/310], Step[0200/1251], Loss: 3.7835(3.7278), Acc: 0.3145(0.3585)
2022-01-06 14:41:37,137 Epoch[235/310], Step[0250/1251], Loss: 3.0516(3.7327), Acc: 0.4590(0.3578)
2022-01-06 14:42:35,387 Epoch[235/310], Step[0300/1251], Loss: 3.3833(3.7212), Acc: 0.3936(0.3572)
2022-01-06 14:43:34,991 Epoch[235/310], Step[0350/1251], Loss: 3.9688(3.7213), Acc: 0.3428(0.3606)
2022-01-06 14:44:33,817 Epoch[235/310], Step[0400/1251], Loss: 4.0046(3.7121), Acc: 0.4434(0.3615)
2022-01-06 14:45:32,906 Epoch[235/310], Step[0450/1251], Loss: 3.4415(3.7127), Acc: 0.5527(0.3633)
2022-01-06 14:46:31,545 Epoch[235/310], Step[0500/1251], Loss: 4.1466(3.7078), Acc: 0.3828(0.3652)
2022-01-06 14:47:29,218 Epoch[235/310], Step[0550/1251], Loss: 3.6902(3.7059), Acc: 0.2598(0.3672)
2022-01-06 14:48:28,180 Epoch[235/310], Step[0600/1251], Loss: 3.3791(3.7084), Acc: 0.3496(0.3674)
2022-01-06 14:49:27,758 Epoch[235/310], Step[0650/1251], Loss: 3.5706(3.7095), Acc: 0.5088(0.3659)
2022-01-06 14:50:26,458 Epoch[235/310], Step[0700/1251], Loss: 4.0775(3.7128), Acc: 0.4414(0.3653)
2022-01-06 14:51:25,408 Epoch[235/310], Step[0750/1251], Loss: 3.5259(3.7070), Acc: 0.4883(0.3637)
2022-01-06 14:52:23,259 Epoch[235/310], Step[0800/1251], Loss: 3.3412(3.7069), Acc: 0.2324(0.3643)
2022-01-06 14:53:22,184 Epoch[235/310], Step[0850/1251], Loss: 3.8404(3.7037), Acc: 0.3682(0.3655)
2022-01-06 14:54:21,127 Epoch[235/310], Step[0900/1251], Loss: 3.5331(3.7000), Acc: 0.5205(0.3668)
2022-01-06 14:55:21,989 Epoch[235/310], Step[0950/1251], Loss: 3.9274(3.6993), Acc: 0.3945(0.3666)
2022-01-06 14:56:22,115 Epoch[235/310], Step[1000/1251], Loss: 4.2432(3.7011), Acc: 0.3447(0.3647)
2022-01-06 14:57:22,175 Epoch[235/310], Step[1050/1251], Loss: 3.4643(3.7018), Acc: 0.4043(0.3637)
2022-01-06 14:58:22,688 Epoch[235/310], Step[1100/1251], Loss: 4.0464(3.7020), Acc: 0.3730(0.3632)
2022-01-06 14:59:22,300 Epoch[235/310], Step[1150/1251], Loss: 3.7994(3.7003), Acc: 0.2695(0.3632)
2022-01-06 15:00:21,938 Epoch[235/310], Step[1200/1251], Loss: 3.1953(3.6994), Acc: 0.4912(0.3651)
2022-01-06 15:01:21,789 Epoch[235/310], Step[1250/1251], Loss: 4.0955(3.7014), Acc: 0.2979(0.3647)
2022-01-06 15:01:23,235 ----- Epoch[235/310], Train Loss: 3.7014, Train Acc: 0.3647, time: 1561.39, Best Val(epoch230) Acc@1: 0.7116
2022-01-06 15:01:23,406 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 15:01:23,406 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 15:01:23,549 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 15:01:23,549 Now training epoch 236. LR=0.000113
2022-01-06 15:02:38,607 Epoch[236/310], Step[0000/1251], Loss: 3.6529(3.6529), Acc: 0.3838(0.3838)
2022-01-06 15:03:38,247 Epoch[236/310], Step[0050/1251], Loss: 3.4705(3.6854), Acc: 0.5068(0.3674)
2022-01-06 15:04:37,282 Epoch[236/310], Step[0100/1251], Loss: 3.4332(3.7168), Acc: 0.4102(0.3543)
2022-01-06 15:05:36,727 Epoch[236/310], Step[0150/1251], Loss: 4.2058(3.7031), Acc: 0.1699(0.3545)
2022-01-06 15:06:33,769 Epoch[236/310], Step[0200/1251], Loss: 3.9697(3.7038), Acc: 0.2920(0.3606)
2022-01-06 15:07:31,834 Epoch[236/310], Step[0250/1251], Loss: 3.7737(3.7086), Acc: 0.1914(0.3589)
2022-01-06 15:08:29,591 Epoch[236/310], Step[0300/1251], Loss: 4.1853(3.6976), Acc: 0.3330(0.3589)
2022-01-06 15:09:26,999 Epoch[236/310], Step[0350/1251], Loss: 2.8128(3.6931), Acc: 0.4824(0.3617)
2022-01-06 15:10:25,697 Epoch[236/310], Step[0400/1251], Loss: 3.7381(3.6982), Acc: 0.2725(0.3605)
2022-01-06 15:11:25,723 Epoch[236/310], Step[0450/1251], Loss: 4.0174(3.6948), Acc: 0.2930(0.3639)
2022-01-06 15:12:24,676 Epoch[236/310], Step[0500/1251], Loss: 4.0092(3.6908), Acc: 0.4346(0.3659)
2022-01-06 15:13:23,317 Epoch[236/310], Step[0550/1251], Loss: 3.6359(3.6927), Acc: 0.4893(0.3660)
2022-01-06 15:14:23,157 Epoch[236/310], Step[0600/1251], Loss: 3.3526(3.6943), Acc: 0.3398(0.3657)
2022-01-06 15:15:22,586 Epoch[236/310], Step[0650/1251], Loss: 4.0289(3.6954), Acc: 0.3535(0.3644)
2022-01-06 15:16:21,796 Epoch[236/310], Step[0700/1251], Loss: 3.7020(3.6926), Acc: 0.5303(0.3662)
2022-01-06 15:17:21,483 Epoch[236/310], Step[0750/1251], Loss: 3.3568(3.6950), Acc: 0.3721(0.3647)
2022-01-06 15:18:20,215 Epoch[236/310], Step[0800/1251], Loss: 3.0557(3.6937), Acc: 0.5664(0.3656)
2022-01-06 15:19:19,851 Epoch[236/310], Step[0850/1251], Loss: 3.4314(3.6917), Acc: 0.3447(0.3662)
2022-01-06 15:20:19,290 Epoch[236/310], Step[0900/1251], Loss: 3.4931(3.6917), Acc: 0.3125(0.3659)
2022-01-06 15:21:18,577 Epoch[236/310], Step[0950/1251], Loss: 3.7656(3.6931), Acc: 0.2275(0.3652)
2022-01-06 15:22:18,683 Epoch[236/310], Step[1000/1251], Loss: 3.8523(3.6931), Acc: 0.5049(0.3651)
2022-01-06 15:23:16,577 Epoch[236/310], Step[1050/1251], Loss: 3.4951(3.6886), Acc: 0.4990(0.3663)
2022-01-06 15:24:15,888 Epoch[236/310], Step[1100/1251], Loss: 4.0780(3.6826), Acc: 0.4072(0.3680)
2022-01-06 15:25:13,082 Epoch[236/310], Step[1150/1251], Loss: 3.7069(3.6811), Acc: 0.3486(0.3689)
2022-01-06 15:26:12,900 Epoch[236/310], Step[1200/1251], Loss: 3.8082(3.6823), Acc: 0.3867(0.3687)
2022-01-06 15:27:12,265 Epoch[236/310], Step[1250/1251], Loss: 4.0292(3.6839), Acc: 0.4385(0.3696)
2022-01-06 15:27:13,730 ----- Validation after Epoch: 236
2022-01-06 15:28:13,942 Val Step[0000/1563], Loss: 0.7000 (0.7000), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 15:28:15,242 Val Step[0050/1563], Loss: 2.0601 (0.7701), Acc@1: 0.4688 (0.8358), Acc@5: 0.8438 (0.9583)
2022-01-06 15:28:16,497 Val Step[0100/1563], Loss: 1.7969 (1.0318), Acc@1: 0.5625 (0.7695), Acc@5: 0.8438 (0.9378)
2022-01-06 15:28:17,797 Val Step[0150/1563], Loss: 0.3988 (0.9823), Acc@1: 0.9688 (0.7827), Acc@5: 1.0000 (0.9402)
2022-01-06 15:28:19,055 Val Step[0200/1563], Loss: 1.1068 (0.9934), Acc@1: 0.7188 (0.7845), Acc@5: 0.9062 (0.9381)
2022-01-06 15:28:20,324 Val Step[0250/1563], Loss: 0.4371 (0.9454), Acc@1: 0.9688 (0.7969), Acc@5: 1.0000 (0.9430)
2022-01-06 15:28:21,600 Val Step[0300/1563], Loss: 1.1252 (1.0046), Acc@1: 0.6562 (0.7792), Acc@5: 0.9688 (0.9380)
2022-01-06 15:28:22,878 Val Step[0350/1563], Loss: 1.0951 (1.0161), Acc@1: 0.7500 (0.7743), Acc@5: 0.9062 (0.9395)
2022-01-06 15:28:24,189 Val Step[0400/1563], Loss: 0.9097 (1.0210), Acc@1: 0.8750 (0.7703), Acc@5: 0.9688 (0.9393)
2022-01-06 15:28:25,464 Val Step[0450/1563], Loss: 1.1129 (1.0284), Acc@1: 0.6562 (0.7671), Acc@5: 1.0000 (0.9399)
2022-01-06 15:28:26,741 Val Step[0500/1563], Loss: 0.5380 (1.0202), Acc@1: 0.8750 (0.7695), Acc@5: 1.0000 (0.9409)
2022-01-06 15:28:28,134 Val Step[0550/1563], Loss: 0.8214 (0.9995), Acc@1: 0.7812 (0.7752), Acc@5: 0.9688 (0.9429)
2022-01-06 15:28:29,425 Val Step[0600/1563], Loss: 0.7065 (1.0072), Acc@1: 0.9062 (0.7739), Acc@5: 0.9688 (0.9421)
2022-01-06 15:28:30,752 Val Step[0650/1563], Loss: 0.7181 (1.0260), Acc@1: 0.8750 (0.7699), Acc@5: 1.0000 (0.9393)
2022-01-06 15:28:32,078 Val Step[0700/1563], Loss: 0.9375 (1.0537), Acc@1: 0.8438 (0.7633), Acc@5: 1.0000 (0.9359)
2022-01-06 15:28:33,390 Val Step[0750/1563], Loss: 1.4795 (1.0865), Acc@1: 0.7500 (0.7569), Acc@5: 0.9062 (0.9313)
2022-01-06 15:28:34,714 Val Step[0800/1563], Loss: 0.8156 (1.1261), Acc@1: 0.7500 (0.7470), Acc@5: 1.0000 (0.9262)
2022-01-06 15:28:35,968 Val Step[0850/1563], Loss: 1.2328 (1.1499), Acc@1: 0.7188 (0.7414), Acc@5: 0.9375 (0.9230)
2022-01-06 15:28:37,284 Val Step[0900/1563], Loss: 0.2780 (1.1501), Acc@1: 1.0000 (0.7428), Acc@5: 1.0000 (0.9224)
2022-01-06 15:28:38,651 Val Step[0950/1563], Loss: 1.3649 (1.1693), Acc@1: 0.7188 (0.7391), Acc@5: 0.9062 (0.9192)
2022-01-06 15:28:40,042 Val Step[1000/1563], Loss: 0.7029 (1.1934), Acc@1: 0.9375 (0.7329), Acc@5: 0.9375 (0.9155)
2022-01-06 15:28:41,454 Val Step[1050/1563], Loss: 0.3630 (1.2060), Acc@1: 0.9375 (0.7298), Acc@5: 0.9688 (0.9136)
2022-01-06 15:28:42,865 Val Step[1100/1563], Loss: 0.9979 (1.2202), Acc@1: 0.7500 (0.7267), Acc@5: 0.9375 (0.9117)
2022-01-06 15:28:44,279 Val Step[1150/1563], Loss: 1.1523 (1.2338), Acc@1: 0.7812 (0.7240), Acc@5: 0.8438 (0.9098)
2022-01-06 15:28:45,661 Val Step[1200/1563], Loss: 1.2092 (1.2482), Acc@1: 0.8125 (0.7206), Acc@5: 0.8438 (0.9076)
2022-01-06 15:28:47,045 Val Step[1250/1563], Loss: 0.7061 (1.2598), Acc@1: 0.8750 (0.7187), Acc@5: 0.9375 (0.9057)
2022-01-06 15:28:48,440 Val Step[1300/1563], Loss: 0.9067 (1.2680), Acc@1: 0.8125 (0.7170), Acc@5: 0.9062 (0.9047)
2022-01-06 15:28:49,833 Val Step[1350/1563], Loss: 1.3839 (1.2847), Acc@1: 0.5625 (0.7132), Acc@5: 0.9062 (0.9024)
2022-01-06 15:28:51,242 Val Step[1400/1563], Loss: 1.0928 (1.2907), Acc@1: 0.7500 (0.7116), Acc@5: 0.9375 (0.9017)
2022-01-06 15:28:52,652 Val Step[1450/1563], Loss: 1.5790 (1.2961), Acc@1: 0.5938 (0.7097), Acc@5: 0.8750 (0.9016)
2022-01-06 15:28:54,055 Val Step[1500/1563], Loss: 1.9026 (1.2855), Acc@1: 0.5938 (0.7124), Acc@5: 0.8438 (0.9029)
2022-01-06 15:28:55,459 Val Step[1550/1563], Loss: 0.9736 (1.2872), Acc@1: 0.8750 (0.7117), Acc@5: 0.9062 (0.9027)
2022-01-06 15:28:56,244 ----- Epoch[236/310], Validation Loss: 1.2858, Validation Acc@1: 0.7119, Validation Acc@5: 0.9029, time: 102.51
2022-01-06 15:28:56,244 ----- Epoch[236/310], Train Loss: 3.6839, Train Acc: 0.3696, time: 1550.18, Best Val(epoch236) Acc@1: 0.7119
2022-01-06 15:28:56,424 Max accuracy so far: 0.7119 at epoch_236
2022-01-06 15:28:56,424 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-06 15:28:56,424 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-06 15:28:56,525 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-06 15:28:56,908 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 15:28:56,908 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 15:28:57,038 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 15:28:57,038 Now training epoch 237. LR=0.000109
2022-01-06 15:30:16,844 Epoch[237/310], Step[0000/1251], Loss: 4.0870(4.0870), Acc: 0.2588(0.2588)
2022-01-06 15:31:16,231 Epoch[237/310], Step[0050/1251], Loss: 3.9107(3.6458), Acc: 0.4863(0.3889)
2022-01-06 15:32:15,529 Epoch[237/310], Step[0100/1251], Loss: 3.5500(3.6455), Acc: 0.4824(0.3788)
2022-01-06 15:33:15,816 Epoch[237/310], Step[0150/1251], Loss: 3.4766(3.6440), Acc: 0.2637(0.3784)
2022-01-06 15:34:15,555 Epoch[237/310], Step[0200/1251], Loss: 3.3011(3.6428), Acc: 0.3975(0.3738)
2022-01-06 15:35:15,610 Epoch[237/310], Step[0250/1251], Loss: 3.7157(3.6432), Acc: 0.3467(0.3749)
2022-01-06 15:36:15,369 Epoch[237/310], Step[0300/1251], Loss: 3.7410(3.6391), Acc: 0.4590(0.3745)
2022-01-06 15:37:14,155 Epoch[237/310], Step[0350/1251], Loss: 3.6462(3.6471), Acc: 0.1953(0.3761)
2022-01-06 15:38:14,070 Epoch[237/310], Step[0400/1251], Loss: 4.2163(3.6561), Acc: 0.1426(0.3725)
2022-01-06 15:39:13,778 Epoch[237/310], Step[0450/1251], Loss: 3.5594(3.6619), Acc: 0.1855(0.3701)
2022-01-06 15:40:12,564 Epoch[237/310], Step[0500/1251], Loss: 3.7960(3.6601), Acc: 0.4434(0.3710)
2022-01-06 15:41:12,217 Epoch[237/310], Step[0550/1251], Loss: 3.3136(3.6539), Acc: 0.5557(0.3710)
2022-01-06 15:42:11,733 Epoch[237/310], Step[0600/1251], Loss: 3.6018(3.6530), Acc: 0.1768(0.3731)
2022-01-06 15:43:11,168 Epoch[237/310], Step[0650/1251], Loss: 3.4799(3.6627), Acc: 0.3955(0.3726)
2022-01-06 15:44:12,127 Epoch[237/310], Step[0700/1251], Loss: 3.7754(3.6634), Acc: 0.2510(0.3732)
2022-01-06 15:45:12,942 Epoch[237/310], Step[0750/1251], Loss: 3.7831(3.6694), Acc: 0.2910(0.3699)
2022-01-06 15:46:12,917 Epoch[237/310], Step[0800/1251], Loss: 3.3271(3.6700), Acc: 0.4092(0.3692)
2022-01-06 15:47:12,081 Epoch[237/310], Step[0850/1251], Loss: 3.8752(3.6721), Acc: 0.1875(0.3689)
2022-01-06 15:48:12,463 Epoch[237/310], Step[0900/1251], Loss: 3.7316(3.6744), Acc: 0.3857(0.3688)
2022-01-06 15:49:11,861 Epoch[237/310], Step[0950/1251], Loss: 3.2304(3.6697), Acc: 0.3184(0.3690)
2022-01-06 15:50:12,172 Epoch[237/310], Step[1000/1251], Loss: 3.6585(3.6711), Acc: 0.4375(0.3692)
2022-01-06 15:51:12,499 Epoch[237/310], Step[1050/1251], Loss: 3.4849(3.6705), Acc: 0.5186(0.3694)
2022-01-06 15:52:12,993 Epoch[237/310], Step[1100/1251], Loss: 4.0415(3.6738), Acc: 0.4629(0.3687)
2022-01-06 15:53:12,994 Epoch[237/310], Step[1150/1251], Loss: 3.9604(3.6742), Acc: 0.3535(0.3686)
2022-01-06 15:54:12,442 Epoch[237/310], Step[1200/1251], Loss: 3.6635(3.6706), Acc: 0.5498(0.3689)
2022-01-06 15:55:12,953 Epoch[237/310], Step[1250/1251], Loss: 4.1054(3.6727), Acc: 0.3047(0.3688)
2022-01-06 15:55:14,945 ----- Epoch[237/310], Train Loss: 3.6727, Train Acc: 0.3688, time: 1577.90, Best Val(epoch236) Acc@1: 0.7119
2022-01-06 15:55:15,117 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 15:55:15,117 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 15:55:15,224 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 15:55:15,224 Now training epoch 238. LR=0.000106
2022-01-06 15:56:35,719 Epoch[238/310], Step[0000/1251], Loss: 3.6684(3.6684), Acc: 0.3564(0.3564)
2022-01-06 15:57:34,277 Epoch[238/310], Step[0050/1251], Loss: 3.5566(3.6171), Acc: 0.3213(0.3997)
2022-01-06 15:58:34,654 Epoch[238/310], Step[0100/1251], Loss: 3.6719(3.6535), Acc: 0.5293(0.3776)
2022-01-06 15:59:34,829 Epoch[238/310], Step[0150/1251], Loss: 3.3092(3.6781), Acc: 0.5781(0.3664)
2022-01-06 16:00:34,619 Epoch[238/310], Step[0200/1251], Loss: 3.5728(3.6758), Acc: 0.5283(0.3692)
2022-01-06 16:01:34,033 Epoch[238/310], Step[0250/1251], Loss: 3.6828(3.6804), Acc: 0.3604(0.3658)
2022-01-06 16:02:34,578 Epoch[238/310], Step[0300/1251], Loss: 3.7773(3.6919), Acc: 0.1816(0.3615)
2022-01-06 16:03:33,316 Epoch[238/310], Step[0350/1251], Loss: 3.9309(3.6909), Acc: 0.3027(0.3612)
2022-01-06 16:04:33,099 Epoch[238/310], Step[0400/1251], Loss: 3.8406(3.6901), Acc: 0.3506(0.3627)
2022-01-06 16:05:32,887 Epoch[238/310], Step[0450/1251], Loss: 3.1757(3.6847), Acc: 0.3184(0.3660)
2022-01-06 16:06:32,012 Epoch[238/310], Step[0500/1251], Loss: 4.4080(3.6758), Acc: 0.2422(0.3686)
2022-01-06 16:07:31,133 Epoch[238/310], Step[0550/1251], Loss: 4.1880(3.6839), Acc: 0.3584(0.3682)
2022-01-06 16:08:30,809 Epoch[238/310], Step[0600/1251], Loss: 3.9725(3.6846), Acc: 0.3906(0.3646)
2022-01-06 16:09:30,265 Epoch[238/310], Step[0650/1251], Loss: 3.6621(3.6849), Acc: 0.1260(0.3640)
2022-01-06 16:10:29,194 Epoch[238/310], Step[0700/1251], Loss: 3.2140(3.6873), Acc: 0.3105(0.3625)
2022-01-06 16:11:27,806 Epoch[238/310], Step[0750/1251], Loss: 3.6947(3.6887), Acc: 0.2070(0.3642)
2022-01-06 16:12:27,264 Epoch[238/310], Step[0800/1251], Loss: 3.2318(3.6876), Acc: 0.5889(0.3652)
2022-01-06 16:13:26,250 Epoch[238/310], Step[0850/1251], Loss: 3.7474(3.6847), Acc: 0.3027(0.3656)
2022-01-06 16:14:26,196 Epoch[238/310], Step[0900/1251], Loss: 3.4350(3.6866), Acc: 0.3994(0.3646)
2022-01-06 16:15:26,357 Epoch[238/310], Step[0950/1251], Loss: 3.9038(3.6852), Acc: 0.2578(0.3635)
2022-01-06 16:16:25,255 Epoch[238/310], Step[1000/1251], Loss: 4.0845(3.6830), Acc: 0.4600(0.3656)
2022-01-06 16:17:25,209 Epoch[238/310], Step[1050/1251], Loss: 3.9209(3.6863), Acc: 0.3271(0.3652)
2022-01-06 16:18:23,656 Epoch[238/310], Step[1100/1251], Loss: 3.6891(3.6849), Acc: 0.4551(0.3641)
2022-01-06 16:19:24,092 Epoch[238/310], Step[1150/1251], Loss: 3.3822(3.6889), Acc: 0.4443(0.3646)
2022-01-06 16:20:24,471 Epoch[238/310], Step[1200/1251], Loss: 3.6314(3.6877), Acc: 0.5107(0.3645)
2022-01-06 16:21:24,125 Epoch[238/310], Step[1250/1251], Loss: 4.0233(3.6867), Acc: 0.2119(0.3640)
2022-01-06 16:21:25,624 ----- Validation after Epoch: 238
2022-01-06 16:22:25,025 Val Step[0000/1563], Loss: 0.6765 (0.6765), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-06 16:22:26,326 Val Step[0050/1563], Loss: 2.4208 (0.7909), Acc@1: 0.4375 (0.8413), Acc@5: 0.8438 (0.9559)
2022-01-06 16:22:27,710 Val Step[0100/1563], Loss: 2.0344 (1.0491), Acc@1: 0.3750 (0.7683), Acc@5: 0.8750 (0.9319)
2022-01-06 16:22:29,170 Val Step[0150/1563], Loss: 0.3497 (0.9865), Acc@1: 0.9375 (0.7833), Acc@5: 1.0000 (0.9373)
2022-01-06 16:22:30,522 Val Step[0200/1563], Loss: 1.0213 (0.9933), Acc@1: 0.8125 (0.7858), Acc@5: 0.9375 (0.9361)
2022-01-06 16:22:31,879 Val Step[0250/1563], Loss: 0.5271 (0.9419), Acc@1: 0.8750 (0.7989), Acc@5: 1.0000 (0.9420)
2022-01-06 16:22:33,243 Val Step[0300/1563], Loss: 1.0065 (1.0017), Acc@1: 0.7812 (0.7839), Acc@5: 0.9688 (0.9378)
2022-01-06 16:22:34,612 Val Step[0350/1563], Loss: 1.1625 (1.0105), Acc@1: 0.7500 (0.7785), Acc@5: 0.9062 (0.9396)
2022-01-06 16:22:35,928 Val Step[0400/1563], Loss: 0.9206 (1.0199), Acc@1: 0.8125 (0.7731), Acc@5: 0.9688 (0.9398)
2022-01-06 16:22:37,228 Val Step[0450/1563], Loss: 0.8712 (1.0280), Acc@1: 0.7500 (0.7704), Acc@5: 1.0000 (0.9402)
2022-01-06 16:22:38,519 Val Step[0500/1563], Loss: 0.5612 (1.0202), Acc@1: 0.8750 (0.7728), Acc@5: 1.0000 (0.9416)
2022-01-06 16:22:39,907 Val Step[0550/1563], Loss: 0.9531 (1.0010), Acc@1: 0.7812 (0.7782), Acc@5: 0.9688 (0.9432)
2022-01-06 16:22:41,213 Val Step[0600/1563], Loss: 0.9270 (1.0069), Acc@1: 0.7812 (0.7774), Acc@5: 0.9375 (0.9426)
2022-01-06 16:22:42,513 Val Step[0650/1563], Loss: 0.6002 (1.0281), Acc@1: 0.9375 (0.7728), Acc@5: 1.0000 (0.9398)
2022-01-06 16:22:43,909 Val Step[0700/1563], Loss: 1.0573 (1.0562), Acc@1: 0.7812 (0.7657), Acc@5: 0.9375 (0.9365)
2022-01-06 16:22:45,207 Val Step[0750/1563], Loss: 1.1832 (1.0879), Acc@1: 0.7812 (0.7592), Acc@5: 0.9062 (0.9323)
2022-01-06 16:22:46,470 Val Step[0800/1563], Loss: 0.9474 (1.1261), Acc@1: 0.7500 (0.7496), Acc@5: 1.0000 (0.9277)
2022-01-06 16:22:47,731 Val Step[0850/1563], Loss: 1.2018 (1.1512), Acc@1: 0.6875 (0.7433), Acc@5: 0.9375 (0.9245)
2022-01-06 16:22:48,993 Val Step[0900/1563], Loss: 0.2714 (1.1514), Acc@1: 1.0000 (0.7450), Acc@5: 1.0000 (0.9241)
2022-01-06 16:22:50,430 Val Step[0950/1563], Loss: 1.1876 (1.1707), Acc@1: 0.7812 (0.7416), Acc@5: 0.9062 (0.9210)
2022-01-06 16:22:51,828 Val Step[1000/1563], Loss: 0.5932 (1.1938), Acc@1: 0.9375 (0.7354), Acc@5: 0.9688 (0.9177)
2022-01-06 16:22:53,242 Val Step[1050/1563], Loss: 0.3640 (1.2074), Acc@1: 0.9375 (0.7323), Acc@5: 1.0000 (0.9158)
2022-01-06 16:22:54,647 Val Step[1100/1563], Loss: 0.9415 (1.2213), Acc@1: 0.7812 (0.7295), Acc@5: 0.9688 (0.9137)
2022-01-06 16:22:56,034 Val Step[1150/1563], Loss: 1.2334 (1.2362), Acc@1: 0.7812 (0.7262), Acc@5: 0.8438 (0.9118)
2022-01-06 16:22:57,413 Val Step[1200/1563], Loss: 1.3313 (1.2518), Acc@1: 0.7812 (0.7225), Acc@5: 0.8438 (0.9094)
2022-01-06 16:22:58,826 Val Step[1250/1563], Loss: 0.7585 (1.2633), Acc@1: 0.8750 (0.7208), Acc@5: 0.9375 (0.9074)
2022-01-06 16:23:00,252 Val Step[1300/1563], Loss: 0.9706 (1.2734), Acc@1: 0.7812 (0.7187), Acc@5: 0.9375 (0.9063)
2022-01-06 16:23:01,685 Val Step[1350/1563], Loss: 1.7762 (1.2903), Acc@1: 0.5312 (0.7145), Acc@5: 0.8438 (0.9039)
2022-01-06 16:23:03,148 Val Step[1400/1563], Loss: 1.1265 (1.2970), Acc@1: 0.7500 (0.7133), Acc@5: 0.9375 (0.9032)
2022-01-06 16:23:04,594 Val Step[1450/1563], Loss: 1.6770 (1.3029), Acc@1: 0.5000 (0.7118), Acc@5: 0.9062 (0.9028)
2022-01-06 16:23:06,036 Val Step[1500/1563], Loss: 1.9757 (1.2925), Acc@1: 0.5000 (0.7142), Acc@5: 0.8125 (0.9042)
2022-01-06 16:23:07,363 Val Step[1550/1563], Loss: 0.9563 (1.2948), Acc@1: 0.8750 (0.7135), Acc@5: 0.9062 (0.9042)
2022-01-06 16:23:08,087 ----- Epoch[238/310], Validation Loss: 1.2930, Validation Acc@1: 0.7138, Validation Acc@5: 0.9044, time: 102.46
2022-01-06 16:23:08,087 ----- Epoch[238/310], Train Loss: 3.6867, Train Acc: 0.3640, time: 1570.40, Best Val(epoch238) Acc@1: 0.7138
2022-01-06 16:23:08,272 Max accuracy so far: 0.7138 at epoch_238
2022-01-06 16:23:08,272 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-06 16:23:08,272 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-06 16:23:08,360 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-06 16:23:08,836 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 16:23:08,836 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 16:23:09,019 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 16:23:09,020 Now training epoch 239. LR=0.000103
2022-01-06 16:24:26,648 Epoch[239/310], Step[0000/1251], Loss: 3.2850(3.2850), Acc: 0.2344(0.2344)
2022-01-06 16:25:26,879 Epoch[239/310], Step[0050/1251], Loss: 3.4289(3.6336), Acc: 0.4502(0.3923)
2022-01-06 16:26:27,069 Epoch[239/310], Step[0100/1251], Loss: 3.3031(3.6211), Acc: 0.2227(0.3710)
2022-01-06 16:27:26,517 Epoch[239/310], Step[0150/1251], Loss: 3.7723(3.6265), Acc: 0.4463(0.3636)
2022-01-06 16:28:24,867 Epoch[239/310], Step[0200/1251], Loss: 3.6727(3.6240), Acc: 0.3574(0.3696)
2022-01-06 16:29:23,694 Epoch[239/310], Step[0250/1251], Loss: 3.9274(3.6428), Acc: 0.2305(0.3701)
2022-01-06 16:30:22,564 Epoch[239/310], Step[0300/1251], Loss: 3.6385(3.6419), Acc: 0.2529(0.3655)
2022-01-06 16:31:22,353 Epoch[239/310], Step[0350/1251], Loss: 3.2939(3.6541), Acc: 0.5430(0.3658)
2022-01-06 16:32:23,195 Epoch[239/310], Step[0400/1251], Loss: 3.4514(3.6585), Acc: 0.2959(0.3657)
2022-01-06 16:33:22,062 Epoch[239/310], Step[0450/1251], Loss: 3.5957(3.6644), Acc: 0.4990(0.3666)
2022-01-06 16:34:22,110 Epoch[239/310], Step[0500/1251], Loss: 4.3966(3.6615), Acc: 0.2305(0.3693)
2022-01-06 16:35:23,045 Epoch[239/310], Step[0550/1251], Loss: 4.2164(3.6642), Acc: 0.3711(0.3699)
2022-01-06 16:36:24,026 Epoch[239/310], Step[0600/1251], Loss: 3.4057(3.6689), Acc: 0.3301(0.3703)
2022-01-06 16:37:23,824 Epoch[239/310], Step[0650/1251], Loss: 3.7384(3.6708), Acc: 0.3018(0.3697)
2022-01-06 16:38:23,269 Epoch[239/310], Step[0700/1251], Loss: 3.6227(3.6694), Acc: 0.4658(0.3693)
2022-01-06 16:39:22,212 Epoch[239/310], Step[0750/1251], Loss: 3.8378(3.6713), Acc: 0.3193(0.3696)
2022-01-06 16:40:22,514 Epoch[239/310], Step[0800/1251], Loss: 3.5969(3.6736), Acc: 0.2041(0.3670)
2022-01-06 16:41:19,464 Epoch[239/310], Step[0850/1251], Loss: 3.6581(3.6685), Acc: 0.3750(0.3663)
2022-01-06 16:42:18,246 Epoch[239/310], Step[0900/1251], Loss: 3.3322(3.6715), Acc: 0.5381(0.3658)
2022-01-06 16:43:17,071 Epoch[239/310], Step[0950/1251], Loss: 3.6000(3.6712), Acc: 0.4219(0.3650)
2022-01-06 16:44:17,065 Epoch[239/310], Step[1000/1251], Loss: 3.8145(3.6711), Acc: 0.3779(0.3650)
2022-01-06 16:45:16,887 Epoch[239/310], Step[1050/1251], Loss: 3.8633(3.6697), Acc: 0.4902(0.3654)
2022-01-06 16:46:15,219 Epoch[239/310], Step[1100/1251], Loss: 3.8632(3.6715), Acc: 0.4355(0.3645)
2022-01-06 16:47:14,261 Epoch[239/310], Step[1150/1251], Loss: 3.4090(3.6687), Acc: 0.5410(0.3651)
2022-01-06 16:48:12,307 Epoch[239/310], Step[1200/1251], Loss: 3.6136(3.6671), Acc: 0.3330(0.3652)
2022-01-06 16:49:12,057 Epoch[239/310], Step[1250/1251], Loss: 3.9996(3.6642), Acc: 0.4766(0.3664)
2022-01-06 16:49:13,541 ----- Epoch[239/310], Train Loss: 3.6642, Train Acc: 0.3664, time: 1564.52, Best Val(epoch238) Acc@1: 0.7138
2022-01-06 16:49:13,712 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 16:49:13,713 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 16:49:13,818 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 16:49:13,818 Now training epoch 240. LR=0.000100
2022-01-06 16:50:33,941 Epoch[240/310], Step[0000/1251], Loss: 3.5439(3.5439), Acc: 0.4854(0.4854)
2022-01-06 16:51:33,678 Epoch[240/310], Step[0050/1251], Loss: 3.6384(3.5946), Acc: 0.3555(0.3584)
2022-01-06 16:52:32,410 Epoch[240/310], Step[0100/1251], Loss: 2.8880(3.6194), Acc: 0.3105(0.3603)
2022-01-06 16:53:31,900 Epoch[240/310], Step[0150/1251], Loss: 3.5390(3.6084), Acc: 0.5107(0.3639)
2022-01-06 16:54:30,855 Epoch[240/310], Step[0200/1251], Loss: 3.7407(3.6255), Acc: 0.2080(0.3673)
2022-01-06 16:55:30,760 Epoch[240/310], Step[0250/1251], Loss: 3.7278(3.6354), Acc: 0.2832(0.3657)
2022-01-06 16:56:29,280 Epoch[240/310], Step[0300/1251], Loss: 3.3200(3.6345), Acc: 0.4307(0.3674)
2022-01-06 16:57:26,988 Epoch[240/310], Step[0350/1251], Loss: 3.1571(3.6354), Acc: 0.2451(0.3709)
2022-01-06 16:58:25,225 Epoch[240/310], Step[0400/1251], Loss: 3.6710(3.6444), Acc: 0.1865(0.3698)
2022-01-06 16:59:22,417 Epoch[240/310], Step[0450/1251], Loss: 3.9929(3.6468), Acc: 0.1641(0.3709)
2022-01-06 17:00:21,386 Epoch[240/310], Step[0500/1251], Loss: 3.5079(3.6396), Acc: 0.1025(0.3709)
2022-01-06 17:01:21,294 Epoch[240/310], Step[0550/1251], Loss: 2.9864(3.6428), Acc: 0.5938(0.3708)
2022-01-06 17:02:21,718 Epoch[240/310], Step[0600/1251], Loss: 4.2022(3.6540), Acc: 0.3320(0.3678)
2022-01-06 17:03:21,984 Epoch[240/310], Step[0650/1251], Loss: 3.5965(3.6556), Acc: 0.3037(0.3706)
2022-01-06 17:04:21,533 Epoch[240/310], Step[0700/1251], Loss: 3.7114(3.6601), Acc: 0.3154(0.3702)
2022-01-06 17:05:20,519 Epoch[240/310], Step[0750/1251], Loss: 3.5289(3.6636), Acc: 0.4727(0.3685)
2022-01-06 17:06:19,683 Epoch[240/310], Step[0800/1251], Loss: 3.5179(3.6630), Acc: 0.2354(0.3686)
2022-01-06 17:07:18,856 Epoch[240/310], Step[0850/1251], Loss: 3.7054(3.6632), Acc: 0.3906(0.3686)
2022-01-06 17:08:17,220 Epoch[240/310], Step[0900/1251], Loss: 3.4779(3.6664), Acc: 0.4805(0.3687)
2022-01-06 17:09:16,886 Epoch[240/310], Step[0950/1251], Loss: 3.4705(3.6677), Acc: 0.5635(0.3699)
2022-01-06 17:10:17,384 Epoch[240/310], Step[1000/1251], Loss: 3.3495(3.6675), Acc: 0.2949(0.3701)
2022-01-06 17:11:16,645 Epoch[240/310], Step[1050/1251], Loss: 3.6724(3.6676), Acc: 0.2627(0.3697)
2022-01-06 17:12:16,164 Epoch[240/310], Step[1100/1251], Loss: 4.1001(3.6686), Acc: 0.3350(0.3701)
2022-01-06 17:13:16,412 Epoch[240/310], Step[1150/1251], Loss: 3.8832(3.6656), Acc: 0.4150(0.3710)
2022-01-06 17:14:15,049 Epoch[240/310], Step[1200/1251], Loss: 3.5267(3.6649), Acc: 0.4893(0.3714)
2022-01-06 17:15:11,884 Epoch[240/310], Step[1250/1251], Loss: 3.9720(3.6658), Acc: 0.2773(0.3714)
2022-01-06 17:15:13,346 ----- Validation after Epoch: 240
2022-01-06 17:16:16,282 Val Step[0000/1563], Loss: 0.8038 (0.8038), Acc@1: 0.8750 (0.8750), Acc@5: 0.9375 (0.9375)
2022-01-06 17:16:17,706 Val Step[0050/1563], Loss: 2.1354 (0.7714), Acc@1: 0.4375 (0.8431), Acc@5: 0.9062 (0.9583)
2022-01-06 17:16:18,999 Val Step[0100/1563], Loss: 2.1672 (1.0161), Acc@1: 0.4375 (0.7782), Acc@5: 0.8125 (0.9369)
2022-01-06 17:16:20,290 Val Step[0150/1563], Loss: 0.4731 (0.9606), Acc@1: 0.9062 (0.7914), Acc@5: 1.0000 (0.9416)
2022-01-06 17:16:21,579 Val Step[0200/1563], Loss: 1.0265 (0.9701), Acc@1: 0.7812 (0.7935), Acc@5: 0.9062 (0.9400)
2022-01-06 17:16:22,958 Val Step[0250/1563], Loss: 0.4487 (0.9172), Acc@1: 0.9688 (0.8065), Acc@5: 1.0000 (0.9455)
2022-01-06 17:16:24,372 Val Step[0300/1563], Loss: 0.9809 (0.9844), Acc@1: 0.7188 (0.7882), Acc@5: 0.9688 (0.9400)
2022-01-06 17:16:25,779 Val Step[0350/1563], Loss: 1.1908 (0.9980), Acc@1: 0.6562 (0.7812), Acc@5: 0.9062 (0.9407)
2022-01-06 17:16:27,168 Val Step[0400/1563], Loss: 0.8732 (1.0060), Acc@1: 0.8750 (0.7746), Acc@5: 0.9688 (0.9412)
2022-01-06 17:16:28,642 Val Step[0450/1563], Loss: 0.9620 (1.0115), Acc@1: 0.7188 (0.7726), Acc@5: 1.0000 (0.9421)
2022-01-06 17:16:30,178 Val Step[0500/1563], Loss: 0.4204 (1.0032), Acc@1: 0.9375 (0.7747), Acc@5: 1.0000 (0.9431)
2022-01-06 17:16:31,548 Val Step[0550/1563], Loss: 0.9118 (0.9849), Acc@1: 0.7500 (0.7801), Acc@5: 0.9688 (0.9447)
2022-01-06 17:16:32,842 Val Step[0600/1563], Loss: 0.9468 (0.9917), Acc@1: 0.7812 (0.7793), Acc@5: 0.9375 (0.9445)
2022-01-06 17:16:34,146 Val Step[0650/1563], Loss: 0.5209 (1.0131), Acc@1: 0.9062 (0.7748), Acc@5: 1.0000 (0.9415)
2022-01-06 17:16:35,599 Val Step[0700/1563], Loss: 1.0422 (1.0409), Acc@1: 0.8125 (0.7674), Acc@5: 0.9375 (0.9382)
2022-01-06 17:16:37,011 Val Step[0750/1563], Loss: 1.5960 (1.0768), Acc@1: 0.7500 (0.7605), Acc@5: 0.8125 (0.9332)
2022-01-06 17:16:38,415 Val Step[0800/1563], Loss: 0.7961 (1.1159), Acc@1: 0.8438 (0.7509), Acc@5: 1.0000 (0.9282)
2022-01-06 17:16:39,835 Val Step[0850/1563], Loss: 1.2897 (1.1431), Acc@1: 0.6562 (0.7439), Acc@5: 0.9375 (0.9247)
2022-01-06 17:16:41,246 Val Step[0900/1563], Loss: 0.2939 (1.1428), Acc@1: 0.9688 (0.7456), Acc@5: 1.0000 (0.9240)
2022-01-06 17:16:42,712 Val Step[0950/1563], Loss: 1.3483 (1.1631), Acc@1: 0.7188 (0.7415), Acc@5: 0.9062 (0.9207)
2022-01-06 17:16:44,179 Val Step[1000/1563], Loss: 0.6622 (1.1874), Acc@1: 0.9375 (0.7350), Acc@5: 0.9375 (0.9174)
2022-01-06 17:16:45,555 Val Step[1050/1563], Loss: 0.3904 (1.2010), Acc@1: 0.9688 (0.7314), Acc@5: 1.0000 (0.9155)
2022-01-06 17:16:46,934 Val Step[1100/1563], Loss: 0.9296 (1.2150), Acc@1: 0.7812 (0.7284), Acc@5: 0.9688 (0.9135)
2022-01-06 17:16:48,333 Val Step[1150/1563], Loss: 1.3246 (1.2296), Acc@1: 0.7812 (0.7252), Acc@5: 0.8438 (0.9115)
2022-01-06 17:16:49,745 Val Step[1200/1563], Loss: 1.2130 (1.2436), Acc@1: 0.7500 (0.7223), Acc@5: 0.8438 (0.9091)
2022-01-06 17:16:51,146 Val Step[1250/1563], Loss: 0.7633 (1.2545), Acc@1: 0.8750 (0.7202), Acc@5: 0.9375 (0.9075)
2022-01-06 17:16:52,624 Val Step[1300/1563], Loss: 0.7477 (1.2629), Acc@1: 0.9062 (0.7183), Acc@5: 0.9062 (0.9064)
2022-01-06 17:16:54,029 Val Step[1350/1563], Loss: 1.4659 (1.2800), Acc@1: 0.5938 (0.7142), Acc@5: 0.9062 (0.9038)
2022-01-06 17:16:55,340 Val Step[1400/1563], Loss: 1.0507 (1.2871), Acc@1: 0.7812 (0.7123), Acc@5: 0.9375 (0.9030)
2022-01-06 17:16:56,594 Val Step[1450/1563], Loss: 1.3567 (1.2936), Acc@1: 0.7188 (0.7105), Acc@5: 0.9375 (0.9028)
2022-01-06 17:16:57,854 Val Step[1500/1563], Loss: 1.7142 (1.2836), Acc@1: 0.6250 (0.7126), Acc@5: 0.8750 (0.9039)
2022-01-06 17:16:59,146 Val Step[1550/1563], Loss: 0.9214 (1.2857), Acc@1: 0.8750 (0.7119), Acc@5: 0.9062 (0.9038)
2022-01-06 17:16:59,889 ----- Epoch[240/310], Validation Loss: 1.2842, Validation Acc@1: 0.7123, Validation Acc@5: 0.9039, time: 106.54
2022-01-06 17:16:59,889 ----- Epoch[240/310], Train Loss: 3.6658, Train Acc: 0.3714, time: 1559.52, Best Val(epoch238) Acc@1: 0.7138
2022-01-06 17:17:00,050 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-240-Loss-3.662466006694461.pdparams
2022-01-06 17:17:00,050 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-240-Loss-3.662466006694461.pdopt
2022-01-06 17:17:00,091 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-240-Loss-3.662466006694461-EMA.pdparams
2022-01-06 17:17:00,091 Now training epoch 241. LR=0.000097
2022-01-06 17:18:22,834 Epoch[241/310], Step[0000/1251], Loss: 4.0162(4.0162), Acc: 0.3613(0.3613)
2022-01-06 17:19:21,899 Epoch[241/310], Step[0050/1251], Loss: 3.2303(3.6532), Acc: 0.3428(0.3650)
2022-01-06 17:20:21,638 Epoch[241/310], Step[0100/1251], Loss: 4.1394(3.6392), Acc: 0.3613(0.3754)
2022-01-06 17:21:22,603 Epoch[241/310], Step[0150/1251], Loss: 3.0831(3.6397), Acc: 0.3340(0.3797)
2022-01-06 17:22:22,830 Epoch[241/310], Step[0200/1251], Loss: 3.7547(3.6470), Acc: 0.5254(0.3758)
2022-01-06 17:23:22,297 Epoch[241/310], Step[0250/1251], Loss: 3.6912(3.6548), Acc: 0.5332(0.3759)
2022-01-06 17:24:22,537 Epoch[241/310], Step[0300/1251], Loss: 3.6489(3.6509), Acc: 0.3311(0.3751)
2022-01-06 17:25:23,126 Epoch[241/310], Step[0350/1251], Loss: 3.5441(3.6565), Acc: 0.2627(0.3717)
2022-01-06 17:26:23,770 Epoch[241/310], Step[0400/1251], Loss: 3.2380(3.6532), Acc: 0.3906(0.3700)
2022-01-06 17:27:23,022 Epoch[241/310], Step[0450/1251], Loss: 4.3701(3.6579), Acc: 0.2920(0.3671)
2022-01-06 17:28:22,508 Epoch[241/310], Step[0500/1251], Loss: 3.7433(3.6612), Acc: 0.3184(0.3702)
2022-01-06 17:29:22,808 Epoch[241/310], Step[0550/1251], Loss: 4.0432(3.6624), Acc: 0.4199(0.3691)
2022-01-06 17:30:23,366 Epoch[241/310], Step[0600/1251], Loss: 3.3580(3.6658), Acc: 0.3662(0.3686)
2022-01-06 17:31:21,707 Epoch[241/310], Step[0650/1251], Loss: 4.0690(3.6633), Acc: 0.4385(0.3700)
2022-01-06 17:32:21,216 Epoch[241/310], Step[0700/1251], Loss: 3.6735(3.6665), Acc: 0.4229(0.3690)
2022-01-06 17:33:20,070 Epoch[241/310], Step[0750/1251], Loss: 4.3381(3.6630), Acc: 0.3096(0.3710)
2022-01-06 17:34:19,918 Epoch[241/310], Step[0800/1251], Loss: 3.5618(3.6642), Acc: 0.5303(0.3718)
2022-01-06 17:35:20,571 Epoch[241/310], Step[0850/1251], Loss: 3.0069(3.6640), Acc: 0.4531(0.3719)
2022-01-06 17:36:21,277 Epoch[241/310], Step[0900/1251], Loss: 3.3967(3.6608), Acc: 0.5098(0.3719)
2022-01-06 17:37:20,420 Epoch[241/310], Step[0950/1251], Loss: 3.6593(3.6606), Acc: 0.3408(0.3719)
2022-01-06 17:38:20,743 Epoch[241/310], Step[1000/1251], Loss: 4.0759(3.6604), Acc: 0.3389(0.3719)
2022-01-06 17:39:21,064 Epoch[241/310], Step[1050/1251], Loss: 3.3889(3.6587), Acc: 0.2852(0.3721)
2022-01-06 17:40:19,622 Epoch[241/310], Step[1100/1251], Loss: 3.8809(3.6594), Acc: 0.4453(0.3718)
2022-01-06 17:41:20,730 Epoch[241/310], Step[1150/1251], Loss: 3.7509(3.6581), Acc: 0.2490(0.3721)
2022-01-06 17:42:20,546 Epoch[241/310], Step[1200/1251], Loss: 3.6823(3.6576), Acc: 0.2832(0.3728)
2022-01-06 17:43:19,922 Epoch[241/310], Step[1250/1251], Loss: 3.8213(3.6586), Acc: 0.3193(0.3726)
2022-01-06 17:43:22,171 ----- Epoch[241/310], Train Loss: 3.6586, Train Acc: 0.3726, time: 1582.08, Best Val(epoch238) Acc@1: 0.7138
2022-01-06 17:43:22,348 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 17:43:22,349 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 17:43:22,458 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 17:43:22,458 Now training epoch 242. LR=0.000094
2022-01-06 17:44:40,634 Epoch[242/310], Step[0000/1251], Loss: 3.4017(3.4017), Acc: 0.4395(0.4395)
2022-01-06 17:45:39,308 Epoch[242/310], Step[0050/1251], Loss: 3.4158(3.7037), Acc: 0.3779(0.3870)
2022-01-06 17:46:38,427 Epoch[242/310], Step[0100/1251], Loss: 3.9703(3.6794), Acc: 0.1562(0.3820)
2022-01-06 17:47:35,810 Epoch[242/310], Step[0150/1251], Loss: 3.5685(3.6433), Acc: 0.3945(0.3887)
2022-01-06 17:48:34,999 Epoch[242/310], Step[0200/1251], Loss: 3.8926(3.6299), Acc: 0.3418(0.3833)
2022-01-06 17:49:35,011 Epoch[242/310], Step[0250/1251], Loss: 3.6812(3.6293), Acc: 0.5078(0.3841)
2022-01-06 17:50:34,095 Epoch[242/310], Step[0300/1251], Loss: 3.6031(3.6301), Acc: 0.3096(0.3860)
2022-01-06 17:51:32,182 Epoch[242/310], Step[0350/1251], Loss: 3.5298(3.6371), Acc: 0.2852(0.3861)
2022-01-06 17:52:31,028 Epoch[242/310], Step[0400/1251], Loss: 3.0995(3.6363), Acc: 0.4189(0.3870)
2022-01-06 17:53:29,331 Epoch[242/310], Step[0450/1251], Loss: 3.6332(3.6444), Acc: 0.2637(0.3840)
2022-01-06 17:54:28,529 Epoch[242/310], Step[0500/1251], Loss: 3.5911(3.6507), Acc: 0.3760(0.3836)
2022-01-06 17:55:27,818 Epoch[242/310], Step[0550/1251], Loss: 3.2621(3.6498), Acc: 0.2080(0.3834)
2022-01-06 17:56:25,070 Epoch[242/310], Step[0600/1251], Loss: 3.1674(3.6471), Acc: 0.4189(0.3828)
2022-01-06 17:57:23,399 Epoch[242/310], Step[0650/1251], Loss: 3.6408(3.6528), Acc: 0.1084(0.3814)
2022-01-06 17:58:21,794 Epoch[242/310], Step[0700/1251], Loss: 4.2363(3.6567), Acc: 0.3076(0.3794)
2022-01-06 17:59:19,726 Epoch[242/310], Step[0750/1251], Loss: 3.6691(3.6564), Acc: 0.2705(0.3775)
2022-01-06 18:00:18,054 Epoch[242/310], Step[0800/1251], Loss: 3.6021(3.6547), Acc: 0.5010(0.3768)
2022-01-06 18:01:17,467 Epoch[242/310], Step[0850/1251], Loss: 3.3433(3.6567), Acc: 0.3223(0.3768)
2022-01-06 18:02:17,350 Epoch[242/310], Step[0900/1251], Loss: 4.1488(3.6600), Acc: 0.2676(0.3764)
2022-01-06 18:03:17,105 Epoch[242/310], Step[0950/1251], Loss: 3.9406(3.6589), Acc: 0.1201(0.3759)
2022-01-06 18:04:16,183 Epoch[242/310], Step[1000/1251], Loss: 3.9282(3.6586), Acc: 0.5146(0.3755)
2022-01-06 18:05:15,653 Epoch[242/310], Step[1050/1251], Loss: 3.8116(3.6556), Acc: 0.2510(0.3755)
2022-01-06 18:06:15,469 Epoch[242/310], Step[1100/1251], Loss: 3.9544(3.6559), Acc: 0.3496(0.3749)
2022-01-06 18:07:13,759 Epoch[242/310], Step[1150/1251], Loss: 3.4184(3.6568), Acc: 0.5957(0.3750)
2022-01-06 18:08:14,750 Epoch[242/310], Step[1200/1251], Loss: 4.1321(3.6573), Acc: 0.4287(0.3756)
2022-01-06 18:09:14,311 Epoch[242/310], Step[1250/1251], Loss: 3.8849(3.6580), Acc: 0.3281(0.3751)
2022-01-06 18:09:15,703 ----- Validation after Epoch: 242
2022-01-06 18:10:19,055 Val Step[0000/1563], Loss: 0.6792 (0.6792), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 18:10:20,520 Val Step[0050/1563], Loss: 2.2079 (0.7384), Acc@1: 0.4375 (0.8462), Acc@5: 0.8750 (0.9596)
2022-01-06 18:10:21,982 Val Step[0100/1563], Loss: 1.9182 (1.0181), Acc@1: 0.5312 (0.7720), Acc@5: 0.8438 (0.9372)
2022-01-06 18:10:23,426 Val Step[0150/1563], Loss: 0.4254 (0.9630), Acc@1: 0.9375 (0.7873), Acc@5: 1.0000 (0.9414)
2022-01-06 18:10:24,818 Val Step[0200/1563], Loss: 1.0443 (0.9636), Acc@1: 0.8125 (0.7903), Acc@5: 0.9062 (0.9401)
2022-01-06 18:10:26,082 Val Step[0250/1563], Loss: 0.5623 (0.9126), Acc@1: 0.9375 (0.8030), Acc@5: 1.0000 (0.9450)
2022-01-06 18:10:27,351 Val Step[0300/1563], Loss: 0.9835 (0.9761), Acc@1: 0.7500 (0.7860), Acc@5: 1.0000 (0.9398)
2022-01-06 18:10:28,634 Val Step[0350/1563], Loss: 0.9957 (0.9865), Acc@1: 0.8125 (0.7820), Acc@5: 0.9062 (0.9414)
2022-01-06 18:10:30,106 Val Step[0400/1563], Loss: 0.8736 (0.9977), Acc@1: 0.8438 (0.7756), Acc@5: 0.9688 (0.9416)
2022-01-06 18:10:31,412 Val Step[0450/1563], Loss: 0.9434 (1.0032), Acc@1: 0.7188 (0.7729), Acc@5: 1.0000 (0.9420)
2022-01-06 18:10:32,716 Val Step[0500/1563], Loss: 0.3519 (0.9943), Acc@1: 0.9688 (0.7756), Acc@5: 1.0000 (0.9432)
2022-01-06 18:10:34,112 Val Step[0550/1563], Loss: 0.8237 (0.9708), Acc@1: 0.8438 (0.7820), Acc@5: 0.9375 (0.9450)
2022-01-06 18:10:35,388 Val Step[0600/1563], Loss: 0.8133 (0.9793), Acc@1: 0.8125 (0.7807), Acc@5: 0.9062 (0.9441)
2022-01-06 18:10:36,664 Val Step[0650/1563], Loss: 0.7337 (1.0025), Acc@1: 0.8750 (0.7749), Acc@5: 1.0000 (0.9412)
2022-01-06 18:10:37,939 Val Step[0700/1563], Loss: 0.7794 (1.0307), Acc@1: 0.8438 (0.7679), Acc@5: 1.0000 (0.9379)
2022-01-06 18:10:39,222 Val Step[0750/1563], Loss: 1.2142 (1.0637), Acc@1: 0.7812 (0.7615), Acc@5: 0.9062 (0.9333)
2022-01-06 18:10:40,491 Val Step[0800/1563], Loss: 0.7218 (1.0989), Acc@1: 0.7812 (0.7528), Acc@5: 1.0000 (0.9284)
2022-01-06 18:10:41,795 Val Step[0850/1563], Loss: 1.2760 (1.1232), Acc@1: 0.6562 (0.7469), Acc@5: 0.9375 (0.9255)
2022-01-06 18:10:43,069 Val Step[0900/1563], Loss: 0.2937 (1.1231), Acc@1: 0.9688 (0.7485), Acc@5: 1.0000 (0.9247)
2022-01-06 18:10:44,472 Val Step[0950/1563], Loss: 1.2701 (1.1429), Acc@1: 0.7500 (0.7445), Acc@5: 0.9375 (0.9216)
2022-01-06 18:10:45,809 Val Step[1000/1563], Loss: 0.6034 (1.1655), Acc@1: 0.9375 (0.7388), Acc@5: 0.9688 (0.9186)
2022-01-06 18:10:47,107 Val Step[1050/1563], Loss: 0.3078 (1.1779), Acc@1: 0.9688 (0.7355), Acc@5: 1.0000 (0.9172)
2022-01-06 18:10:48,381 Val Step[1100/1563], Loss: 0.6892 (1.1921), Acc@1: 0.9062 (0.7327), Acc@5: 1.0000 (0.9152)
2022-01-06 18:10:49,744 Val Step[1150/1563], Loss: 1.3506 (1.2056), Acc@1: 0.7812 (0.7305), Acc@5: 0.8125 (0.9134)
2022-01-06 18:10:51,099 Val Step[1200/1563], Loss: 1.1548 (1.2198), Acc@1: 0.8125 (0.7272), Acc@5: 0.8438 (0.9111)
2022-01-06 18:10:52,380 Val Step[1250/1563], Loss: 0.7780 (1.2314), Acc@1: 0.8750 (0.7254), Acc@5: 0.9375 (0.9094)
2022-01-06 18:10:53,767 Val Step[1300/1563], Loss: 0.8129 (1.2400), Acc@1: 0.8438 (0.7236), Acc@5: 0.9062 (0.9084)
2022-01-06 18:10:55,047 Val Step[1350/1563], Loss: 2.0042 (1.2571), Acc@1: 0.4375 (0.7196), Acc@5: 0.8438 (0.9060)
2022-01-06 18:10:56,297 Val Step[1400/1563], Loss: 1.0326 (1.2637), Acc@1: 0.7812 (0.7179), Acc@5: 0.9688 (0.9051)
2022-01-06 18:10:57,542 Val Step[1450/1563], Loss: 1.7163 (1.2705), Acc@1: 0.5312 (0.7159), Acc@5: 0.8750 (0.9048)
2022-01-06 18:10:58,840 Val Step[1500/1563], Loss: 1.9454 (1.2592), Acc@1: 0.5312 (0.7185), Acc@5: 0.7812 (0.9062)
2022-01-06 18:11:00,155 Val Step[1550/1563], Loss: 0.8206 (1.2609), Acc@1: 0.8750 (0.7180), Acc@5: 0.9062 (0.9059)
2022-01-06 18:11:00,940 ----- Epoch[242/310], Validation Loss: 1.2594, Validation Acc@1: 0.7183, Validation Acc@5: 0.9061, time: 105.23
2022-01-06 18:11:00,940 ----- Epoch[242/310], Train Loss: 3.6580, Train Acc: 0.3751, time: 1553.24, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 18:11:01,122 Max accuracy so far: 0.7183 at epoch_242
2022-01-06 18:11:01,122 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-06 18:11:01,122 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-06 18:11:01,228 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-06 18:11:01,611 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 18:11:01,611 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 18:11:01,757 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 18:11:01,758 Now training epoch 243. LR=0.000091
2022-01-06 18:12:25,630 Epoch[243/310], Step[0000/1251], Loss: 3.5626(3.5626), Acc: 0.5381(0.5381)
2022-01-06 18:13:23,892 Epoch[243/310], Step[0050/1251], Loss: 4.0426(3.6556), Acc: 0.3340(0.3432)
2022-01-06 18:14:21,382 Epoch[243/310], Step[0100/1251], Loss: 3.4251(3.6821), Acc: 0.2598(0.3551)
2022-01-06 18:15:20,193 Epoch[243/310], Step[0150/1251], Loss: 2.9083(3.6721), Acc: 0.3242(0.3569)
2022-01-06 18:16:19,088 Epoch[243/310], Step[0200/1251], Loss: 4.0065(3.6531), Acc: 0.2432(0.3632)
2022-01-06 18:17:17,186 Epoch[243/310], Step[0250/1251], Loss: 3.6275(3.6516), Acc: 0.4717(0.3690)
2022-01-06 18:18:15,971 Epoch[243/310], Step[0300/1251], Loss: 3.8079(3.6491), Acc: 0.4668(0.3721)
2022-01-06 18:19:14,170 Epoch[243/310], Step[0350/1251], Loss: 3.1186(3.6476), Acc: 0.2549(0.3718)
2022-01-06 18:20:13,439 Epoch[243/310], Step[0400/1251], Loss: 3.5196(3.6544), Acc: 0.0293(0.3679)
2022-01-06 18:21:12,910 Epoch[243/310], Step[0450/1251], Loss: 3.4539(3.6571), Acc: 0.3457(0.3688)
2022-01-06 18:22:12,953 Epoch[243/310], Step[0500/1251], Loss: 3.3191(3.6538), Acc: 0.3867(0.3701)
2022-01-06 18:23:12,644 Epoch[243/310], Step[0550/1251], Loss: 3.7120(3.6547), Acc: 0.4707(0.3700)
2022-01-06 18:24:13,526 Epoch[243/310], Step[0600/1251], Loss: 3.6761(3.6528), Acc: 0.4883(0.3693)
2022-01-06 18:25:13,487 Epoch[243/310], Step[0650/1251], Loss: 3.4935(3.6563), Acc: 0.4248(0.3702)
2022-01-06 18:26:13,576 Epoch[243/310], Step[0700/1251], Loss: 3.3640(3.6545), Acc: 0.3467(0.3719)
2022-01-06 18:27:13,471 Epoch[243/310], Step[0750/1251], Loss: 3.4303(3.6574), Acc: 0.3623(0.3714)
2022-01-06 18:28:14,273 Epoch[243/310], Step[0800/1251], Loss: 3.7011(3.6550), Acc: 0.5166(0.3712)
2022-01-06 18:29:13,056 Epoch[243/310], Step[0850/1251], Loss: 3.7979(3.6559), Acc: 0.3594(0.3718)
2022-01-06 18:30:13,629 Epoch[243/310], Step[0900/1251], Loss: 3.7874(3.6568), Acc: 0.2334(0.3693)
2022-01-06 18:31:12,770 Epoch[243/310], Step[0950/1251], Loss: 3.5897(3.6642), Acc: 0.2686(0.3674)
2022-01-06 18:32:12,166 Epoch[243/310], Step[1000/1251], Loss: 4.0118(3.6654), Acc: 0.1826(0.3672)
2022-01-06 18:33:12,404 Epoch[243/310], Step[1050/1251], Loss: 3.3402(3.6668), Acc: 0.5791(0.3664)
2022-01-06 18:34:11,847 Epoch[243/310], Step[1100/1251], Loss: 3.8813(3.6697), Acc: 0.3447(0.3664)
2022-01-06 18:35:12,730 Epoch[243/310], Step[1150/1251], Loss: 4.1798(3.6678), Acc: 0.2441(0.3660)
2022-01-06 18:36:13,197 Epoch[243/310], Step[1200/1251], Loss: 3.7359(3.6698), Acc: 0.3779(0.3663)
2022-01-06 18:37:12,816 Epoch[243/310], Step[1250/1251], Loss: 3.7550(3.6675), Acc: 0.3408(0.3668)
2022-01-06 18:37:14,258 ----- Epoch[243/310], Train Loss: 3.6675, Train Acc: 0.3668, time: 1572.50, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 18:37:14,431 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 18:37:14,432 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 18:37:14,536 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 18:37:14,537 Now training epoch 244. LR=0.000088
2022-01-06 18:38:36,549 Epoch[244/310], Step[0000/1251], Loss: 3.8800(3.8800), Acc: 0.2217(0.2217)
2022-01-06 18:39:35,844 Epoch[244/310], Step[0050/1251], Loss: 3.2952(3.6397), Acc: 0.3174(0.3797)
2022-01-06 18:40:36,694 Epoch[244/310], Step[0100/1251], Loss: 2.8937(3.6465), Acc: 0.4736(0.3813)
2022-01-06 18:41:37,357 Epoch[244/310], Step[0150/1251], Loss: 3.5534(3.6344), Acc: 0.2861(0.3780)
2022-01-06 18:42:37,015 Epoch[244/310], Step[0200/1251], Loss: 4.0249(3.6352), Acc: 0.3477(0.3830)
2022-01-06 18:43:38,152 Epoch[244/310], Step[0250/1251], Loss: 3.4935(3.6434), Acc: 0.3984(0.3788)
2022-01-06 18:44:38,291 Epoch[244/310], Step[0300/1251], Loss: 3.7936(3.6471), Acc: 0.3652(0.3788)
2022-01-06 18:45:38,446 Epoch[244/310], Step[0350/1251], Loss: 3.2581(3.6403), Acc: 0.2285(0.3798)
2022-01-06 18:46:38,270 Epoch[244/310], Step[0400/1251], Loss: 3.6783(3.6452), Acc: 0.4561(0.3775)
2022-01-06 18:47:38,010 Epoch[244/310], Step[0450/1251], Loss: 3.4143(3.6411), Acc: 0.3281(0.3766)
2022-01-06 18:48:37,104 Epoch[244/310], Step[0500/1251], Loss: 3.6292(3.6462), Acc: 0.5449(0.3785)
2022-01-06 18:49:37,202 Epoch[244/310], Step[0550/1251], Loss: 4.3734(3.6460), Acc: 0.2744(0.3786)
2022-01-06 18:50:38,120 Epoch[244/310], Step[0600/1251], Loss: 4.1799(3.6463), Acc: 0.3408(0.3780)
2022-01-06 18:51:37,249 Epoch[244/310], Step[0650/1251], Loss: 3.2876(3.6463), Acc: 0.5361(0.3774)
2022-01-06 18:52:36,901 Epoch[244/310], Step[0700/1251], Loss: 3.3581(3.6468), Acc: 0.3350(0.3773)
2022-01-06 18:53:36,317 Epoch[244/310], Step[0750/1251], Loss: 3.7223(3.6506), Acc: 0.1914(0.3769)
2022-01-06 18:54:36,321 Epoch[244/310], Step[0800/1251], Loss: 3.4578(3.6527), Acc: 0.0361(0.3747)
2022-01-06 18:55:36,051 Epoch[244/310], Step[0850/1251], Loss: 3.9544(3.6510), Acc: 0.3535(0.3746)
2022-01-06 18:56:35,438 Epoch[244/310], Step[0900/1251], Loss: 3.8633(3.6457), Acc: 0.2012(0.3735)
2022-01-06 18:57:34,821 Epoch[244/310], Step[0950/1251], Loss: 4.3110(3.6478), Acc: 0.3057(0.3731)
2022-01-06 18:58:34,686 Epoch[244/310], Step[1000/1251], Loss: 3.5520(3.6473), Acc: 0.4482(0.3723)
2022-01-06 18:59:33,440 Epoch[244/310], Step[1050/1251], Loss: 3.8635(3.6498), Acc: 0.4600(0.3727)
2022-01-06 19:00:33,225 Epoch[244/310], Step[1100/1251], Loss: 3.8523(3.6529), Acc: 0.4463(0.3722)
2022-01-06 19:01:32,320 Epoch[244/310], Step[1150/1251], Loss: 3.8205(3.6528), Acc: 0.4189(0.3720)
2022-01-06 19:02:30,559 Epoch[244/310], Step[1200/1251], Loss: 3.4237(3.6522), Acc: 0.5244(0.3725)
2022-01-06 19:03:29,491 Epoch[244/310], Step[1250/1251], Loss: 3.9998(3.6535), Acc: 0.3525(0.3720)
2022-01-06 19:03:30,981 ----- Validation after Epoch: 244
2022-01-06 19:04:39,430 Val Step[0000/1563], Loss: 0.7328 (0.7328), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 19:04:40,791 Val Step[0050/1563], Loss: 2.1090 (0.7545), Acc@1: 0.4062 (0.8431), Acc@5: 0.9062 (0.9602)
2022-01-06 19:04:42,056 Val Step[0100/1563], Loss: 2.0191 (1.0253), Acc@1: 0.5000 (0.7720), Acc@5: 0.8438 (0.9350)
2022-01-06 19:04:43,374 Val Step[0150/1563], Loss: 0.4781 (0.9683), Acc@1: 0.9062 (0.7858), Acc@5: 1.0000 (0.9398)
2022-01-06 19:04:44,769 Val Step[0200/1563], Loss: 0.9611 (0.9736), Acc@1: 0.7812 (0.7892), Acc@5: 0.9062 (0.9380)
2022-01-06 19:04:46,155 Val Step[0250/1563], Loss: 0.3863 (0.9187), Acc@1: 0.9688 (0.8010), Acc@5: 1.0000 (0.9436)
2022-01-06 19:04:47,534 Val Step[0300/1563], Loss: 0.9344 (0.9785), Acc@1: 0.7188 (0.7835), Acc@5: 0.9688 (0.9390)
2022-01-06 19:04:48,927 Val Step[0350/1563], Loss: 1.0134 (0.9865), Acc@1: 0.7500 (0.7784), Acc@5: 0.9062 (0.9403)
2022-01-06 19:04:50,334 Val Step[0400/1563], Loss: 0.9227 (0.9953), Acc@1: 0.8438 (0.7740), Acc@5: 0.9688 (0.9405)
2022-01-06 19:04:51,743 Val Step[0450/1563], Loss: 0.7712 (0.9984), Acc@1: 0.8125 (0.7722), Acc@5: 1.0000 (0.9417)
2022-01-06 19:04:53,147 Val Step[0500/1563], Loss: 0.4809 (0.9896), Acc@1: 0.9062 (0.7748), Acc@5: 1.0000 (0.9431)
2022-01-06 19:04:54,639 Val Step[0550/1563], Loss: 0.8304 (0.9693), Acc@1: 0.8438 (0.7803), Acc@5: 0.9688 (0.9445)
2022-01-06 19:04:56,078 Val Step[0600/1563], Loss: 0.7170 (0.9758), Acc@1: 0.8438 (0.7790), Acc@5: 0.9688 (0.9444)
2022-01-06 19:04:57,562 Val Step[0650/1563], Loss: 0.6142 (0.9971), Acc@1: 0.8438 (0.7742), Acc@5: 1.0000 (0.9412)
2022-01-06 19:04:59,021 Val Step[0700/1563], Loss: 0.8651 (1.0254), Acc@1: 0.8750 (0.7676), Acc@5: 0.9375 (0.9383)
2022-01-06 19:05:00,474 Val Step[0750/1563], Loss: 1.2855 (1.0596), Acc@1: 0.7812 (0.7606), Acc@5: 0.9062 (0.9334)
2022-01-06 19:05:01,914 Val Step[0800/1563], Loss: 0.8089 (1.0955), Acc@1: 0.7500 (0.7519), Acc@5: 1.0000 (0.9286)
2022-01-06 19:05:03,319 Val Step[0850/1563], Loss: 1.2341 (1.1199), Acc@1: 0.6562 (0.7456), Acc@5: 0.9375 (0.9255)
2022-01-06 19:05:04,746 Val Step[0900/1563], Loss: 0.2174 (1.1191), Acc@1: 1.0000 (0.7471), Acc@5: 1.0000 (0.9250)
2022-01-06 19:05:06,169 Val Step[0950/1563], Loss: 1.2611 (1.1381), Acc@1: 0.7500 (0.7437), Acc@5: 0.9375 (0.9220)
2022-01-06 19:05:07,426 Val Step[1000/1563], Loss: 0.6859 (1.1612), Acc@1: 0.9062 (0.7382), Acc@5: 1.0000 (0.9190)
2022-01-06 19:05:08,760 Val Step[1050/1563], Loss: 0.2980 (1.1768), Acc@1: 0.9688 (0.7343), Acc@5: 1.0000 (0.9169)
2022-01-06 19:05:10,010 Val Step[1100/1563], Loss: 0.7321 (1.1915), Acc@1: 0.8750 (0.7313), Acc@5: 0.9688 (0.9147)
2022-01-06 19:05:11,269 Val Step[1150/1563], Loss: 1.2101 (1.2043), Acc@1: 0.7812 (0.7289), Acc@5: 0.8125 (0.9129)
2022-01-06 19:05:12,634 Val Step[1200/1563], Loss: 1.0951 (1.2184), Acc@1: 0.8125 (0.7257), Acc@5: 0.8438 (0.9107)
2022-01-06 19:05:13,906 Val Step[1250/1563], Loss: 0.6652 (1.2298), Acc@1: 0.8750 (0.7238), Acc@5: 0.9375 (0.9087)
2022-01-06 19:05:15,179 Val Step[1300/1563], Loss: 0.8032 (1.2378), Acc@1: 0.9062 (0.7222), Acc@5: 0.9062 (0.9076)
2022-01-06 19:05:16,441 Val Step[1350/1563], Loss: 1.7246 (1.2551), Acc@1: 0.4688 (0.7180), Acc@5: 0.8125 (0.9050)
2022-01-06 19:05:17,698 Val Step[1400/1563], Loss: 1.0905 (1.2612), Acc@1: 0.7500 (0.7167), Acc@5: 0.9688 (0.9040)
2022-01-06 19:05:19,010 Val Step[1450/1563], Loss: 1.3067 (1.2682), Acc@1: 0.6875 (0.7141), Acc@5: 0.9375 (0.9036)
2022-01-06 19:05:20,365 Val Step[1500/1563], Loss: 1.6966 (1.2581), Acc@1: 0.6250 (0.7163), Acc@5: 0.8750 (0.9050)
2022-01-06 19:05:21,623 Val Step[1550/1563], Loss: 0.8938 (1.2601), Acc@1: 0.8750 (0.7153), Acc@5: 0.9062 (0.9046)
2022-01-06 19:05:22,366 ----- Epoch[244/310], Validation Loss: 1.2587, Validation Acc@1: 0.7155, Validation Acc@5: 0.9047, time: 111.38
2022-01-06 19:05:22,366 ----- Epoch[244/310], Train Loss: 3.6535, Train Acc: 0.3720, time: 1576.44, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 19:05:22,546 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 19:05:22,547 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 19:05:22,885 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 19:05:22,886 Now training epoch 245. LR=0.000085
2022-01-06 19:06:48,272 Epoch[245/310], Step[0000/1251], Loss: 3.3505(3.3505), Acc: 0.4375(0.4375)
2022-01-06 19:07:47,184 Epoch[245/310], Step[0050/1251], Loss: 3.5853(3.6120), Acc: 0.3838(0.3800)
2022-01-06 19:08:46,840 Epoch[245/310], Step[0100/1251], Loss: 3.7821(3.6777), Acc: 0.5195(0.3686)
2022-01-06 19:09:46,595 Epoch[245/310], Step[0150/1251], Loss: 3.2639(3.6703), Acc: 0.6035(0.3673)
2022-01-06 19:10:44,958 Epoch[245/310], Step[0200/1251], Loss: 3.4980(3.6683), Acc: 0.4990(0.3707)
2022-01-06 19:11:44,578 Epoch[245/310], Step[0250/1251], Loss: 3.3135(3.6630), Acc: 0.5605(0.3760)
2022-01-06 19:12:43,268 Epoch[245/310], Step[0300/1251], Loss: 3.4553(3.6517), Acc: 0.3975(0.3773)
2022-01-06 19:13:41,867 Epoch[245/310], Step[0350/1251], Loss: 3.8837(3.6505), Acc: 0.3379(0.3739)
2022-01-06 19:14:40,373 Epoch[245/310], Step[0400/1251], Loss: 3.1191(3.6595), Acc: 0.4531(0.3739)
2022-01-06 19:15:39,826 Epoch[245/310], Step[0450/1251], Loss: 3.6575(3.6581), Acc: 0.3545(0.3709)
2022-01-06 19:16:40,010 Epoch[245/310], Step[0500/1251], Loss: 3.8095(3.6662), Acc: 0.3896(0.3708)
2022-01-06 19:17:38,501 Epoch[245/310], Step[0550/1251], Loss: 3.7332(3.6672), Acc: 0.4727(0.3721)
2022-01-06 19:18:38,264 Epoch[245/310], Step[0600/1251], Loss: 3.9169(3.6682), Acc: 0.3428(0.3716)
2022-01-06 19:19:38,756 Epoch[245/310], Step[0650/1251], Loss: 3.6028(3.6647), Acc: 0.2549(0.3709)
2022-01-06 19:20:38,324 Epoch[245/310], Step[0700/1251], Loss: 3.8285(3.6588), Acc: 0.4531(0.3700)
2022-01-06 19:21:36,851 Epoch[245/310], Step[0750/1251], Loss: 3.4927(3.6552), Acc: 0.5107(0.3684)
2022-01-06 19:22:36,144 Epoch[245/310], Step[0800/1251], Loss: 3.3691(3.6567), Acc: 0.2471(0.3680)
2022-01-06 19:23:35,357 Epoch[245/310], Step[0850/1251], Loss: 3.8176(3.6580), Acc: 0.3975(0.3677)
2022-01-06 19:24:35,129 Epoch[245/310], Step[0900/1251], Loss: 3.8820(3.6571), Acc: 0.4336(0.3689)
2022-01-06 19:25:33,459 Epoch[245/310], Step[0950/1251], Loss: 4.2339(3.6572), Acc: 0.3115(0.3677)
2022-01-06 19:26:31,455 Epoch[245/310], Step[1000/1251], Loss: 3.9512(3.6600), Acc: 0.2139(0.3679)
2022-01-06 19:27:31,188 Epoch[245/310], Step[1050/1251], Loss: 3.9674(3.6574), Acc: 0.2627(0.3682)
2022-01-06 19:28:30,685 Epoch[245/310], Step[1100/1251], Loss: 3.8725(3.6571), Acc: 0.3350(0.3686)
2022-01-06 19:29:29,972 Epoch[245/310], Step[1150/1251], Loss: 3.5556(3.6533), Acc: 0.5488(0.3689)
2022-01-06 19:30:30,361 Epoch[245/310], Step[1200/1251], Loss: 3.8115(3.6553), Acc: 0.2168(0.3692)
2022-01-06 19:31:30,616 Epoch[245/310], Step[1250/1251], Loss: 3.8529(3.6557), Acc: 0.2744(0.3690)
2022-01-06 19:31:32,948 ----- Epoch[245/310], Train Loss: 3.6557, Train Acc: 0.3690, time: 1570.06, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 19:31:33,140 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 19:31:33,140 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 19:31:33,231 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 19:31:33,231 Now training epoch 246. LR=0.000082
2022-01-06 19:33:01,319 Epoch[246/310], Step[0000/1251], Loss: 3.5672(3.5672), Acc: 0.3916(0.3916)
2022-01-06 19:33:58,998 Epoch[246/310], Step[0050/1251], Loss: 3.3390(3.6376), Acc: 0.2090(0.3689)
2022-01-06 19:34:55,730 Epoch[246/310], Step[0100/1251], Loss: 3.7625(3.6139), Acc: 0.4092(0.3885)
2022-01-06 19:35:54,105 Epoch[246/310], Step[0150/1251], Loss: 3.5507(3.6273), Acc: 0.3311(0.3878)
2022-01-06 19:36:50,630 Epoch[246/310], Step[0200/1251], Loss: 3.9298(3.6446), Acc: 0.4463(0.3801)
2022-01-06 19:37:48,309 Epoch[246/310], Step[0250/1251], Loss: 3.8771(3.6634), Acc: 0.4531(0.3813)
2022-01-06 19:38:48,472 Epoch[246/310], Step[0300/1251], Loss: 3.5034(3.6554), Acc: 0.3232(0.3846)
2022-01-06 19:39:47,172 Epoch[246/310], Step[0350/1251], Loss: 4.1168(3.6552), Acc: 0.1406(0.3845)
2022-01-06 19:40:45,538 Epoch[246/310], Step[0400/1251], Loss: 3.6473(3.6555), Acc: 0.5635(0.3860)
2022-01-06 19:41:45,726 Epoch[246/310], Step[0450/1251], Loss: 3.9572(3.6548), Acc: 0.3379(0.3859)
2022-01-06 19:42:44,611 Epoch[246/310], Step[0500/1251], Loss: 3.5660(3.6559), Acc: 0.4951(0.3831)
2022-01-06 19:43:42,157 Epoch[246/310], Step[0550/1251], Loss: 3.7277(3.6460), Acc: 0.4707(0.3851)
2022-01-06 19:44:41,489 Epoch[246/310], Step[0600/1251], Loss: 3.8542(3.6455), Acc: 0.2949(0.3847)
2022-01-06 19:45:41,019 Epoch[246/310], Step[0650/1251], Loss: 3.7164(3.6448), Acc: 0.1133(0.3829)
2022-01-06 19:46:41,471 Epoch[246/310], Step[0700/1251], Loss: 3.9658(3.6489), Acc: 0.3145(0.3813)
2022-01-06 19:47:41,391 Epoch[246/310], Step[0750/1251], Loss: 3.7355(3.6497), Acc: 0.1533(0.3806)
2022-01-06 19:48:41,301 Epoch[246/310], Step[0800/1251], Loss: 3.8401(3.6486), Acc: 0.4482(0.3817)
2022-01-06 19:49:41,353 Epoch[246/310], Step[0850/1251], Loss: 3.7703(3.6497), Acc: 0.4355(0.3811)
2022-01-06 19:50:39,672 Epoch[246/310], Step[0900/1251], Loss: 4.1439(3.6424), Acc: 0.3828(0.3825)
2022-01-06 19:51:37,820 Epoch[246/310], Step[0950/1251], Loss: 3.4137(3.6407), Acc: 0.2881(0.3817)
2022-01-06 19:52:36,402 Epoch[246/310], Step[1000/1251], Loss: 3.5837(3.6406), Acc: 0.1914(0.3816)
2022-01-06 19:53:36,162 Epoch[246/310], Step[1050/1251], Loss: 3.5217(3.6420), Acc: 0.3818(0.3810)
2022-01-06 19:54:37,302 Epoch[246/310], Step[1100/1251], Loss: 3.7514(3.6438), Acc: 0.4395(0.3806)
2022-01-06 19:55:37,440 Epoch[246/310], Step[1150/1251], Loss: 4.4104(3.6463), Acc: 0.3477(0.3792)
2022-01-06 19:56:38,966 Epoch[246/310], Step[1200/1251], Loss: 3.9956(3.6458), Acc: 0.2988(0.3788)
2022-01-06 19:57:38,718 Epoch[246/310], Step[1250/1251], Loss: 3.6208(3.6443), Acc: 0.3457(0.3787)
2022-01-06 19:57:40,871 ----- Validation after Epoch: 246
2022-01-06 19:58:51,856 Val Step[0000/1563], Loss: 0.7485 (0.7485), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 19:58:53,217 Val Step[0050/1563], Loss: 2.2816 (0.7550), Acc@1: 0.4688 (0.8480), Acc@5: 0.8125 (0.9571)
2022-01-06 19:58:54,478 Val Step[0100/1563], Loss: 1.8803 (1.0233), Acc@1: 0.5000 (0.7735), Acc@5: 0.8438 (0.9375)
2022-01-06 19:58:55,798 Val Step[0150/1563], Loss: 0.4885 (0.9709), Acc@1: 0.8750 (0.7862), Acc@5: 1.0000 (0.9406)
2022-01-06 19:58:57,203 Val Step[0200/1563], Loss: 1.2615 (0.9827), Acc@1: 0.8125 (0.7890), Acc@5: 0.9062 (0.9391)
2022-01-06 19:58:58,615 Val Step[0250/1563], Loss: 0.5481 (0.9291), Acc@1: 0.9688 (0.7994), Acc@5: 1.0000 (0.9447)
2022-01-06 19:59:00,083 Val Step[0300/1563], Loss: 1.1542 (0.9837), Acc@1: 0.6562 (0.7841), Acc@5: 0.9688 (0.9411)
2022-01-06 19:59:01,518 Val Step[0350/1563], Loss: 0.9969 (0.9894), Acc@1: 0.7188 (0.7804), Acc@5: 0.9062 (0.9431)
2022-01-06 19:59:02,925 Val Step[0400/1563], Loss: 1.0189 (1.0000), Acc@1: 0.8438 (0.7745), Acc@5: 0.9688 (0.9422)
2022-01-06 19:59:04,218 Val Step[0450/1563], Loss: 1.1368 (1.0067), Acc@1: 0.6875 (0.7713), Acc@5: 1.0000 (0.9431)
2022-01-06 19:59:05,531 Val Step[0500/1563], Loss: 0.4446 (0.9975), Acc@1: 0.9688 (0.7745), Acc@5: 1.0000 (0.9442)
2022-01-06 19:59:06,944 Val Step[0550/1563], Loss: 0.7900 (0.9775), Acc@1: 0.8125 (0.7798), Acc@5: 0.9688 (0.9457)
2022-01-06 19:59:08,297 Val Step[0600/1563], Loss: 0.8334 (0.9842), Acc@1: 0.7812 (0.7784), Acc@5: 0.9375 (0.9454)
2022-01-06 19:59:09,773 Val Step[0650/1563], Loss: 0.6361 (1.0041), Acc@1: 0.8125 (0.7737), Acc@5: 1.0000 (0.9426)
2022-01-06 19:59:11,212 Val Step[0700/1563], Loss: 1.0303 (1.0310), Acc@1: 0.8125 (0.7673), Acc@5: 0.9688 (0.9393)
2022-01-06 19:59:12,640 Val Step[0750/1563], Loss: 1.4887 (1.0643), Acc@1: 0.6562 (0.7609), Acc@5: 0.9062 (0.9346)
2022-01-06 19:59:14,101 Val Step[0800/1563], Loss: 0.9789 (1.1018), Acc@1: 0.7812 (0.7516), Acc@5: 1.0000 (0.9295)
2022-01-06 19:59:15,372 Val Step[0850/1563], Loss: 1.2766 (1.1249), Acc@1: 0.6250 (0.7456), Acc@5: 0.9375 (0.9266)
2022-01-06 19:59:16,751 Val Step[0900/1563], Loss: 0.2991 (1.1260), Acc@1: 0.9688 (0.7466), Acc@5: 1.0000 (0.9258)
2022-01-06 19:59:18,138 Val Step[0950/1563], Loss: 1.1874 (1.1469), Acc@1: 0.7500 (0.7422), Acc@5: 0.9062 (0.9225)
2022-01-06 19:59:19,434 Val Step[1000/1563], Loss: 0.5932 (1.1700), Acc@1: 0.9375 (0.7361), Acc@5: 1.0000 (0.9193)
2022-01-06 19:59:20,718 Val Step[1050/1563], Loss: 0.2654 (1.1848), Acc@1: 0.9688 (0.7325), Acc@5: 1.0000 (0.9175)
2022-01-06 19:59:21,997 Val Step[1100/1563], Loss: 0.8939 (1.1979), Acc@1: 0.7500 (0.7298), Acc@5: 0.9688 (0.9154)
2022-01-06 19:59:23,258 Val Step[1150/1563], Loss: 1.4312 (1.2120), Acc@1: 0.7812 (0.7275), Acc@5: 0.7812 (0.9133)
2022-01-06 19:59:24,563 Val Step[1200/1563], Loss: 1.4427 (1.2265), Acc@1: 0.7500 (0.7243), Acc@5: 0.8438 (0.9109)
2022-01-06 19:59:25,844 Val Step[1250/1563], Loss: 0.7391 (1.2366), Acc@1: 0.8438 (0.7228), Acc@5: 0.9375 (0.9092)
2022-01-06 19:59:27,216 Val Step[1300/1563], Loss: 0.9312 (1.2441), Acc@1: 0.8125 (0.7212), Acc@5: 0.9375 (0.9081)
2022-01-06 19:59:28,472 Val Step[1350/1563], Loss: 2.0381 (1.2594), Acc@1: 0.5000 (0.7175), Acc@5: 0.7812 (0.9057)
2022-01-06 19:59:29,774 Val Step[1400/1563], Loss: 1.0667 (1.2661), Acc@1: 0.7812 (0.7157), Acc@5: 0.9375 (0.9048)
2022-01-06 19:59:31,036 Val Step[1450/1563], Loss: 1.2817 (1.2718), Acc@1: 0.7812 (0.7140), Acc@5: 0.9375 (0.9047)
2022-01-06 19:59:32,299 Val Step[1500/1563], Loss: 1.8781 (1.2613), Acc@1: 0.5938 (0.7164), Acc@5: 0.8438 (0.9060)
2022-01-06 19:59:33,583 Val Step[1550/1563], Loss: 0.9358 (1.2635), Acc@1: 0.8750 (0.7158), Acc@5: 0.9062 (0.9058)
2022-01-06 19:59:34,382 ----- Epoch[246/310], Validation Loss: 1.2620, Validation Acc@1: 0.7160, Validation Acc@5: 0.9060, time: 113.51
2022-01-06 19:59:34,383 ----- Epoch[246/310], Train Loss: 3.6443, Train Acc: 0.3787, time: 1567.64, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 19:59:34,577 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 19:59:34,577 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 19:59:34,683 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 19:59:34,684 Now training epoch 247. LR=0.000080
2022-01-06 20:00:58,534 Epoch[247/310], Step[0000/1251], Loss: 3.2998(3.2998), Acc: 0.3291(0.3291)
2022-01-06 20:01:57,035 Epoch[247/310], Step[0050/1251], Loss: 3.6994(3.5127), Acc: 0.4688(0.3698)
2022-01-06 20:02:55,337 Epoch[247/310], Step[0100/1251], Loss: 3.7501(3.5867), Acc: 0.4482(0.3792)
2022-01-06 20:03:54,474 Epoch[247/310], Step[0150/1251], Loss: 3.3145(3.5968), Acc: 0.1875(0.3823)
2022-01-06 20:04:53,339 Epoch[247/310], Step[0200/1251], Loss: 3.4602(3.6124), Acc: 0.3301(0.3740)
2022-01-06 20:05:51,875 Epoch[247/310], Step[0250/1251], Loss: 3.8469(3.6148), Acc: 0.2031(0.3767)
2022-01-06 20:06:50,434 Epoch[247/310], Step[0300/1251], Loss: 3.6034(3.6304), Acc: 0.4043(0.3804)
2022-01-06 20:07:47,831 Epoch[247/310], Step[0350/1251], Loss: 3.8104(3.6278), Acc: 0.3564(0.3816)
2022-01-06 20:08:46,656 Epoch[247/310], Step[0400/1251], Loss: 3.6587(3.6416), Acc: 0.2256(0.3783)
2022-01-06 20:09:45,242 Epoch[247/310], Step[0450/1251], Loss: 3.2016(3.6489), Acc: 0.5312(0.3782)
2022-01-06 20:10:44,468 Epoch[247/310], Step[0500/1251], Loss: 3.4047(3.6469), Acc: 0.4639(0.3770)
2022-01-06 20:11:43,233 Epoch[247/310], Step[0550/1251], Loss: 3.8003(3.6521), Acc: 0.3154(0.3768)
2022-01-06 20:12:42,105 Epoch[247/310], Step[0600/1251], Loss: 3.7362(3.6520), Acc: 0.3037(0.3756)
2022-01-06 20:13:41,447 Epoch[247/310], Step[0650/1251], Loss: 3.9688(3.6509), Acc: 0.2979(0.3761)
2022-01-06 20:14:40,609 Epoch[247/310], Step[0700/1251], Loss: 3.1639(3.6467), Acc: 0.1641(0.3759)
2022-01-06 20:15:38,248 Epoch[247/310], Step[0750/1251], Loss: 3.7675(3.6414), Acc: 0.2471(0.3749)
2022-01-06 20:16:36,214 Epoch[247/310], Step[0800/1251], Loss: 2.9140(3.6392), Acc: 0.3037(0.3753)
2022-01-06 20:17:35,901 Epoch[247/310], Step[0850/1251], Loss: 3.6154(3.6419), Acc: 0.3369(0.3746)
2022-01-06 20:18:35,230 Epoch[247/310], Step[0900/1251], Loss: 3.9124(3.6425), Acc: 0.3984(0.3745)
2022-01-06 20:19:34,698 Epoch[247/310], Step[0950/1251], Loss: 3.4144(3.6444), Acc: 0.5635(0.3735)
2022-01-06 20:20:33,147 Epoch[247/310], Step[1000/1251], Loss: 3.8513(3.6438), Acc: 0.3936(0.3740)
2022-01-06 20:21:31,657 Epoch[247/310], Step[1050/1251], Loss: 4.2675(3.6442), Acc: 0.4375(0.3739)
2022-01-06 20:22:30,376 Epoch[247/310], Step[1100/1251], Loss: 3.8579(3.6449), Acc: 0.3506(0.3748)
2022-01-06 20:23:28,711 Epoch[247/310], Step[1150/1251], Loss: 3.0106(3.6425), Acc: 0.6133(0.3746)
2022-01-06 20:24:28,213 Epoch[247/310], Step[1200/1251], Loss: 3.2103(3.6409), Acc: 0.2871(0.3762)
2022-01-06 20:25:28,787 Epoch[247/310], Step[1250/1251], Loss: 3.4449(3.6408), Acc: 0.3438(0.3757)
2022-01-06 20:25:30,509 ----- Epoch[247/310], Train Loss: 3.6408, Train Acc: 0.3757, time: 1555.82, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 20:25:30,686 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 20:25:30,687 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 20:25:30,798 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 20:25:30,798 Now training epoch 248. LR=0.000077
2022-01-06 20:27:00,302 Epoch[248/310], Step[0000/1251], Loss: 3.6262(3.6262), Acc: 0.3486(0.3486)
2022-01-06 20:27:59,232 Epoch[248/310], Step[0050/1251], Loss: 3.8068(3.6608), Acc: 0.4365(0.3645)
2022-01-06 20:28:58,062 Epoch[248/310], Step[0100/1251], Loss: 3.5235(3.6417), Acc: 0.1777(0.3649)
2022-01-06 20:29:57,457 Epoch[248/310], Step[0150/1251], Loss: 3.5453(3.6424), Acc: 0.3398(0.3652)
2022-01-06 20:30:56,248 Epoch[248/310], Step[0200/1251], Loss: 4.0366(3.6450), Acc: 0.2344(0.3698)
2022-01-06 20:31:55,114 Epoch[248/310], Step[0250/1251], Loss: 3.8642(3.6461), Acc: 0.4502(0.3640)
2022-01-06 20:32:54,019 Epoch[248/310], Step[0300/1251], Loss: 3.7087(3.6491), Acc: 0.5137(0.3690)
2022-01-06 20:33:53,314 Epoch[248/310], Step[0350/1251], Loss: 3.2243(3.6412), Acc: 0.2451(0.3671)
2022-01-06 20:34:52,245 Epoch[248/310], Step[0400/1251], Loss: 3.8299(3.6368), Acc: 0.4668(0.3677)
2022-01-06 20:35:52,320 Epoch[248/310], Step[0450/1251], Loss: 3.5330(3.6357), Acc: 0.3535(0.3694)
2022-01-06 20:36:52,117 Epoch[248/310], Step[0500/1251], Loss: 3.5978(3.6361), Acc: 0.3418(0.3701)
2022-01-06 20:37:50,711 Epoch[248/310], Step[0550/1251], Loss: 3.6613(3.6418), Acc: 0.4199(0.3721)
2022-01-06 20:38:50,157 Epoch[248/310], Step[0600/1251], Loss: 3.7899(3.6421), Acc: 0.2148(0.3714)
2022-01-06 20:39:49,871 Epoch[248/310], Step[0650/1251], Loss: 3.7813(3.6414), Acc: 0.2256(0.3705)
2022-01-06 20:40:48,884 Epoch[248/310], Step[0700/1251], Loss: 3.4686(3.6392), Acc: 0.5771(0.3715)
2022-01-06 20:41:46,987 Epoch[248/310], Step[0750/1251], Loss: 3.3672(3.6376), Acc: 0.5664(0.3744)
2022-01-06 20:42:44,636 Epoch[248/310], Step[0800/1251], Loss: 3.4402(3.6379), Acc: 0.5557(0.3738)
2022-01-06 20:43:43,276 Epoch[248/310], Step[0850/1251], Loss: 3.5171(3.6359), Acc: 0.4209(0.3753)
2022-01-06 20:44:42,001 Epoch[248/310], Step[0900/1251], Loss: 3.8780(3.6310), Acc: 0.4512(0.3762)
2022-01-06 20:45:41,105 Epoch[248/310], Step[0950/1251], Loss: 3.3115(3.6303), Acc: 0.5840(0.3769)
2022-01-06 20:46:40,699 Epoch[248/310], Step[1000/1251], Loss: 3.2496(3.6303), Acc: 0.3330(0.3755)
2022-01-06 20:47:40,891 Epoch[248/310], Step[1050/1251], Loss: 3.5598(3.6331), Acc: 0.3223(0.3744)
2022-01-06 20:48:40,171 Epoch[248/310], Step[1100/1251], Loss: 3.9617(3.6371), Acc: 0.3232(0.3735)
2022-01-06 20:49:38,994 Epoch[248/310], Step[1150/1251], Loss: 2.9275(3.6357), Acc: 0.4873(0.3723)
2022-01-06 20:50:38,811 Epoch[248/310], Step[1200/1251], Loss: 4.0505(3.6381), Acc: 0.2959(0.3729)
2022-01-06 20:51:38,474 Epoch[248/310], Step[1250/1251], Loss: 3.5417(3.6397), Acc: 0.5508(0.3718)
2022-01-06 20:51:39,956 ----- Validation after Epoch: 248
2022-01-06 20:52:51,156 Val Step[0000/1563], Loss: 0.7444 (0.7444), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 20:52:52,441 Val Step[0050/1563], Loss: 2.2809 (0.7691), Acc@1: 0.4688 (0.8529), Acc@5: 0.8750 (0.9577)
2022-01-06 20:52:53,701 Val Step[0100/1563], Loss: 1.8840 (1.0366), Acc@1: 0.5000 (0.7744), Acc@5: 0.8125 (0.9350)
2022-01-06 20:52:54,959 Val Step[0150/1563], Loss: 0.4124 (0.9907), Acc@1: 0.9375 (0.7877), Acc@5: 1.0000 (0.9398)
2022-01-06 20:52:56,220 Val Step[0200/1563], Loss: 0.9145 (1.0043), Acc@1: 0.8438 (0.7882), Acc@5: 0.9375 (0.9380)
2022-01-06 20:52:57,468 Val Step[0250/1563], Loss: 0.4862 (0.9520), Acc@1: 0.9375 (0.8009), Acc@5: 1.0000 (0.9427)
2022-01-06 20:52:58,909 Val Step[0300/1563], Loss: 1.1467 (1.0099), Acc@1: 0.6562 (0.7846), Acc@5: 0.9688 (0.9385)
2022-01-06 20:53:00,313 Val Step[0350/1563], Loss: 0.8973 (1.0169), Acc@1: 0.7812 (0.7792), Acc@5: 0.9062 (0.9398)
2022-01-06 20:53:01,688 Val Step[0400/1563], Loss: 0.8860 (1.0229), Acc@1: 0.7812 (0.7745), Acc@5: 0.9688 (0.9403)
2022-01-06 20:53:02,989 Val Step[0450/1563], Loss: 1.0690 (1.0310), Acc@1: 0.6875 (0.7722), Acc@5: 1.0000 (0.9408)
2022-01-06 20:53:04,457 Val Step[0500/1563], Loss: 0.5101 (1.0221), Acc@1: 0.8438 (0.7745), Acc@5: 1.0000 (0.9425)
2022-01-06 20:53:05,845 Val Step[0550/1563], Loss: 0.9718 (1.0011), Acc@1: 0.7500 (0.7801), Acc@5: 0.9688 (0.9442)
2022-01-06 20:53:07,128 Val Step[0600/1563], Loss: 1.0554 (1.0093), Acc@1: 0.7500 (0.7795), Acc@5: 0.9375 (0.9434)
2022-01-06 20:53:08,412 Val Step[0650/1563], Loss: 0.5229 (1.0302), Acc@1: 0.9375 (0.7749), Acc@5: 1.0000 (0.9407)
2022-01-06 20:53:09,738 Val Step[0700/1563], Loss: 0.9432 (1.0586), Acc@1: 0.8125 (0.7680), Acc@5: 0.9688 (0.9371)
2022-01-06 20:53:11,079 Val Step[0750/1563], Loss: 1.1797 (1.0907), Acc@1: 0.7500 (0.7610), Acc@5: 0.9062 (0.9328)
2022-01-06 20:53:12,352 Val Step[0800/1563], Loss: 0.9617 (1.1278), Acc@1: 0.7500 (0.7518), Acc@5: 1.0000 (0.9278)
2022-01-06 20:53:13,630 Val Step[0850/1563], Loss: 1.2568 (1.1520), Acc@1: 0.6562 (0.7454), Acc@5: 0.9688 (0.9251)
2022-01-06 20:53:14,892 Val Step[0900/1563], Loss: 0.2580 (1.1505), Acc@1: 0.9688 (0.7473), Acc@5: 1.0000 (0.9245)
2022-01-06 20:53:16,269 Val Step[0950/1563], Loss: 1.1705 (1.1707), Acc@1: 0.8125 (0.7433), Acc@5: 0.9062 (0.9208)
2022-01-06 20:53:17,524 Val Step[1000/1563], Loss: 0.6645 (1.1923), Acc@1: 0.9375 (0.7373), Acc@5: 1.0000 (0.9180)
2022-01-06 20:53:18,782 Val Step[1050/1563], Loss: 0.4012 (1.2045), Acc@1: 0.9688 (0.7341), Acc@5: 1.0000 (0.9168)
2022-01-06 20:53:20,048 Val Step[1100/1563], Loss: 0.8756 (1.2182), Acc@1: 0.8125 (0.7312), Acc@5: 0.9688 (0.9147)
2022-01-06 20:53:21,392 Val Step[1150/1563], Loss: 1.2707 (1.2319), Acc@1: 0.7812 (0.7284), Acc@5: 0.8438 (0.9129)
2022-01-06 20:53:22,739 Val Step[1200/1563], Loss: 1.1907 (1.2469), Acc@1: 0.7500 (0.7253), Acc@5: 0.8438 (0.9103)
2022-01-06 20:53:24,000 Val Step[1250/1563], Loss: 0.7487 (1.2576), Acc@1: 0.8750 (0.7233), Acc@5: 0.9375 (0.9086)
2022-01-06 20:53:25,276 Val Step[1300/1563], Loss: 0.9040 (1.2662), Acc@1: 0.8438 (0.7217), Acc@5: 0.9062 (0.9075)
2022-01-06 20:53:26,535 Val Step[1350/1563], Loss: 1.8395 (1.2825), Acc@1: 0.5000 (0.7172), Acc@5: 0.8125 (0.9052)
2022-01-06 20:53:27,819 Val Step[1400/1563], Loss: 1.1708 (1.2896), Acc@1: 0.6875 (0.7158), Acc@5: 0.9375 (0.9040)
2022-01-06 20:53:29,089 Val Step[1450/1563], Loss: 1.2996 (1.2955), Acc@1: 0.7500 (0.7141), Acc@5: 0.9375 (0.9039)
2022-01-06 20:53:30,420 Val Step[1500/1563], Loss: 1.7565 (1.2852), Acc@1: 0.5938 (0.7165), Acc@5: 0.8750 (0.9055)
2022-01-06 20:53:31,782 Val Step[1550/1563], Loss: 0.8971 (1.2866), Acc@1: 0.8750 (0.7158), Acc@5: 0.9062 (0.9053)
2022-01-06 20:53:32,523 ----- Epoch[248/310], Validation Loss: 1.2856, Validation Acc@1: 0.7159, Validation Acc@5: 0.9055, time: 112.56
2022-01-06 20:53:32,523 ----- Epoch[248/310], Train Loss: 3.6397, Train Acc: 0.3718, time: 1569.15, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 20:53:32,710 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 20:53:32,711 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 20:53:32,797 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 20:53:32,798 Now training epoch 249. LR=0.000074
2022-01-06 20:54:55,359 Epoch[249/310], Step[0000/1251], Loss: 3.6436(3.6436), Acc: 0.2334(0.2334)
2022-01-06 20:55:54,764 Epoch[249/310], Step[0050/1251], Loss: 3.8513(3.6865), Acc: 0.4941(0.3812)
2022-01-06 20:56:54,450 Epoch[249/310], Step[0100/1251], Loss: 3.5244(3.6812), Acc: 0.5156(0.3602)
2022-01-06 20:57:53,795 Epoch[249/310], Step[0150/1251], Loss: 3.3968(3.6383), Acc: 0.3945(0.3718)
2022-01-06 20:58:52,493 Epoch[249/310], Step[0200/1251], Loss: 3.4631(3.6303), Acc: 0.3604(0.3774)
2022-01-06 20:59:52,644 Epoch[249/310], Step[0250/1251], Loss: 3.2757(3.6288), Acc: 0.1094(0.3723)
2022-01-06 21:00:51,755 Epoch[249/310], Step[0300/1251], Loss: 3.3027(3.6280), Acc: 0.4014(0.3710)
2022-01-06 21:01:50,782 Epoch[249/310], Step[0350/1251], Loss: 2.8774(3.6262), Acc: 0.4668(0.3729)
2022-01-06 21:02:50,719 Epoch[249/310], Step[0400/1251], Loss: 3.3904(3.6237), Acc: 0.4277(0.3770)
2022-01-06 21:03:51,215 Epoch[249/310], Step[0450/1251], Loss: 3.7065(3.6252), Acc: 0.3867(0.3776)
2022-01-06 21:04:50,076 Epoch[249/310], Step[0500/1251], Loss: 3.9027(3.6273), Acc: 0.4229(0.3769)
2022-01-06 21:05:48,590 Epoch[249/310], Step[0550/1251], Loss: 3.3519(3.6294), Acc: 0.5527(0.3755)
2022-01-06 21:06:48,805 Epoch[249/310], Step[0600/1251], Loss: 3.9036(3.6328), Acc: 0.3965(0.3752)
2022-01-06 21:07:46,915 Epoch[249/310], Step[0650/1251], Loss: 3.5861(3.6318), Acc: 0.2197(0.3757)
2022-01-06 21:08:47,498 Epoch[249/310], Step[0700/1251], Loss: 3.7813(3.6276), Acc: 0.3525(0.3757)
2022-01-06 21:09:48,277 Epoch[249/310], Step[0750/1251], Loss: 3.8464(3.6288), Acc: 0.3057(0.3763)
2022-01-06 21:10:48,689 Epoch[249/310], Step[0800/1251], Loss: 3.9124(3.6255), Acc: 0.4561(0.3757)
2022-01-06 21:11:47,227 Epoch[249/310], Step[0850/1251], Loss: 3.5262(3.6220), Acc: 0.4766(0.3756)
2022-01-06 21:12:48,110 Epoch[249/310], Step[0900/1251], Loss: 4.0950(3.6258), Acc: 0.4453(0.3745)
2022-01-06 21:13:47,247 Epoch[249/310], Step[0950/1251], Loss: 3.3588(3.6261), Acc: 0.4766(0.3736)
2022-01-06 21:14:47,582 Epoch[249/310], Step[1000/1251], Loss: 3.2032(3.6275), Acc: 0.5781(0.3736)
2022-01-06 21:15:48,659 Epoch[249/310], Step[1050/1251], Loss: 3.7581(3.6261), Acc: 0.1816(0.3730)
2022-01-06 21:16:49,152 Epoch[249/310], Step[1100/1251], Loss: 3.4157(3.6278), Acc: 0.5576(0.3732)
2022-01-06 21:17:48,637 Epoch[249/310], Step[1150/1251], Loss: 3.9897(3.6271), Acc: 0.3545(0.3750)
2022-01-06 21:18:48,567 Epoch[249/310], Step[1200/1251], Loss: 3.2599(3.6273), Acc: 0.4014(0.3751)
2022-01-06 21:19:48,616 Epoch[249/310], Step[1250/1251], Loss: 3.4128(3.6309), Acc: 0.4180(0.3745)
2022-01-06 21:19:50,105 ----- Epoch[249/310], Train Loss: 3.6309, Train Acc: 0.3745, time: 1577.30, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 21:19:50,282 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 21:19:50,282 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 21:19:50,391 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 21:19:50,392 Now training epoch 250. LR=0.000072
2022-01-06 21:21:16,193 Epoch[250/310], Step[0000/1251], Loss: 3.6888(3.6888), Acc: 0.4521(0.4521)
2022-01-06 21:22:14,810 Epoch[250/310], Step[0050/1251], Loss: 3.7089(3.6398), Acc: 0.3066(0.3724)
2022-01-06 21:23:13,044 Epoch[250/310], Step[0100/1251], Loss: 3.9522(3.6570), Acc: 0.3340(0.3682)
2022-01-06 21:24:12,317 Epoch[250/310], Step[0150/1251], Loss: 3.9336(3.6609), Acc: 0.4473(0.3693)
2022-01-06 21:25:11,772 Epoch[250/310], Step[0200/1251], Loss: 2.7013(3.6576), Acc: 0.4580(0.3705)
2022-01-06 21:26:10,868 Epoch[250/310], Step[0250/1251], Loss: 3.4227(3.6612), Acc: 0.5068(0.3714)
2022-01-06 21:27:10,932 Epoch[250/310], Step[0300/1251], Loss: 3.6433(3.6570), Acc: 0.4912(0.3692)
2022-01-06 21:28:10,696 Epoch[250/310], Step[0350/1251], Loss: 3.4276(3.6544), Acc: 0.2441(0.3692)
2022-01-06 21:29:09,847 Epoch[250/310], Step[0400/1251], Loss: 3.6569(3.6530), Acc: 0.5205(0.3726)
2022-01-06 21:30:08,525 Epoch[250/310], Step[0450/1251], Loss: 3.5203(3.6601), Acc: 0.5723(0.3734)
2022-01-06 21:31:08,598 Epoch[250/310], Step[0500/1251], Loss: 3.6125(3.6561), Acc: 0.4121(0.3739)
2022-01-06 21:32:08,370 Epoch[250/310], Step[0550/1251], Loss: 3.5762(3.6504), Acc: 0.5283(0.3730)
2022-01-06 21:33:08,393 Epoch[250/310], Step[0600/1251], Loss: 3.8900(3.6459), Acc: 0.1973(0.3733)
2022-01-06 21:34:06,092 Epoch[250/310], Step[0650/1251], Loss: 4.2383(3.6415), Acc: 0.2881(0.3740)
2022-01-06 21:35:04,168 Epoch[250/310], Step[0700/1251], Loss: 3.3878(3.6428), Acc: 0.3955(0.3717)
2022-01-06 21:36:01,952 Epoch[250/310], Step[0750/1251], Loss: 3.6988(3.6422), Acc: 0.4395(0.3708)
2022-01-06 21:37:01,966 Epoch[250/310], Step[0800/1251], Loss: 4.2225(3.6460), Acc: 0.3418(0.3711)
2022-01-06 21:38:01,643 Epoch[250/310], Step[0850/1251], Loss: 3.4369(3.6479), Acc: 0.3721(0.3713)
2022-01-06 21:39:01,802 Epoch[250/310], Step[0900/1251], Loss: 3.6254(3.6468), Acc: 0.2061(0.3710)
2022-01-06 21:40:02,478 Epoch[250/310], Step[0950/1251], Loss: 3.3668(3.6451), Acc: 0.5557(0.3720)
2022-01-06 21:41:04,024 Epoch[250/310], Step[1000/1251], Loss: 4.3958(3.6421), Acc: 0.3008(0.3719)
2022-01-06 21:42:03,761 Epoch[250/310], Step[1050/1251], Loss: 3.3640(3.6431), Acc: 0.4424(0.3721)
2022-01-06 21:43:04,999 Epoch[250/310], Step[1100/1251], Loss: 3.6271(3.6439), Acc: 0.2441(0.3710)
2022-01-06 21:44:03,893 Epoch[250/310], Step[1150/1251], Loss: 4.0681(3.6468), Acc: 0.2246(0.3716)
2022-01-06 21:45:04,366 Epoch[250/310], Step[1200/1251], Loss: 3.1628(3.6448), Acc: 0.1406(0.3718)
2022-01-06 21:46:05,355 Epoch[250/310], Step[1250/1251], Loss: 2.9991(3.6478), Acc: 0.3359(0.3715)
2022-01-06 21:46:07,554 ----- Validation after Epoch: 250
2022-01-06 21:47:13,873 Val Step[0000/1563], Loss: 0.7581 (0.7581), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-06 21:47:15,160 Val Step[0050/1563], Loss: 2.3472 (0.7536), Acc@1: 0.4062 (0.8511), Acc@5: 0.8750 (0.9608)
2022-01-06 21:47:16,557 Val Step[0100/1563], Loss: 1.8282 (1.0410), Acc@1: 0.5625 (0.7695), Acc@5: 0.8438 (0.9353)
2022-01-06 21:47:17,960 Val Step[0150/1563], Loss: 0.4803 (0.9830), Acc@1: 0.9062 (0.7875), Acc@5: 1.0000 (0.9410)
2022-01-06 21:47:19,477 Val Step[0200/1563], Loss: 1.1013 (0.9849), Acc@1: 0.7500 (0.7901), Acc@5: 0.9062 (0.9401)
2022-01-06 21:47:20,907 Val Step[0250/1563], Loss: 0.3960 (0.9349), Acc@1: 0.9688 (0.8027), Acc@5: 1.0000 (0.9448)
2022-01-06 21:47:22,320 Val Step[0300/1563], Loss: 1.2467 (0.9917), Acc@1: 0.6562 (0.7847), Acc@5: 0.9375 (0.9405)
2022-01-06 21:47:23,701 Val Step[0350/1563], Loss: 0.9789 (0.9994), Acc@1: 0.7500 (0.7791), Acc@5: 0.9375 (0.9424)
2022-01-06 21:47:25,051 Val Step[0400/1563], Loss: 0.9481 (1.0043), Acc@1: 0.8750 (0.7754), Acc@5: 0.9688 (0.9426)
2022-01-06 21:47:26,446 Val Step[0450/1563], Loss: 1.0411 (1.0087), Acc@1: 0.6875 (0.7727), Acc@5: 1.0000 (0.9436)
2022-01-06 21:47:27,945 Val Step[0500/1563], Loss: 0.5528 (1.0019), Acc@1: 0.8438 (0.7749), Acc@5: 1.0000 (0.9448)
2022-01-06 21:47:29,479 Val Step[0550/1563], Loss: 0.7718 (0.9811), Acc@1: 0.8125 (0.7805), Acc@5: 0.9375 (0.9463)
2022-01-06 21:47:30,802 Val Step[0600/1563], Loss: 0.8759 (0.9890), Acc@1: 0.8125 (0.7794), Acc@5: 0.9062 (0.9452)
2022-01-06 21:47:32,072 Val Step[0650/1563], Loss: 0.5581 (1.0079), Acc@1: 0.8750 (0.7752), Acc@5: 1.0000 (0.9426)
2022-01-06 21:47:33,386 Val Step[0700/1563], Loss: 1.1085 (1.0362), Acc@1: 0.7812 (0.7687), Acc@5: 0.9375 (0.9390)
2022-01-06 21:47:34,663 Val Step[0750/1563], Loss: 1.3369 (1.0696), Acc@1: 0.7500 (0.7617), Acc@5: 0.9062 (0.9345)
2022-01-06 21:47:36,037 Val Step[0800/1563], Loss: 0.9034 (1.1082), Acc@1: 0.8125 (0.7518), Acc@5: 1.0000 (0.9297)
2022-01-06 21:47:37,424 Val Step[0850/1563], Loss: 1.3622 (1.1339), Acc@1: 0.6562 (0.7453), Acc@5: 0.9375 (0.9267)
2022-01-06 21:47:38,688 Val Step[0900/1563], Loss: 0.2926 (1.1316), Acc@1: 0.9688 (0.7469), Acc@5: 1.0000 (0.9265)
2022-01-06 21:47:40,069 Val Step[0950/1563], Loss: 1.3249 (1.1507), Acc@1: 0.7500 (0.7435), Acc@5: 0.9062 (0.9236)
2022-01-06 21:47:41,542 Val Step[1000/1563], Loss: 0.7131 (1.1722), Acc@1: 0.9375 (0.7376), Acc@5: 0.9688 (0.9207)
2022-01-06 21:47:42,862 Val Step[1050/1563], Loss: 0.3229 (1.1843), Acc@1: 0.9688 (0.7348), Acc@5: 1.0000 (0.9194)
2022-01-06 21:47:44,136 Val Step[1100/1563], Loss: 0.7953 (1.1985), Acc@1: 0.8438 (0.7316), Acc@5: 0.9688 (0.9173)
2022-01-06 21:47:45,444 Val Step[1150/1563], Loss: 1.2336 (1.2130), Acc@1: 0.7812 (0.7285), Acc@5: 0.8125 (0.9153)
2022-01-06 21:47:46,713 Val Step[1200/1563], Loss: 1.1270 (1.2266), Acc@1: 0.8125 (0.7250), Acc@5: 0.8750 (0.9130)
2022-01-06 21:47:48,027 Val Step[1250/1563], Loss: 0.7118 (1.2385), Acc@1: 0.8750 (0.7230), Acc@5: 0.9375 (0.9114)
2022-01-06 21:47:49,333 Val Step[1300/1563], Loss: 0.8676 (1.2480), Acc@1: 0.8438 (0.7214), Acc@5: 0.9062 (0.9102)
2022-01-06 21:47:50,739 Val Step[1350/1563], Loss: 1.6674 (1.2645), Acc@1: 0.5625 (0.7175), Acc@5: 0.8750 (0.9078)
2022-01-06 21:47:52,014 Val Step[1400/1563], Loss: 1.3079 (1.2706), Acc@1: 0.6875 (0.7162), Acc@5: 0.8750 (0.9069)
2022-01-06 21:47:53,295 Val Step[1450/1563], Loss: 1.3500 (1.2772), Acc@1: 0.6875 (0.7144), Acc@5: 0.9375 (0.9066)
2022-01-06 21:47:54,611 Val Step[1500/1563], Loss: 1.7859 (1.2674), Acc@1: 0.5312 (0.7167), Acc@5: 0.8125 (0.9079)
2022-01-06 21:47:55,877 Val Step[1550/1563], Loss: 0.8983 (1.2688), Acc@1: 0.8750 (0.7162), Acc@5: 0.9062 (0.9076)
2022-01-06 21:47:56,605 ----- Epoch[250/310], Validation Loss: 1.2670, Validation Acc@1: 0.7165, Validation Acc@5: 0.9079, time: 109.05
2022-01-06 21:47:56,605 ----- Epoch[250/310], Train Loss: 3.6478, Train Acc: 0.3715, time: 1577.16, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 21:47:56,765 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-250-Loss-3.6592920194331597.pdparams
2022-01-06 21:47:56,765 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-250-Loss-3.6592920194331597.pdopt
2022-01-06 21:47:56,805 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-250-Loss-3.6592920194331597-EMA.pdparams
2022-01-06 21:47:56,806 Now training epoch 251. LR=0.000069
2022-01-06 21:49:20,734 Epoch[251/310], Step[0000/1251], Loss: 3.5782(3.5782), Acc: 0.5029(0.5029)
2022-01-06 21:50:20,244 Epoch[251/310], Step[0050/1251], Loss: 3.5720(3.6864), Acc: 0.2676(0.3857)
2022-01-06 21:51:18,792 Epoch[251/310], Step[0100/1251], Loss: 3.8085(3.6693), Acc: 0.4775(0.3793)
2022-01-06 21:52:16,442 Epoch[251/310], Step[0150/1251], Loss: 3.6258(3.6400), Acc: 0.2607(0.3805)
2022-01-06 21:53:15,376 Epoch[251/310], Step[0200/1251], Loss: 3.7893(3.6347), Acc: 0.2100(0.3719)
2022-01-06 21:54:14,770 Epoch[251/310], Step[0250/1251], Loss: 3.5770(3.6251), Acc: 0.4229(0.3716)
2022-01-06 21:55:14,180 Epoch[251/310], Step[0300/1251], Loss: 3.4472(3.6203), Acc: 0.4355(0.3743)
2022-01-06 21:56:13,884 Epoch[251/310], Step[0350/1251], Loss: 3.5070(3.6178), Acc: 0.4775(0.3752)
2022-01-06 21:57:13,404 Epoch[251/310], Step[0400/1251], Loss: 3.3028(3.6179), Acc: 0.5244(0.3744)
2022-01-06 21:58:12,705 Epoch[251/310], Step[0450/1251], Loss: 3.6447(3.6317), Acc: 0.3857(0.3747)
2022-01-06 21:59:12,464 Epoch[251/310], Step[0500/1251], Loss: 3.7926(3.6328), Acc: 0.3936(0.3700)
2022-01-06 22:00:11,521 Epoch[251/310], Step[0550/1251], Loss: 3.6010(3.6360), Acc: 0.3486(0.3697)
2022-01-06 22:01:10,189 Epoch[251/310], Step[0600/1251], Loss: 4.0401(3.6338), Acc: 0.2646(0.3702)
2022-01-06 22:02:09,315 Epoch[251/310], Step[0650/1251], Loss: 3.5366(3.6364), Acc: 0.1523(0.3694)
2022-01-06 22:03:07,523 Epoch[251/310], Step[0700/1251], Loss: 3.5051(3.6356), Acc: 0.1914(0.3692)
2022-01-06 22:04:06,478 Epoch[251/310], Step[0750/1251], Loss: 3.7509(3.6329), Acc: 0.3154(0.3708)
2022-01-06 22:05:06,700 Epoch[251/310], Step[0800/1251], Loss: 3.2323(3.6344), Acc: 0.6152(0.3708)
2022-01-06 22:06:06,654 Epoch[251/310], Step[0850/1251], Loss: 3.4614(3.6377), Acc: 0.4043(0.3712)
2022-01-06 22:07:06,976 Epoch[251/310], Step[0900/1251], Loss: 3.8168(3.6380), Acc: 0.5508(0.3716)
2022-01-06 22:08:06,766 Epoch[251/310], Step[0950/1251], Loss: 3.6146(3.6426), Acc: 0.2734(0.3702)
2022-01-06 22:09:06,426 Epoch[251/310], Step[1000/1251], Loss: 3.5723(3.6391), Acc: 0.4727(0.3691)
2022-01-06 22:10:05,555 Epoch[251/310], Step[1050/1251], Loss: 3.3523(3.6406), Acc: 0.3984(0.3695)
2022-01-06 22:11:05,811 Epoch[251/310], Step[1100/1251], Loss: 3.2242(3.6410), Acc: 0.4590(0.3689)
2022-01-06 22:12:05,748 Epoch[251/310], Step[1150/1251], Loss: 3.3898(3.6406), Acc: 0.2627(0.3695)
2022-01-06 22:13:05,723 Epoch[251/310], Step[1200/1251], Loss: 3.8381(3.6403), Acc: 0.2188(0.3697)
2022-01-06 22:14:04,259 Epoch[251/310], Step[1250/1251], Loss: 4.1524(3.6369), Acc: 0.4297(0.3714)
2022-01-06 22:14:05,907 ----- Epoch[251/310], Train Loss: 3.6369, Train Acc: 0.3714, time: 1569.10, Best Val(epoch242) Acc@1: 0.7183
2022-01-06 22:14:06,079 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 22:14:06,079 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 22:14:06,184 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 22:14:06,185 Now training epoch 252. LR=0.000067
2022-01-06 22:15:30,590 Epoch[252/310], Step[0000/1251], Loss: 3.3043(3.3043), Acc: 0.5020(0.5020)
2022-01-06 22:16:30,430 Epoch[252/310], Step[0050/1251], Loss: 4.1216(3.6634), Acc: 0.3496(0.3848)
2022-01-06 22:17:30,234 Epoch[252/310], Step[0100/1251], Loss: 3.7441(3.6352), Acc: 0.3340(0.3942)
2022-01-06 22:18:28,291 Epoch[252/310], Step[0150/1251], Loss: 2.9706(3.6295), Acc: 0.6338(0.3894)
2022-01-06 22:19:27,325 Epoch[252/310], Step[0200/1251], Loss: 3.7015(3.6316), Acc: 0.5049(0.3859)
2022-01-06 22:20:25,944 Epoch[252/310], Step[0250/1251], Loss: 4.0045(3.6261), Acc: 0.3623(0.3836)
2022-01-06 22:21:25,585 Epoch[252/310], Step[0300/1251], Loss: 4.3685(3.6291), Acc: 0.2197(0.3831)
2022-01-06 22:22:24,350 Epoch[252/310], Step[0350/1251], Loss: 3.6276(3.6318), Acc: 0.3672(0.3816)
2022-01-06 22:23:23,387 Epoch[252/310], Step[0400/1251], Loss: 4.0149(3.6259), Acc: 0.3740(0.3808)
2022-01-06 22:24:23,433 Epoch[252/310], Step[0450/1251], Loss: 3.8455(3.6247), Acc: 0.5361(0.3796)
2022-01-06 22:25:23,579 Epoch[252/310], Step[0500/1251], Loss: 4.0677(3.6282), Acc: 0.3008(0.3772)
2022-01-06 22:26:22,830 Epoch[252/310], Step[0550/1251], Loss: 3.2766(3.6309), Acc: 0.5723(0.3786)
2022-01-06 22:27:23,122 Epoch[252/310], Step[0600/1251], Loss: 3.8416(3.6334), Acc: 0.4541(0.3765)
2022-01-06 22:28:22,853 Epoch[252/310], Step[0650/1251], Loss: 3.7131(3.6327), Acc: 0.0928(0.3752)
2022-01-06 22:29:21,901 Epoch[252/310], Step[0700/1251], Loss: 3.3675(3.6342), Acc: 0.4854(0.3746)
2022-01-06 22:30:21,486 Epoch[252/310], Step[0750/1251], Loss: 3.5408(3.6354), Acc: 0.2109(0.3753)
2022-01-06 22:31:21,558 Epoch[252/310], Step[0800/1251], Loss: 3.8723(3.6398), Acc: 0.2930(0.3734)
2022-01-06 22:32:21,629 Epoch[252/310], Step[0850/1251], Loss: 3.7147(3.6403), Acc: 0.1924(0.3724)
2022-01-06 22:33:21,634 Epoch[252/310], Step[0900/1251], Loss: 3.8103(3.6395), Acc: 0.3330(0.3725)
2022-01-06 22:34:22,262 Epoch[252/310], Step[0950/1251], Loss: 3.8871(3.6384), Acc: 0.3574(0.3739)
2022-01-06 22:35:20,491 Epoch[252/310], Step[1000/1251], Loss: 3.5434(3.6369), Acc: 0.2139(0.3740)
2022-01-06 22:36:20,456 Epoch[252/310], Step[1050/1251], Loss: 3.2279(3.6384), Acc: 0.4307(0.3745)
2022-01-06 22:37:20,051 Epoch[252/310], Step[1100/1251], Loss: 3.8495(3.6388), Acc: 0.5146(0.3743)
2022-01-06 22:38:20,284 Epoch[252/310], Step[1150/1251], Loss: 3.0866(3.6359), Acc: 0.4785(0.3758)
2022-01-06 22:39:20,121 Epoch[252/310], Step[1200/1251], Loss: 3.8312(3.6339), Acc: 0.4307(0.3757)
2022-01-06 22:40:20,291 Epoch[252/310], Step[1250/1251], Loss: 3.6543(3.6330), Acc: 0.4326(0.3768)
2022-01-06 22:40:21,771 ----- Validation after Epoch: 252
2022-01-06 22:41:31,308 Val Step[0000/1563], Loss: 0.7462 (0.7462), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-06 22:41:32,654 Val Step[0050/1563], Loss: 2.2584 (0.7224), Acc@1: 0.4062 (0.8554), Acc@5: 0.9062 (0.9626)
2022-01-06 22:41:33,925 Val Step[0100/1563], Loss: 1.9590 (0.9942), Acc@1: 0.5000 (0.7812), Acc@5: 0.8125 (0.9375)
2022-01-06 22:41:35,192 Val Step[0150/1563], Loss: 0.3656 (0.9437), Acc@1: 0.9688 (0.7945), Acc@5: 1.0000 (0.9408)
2022-01-06 22:41:36,487 Val Step[0200/1563], Loss: 1.0796 (0.9574), Acc@1: 0.7812 (0.7948), Acc@5: 0.8750 (0.9377)
2022-01-06 22:41:37,744 Val Step[0250/1563], Loss: 0.4822 (0.9022), Acc@1: 0.9375 (0.8066), Acc@5: 1.0000 (0.9437)
2022-01-06 22:41:39,090 Val Step[0300/1563], Loss: 1.0213 (0.9577), Acc@1: 0.7500 (0.7905), Acc@5: 0.9688 (0.9396)
2022-01-06 22:41:40,358 Val Step[0350/1563], Loss: 0.8301 (0.9689), Acc@1: 0.8125 (0.7862), Acc@5: 0.9375 (0.9411)
2022-01-06 22:41:41,625 Val Step[0400/1563], Loss: 0.8727 (0.9779), Acc@1: 0.8125 (0.7805), Acc@5: 0.9688 (0.9418)
2022-01-06 22:41:42,910 Val Step[0450/1563], Loss: 1.0267 (0.9853), Acc@1: 0.7188 (0.7774), Acc@5: 1.0000 (0.9419)
2022-01-06 22:41:44,235 Val Step[0500/1563], Loss: 0.3956 (0.9768), Acc@1: 0.9688 (0.7793), Acc@5: 1.0000 (0.9429)
2022-01-06 22:41:45,674 Val Step[0550/1563], Loss: 0.7867 (0.9556), Acc@1: 0.8438 (0.7843), Acc@5: 0.9375 (0.9449)
2022-01-06 22:41:47,085 Val Step[0600/1563], Loss: 0.8158 (0.9618), Acc@1: 0.8125 (0.7835), Acc@5: 0.9062 (0.9443)
2022-01-06 22:41:48,363 Val Step[0650/1563], Loss: 0.5462 (0.9822), Acc@1: 0.9062 (0.7794), Acc@5: 1.0000 (0.9415)
2022-01-06 22:41:49,670 Val Step[0700/1563], Loss: 0.9519 (1.0112), Acc@1: 0.7812 (0.7719), Acc@5: 0.9688 (0.9382)
2022-01-06 22:41:50,991 Val Step[0750/1563], Loss: 1.0167 (1.0433), Acc@1: 0.8125 (0.7654), Acc@5: 0.9375 (0.9341)
2022-01-06 22:41:52,267 Val Step[0800/1563], Loss: 0.8520 (1.0820), Acc@1: 0.7812 (0.7559), Acc@5: 1.0000 (0.9292)
2022-01-06 22:41:53,549 Val Step[0850/1563], Loss: 1.4086 (1.1075), Acc@1: 0.6250 (0.7493), Acc@5: 0.9062 (0.9262)
2022-01-06 22:41:54,875 Val Step[0900/1563], Loss: 0.2290 (1.1058), Acc@1: 0.9688 (0.7510), Acc@5: 1.0000 (0.9260)
2022-01-06 22:41:56,259 Val Step[0950/1563], Loss: 1.3289 (1.1273), Acc@1: 0.7812 (0.7469), Acc@5: 0.9062 (0.9227)
2022-01-06 22:41:57,515 Val Step[1000/1563], Loss: 0.4389 (1.1505), Acc@1: 0.9688 (0.7412), Acc@5: 1.0000 (0.9196)
2022-01-06 22:41:58,817 Val Step[1050/1563], Loss: 0.2764 (1.1634), Acc@1: 0.9688 (0.7375), Acc@5: 1.0000 (0.9181)
2022-01-06 22:42:00,179 Val Step[1100/1563], Loss: 0.6428 (1.1780), Acc@1: 0.8438 (0.7345), Acc@5: 1.0000 (0.9161)
2022-01-06 22:42:01,639 Val Step[1150/1563], Loss: 1.2061 (1.1906), Acc@1: 0.7812 (0.7321), Acc@5: 0.8125 (0.9144)
2022-01-06 22:42:02,930 Val Step[1200/1563], Loss: 1.2962 (1.2039), Acc@1: 0.7188 (0.7289), Acc@5: 0.8438 (0.9125)
2022-01-06 22:42:04,201 Val Step[1250/1563], Loss: 0.6466 (1.2159), Acc@1: 0.8750 (0.7267), Acc@5: 0.9375 (0.9107)
2022-01-06 22:42:05,479 Val Step[1300/1563], Loss: 0.7632 (1.2245), Acc@1: 0.9062 (0.7254), Acc@5: 0.9375 (0.9096)
2022-01-06 22:42:06,754 Val Step[1350/1563], Loss: 2.1121 (1.2423), Acc@1: 0.3750 (0.7214), Acc@5: 0.8125 (0.9070)
2022-01-06 22:42:08,045 Val Step[1400/1563], Loss: 1.0088 (1.2486), Acc@1: 0.7500 (0.7200), Acc@5: 0.9375 (0.9061)
2022-01-06 22:42:09,422 Val Step[1450/1563], Loss: 1.3276 (1.2540), Acc@1: 0.6875 (0.7182), Acc@5: 0.9375 (0.9059)
2022-01-06 22:42:10,731 Val Step[1500/1563], Loss: 1.6027 (1.2436), Acc@1: 0.5938 (0.7205), Acc@5: 0.8750 (0.9072)
2022-01-06 22:42:12,008 Val Step[1550/1563], Loss: 0.9221 (1.2447), Acc@1: 0.8750 (0.7199), Acc@5: 0.9062 (0.9070)
2022-01-06 22:42:12,817 ----- Epoch[252/310], Validation Loss: 1.2429, Validation Acc@1: 0.7202, Validation Acc@5: 0.9073, time: 111.04
2022-01-06 22:42:12,817 ----- Epoch[252/310], Train Loss: 3.6330, Train Acc: 0.3768, time: 1575.58, Best Val(epoch252) Acc@1: 0.7202
2022-01-06 22:42:13,003 Max accuracy so far: 0.7202 at epoch_252
2022-01-06 22:42:13,004 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-06 22:42:13,004 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-06 22:42:13,112 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-06 22:42:13,490 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 22:42:13,490 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 22:42:13,621 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 22:42:13,622 Now training epoch 253. LR=0.000064
2022-01-06 22:43:32,904 Epoch[253/310], Step[0000/1251], Loss: 3.7839(3.7839), Acc: 0.1855(0.1855)
2022-01-06 22:44:32,915 Epoch[253/310], Step[0050/1251], Loss: 3.9948(3.6264), Acc: 0.2051(0.3643)
2022-01-06 22:45:30,948 Epoch[253/310], Step[0100/1251], Loss: 3.9214(3.6691), Acc: 0.3105(0.3674)
2022-01-06 22:46:28,645 Epoch[253/310], Step[0150/1251], Loss: 3.8934(3.6367), Acc: 0.3320(0.3655)
2022-01-06 22:47:27,730 Epoch[253/310], Step[0200/1251], Loss: 3.8795(3.6390), Acc: 0.2930(0.3643)
2022-01-06 22:48:27,309 Epoch[253/310], Step[0250/1251], Loss: 3.7054(3.6316), Acc: 0.3398(0.3706)
2022-01-06 22:49:26,738 Epoch[253/310], Step[0300/1251], Loss: 4.0199(3.6342), Acc: 0.4414(0.3712)
2022-01-06 22:50:26,608 Epoch[253/310], Step[0350/1251], Loss: 3.4582(3.6223), Acc: 0.4111(0.3710)
2022-01-06 22:51:25,991 Epoch[253/310], Step[0400/1251], Loss: 3.1437(3.6163), Acc: 0.6162(0.3724)
2022-01-06 22:52:26,149 Epoch[253/310], Step[0450/1251], Loss: 3.3026(3.6143), Acc: 0.3115(0.3724)
2022-01-06 22:53:27,839 Epoch[253/310], Step[0500/1251], Loss: 3.1640(3.6150), Acc: 0.4248(0.3730)
2022-01-06 22:54:28,167 Epoch[253/310], Step[0550/1251], Loss: 3.7241(3.6139), Acc: 0.3682(0.3755)
2022-01-06 22:55:28,218 Epoch[253/310], Step[0600/1251], Loss: 3.7956(3.6152), Acc: 0.3037(0.3766)
2022-01-06 22:56:29,001 Epoch[253/310], Step[0650/1251], Loss: 3.6764(3.6189), Acc: 0.2832(0.3775)
2022-01-06 22:57:29,488 Epoch[253/310], Step[0700/1251], Loss: 3.6213(3.6170), Acc: 0.5625(0.3753)
2022-01-06 22:58:31,081 Epoch[253/310], Step[0750/1251], Loss: 3.8331(3.6188), Acc: 0.3027(0.3746)
2022-01-06 22:59:30,853 Epoch[253/310], Step[0800/1251], Loss: 3.7939(3.6193), Acc: 0.3252(0.3761)
2022-01-06 23:00:31,531 Epoch[253/310], Step[0850/1251], Loss: 3.6632(3.6184), Acc: 0.4053(0.3767)
2022-01-06 23:01:31,989 Epoch[253/310], Step[0900/1251], Loss: 3.5493(3.6171), Acc: 0.3291(0.3763)
2022-01-06 23:02:32,749 Epoch[253/310], Step[0950/1251], Loss: 3.4930(3.6216), Acc: 0.3232(0.3757)
2022-01-06 23:03:33,445 Epoch[253/310], Step[1000/1251], Loss: 3.6794(3.6234), Acc: 0.5234(0.3761)
2022-01-06 23:04:34,915 Epoch[253/310], Step[1050/1251], Loss: 3.8396(3.6239), Acc: 0.3047(0.3763)
2022-01-06 23:05:35,408 Epoch[253/310], Step[1100/1251], Loss: 3.3329(3.6242), Acc: 0.4346(0.3766)
2022-01-06 23:06:34,730 Epoch[253/310], Step[1150/1251], Loss: 3.6503(3.6215), Acc: 0.4443(0.3764)
2022-01-06 23:07:35,455 Epoch[253/310], Step[1200/1251], Loss: 3.4292(3.6231), Acc: 0.5605(0.3764)
2022-01-06 23:08:36,220 Epoch[253/310], Step[1250/1251], Loss: 3.3124(3.6210), Acc: 0.4424(0.3774)
2022-01-06 23:08:37,824 ----- Epoch[253/310], Train Loss: 3.6210, Train Acc: 0.3774, time: 1584.20, Best Val(epoch252) Acc@1: 0.7202
2022-01-06 23:08:38,012 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 23:08:38,013 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 23:08:38,098 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 23:08:38,098 Now training epoch 254. LR=0.000062
2022-01-06 23:10:01,075 Epoch[254/310], Step[0000/1251], Loss: 3.8541(3.8541), Acc: 0.3330(0.3330)
2022-01-06 23:10:59,746 Epoch[254/310], Step[0050/1251], Loss: 3.2358(3.5706), Acc: 0.5713(0.3820)
2022-01-06 23:11:57,752 Epoch[254/310], Step[0100/1251], Loss: 4.1133(3.6080), Acc: 0.2490(0.3846)
2022-01-06 23:12:56,196 Epoch[254/310], Step[0150/1251], Loss: 3.8980(3.6369), Acc: 0.2949(0.3738)
2022-01-06 23:13:55,087 Epoch[254/310], Step[0200/1251], Loss: 3.9047(3.6490), Acc: 0.4072(0.3737)
2022-01-06 23:14:54,188 Epoch[254/310], Step[0250/1251], Loss: 3.7529(3.6481), Acc: 0.3848(0.3719)
2022-01-06 23:15:53,026 Epoch[254/310], Step[0300/1251], Loss: 3.5426(3.6331), Acc: 0.3457(0.3731)
2022-01-06 23:16:50,401 Epoch[254/310], Step[0350/1251], Loss: 3.3152(3.6315), Acc: 0.3135(0.3691)
2022-01-06 23:17:50,479 Epoch[254/310], Step[0400/1251], Loss: 3.5471(3.6325), Acc: 0.2422(0.3687)
2022-01-06 23:18:50,558 Epoch[254/310], Step[0450/1251], Loss: 3.2025(3.6281), Acc: 0.3975(0.3676)
2022-01-06 23:19:50,492 Epoch[254/310], Step[0500/1251], Loss: 3.5211(3.6260), Acc: 0.4648(0.3672)
2022-01-06 23:20:51,561 Epoch[254/310], Step[0550/1251], Loss: 3.5465(3.6248), Acc: 0.3643(0.3663)
2022-01-06 23:21:50,842 Epoch[254/310], Step[0600/1251], Loss: 3.0767(3.6216), Acc: 0.4014(0.3675)
2022-01-06 23:22:49,720 Epoch[254/310], Step[0650/1251], Loss: 3.7612(3.6151), Acc: 0.4688(0.3691)
2022-01-06 23:23:49,726 Epoch[254/310], Step[0700/1251], Loss: 3.4134(3.6158), Acc: 0.5752(0.3698)
2022-01-06 23:24:48,441 Epoch[254/310], Step[0750/1251], Loss: 3.2426(3.6112), Acc: 0.5029(0.3696)
2022-01-06 23:25:47,428 Epoch[254/310], Step[0800/1251], Loss: 3.8763(3.6138), Acc: 0.5020(0.3688)
2022-01-06 23:26:45,789 Epoch[254/310], Step[0850/1251], Loss: 3.5999(3.6157), Acc: 0.2773(0.3700)
2022-01-06 23:27:44,902 Epoch[254/310], Step[0900/1251], Loss: 3.4976(3.6115), Acc: 0.3672(0.3708)
2022-01-06 23:28:44,451 Epoch[254/310], Step[0950/1251], Loss: 3.3003(3.6126), Acc: 0.5811(0.3710)
2022-01-06 23:29:44,287 Epoch[254/310], Step[1000/1251], Loss: 3.6894(3.6105), Acc: 0.2910(0.3720)
2022-01-06 23:30:44,221 Epoch[254/310], Step[1050/1251], Loss: 3.1744(3.6120), Acc: 0.2490(0.3723)
2022-01-06 23:31:43,704 Epoch[254/310], Step[1100/1251], Loss: 3.9897(3.6117), Acc: 0.3496(0.3721)
2022-01-06 23:32:43,966 Epoch[254/310], Step[1150/1251], Loss: 3.8079(3.6106), Acc: 0.3008(0.3717)
2022-01-06 23:33:42,965 Epoch[254/310], Step[1200/1251], Loss: 3.9997(3.6129), Acc: 0.2793(0.3718)
2022-01-06 23:34:42,280 Epoch[254/310], Step[1250/1251], Loss: 3.6783(3.6117), Acc: 0.4795(0.3724)
2022-01-06 23:34:43,766 ----- Validation after Epoch: 254
2022-01-06 23:35:44,643 Val Step[0000/1563], Loss: 0.6798 (0.6798), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-06 23:35:46,027 Val Step[0050/1563], Loss: 2.1432 (0.7581), Acc@1: 0.4375 (0.8505), Acc@5: 0.8750 (0.9620)
2022-01-06 23:35:47,284 Val Step[0100/1563], Loss: 1.9624 (1.0258), Acc@1: 0.4375 (0.7741), Acc@5: 0.8438 (0.9372)
2022-01-06 23:35:48,552 Val Step[0150/1563], Loss: 0.4671 (0.9659), Acc@1: 0.9062 (0.7895), Acc@5: 1.0000 (0.9410)
2022-01-06 23:35:49,822 Val Step[0200/1563], Loss: 1.1237 (0.9757), Acc@1: 0.7812 (0.7903), Acc@5: 0.9375 (0.9403)
2022-01-06 23:35:51,244 Val Step[0250/1563], Loss: 0.6827 (0.9270), Acc@1: 0.8438 (0.8017), Acc@5: 1.0000 (0.9452)
2022-01-06 23:35:52,679 Val Step[0300/1563], Loss: 0.9651 (0.9810), Acc@1: 0.7812 (0.7860), Acc@5: 1.0000 (0.9415)
2022-01-06 23:35:53,984 Val Step[0350/1563], Loss: 0.9345 (0.9919), Acc@1: 0.7812 (0.7796), Acc@5: 0.9062 (0.9430)
2022-01-06 23:35:55,251 Val Step[0400/1563], Loss: 0.7837 (0.9964), Acc@1: 0.8750 (0.7755), Acc@5: 0.9688 (0.9433)
2022-01-06 23:35:56,556 Val Step[0450/1563], Loss: 0.9293 (1.0018), Acc@1: 0.7188 (0.7734), Acc@5: 1.0000 (0.9433)
2022-01-06 23:35:57,894 Val Step[0500/1563], Loss: 0.4102 (0.9925), Acc@1: 0.9688 (0.7764), Acc@5: 1.0000 (0.9444)
2022-01-06 23:35:59,374 Val Step[0550/1563], Loss: 0.7552 (0.9717), Acc@1: 0.8438 (0.7817), Acc@5: 0.9375 (0.9459)
2022-01-06 23:36:00,795 Val Step[0600/1563], Loss: 0.8859 (0.9790), Acc@1: 0.8125 (0.7808), Acc@5: 0.9375 (0.9453)
2022-01-06 23:36:02,165 Val Step[0650/1563], Loss: 0.7231 (1.0001), Acc@1: 0.8750 (0.7759), Acc@5: 1.0000 (0.9426)
2022-01-06 23:36:03,601 Val Step[0700/1563], Loss: 1.2852 (1.0266), Acc@1: 0.7188 (0.7693), Acc@5: 0.8750 (0.9396)
2022-01-06 23:36:05,022 Val Step[0750/1563], Loss: 1.1739 (1.0587), Acc@1: 0.7812 (0.7632), Acc@5: 0.9062 (0.9350)
2022-01-06 23:36:06,450 Val Step[0800/1563], Loss: 0.8014 (1.0935), Acc@1: 0.8438 (0.7551), Acc@5: 1.0000 (0.9303)
2022-01-06 23:36:07,769 Val Step[0850/1563], Loss: 1.2251 (1.1182), Acc@1: 0.6250 (0.7489), Acc@5: 0.9375 (0.9274)
2022-01-06 23:36:09,050 Val Step[0900/1563], Loss: 0.3220 (1.1182), Acc@1: 0.9375 (0.7499), Acc@5: 1.0000 (0.9269)
2022-01-06 23:36:10,422 Val Step[0950/1563], Loss: 1.2966 (1.1379), Acc@1: 0.7188 (0.7460), Acc@5: 0.9062 (0.9235)
2022-01-06 23:36:11,709 Val Step[1000/1563], Loss: 0.5303 (1.1607), Acc@1: 0.9375 (0.7402), Acc@5: 1.0000 (0.9204)
2022-01-06 23:36:12,988 Val Step[1050/1563], Loss: 0.3159 (1.1741), Acc@1: 0.9375 (0.7366), Acc@5: 1.0000 (0.9190)
2022-01-06 23:36:14,264 Val Step[1100/1563], Loss: 0.7742 (1.1882), Acc@1: 0.8750 (0.7336), Acc@5: 0.9688 (0.9169)
2022-01-06 23:36:15,564 Val Step[1150/1563], Loss: 1.2148 (1.2013), Acc@1: 0.7812 (0.7313), Acc@5: 0.8438 (0.9151)
2022-01-06 23:36:16,836 Val Step[1200/1563], Loss: 1.2033 (1.2147), Acc@1: 0.8125 (0.7283), Acc@5: 0.8438 (0.9129)
2022-01-06 23:36:18,129 Val Step[1250/1563], Loss: 0.7597 (1.2248), Acc@1: 0.8750 (0.7262), Acc@5: 0.9375 (0.9114)
2022-01-06 23:36:19,569 Val Step[1300/1563], Loss: 0.8305 (1.2336), Acc@1: 0.8438 (0.7243), Acc@5: 0.9062 (0.9103)
2022-01-06 23:36:21,027 Val Step[1350/1563], Loss: 1.7930 (1.2500), Acc@1: 0.5625 (0.7200), Acc@5: 0.8125 (0.9078)
2022-01-06 23:36:22,485 Val Step[1400/1563], Loss: 0.9437 (1.2571), Acc@1: 0.7500 (0.7185), Acc@5: 1.0000 (0.9070)
2022-01-06 23:36:23,939 Val Step[1450/1563], Loss: 1.3840 (1.2632), Acc@1: 0.6562 (0.7167), Acc@5: 0.9375 (0.9068)
2022-01-06 23:36:25,371 Val Step[1500/1563], Loss: 1.6533 (1.2524), Acc@1: 0.6250 (0.7194), Acc@5: 0.8750 (0.9083)
2022-01-06 23:36:26,796 Val Step[1550/1563], Loss: 0.9301 (1.2549), Acc@1: 0.8750 (0.7189), Acc@5: 0.9062 (0.9078)
2022-01-06 23:36:27,569 ----- Epoch[254/310], Validation Loss: 1.2538, Validation Acc@1: 0.7191, Validation Acc@5: 0.9079, time: 103.80
2022-01-06 23:36:27,569 ----- Epoch[254/310], Train Loss: 3.6117, Train Acc: 0.3724, time: 1565.66, Best Val(epoch252) Acc@1: 0.7202
2022-01-06 23:36:27,751 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-06 23:36:27,751 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-06 23:36:27,855 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-06 23:36:27,855 Now training epoch 255. LR=0.000059
2022-01-06 23:37:44,995 Epoch[255/310], Step[0000/1251], Loss: 3.4001(3.4001), Acc: 0.3574(0.3574)
2022-01-06 23:38:44,935 Epoch[255/310], Step[0050/1251], Loss: 3.1123(3.5996), Acc: 0.4531(0.3626)
2022-01-06 23:39:43,782 Epoch[255/310], Step[0100/1251], Loss: 4.2594(3.6054), Acc: 0.2422(0.3682)
2022-01-06 23:40:43,196 Epoch[255/310], Step[0150/1251], Loss: 3.5633(3.6150), Acc: 0.3193(0.3662)
2022-01-06 23:41:42,753 Epoch[255/310], Step[0200/1251], Loss: 3.4139(3.5981), Acc: 0.3076(0.3669)
2022-01-06 23:42:42,059 Epoch[255/310], Step[0250/1251], Loss: 3.8762(3.6081), Acc: 0.2539(0.3706)
2022-01-06 23:43:41,481 Epoch[255/310], Step[0300/1251], Loss: 3.7562(3.6080), Acc: 0.5068(0.3727)
2022-01-06 23:44:41,031 Epoch[255/310], Step[0350/1251], Loss: 3.3445(3.6064), Acc: 0.1426(0.3732)
2022-01-06 23:45:40,619 Epoch[255/310], Step[0400/1251], Loss: 3.7035(3.6093), Acc: 0.2305(0.3753)
2022-01-06 23:46:40,402 Epoch[255/310], Step[0450/1251], Loss: 3.0326(3.6079), Acc: 0.4707(0.3750)
2022-01-06 23:47:40,538 Epoch[255/310], Step[0500/1251], Loss: 3.3939(3.6107), Acc: 0.4346(0.3760)
2022-01-06 23:48:41,202 Epoch[255/310], Step[0550/1251], Loss: 3.1434(3.6093), Acc: 0.4756(0.3745)
2022-01-06 23:49:41,516 Epoch[255/310], Step[0600/1251], Loss: 3.5661(3.6085), Acc: 0.4668(0.3727)
2022-01-06 23:50:41,322 Epoch[255/310], Step[0650/1251], Loss: 3.6528(3.6099), Acc: 0.3193(0.3721)
2022-01-06 23:51:40,933 Epoch[255/310], Step[0700/1251], Loss: 3.9127(3.6140), Acc: 0.5293(0.3725)
2022-01-06 23:52:41,103 Epoch[255/310], Step[0750/1251], Loss: 3.8242(3.6159), Acc: 0.3457(0.3716)
2022-01-06 23:53:41,574 Epoch[255/310], Step[0800/1251], Loss: 3.5278(3.6163), Acc: 0.4229(0.3704)
2022-01-06 23:54:42,482 Epoch[255/310], Step[0850/1251], Loss: 3.6724(3.6172), Acc: 0.2861(0.3712)
2022-01-06 23:55:41,688 Epoch[255/310], Step[0900/1251], Loss: 3.7757(3.6199), Acc: 0.2861(0.3712)
2022-01-06 23:56:40,535 Epoch[255/310], Step[0950/1251], Loss: 3.3551(3.6154), Acc: 0.5293(0.3727)
2022-01-06 23:57:39,340 Epoch[255/310], Step[1000/1251], Loss: 4.0863(3.6167), Acc: 0.3135(0.3728)
2022-01-06 23:58:40,365 Epoch[255/310], Step[1050/1251], Loss: 3.3768(3.6150), Acc: 0.5605(0.3737)
2022-01-06 23:59:41,200 Epoch[255/310], Step[1100/1251], Loss: 4.0684(3.6135), Acc: 0.4023(0.3732)
2022-01-07 00:00:42,363 Epoch[255/310], Step[1150/1251], Loss: 3.3715(3.6158), Acc: 0.4229(0.3727)
2022-01-07 00:01:42,645 Epoch[255/310], Step[1200/1251], Loss: 4.0450(3.6178), Acc: 0.3652(0.3729)
2022-01-07 00:02:43,927 Epoch[255/310], Step[1250/1251], Loss: 3.5623(3.6147), Acc: 0.4658(0.3740)
2022-01-07 00:02:45,684 ----- Epoch[255/310], Train Loss: 3.6147, Train Acc: 0.3740, time: 1577.82, Best Val(epoch252) Acc@1: 0.7202
2022-01-07 00:02:45,855 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 00:02:45,856 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 00:02:45,962 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 00:02:45,962 Now training epoch 256. LR=0.000057
2022-01-07 00:04:13,470 Epoch[256/310], Step[0000/1251], Loss: 3.7009(3.7009), Acc: 0.4482(0.4482)
2022-01-07 00:05:13,506 Epoch[256/310], Step[0050/1251], Loss: 3.4952(3.5548), Acc: 0.3086(0.3710)
2022-01-07 00:06:12,697 Epoch[256/310], Step[0100/1251], Loss: 3.4018(3.6044), Acc: 0.4355(0.3688)
2022-01-07 00:07:12,028 Epoch[256/310], Step[0150/1251], Loss: 3.1486(3.6158), Acc: 0.2705(0.3775)
2022-01-07 00:08:11,953 Epoch[256/310], Step[0200/1251], Loss: 3.1543(3.6097), Acc: 0.4434(0.3844)
2022-01-07 00:09:11,398 Epoch[256/310], Step[0250/1251], Loss: 3.3860(3.6026), Acc: 0.3896(0.3797)
2022-01-07 00:10:12,415 Epoch[256/310], Step[0300/1251], Loss: 3.2582(3.6001), Acc: 0.4375(0.3814)
2022-01-07 00:11:12,958 Epoch[256/310], Step[0350/1251], Loss: 3.4129(3.6034), Acc: 0.4756(0.3827)
2022-01-07 00:12:12,479 Epoch[256/310], Step[0400/1251], Loss: 3.4757(3.5917), Acc: 0.4180(0.3854)
2022-01-07 00:13:11,681 Epoch[256/310], Step[0450/1251], Loss: 3.5811(3.5886), Acc: 0.3525(0.3836)
2022-01-07 00:14:11,198 Epoch[256/310], Step[0500/1251], Loss: 3.6030(3.5791), Acc: 0.4307(0.3839)
2022-01-07 00:15:10,677 Epoch[256/310], Step[0550/1251], Loss: 3.1952(3.5762), Acc: 0.5264(0.3848)
2022-01-07 00:16:11,305 Epoch[256/310], Step[0600/1251], Loss: 3.1616(3.5847), Acc: 0.5938(0.3832)
2022-01-07 00:17:11,872 Epoch[256/310], Step[0650/1251], Loss: 3.3827(3.5865), Acc: 0.3457(0.3836)
2022-01-07 00:18:11,333 Epoch[256/310], Step[0700/1251], Loss: 3.7613(3.5940), Acc: 0.3945(0.3832)
2022-01-07 00:19:10,574 Epoch[256/310], Step[0750/1251], Loss: 3.5288(3.6037), Acc: 0.5713(0.3834)
2022-01-07 00:20:09,608 Epoch[256/310], Step[0800/1251], Loss: 3.7403(3.6050), Acc: 0.2959(0.3835)
2022-01-07 00:21:09,653 Epoch[256/310], Step[0850/1251], Loss: 3.7092(3.6052), Acc: 0.4727(0.3830)
2022-01-07 00:22:07,856 Epoch[256/310], Step[0900/1251], Loss: 3.6079(3.6035), Acc: 0.3086(0.3845)
2022-01-07 00:23:04,555 Epoch[256/310], Step[0950/1251], Loss: 3.6399(3.6066), Acc: 0.5186(0.3857)
2022-01-07 00:24:01,801 Epoch[256/310], Step[1000/1251], Loss: 3.4070(3.6063), Acc: 0.5928(0.3868)
2022-01-07 00:25:00,475 Epoch[256/310], Step[1050/1251], Loss: 3.7585(3.6041), Acc: 0.3096(0.3853)
2022-01-07 00:26:00,127 Epoch[256/310], Step[1100/1251], Loss: 3.9144(3.6052), Acc: 0.3691(0.3847)
2022-01-07 00:27:01,026 Epoch[256/310], Step[1150/1251], Loss: 3.4171(3.6031), Acc: 0.5938(0.3849)
2022-01-07 00:28:01,015 Epoch[256/310], Step[1200/1251], Loss: 3.9587(3.6054), Acc: 0.3809(0.3853)
2022-01-07 00:29:01,919 Epoch[256/310], Step[1250/1251], Loss: 3.6315(3.6053), Acc: 0.3330(0.3852)
2022-01-07 00:29:03,425 ----- Validation after Epoch: 256
2022-01-07 00:30:08,958 Val Step[0000/1563], Loss: 0.7721 (0.7721), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 00:30:10,314 Val Step[0050/1563], Loss: 2.1358 (0.7410), Acc@1: 0.4062 (0.8493), Acc@5: 0.8750 (0.9614)
2022-01-07 00:30:11,594 Val Step[0100/1563], Loss: 1.9718 (1.0151), Acc@1: 0.5312 (0.7775), Acc@5: 0.8125 (0.9353)
2022-01-07 00:30:12,870 Val Step[0150/1563], Loss: 0.4042 (0.9634), Acc@1: 0.9375 (0.7904), Acc@5: 1.0000 (0.9408)
2022-01-07 00:30:14,152 Val Step[0200/1563], Loss: 1.0566 (0.9717), Acc@1: 0.8125 (0.7909), Acc@5: 0.9375 (0.9411)
2022-01-07 00:30:15,466 Val Step[0250/1563], Loss: 0.5167 (0.9218), Acc@1: 0.9375 (0.8027), Acc@5: 1.0000 (0.9461)
2022-01-07 00:30:16,769 Val Step[0300/1563], Loss: 1.0573 (0.9818), Acc@1: 0.7188 (0.7866), Acc@5: 0.9688 (0.9409)
2022-01-07 00:30:18,049 Val Step[0350/1563], Loss: 0.9716 (0.9896), Acc@1: 0.7812 (0.7831), Acc@5: 0.9062 (0.9429)
2022-01-07 00:30:19,427 Val Step[0400/1563], Loss: 0.8138 (1.0004), Acc@1: 0.8125 (0.7777), Acc@5: 0.9688 (0.9427)
2022-01-07 00:30:20,860 Val Step[0450/1563], Loss: 1.0295 (1.0070), Acc@1: 0.6875 (0.7749), Acc@5: 1.0000 (0.9433)
2022-01-07 00:30:22,201 Val Step[0500/1563], Loss: 0.4281 (0.9970), Acc@1: 0.9062 (0.7781), Acc@5: 1.0000 (0.9447)
2022-01-07 00:30:23,663 Val Step[0550/1563], Loss: 0.7846 (0.9762), Acc@1: 0.8438 (0.7833), Acc@5: 0.9688 (0.9464)
2022-01-07 00:30:25,011 Val Step[0600/1563], Loss: 0.7902 (0.9821), Acc@1: 0.8125 (0.7825), Acc@5: 0.9375 (0.9459)
2022-01-07 00:30:26,398 Val Step[0650/1563], Loss: 0.5523 (1.0026), Acc@1: 0.9375 (0.7779), Acc@5: 1.0000 (0.9431)
2022-01-07 00:30:27,716 Val Step[0700/1563], Loss: 0.9956 (1.0280), Acc@1: 0.8438 (0.7713), Acc@5: 0.9688 (0.9402)
2022-01-07 00:30:29,073 Val Step[0750/1563], Loss: 1.2301 (1.0577), Acc@1: 0.8125 (0.7654), Acc@5: 0.8750 (0.9360)
2022-01-07 00:30:30,490 Val Step[0800/1563], Loss: 0.8185 (1.0940), Acc@1: 0.8438 (0.7565), Acc@5: 1.0000 (0.9311)
2022-01-07 00:30:31,869 Val Step[0850/1563], Loss: 1.3343 (1.1179), Acc@1: 0.6875 (0.7502), Acc@5: 0.9375 (0.9280)
2022-01-07 00:30:33,261 Val Step[0900/1563], Loss: 0.2770 (1.1175), Acc@1: 1.0000 (0.7515), Acc@5: 1.0000 (0.9275)
2022-01-07 00:30:34,678 Val Step[0950/1563], Loss: 1.4512 (1.1390), Acc@1: 0.7812 (0.7471), Acc@5: 0.8750 (0.9240)
2022-01-07 00:30:36,030 Val Step[1000/1563], Loss: 0.5923 (1.1620), Acc@1: 0.9375 (0.7411), Acc@5: 0.9688 (0.9211)
2022-01-07 00:30:37,324 Val Step[1050/1563], Loss: 0.3665 (1.1755), Acc@1: 0.9688 (0.7377), Acc@5: 1.0000 (0.9196)
2022-01-07 00:30:38,585 Val Step[1100/1563], Loss: 0.9051 (1.1901), Acc@1: 0.7812 (0.7347), Acc@5: 0.9688 (0.9176)
2022-01-07 00:30:39,889 Val Step[1150/1563], Loss: 1.2864 (1.2032), Acc@1: 0.7812 (0.7321), Acc@5: 0.8125 (0.9156)
2022-01-07 00:30:41,170 Val Step[1200/1563], Loss: 1.1875 (1.2165), Acc@1: 0.7812 (0.7293), Acc@5: 0.8438 (0.9132)
2022-01-07 00:30:42,436 Val Step[1250/1563], Loss: 0.6970 (1.2269), Acc@1: 0.8750 (0.7275), Acc@5: 0.9375 (0.9116)
2022-01-07 00:30:43,719 Val Step[1300/1563], Loss: 0.7474 (1.2357), Acc@1: 0.9062 (0.7257), Acc@5: 0.9375 (0.9107)
2022-01-07 00:30:44,979 Val Step[1350/1563], Loss: 1.8998 (1.2518), Acc@1: 0.5000 (0.7213), Acc@5: 0.8438 (0.9084)
2022-01-07 00:30:46,318 Val Step[1400/1563], Loss: 1.0141 (1.2576), Acc@1: 0.7500 (0.7198), Acc@5: 0.9375 (0.9075)
2022-01-07 00:30:47,573 Val Step[1450/1563], Loss: 1.4368 (1.2638), Acc@1: 0.7188 (0.7179), Acc@5: 0.9375 (0.9072)
2022-01-07 00:30:48,832 Val Step[1500/1563], Loss: 1.6769 (1.2536), Acc@1: 0.6562 (0.7204), Acc@5: 0.8438 (0.9085)
2022-01-07 00:30:50,115 Val Step[1550/1563], Loss: 0.9509 (1.2557), Acc@1: 0.8750 (0.7197), Acc@5: 0.9062 (0.9082)
2022-01-07 00:30:50,969 ----- Epoch[256/310], Validation Loss: 1.2542, Validation Acc@1: 0.7200, Validation Acc@5: 0.9083, time: 107.54
2022-01-07 00:30:50,969 ----- Epoch[256/310], Train Loss: 3.6053, Train Acc: 0.3852, time: 1577.46, Best Val(epoch252) Acc@1: 0.7202
2022-01-07 00:30:51,154 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 00:30:51,155 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 00:30:51,261 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 00:30:51,262 Now training epoch 257. LR=0.000055
2022-01-07 00:32:07,744 Epoch[257/310], Step[0000/1251], Loss: 2.6210(2.6210), Acc: 0.3281(0.3281)
2022-01-07 00:33:07,038 Epoch[257/310], Step[0050/1251], Loss: 3.3341(3.4874), Acc: 0.2666(0.3797)
2022-01-07 00:34:05,641 Epoch[257/310], Step[0100/1251], Loss: 4.2210(3.5590), Acc: 0.3770(0.3847)
2022-01-07 00:35:03,684 Epoch[257/310], Step[0150/1251], Loss: 3.2399(3.5658), Acc: 0.3320(0.3821)
2022-01-07 00:36:02,293 Epoch[257/310], Step[0200/1251], Loss: 3.8282(3.5892), Acc: 0.4746(0.3784)
2022-01-07 00:37:01,836 Epoch[257/310], Step[0250/1251], Loss: 3.9015(3.5880), Acc: 0.1816(0.3808)
2022-01-07 00:38:01,398 Epoch[257/310], Step[0300/1251], Loss: 3.1321(3.5913), Acc: 0.6191(0.3800)
2022-01-07 00:39:01,060 Epoch[257/310], Step[0350/1251], Loss: 3.4986(3.5942), Acc: 0.4180(0.3789)
2022-01-07 00:39:59,693 Epoch[257/310], Step[0400/1251], Loss: 3.9138(3.6072), Acc: 0.3086(0.3786)
2022-01-07 00:40:59,827 Epoch[257/310], Step[0450/1251], Loss: 4.0798(3.6085), Acc: 0.2666(0.3787)
2022-01-07 00:41:58,472 Epoch[257/310], Step[0500/1251], Loss: 3.3135(3.6103), Acc: 0.3682(0.3814)
2022-01-07 00:42:57,044 Epoch[257/310], Step[0550/1251], Loss: 3.0489(3.6088), Acc: 0.0088(0.3801)
2022-01-07 00:43:57,450 Epoch[257/310], Step[0600/1251], Loss: 3.4172(3.6171), Acc: 0.2666(0.3793)
2022-01-07 00:44:58,069 Epoch[257/310], Step[0650/1251], Loss: 3.2516(3.6152), Acc: 0.5703(0.3787)
2022-01-07 00:45:57,637 Epoch[257/310], Step[0700/1251], Loss: 3.9950(3.6178), Acc: 0.2002(0.3796)
2022-01-07 00:46:56,905 Epoch[257/310], Step[0750/1251], Loss: 3.4326(3.6213), Acc: 0.5479(0.3779)
2022-01-07 00:47:55,565 Epoch[257/310], Step[0800/1251], Loss: 3.4106(3.6197), Acc: 0.5762(0.3793)
2022-01-07 00:48:53,800 Epoch[257/310], Step[0850/1251], Loss: 3.2153(3.6233), Acc: 0.4170(0.3795)
2022-01-07 00:49:51,250 Epoch[257/310], Step[0900/1251], Loss: 3.7726(3.6235), Acc: 0.3242(0.3803)
2022-01-07 00:50:49,195 Epoch[257/310], Step[0950/1251], Loss: 3.4572(3.6207), Acc: 0.3936(0.3813)
2022-01-07 00:51:48,680 Epoch[257/310], Step[1000/1251], Loss: 3.2901(3.6202), Acc: 0.4482(0.3805)
2022-01-07 00:52:45,613 Epoch[257/310], Step[1050/1251], Loss: 3.8408(3.6206), Acc: 0.1924(0.3814)
2022-01-07 00:53:44,067 Epoch[257/310], Step[1100/1251], Loss: 3.6961(3.6162), Acc: 0.4736(0.3824)
2022-01-07 00:54:42,663 Epoch[257/310], Step[1150/1251], Loss: 3.2751(3.6159), Acc: 0.5469(0.3821)
2022-01-07 00:55:41,652 Epoch[257/310], Step[1200/1251], Loss: 3.8811(3.6175), Acc: 0.3477(0.3824)
2022-01-07 00:56:40,781 Epoch[257/310], Step[1250/1251], Loss: 3.4627(3.6154), Acc: 0.5850(0.3828)
2022-01-07 00:56:43,062 ----- Epoch[257/310], Train Loss: 3.6154, Train Acc: 0.3828, time: 1551.80, Best Val(epoch252) Acc@1: 0.7202
2022-01-07 00:56:43,235 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 00:56:43,235 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 00:56:43,339 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 00:56:43,340 Now training epoch 258. LR=0.000052
2022-01-07 00:58:02,756 Epoch[258/310], Step[0000/1251], Loss: 3.6235(3.6235), Acc: 0.4980(0.4980)
2022-01-07 00:59:03,038 Epoch[258/310], Step[0050/1251], Loss: 3.7893(3.5636), Acc: 0.2822(0.4082)
2022-01-07 01:00:02,827 Epoch[258/310], Step[0100/1251], Loss: 3.6702(3.5917), Acc: 0.5322(0.3963)
2022-01-07 01:01:03,182 Epoch[258/310], Step[0150/1251], Loss: 2.9624(3.6053), Acc: 0.4473(0.3926)
2022-01-07 01:02:01,957 Epoch[258/310], Step[0200/1251], Loss: 3.6000(3.6105), Acc: 0.2881(0.3844)
2022-01-07 01:03:00,843 Epoch[258/310], Step[0250/1251], Loss: 3.7965(3.5998), Acc: 0.2588(0.3797)
2022-01-07 01:03:58,309 Epoch[258/310], Step[0300/1251], Loss: 3.2654(3.6022), Acc: 0.4658(0.3843)
2022-01-07 01:04:58,111 Epoch[258/310], Step[0350/1251], Loss: 3.8261(3.6057), Acc: 0.4971(0.3836)
2022-01-07 01:05:57,630 Epoch[258/310], Step[0400/1251], Loss: 2.8791(3.6023), Acc: 0.1729(0.3830)
2022-01-07 01:06:57,398 Epoch[258/310], Step[0450/1251], Loss: 3.9221(3.6116), Acc: 0.3623(0.3811)
2022-01-07 01:07:58,040 Epoch[258/310], Step[0500/1251], Loss: 3.3554(3.6198), Acc: 0.3955(0.3805)
2022-01-07 01:08:57,725 Epoch[258/310], Step[0550/1251], Loss: 3.6034(3.6161), Acc: 0.3428(0.3816)
2022-01-07 01:09:57,364 Epoch[258/310], Step[0600/1251], Loss: 3.2284(3.6139), Acc: 0.3662(0.3850)
2022-01-07 01:10:56,870 Epoch[258/310], Step[0650/1251], Loss: 3.3437(3.6158), Acc: 0.1055(0.3833)
2022-01-07 01:11:57,777 Epoch[258/310], Step[0700/1251], Loss: 3.7329(3.6177), Acc: 0.3428(0.3844)
2022-01-07 01:12:57,427 Epoch[258/310], Step[0750/1251], Loss: 3.6728(3.6125), Acc: 0.5410(0.3847)
2022-01-07 01:13:57,333 Epoch[258/310], Step[0800/1251], Loss: 3.4937(3.6111), Acc: 0.2051(0.3830)
2022-01-07 01:14:57,215 Epoch[258/310], Step[0850/1251], Loss: 3.8833(3.6100), Acc: 0.2793(0.3824)
2022-01-07 01:15:58,503 Epoch[258/310], Step[0900/1251], Loss: 3.7144(3.6124), Acc: 0.3564(0.3821)
2022-01-07 01:16:58,044 Epoch[258/310], Step[0950/1251], Loss: 3.6994(3.6117), Acc: 0.4131(0.3820)
2022-01-07 01:17:57,845 Epoch[258/310], Step[1000/1251], Loss: 3.5879(3.6113), Acc: 0.5361(0.3814)
2022-01-07 01:18:57,604 Epoch[258/310], Step[1050/1251], Loss: 3.6815(3.6126), Acc: 0.4355(0.3812)
2022-01-07 01:19:57,729 Epoch[258/310], Step[1100/1251], Loss: 3.7146(3.6130), Acc: 0.4746(0.3809)
2022-01-07 01:20:58,275 Epoch[258/310], Step[1150/1251], Loss: 3.9290(3.6126), Acc: 0.3213(0.3812)
2022-01-07 01:21:56,432 Epoch[258/310], Step[1200/1251], Loss: 4.1021(3.6148), Acc: 0.4141(0.3811)
2022-01-07 01:22:55,829 Epoch[258/310], Step[1250/1251], Loss: 3.5111(3.6163), Acc: 0.5596(0.3811)
2022-01-07 01:22:57,541 ----- Validation after Epoch: 258
2022-01-07 01:23:55,582 Val Step[0000/1563], Loss: 0.6948 (0.6948), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 01:23:56,947 Val Step[0050/1563], Loss: 2.0998 (0.7339), Acc@1: 0.4375 (0.8554), Acc@5: 0.9062 (0.9608)
2022-01-07 01:23:58,377 Val Step[0100/1563], Loss: 1.8088 (0.9885), Acc@1: 0.5000 (0.7837), Acc@5: 0.8438 (0.9381)
2022-01-07 01:23:59,732 Val Step[0150/1563], Loss: 0.4410 (0.9376), Acc@1: 0.9062 (0.7949), Acc@5: 1.0000 (0.9423)
2022-01-07 01:24:01,007 Val Step[0200/1563], Loss: 0.9682 (0.9442), Acc@1: 0.8125 (0.7977), Acc@5: 0.9375 (0.9415)
2022-01-07 01:24:02,415 Val Step[0250/1563], Loss: 0.5330 (0.8955), Acc@1: 0.9062 (0.8104), Acc@5: 1.0000 (0.9470)
2022-01-07 01:24:03,955 Val Step[0300/1563], Loss: 1.0488 (0.9566), Acc@1: 0.6875 (0.7937), Acc@5: 0.9688 (0.9419)
2022-01-07 01:24:05,374 Val Step[0350/1563], Loss: 1.0067 (0.9609), Acc@1: 0.7500 (0.7895), Acc@5: 0.9062 (0.9440)
2022-01-07 01:24:06,829 Val Step[0400/1563], Loss: 0.8776 (0.9702), Acc@1: 0.8438 (0.7841), Acc@5: 0.9688 (0.9442)
2022-01-07 01:24:08,292 Val Step[0450/1563], Loss: 0.8559 (0.9750), Acc@1: 0.7500 (0.7806), Acc@5: 1.0000 (0.9448)
2022-01-07 01:24:09,738 Val Step[0500/1563], Loss: 0.5054 (0.9677), Acc@1: 0.8750 (0.7824), Acc@5: 1.0000 (0.9459)
2022-01-07 01:24:11,296 Val Step[0550/1563], Loss: 0.7242 (0.9477), Acc@1: 0.8438 (0.7875), Acc@5: 0.9688 (0.9475)
2022-01-07 01:24:12,751 Val Step[0600/1563], Loss: 0.8034 (0.9548), Acc@1: 0.8125 (0.7863), Acc@5: 0.9062 (0.9469)
2022-01-07 01:24:14,165 Val Step[0650/1563], Loss: 0.5314 (0.9762), Acc@1: 0.9062 (0.7816), Acc@5: 1.0000 (0.9438)
2022-01-07 01:24:15,638 Val Step[0700/1563], Loss: 1.2274 (1.0059), Acc@1: 0.7188 (0.7741), Acc@5: 0.9062 (0.9402)
2022-01-07 01:24:17,126 Val Step[0750/1563], Loss: 1.2497 (1.0379), Acc@1: 0.7812 (0.7673), Acc@5: 0.9375 (0.9358)
2022-01-07 01:24:18,555 Val Step[0800/1563], Loss: 0.7260 (1.0744), Acc@1: 0.8125 (0.7579), Acc@5: 1.0000 (0.9310)
2022-01-07 01:24:19,970 Val Step[0850/1563], Loss: 1.3471 (1.0981), Acc@1: 0.6562 (0.7520), Acc@5: 0.9375 (0.9282)
2022-01-07 01:24:21,379 Val Step[0900/1563], Loss: 0.2700 (1.0970), Acc@1: 0.9688 (0.7537), Acc@5: 1.0000 (0.9278)
2022-01-07 01:24:22,883 Val Step[0950/1563], Loss: 1.3411 (1.1166), Acc@1: 0.7188 (0.7500), Acc@5: 0.9375 (0.9248)
2022-01-07 01:24:24,321 Val Step[1000/1563], Loss: 0.6072 (1.1402), Acc@1: 0.9375 (0.7436), Acc@5: 1.0000 (0.9216)
2022-01-07 01:24:25,739 Val Step[1050/1563], Loss: 0.4173 (1.1521), Acc@1: 0.9375 (0.7407), Acc@5: 0.9688 (0.9203)
2022-01-07 01:24:27,169 Val Step[1100/1563], Loss: 0.7483 (1.1661), Acc@1: 0.8750 (0.7379), Acc@5: 0.9688 (0.9183)
2022-01-07 01:24:28,695 Val Step[1150/1563], Loss: 1.2771 (1.1800), Acc@1: 0.7812 (0.7353), Acc@5: 0.8125 (0.9165)
2022-01-07 01:24:30,225 Val Step[1200/1563], Loss: 1.1046 (1.1931), Acc@1: 0.8125 (0.7323), Acc@5: 0.8750 (0.9144)
2022-01-07 01:24:31,659 Val Step[1250/1563], Loss: 0.7812 (1.2034), Acc@1: 0.8750 (0.7304), Acc@5: 0.9375 (0.9128)
2022-01-07 01:24:33,088 Val Step[1300/1563], Loss: 0.7634 (1.2123), Acc@1: 0.8750 (0.7286), Acc@5: 0.9062 (0.9115)
2022-01-07 01:24:34,488 Val Step[1350/1563], Loss: 1.9706 (1.2307), Acc@1: 0.4688 (0.7244), Acc@5: 0.7812 (0.9087)
2022-01-07 01:24:35,892 Val Step[1400/1563], Loss: 1.0269 (1.2367), Acc@1: 0.7500 (0.7231), Acc@5: 0.9688 (0.9080)
2022-01-07 01:24:37,310 Val Step[1450/1563], Loss: 1.4285 (1.2421), Acc@1: 0.6250 (0.7217), Acc@5: 0.9375 (0.9078)
2022-01-07 01:24:38,759 Val Step[1500/1563], Loss: 1.4842 (1.2313), Acc@1: 0.6562 (0.7240), Acc@5: 0.8750 (0.9091)
2022-01-07 01:24:40,168 Val Step[1550/1563], Loss: 0.9537 (1.2342), Acc@1: 0.8750 (0.7228), Acc@5: 0.9062 (0.9088)
2022-01-07 01:24:40,977 ----- Epoch[258/310], Validation Loss: 1.2327, Validation Acc@1: 0.7232, Validation Acc@5: 0.9090, time: 103.43
2022-01-07 01:24:40,977 ----- Epoch[258/310], Train Loss: 3.6163, Train Acc: 0.3811, time: 1574.20, Best Val(epoch258) Acc@1: 0.7232
2022-01-07 01:24:41,159 Max accuracy so far: 0.7232 at epoch_258
2022-01-07 01:24:41,159 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 01:24:41,159 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 01:24:41,262 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 01:24:41,652 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 01:24:41,652 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 01:24:41,781 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 01:24:41,782 Now training epoch 259. LR=0.000050
2022-01-07 01:25:55,085 Epoch[259/310], Step[0000/1251], Loss: 4.1130(4.1130), Acc: 0.1895(0.1895)
2022-01-07 01:26:54,800 Epoch[259/310], Step[0050/1251], Loss: 3.6494(3.6300), Acc: 0.3564(0.3789)
2022-01-07 01:27:54,143 Epoch[259/310], Step[0100/1251], Loss: 4.0307(3.6004), Acc: 0.4883(0.3719)
2022-01-07 01:28:53,177 Epoch[259/310], Step[0150/1251], Loss: 4.0136(3.6004), Acc: 0.3936(0.3756)
2022-01-07 01:29:52,176 Epoch[259/310], Step[0200/1251], Loss: 3.6930(3.6010), Acc: 0.3936(0.3790)
2022-01-07 01:30:49,200 Epoch[259/310], Step[0250/1251], Loss: 3.6983(3.5889), Acc: 0.4707(0.3828)
2022-01-07 01:31:46,112 Epoch[259/310], Step[0300/1251], Loss: 3.7075(3.5835), Acc: 0.2236(0.3852)
2022-01-07 01:32:44,671 Epoch[259/310], Step[0350/1251], Loss: 3.6752(3.5827), Acc: 0.2617(0.3869)
2022-01-07 01:33:41,277 Epoch[259/310], Step[0400/1251], Loss: 3.3454(3.5788), Acc: 0.2246(0.3874)
2022-01-07 01:34:39,075 Epoch[259/310], Step[0450/1251], Loss: 3.4106(3.5771), Acc: 0.3213(0.3863)
2022-01-07 01:35:37,376 Epoch[259/310], Step[0500/1251], Loss: 3.8748(3.5798), Acc: 0.3555(0.3852)
2022-01-07 01:36:36,843 Epoch[259/310], Step[0550/1251], Loss: 3.9709(3.5826), Acc: 0.3164(0.3853)
2022-01-07 01:37:34,889 Epoch[259/310], Step[0600/1251], Loss: 3.4822(3.5861), Acc: 0.3574(0.3847)
2022-01-07 01:38:34,786 Epoch[259/310], Step[0650/1251], Loss: 2.9395(3.5873), Acc: 0.0059(0.3827)
2022-01-07 01:39:33,209 Epoch[259/310], Step[0700/1251], Loss: 3.6991(3.5948), Acc: 0.3848(0.3825)
2022-01-07 01:40:32,456 Epoch[259/310], Step[0750/1251], Loss: 3.5023(3.5967), Acc: 0.0371(0.3822)
2022-01-07 01:41:31,908 Epoch[259/310], Step[0800/1251], Loss: 3.6143(3.5956), Acc: 0.3633(0.3826)
2022-01-07 01:42:31,946 Epoch[259/310], Step[0850/1251], Loss: 3.5673(3.5976), Acc: 0.4941(0.3827)
2022-01-07 01:43:31,866 Epoch[259/310], Step[0900/1251], Loss: 3.6896(3.5954), Acc: 0.5322(0.3844)
2022-01-07 01:44:33,225 Epoch[259/310], Step[0950/1251], Loss: 4.0893(3.5952), Acc: 0.4561(0.3839)
2022-01-07 01:45:32,322 Epoch[259/310], Step[1000/1251], Loss: 3.7088(3.5932), Acc: 0.4512(0.3840)
2022-01-07 01:46:32,610 Epoch[259/310], Step[1050/1251], Loss: 3.7660(3.5932), Acc: 0.4736(0.3840)
2022-01-07 01:47:32,560 Epoch[259/310], Step[1100/1251], Loss: 2.8989(3.5921), Acc: 0.4688(0.3832)
2022-01-07 01:48:29,739 Epoch[259/310], Step[1150/1251], Loss: 2.8523(3.5929), Acc: 0.3486(0.3832)
2022-01-07 01:49:28,246 Epoch[259/310], Step[1200/1251], Loss: 3.9242(3.5954), Acc: 0.2197(0.3822)
2022-01-07 01:50:26,089 Epoch[259/310], Step[1250/1251], Loss: 3.7951(3.5966), Acc: 0.4268(0.3822)
2022-01-07 01:50:27,569 ----- Epoch[259/310], Train Loss: 3.5966, Train Acc: 0.3822, time: 1545.78, Best Val(epoch258) Acc@1: 0.7232
2022-01-07 01:50:27,741 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 01:50:27,741 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 01:50:27,844 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 01:50:27,844 Now training epoch 260. LR=0.000048
2022-01-07 01:51:43,756 Epoch[260/310], Step[0000/1251], Loss: 3.3894(3.3894), Acc: 0.5596(0.5596)
2022-01-07 01:52:43,159 Epoch[260/310], Step[0050/1251], Loss: 3.5134(3.4981), Acc: 0.4971(0.3881)
2022-01-07 01:53:42,808 Epoch[260/310], Step[0100/1251], Loss: 3.6459(3.5201), Acc: 0.4629(0.3913)
2022-01-07 01:54:43,072 Epoch[260/310], Step[0150/1251], Loss: 3.6632(3.5660), Acc: 0.2197(0.3866)
2022-01-07 01:55:42,906 Epoch[260/310], Step[0200/1251], Loss: 3.1255(3.5663), Acc: 0.5176(0.3915)
2022-01-07 01:56:43,408 Epoch[260/310], Step[0250/1251], Loss: 4.2542(3.5758), Acc: 0.2412(0.3833)
2022-01-07 01:57:43,859 Epoch[260/310], Step[0300/1251], Loss: 3.4845(3.5772), Acc: 0.4717(0.3809)
2022-01-07 01:58:45,045 Epoch[260/310], Step[0350/1251], Loss: 3.6712(3.5795), Acc: 0.4648(0.3802)
2022-01-07 01:59:45,660 Epoch[260/310], Step[0400/1251], Loss: 3.7503(3.5783), Acc: 0.4590(0.3837)
2022-01-07 02:00:46,892 Epoch[260/310], Step[0450/1251], Loss: 3.5317(3.5797), Acc: 0.1914(0.3819)
2022-01-07 02:01:47,781 Epoch[260/310], Step[0500/1251], Loss: 4.0048(3.5876), Acc: 0.4258(0.3810)
2022-01-07 02:02:49,208 Epoch[260/310], Step[0550/1251], Loss: 3.7895(3.5898), Acc: 0.2090(0.3805)
2022-01-07 02:03:50,835 Epoch[260/310], Step[0600/1251], Loss: 3.6884(3.5924), Acc: 0.5352(0.3816)
2022-01-07 02:04:49,633 Epoch[260/310], Step[0650/1251], Loss: 3.8473(3.5933), Acc: 0.3271(0.3823)
2022-01-07 02:05:50,570 Epoch[260/310], Step[0700/1251], Loss: 3.7758(3.5952), Acc: 0.4678(0.3813)
2022-01-07 02:06:50,118 Epoch[260/310], Step[0750/1251], Loss: 3.9791(3.6043), Acc: 0.2949(0.3805)
2022-01-07 02:07:49,217 Epoch[260/310], Step[0800/1251], Loss: 3.9962(3.6035), Acc: 0.3887(0.3804)
2022-01-07 02:08:49,762 Epoch[260/310], Step[0850/1251], Loss: 4.1068(3.6010), Acc: 0.2471(0.3809)
2022-01-07 02:09:49,639 Epoch[260/310], Step[0900/1251], Loss: 4.1013(3.6021), Acc: 0.3555(0.3811)
2022-01-07 02:10:49,399 Epoch[260/310], Step[0950/1251], Loss: 3.9482(3.6024), Acc: 0.4453(0.3809)
2022-01-07 02:11:48,030 Epoch[260/310], Step[1000/1251], Loss: 3.2529(3.6018), Acc: 0.5615(0.3809)
2022-01-07 02:12:47,678 Epoch[260/310], Step[1050/1251], Loss: 3.5085(3.6025), Acc: 0.1826(0.3799)
2022-01-07 02:13:48,374 Epoch[260/310], Step[1100/1251], Loss: 3.3785(3.6043), Acc: 0.2080(0.3798)
2022-01-07 02:14:47,897 Epoch[260/310], Step[1150/1251], Loss: 3.8749(3.6043), Acc: 0.4375(0.3804)
2022-01-07 02:15:47,709 Epoch[260/310], Step[1200/1251], Loss: 4.2071(3.6051), Acc: 0.2891(0.3801)
2022-01-07 02:16:46,044 Epoch[260/310], Step[1250/1251], Loss: 3.2102(3.6055), Acc: 0.5859(0.3796)
2022-01-07 02:16:47,571 ----- Validation after Epoch: 260
2022-01-07 02:17:42,476 Val Step[0000/1563], Loss: 0.7276 (0.7276), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 02:17:43,806 Val Step[0050/1563], Loss: 2.1673 (0.7270), Acc@1: 0.4062 (0.8536), Acc@5: 0.8750 (0.9645)
2022-01-07 02:17:45,076 Val Step[0100/1563], Loss: 1.8992 (0.9920), Acc@1: 0.5000 (0.7794), Acc@5: 0.8438 (0.9425)
2022-01-07 02:17:46,348 Val Step[0150/1563], Loss: 0.4069 (0.9388), Acc@1: 0.9375 (0.7943), Acc@5: 1.0000 (0.9462)
2022-01-07 02:17:47,682 Val Step[0200/1563], Loss: 1.0281 (0.9461), Acc@1: 0.8125 (0.7970), Acc@5: 0.9375 (0.9436)
2022-01-07 02:17:48,999 Val Step[0250/1563], Loss: 0.5031 (0.8975), Acc@1: 0.9375 (0.8068), Acc@5: 1.0000 (0.9487)
2022-01-07 02:17:50,276 Val Step[0300/1563], Loss: 0.9829 (0.9543), Acc@1: 0.7188 (0.7903), Acc@5: 0.9688 (0.9437)
2022-01-07 02:17:51,555 Val Step[0350/1563], Loss: 1.0743 (0.9620), Acc@1: 0.7500 (0.7861), Acc@5: 0.9062 (0.9450)
2022-01-07 02:17:52,856 Val Step[0400/1563], Loss: 0.8346 (0.9715), Acc@1: 0.8750 (0.7809), Acc@5: 0.9688 (0.9450)
2022-01-07 02:17:54,302 Val Step[0450/1563], Loss: 0.9469 (0.9797), Acc@1: 0.7500 (0.7782), Acc@5: 1.0000 (0.9452)
2022-01-07 02:17:55,697 Val Step[0500/1563], Loss: 0.5186 (0.9739), Acc@1: 0.8750 (0.7800), Acc@5: 1.0000 (0.9459)
2022-01-07 02:17:57,122 Val Step[0550/1563], Loss: 0.8109 (0.9556), Acc@1: 0.8125 (0.7843), Acc@5: 0.9688 (0.9471)
2022-01-07 02:17:58,409 Val Step[0600/1563], Loss: 0.8245 (0.9621), Acc@1: 0.7812 (0.7838), Acc@5: 0.9375 (0.9462)
2022-01-07 02:17:59,703 Val Step[0650/1563], Loss: 0.5852 (0.9824), Acc@1: 0.9062 (0.7799), Acc@5: 1.0000 (0.9430)
2022-01-07 02:18:00,969 Val Step[0700/1563], Loss: 0.9475 (1.0089), Acc@1: 0.8125 (0.7727), Acc@5: 0.9375 (0.9397)
2022-01-07 02:18:02,247 Val Step[0750/1563], Loss: 1.1882 (1.0416), Acc@1: 0.7500 (0.7665), Acc@5: 0.9062 (0.9350)
2022-01-07 02:18:03,520 Val Step[0800/1563], Loss: 0.7766 (1.0775), Acc@1: 0.7812 (0.7579), Acc@5: 1.0000 (0.9305)
2022-01-07 02:18:04,807 Val Step[0850/1563], Loss: 1.2260 (1.1026), Acc@1: 0.7188 (0.7517), Acc@5: 0.9375 (0.9274)
2022-01-07 02:18:06,135 Val Step[0900/1563], Loss: 0.3463 (1.1028), Acc@1: 0.9688 (0.7529), Acc@5: 1.0000 (0.9270)
2022-01-07 02:18:07,515 Val Step[0950/1563], Loss: 1.2725 (1.1212), Acc@1: 0.7500 (0.7491), Acc@5: 0.9062 (0.9240)
2022-01-07 02:18:08,891 Val Step[1000/1563], Loss: 0.5376 (1.1445), Acc@1: 0.9688 (0.7428), Acc@5: 1.0000 (0.9209)
2022-01-07 02:18:10,242 Val Step[1050/1563], Loss: 0.2720 (1.1590), Acc@1: 0.9688 (0.7392), Acc@5: 1.0000 (0.9195)
2022-01-07 02:18:11,618 Val Step[1100/1563], Loss: 0.8560 (1.1723), Acc@1: 0.7188 (0.7364), Acc@5: 1.0000 (0.9174)
2022-01-07 02:18:12,908 Val Step[1150/1563], Loss: 1.3737 (1.1860), Acc@1: 0.7812 (0.7337), Acc@5: 0.7812 (0.9156)
2022-01-07 02:18:14,251 Val Step[1200/1563], Loss: 1.2454 (1.1984), Acc@1: 0.7188 (0.7301), Acc@5: 0.8438 (0.9133)
2022-01-07 02:18:15,509 Val Step[1250/1563], Loss: 0.7000 (1.2087), Acc@1: 0.8750 (0.7282), Acc@5: 0.9375 (0.9119)
2022-01-07 02:18:16,772 Val Step[1300/1563], Loss: 0.7856 (1.2173), Acc@1: 0.9062 (0.7268), Acc@5: 0.9062 (0.9107)
2022-01-07 02:18:18,059 Val Step[1350/1563], Loss: 1.7786 (1.2334), Acc@1: 0.5625 (0.7228), Acc@5: 0.8750 (0.9082)
2022-01-07 02:18:19,331 Val Step[1400/1563], Loss: 1.0835 (1.2395), Acc@1: 0.7188 (0.7214), Acc@5: 0.9062 (0.9072)
2022-01-07 02:18:20,608 Val Step[1450/1563], Loss: 1.3903 (1.2456), Acc@1: 0.7500 (0.7195), Acc@5: 0.9375 (0.9068)
2022-01-07 02:18:21,899 Val Step[1500/1563], Loss: 1.6245 (1.2349), Acc@1: 0.6562 (0.7217), Acc@5: 0.8438 (0.9082)
2022-01-07 02:18:23,177 Val Step[1550/1563], Loss: 0.9106 (1.2368), Acc@1: 0.8750 (0.7208), Acc@5: 0.9062 (0.9081)
2022-01-07 02:18:23,949 ----- Epoch[260/310], Validation Loss: 1.2352, Validation Acc@1: 0.7211, Validation Acc@5: 0.9082, time: 96.38
2022-01-07 02:18:23,949 ----- Epoch[260/310], Train Loss: 3.6055, Train Acc: 0.3796, time: 1579.72, Best Val(epoch258) Acc@1: 0.7232
2022-01-07 02:18:24,112 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-260-Loss-3.595203842571695.pdparams
2022-01-07 02:18:24,112 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-260-Loss-3.595203842571695.pdopt
2022-01-07 02:18:24,152 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-260-Loss-3.595203842571695-EMA.pdparams
2022-01-07 02:18:24,153 Now training epoch 261. LR=0.000046
2022-01-07 02:19:33,901 Epoch[261/310], Step[0000/1251], Loss: 3.9786(3.9786), Acc: 0.2891(0.2891)
2022-01-07 02:20:34,199 Epoch[261/310], Step[0050/1251], Loss: 3.7732(3.6392), Acc: 0.3086(0.3854)
2022-01-07 02:21:34,261 Epoch[261/310], Step[0100/1251], Loss: 3.5014(3.6173), Acc: 0.4863(0.3863)
2022-01-07 02:22:33,375 Epoch[261/310], Step[0150/1251], Loss: 3.9459(3.6247), Acc: 0.2500(0.3843)
2022-01-07 02:23:31,733 Epoch[261/310], Step[0200/1251], Loss: 3.3421(3.6258), Acc: 0.5068(0.3871)
2022-01-07 02:24:31,766 Epoch[261/310], Step[0250/1251], Loss: 3.4398(3.6299), Acc: 0.5527(0.3801)
2022-01-07 02:25:31,608 Epoch[261/310], Step[0300/1251], Loss: 3.6990(3.6355), Acc: 0.4766(0.3817)
2022-01-07 02:26:30,016 Epoch[261/310], Step[0350/1251], Loss: 3.1892(3.6317), Acc: 0.4326(0.3775)
2022-01-07 02:27:30,916 Epoch[261/310], Step[0400/1251], Loss: 3.7560(3.6321), Acc: 0.4502(0.3755)
2022-01-07 02:28:31,838 Epoch[261/310], Step[0450/1251], Loss: 3.5622(3.6289), Acc: 0.4072(0.3755)
2022-01-07 02:29:32,398 Epoch[261/310], Step[0500/1251], Loss: 3.1932(3.6267), Acc: 0.4375(0.3773)
2022-01-07 02:30:31,822 Epoch[261/310], Step[0550/1251], Loss: 3.1604(3.6249), Acc: 0.6250(0.3774)
2022-01-07 02:31:32,558 Epoch[261/310], Step[0600/1251], Loss: 3.7821(3.6261), Acc: 0.4180(0.3779)
2022-01-07 02:32:33,535 Epoch[261/310], Step[0650/1251], Loss: 4.0226(3.6281), Acc: 0.4092(0.3780)
2022-01-07 02:33:31,548 Epoch[261/310], Step[0700/1251], Loss: 3.7835(3.6238), Acc: 0.4219(0.3789)
2022-01-07 02:34:31,090 Epoch[261/310], Step[0750/1251], Loss: 3.3144(3.6191), Acc: 0.5244(0.3785)
2022-01-07 02:35:31,593 Epoch[261/310], Step[0800/1251], Loss: 3.2553(3.6199), Acc: 0.5430(0.3762)
2022-01-07 02:36:31,459 Epoch[261/310], Step[0850/1251], Loss: 4.0013(3.6183), Acc: 0.3320(0.3762)
2022-01-07 02:37:32,374 Epoch[261/310], Step[0900/1251], Loss: 2.7765(3.6167), Acc: 0.4990(0.3757)
2022-01-07 02:38:33,876 Epoch[261/310], Step[0950/1251], Loss: 3.7446(3.6173), Acc: 0.4492(0.3768)
2022-01-07 02:39:31,938 Epoch[261/310], Step[1000/1251], Loss: 3.7132(3.6198), Acc: 0.5127(0.3769)
2022-01-07 02:40:32,406 Epoch[261/310], Step[1050/1251], Loss: 3.0648(3.6181), Acc: 0.3555(0.3768)
2022-01-07 02:41:32,001 Epoch[261/310], Step[1100/1251], Loss: 4.0780(3.6139), Acc: 0.3535(0.3768)
2022-01-07 02:42:32,434 Epoch[261/310], Step[1150/1251], Loss: 3.7315(3.6105), Acc: 0.4199(0.3772)
2022-01-07 02:43:31,633 Epoch[261/310], Step[1200/1251], Loss: 3.5770(3.6095), Acc: 0.2617(0.3778)
2022-01-07 02:44:30,725 Epoch[261/310], Step[1250/1251], Loss: 3.6564(3.6084), Acc: 0.5244(0.3775)
2022-01-07 02:44:32,215 ----- Epoch[261/310], Train Loss: 3.6084, Train Acc: 0.3775, time: 1568.06, Best Val(epoch258) Acc@1: 0.7232
2022-01-07 02:44:32,387 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 02:44:32,387 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 02:44:32,493 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 02:44:32,493 Now training epoch 262. LR=0.000044
2022-01-07 02:45:51,135 Epoch[262/310], Step[0000/1251], Loss: 3.9678(3.9678), Acc: 0.3916(0.3916)
2022-01-07 02:46:50,533 Epoch[262/310], Step[0050/1251], Loss: 3.6675(3.5846), Acc: 0.2012(0.3762)
2022-01-07 02:47:51,420 Epoch[262/310], Step[0100/1251], Loss: 3.6469(3.5809), Acc: 0.1758(0.3668)
2022-01-07 02:48:50,034 Epoch[262/310], Step[0150/1251], Loss: 3.8689(3.5764), Acc: 0.4756(0.3838)
2022-01-07 02:49:49,915 Epoch[262/310], Step[0200/1251], Loss: 3.7215(3.5740), Acc: 0.3672(0.3788)
2022-01-07 02:50:49,982 Epoch[262/310], Step[0250/1251], Loss: 3.7266(3.5807), Acc: 0.1934(0.3802)
2022-01-07 02:51:50,837 Epoch[262/310], Step[0300/1251], Loss: 4.0973(3.5953), Acc: 0.4844(0.3792)
2022-01-07 02:52:49,983 Epoch[262/310], Step[0350/1251], Loss: 3.3873(3.5926), Acc: 0.5791(0.3781)
2022-01-07 02:53:51,255 Epoch[262/310], Step[0400/1251], Loss: 4.3393(3.6025), Acc: 0.3145(0.3782)
2022-01-07 02:54:50,626 Epoch[262/310], Step[0450/1251], Loss: 3.7599(3.6054), Acc: 0.4531(0.3814)
2022-01-07 02:55:51,943 Epoch[262/310], Step[0500/1251], Loss: 3.6679(3.5982), Acc: 0.3691(0.3797)
2022-01-07 02:56:52,573 Epoch[262/310], Step[0550/1251], Loss: 4.1062(3.6012), Acc: 0.1934(0.3789)
2022-01-07 02:57:53,399 Epoch[262/310], Step[0600/1251], Loss: 4.0162(3.5995), Acc: 0.3682(0.3809)
2022-01-07 02:58:54,644 Epoch[262/310], Step[0650/1251], Loss: 3.9855(3.6016), Acc: 0.3682(0.3804)
2022-01-07 02:59:53,262 Epoch[262/310], Step[0700/1251], Loss: 3.5681(3.6021), Acc: 0.3584(0.3821)
2022-01-07 03:00:52,904 Epoch[262/310], Step[0750/1251], Loss: 3.4640(3.5975), Acc: 0.5107(0.3835)
2022-01-07 03:01:50,925 Epoch[262/310], Step[0800/1251], Loss: 3.0832(3.5960), Acc: 0.4805(0.3849)
2022-01-07 03:02:50,377 Epoch[262/310], Step[0850/1251], Loss: 3.5862(3.5936), Acc: 0.5566(0.3856)
2022-01-07 03:03:49,509 Epoch[262/310], Step[0900/1251], Loss: 3.2593(3.5958), Acc: 0.4854(0.3858)
2022-01-07 03:04:48,352 Epoch[262/310], Step[0950/1251], Loss: 3.3989(3.5980), Acc: 0.5498(0.3847)
2022-01-07 03:05:47,915 Epoch[262/310], Step[1000/1251], Loss: 3.7221(3.6003), Acc: 0.4053(0.3844)
2022-01-07 03:06:46,006 Epoch[262/310], Step[1050/1251], Loss: 3.6252(3.6008), Acc: 0.5244(0.3854)
2022-01-07 03:07:44,791 Epoch[262/310], Step[1100/1251], Loss: 3.2321(3.6009), Acc: 0.1943(0.3848)
2022-01-07 03:08:44,908 Epoch[262/310], Step[1150/1251], Loss: 3.1202(3.6019), Acc: 0.5283(0.3858)
2022-01-07 03:09:44,024 Epoch[262/310], Step[1200/1251], Loss: 4.1333(3.6030), Acc: 0.3145(0.3863)
2022-01-07 03:10:42,585 Epoch[262/310], Step[1250/1251], Loss: 3.6883(3.6033), Acc: 0.2959(0.3871)
2022-01-07 03:10:44,086 ----- Validation after Epoch: 262
2022-01-07 03:11:43,652 Val Step[0000/1563], Loss: 0.6452 (0.6452), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 03:11:45,095 Val Step[0050/1563], Loss: 2.1803 (0.7268), Acc@1: 0.4375 (0.8572), Acc@5: 0.8438 (0.9608)
2022-01-07 03:11:46,662 Val Step[0100/1563], Loss: 2.0477 (0.9935), Acc@1: 0.4688 (0.7816), Acc@5: 0.8125 (0.9378)
2022-01-07 03:11:48,041 Val Step[0150/1563], Loss: 0.4308 (0.9527), Acc@1: 0.8750 (0.7910), Acc@5: 1.0000 (0.9421)
2022-01-07 03:11:49,313 Val Step[0200/1563], Loss: 1.0499 (0.9662), Acc@1: 0.7812 (0.7923), Acc@5: 0.9688 (0.9397)
2022-01-07 03:11:50,655 Val Step[0250/1563], Loss: 0.4583 (0.9125), Acc@1: 0.9375 (0.8045), Acc@5: 1.0000 (0.9456)
2022-01-07 03:11:51,935 Val Step[0300/1563], Loss: 1.1766 (0.9715), Acc@1: 0.6875 (0.7881), Acc@5: 0.9062 (0.9408)
2022-01-07 03:11:53,209 Val Step[0350/1563], Loss: 0.8839 (0.9789), Acc@1: 0.8125 (0.7845), Acc@5: 0.9375 (0.9420)
2022-01-07 03:11:54,474 Val Step[0400/1563], Loss: 0.8758 (0.9846), Acc@1: 0.8125 (0.7807), Acc@5: 0.9688 (0.9426)
2022-01-07 03:11:55,797 Val Step[0450/1563], Loss: 0.9094 (0.9907), Acc@1: 0.7500 (0.7779), Acc@5: 1.0000 (0.9432)
2022-01-07 03:11:57,123 Val Step[0500/1563], Loss: 0.4656 (0.9827), Acc@1: 0.8750 (0.7804), Acc@5: 1.0000 (0.9442)
2022-01-07 03:11:58,596 Val Step[0550/1563], Loss: 0.6915 (0.9604), Acc@1: 0.8125 (0.7862), Acc@5: 0.9688 (0.9459)
2022-01-07 03:12:00,025 Val Step[0600/1563], Loss: 0.8424 (0.9672), Acc@1: 0.7812 (0.7852), Acc@5: 0.9375 (0.9451)
2022-01-07 03:12:01,287 Val Step[0650/1563], Loss: 0.5651 (0.9856), Acc@1: 0.9062 (0.7814), Acc@5: 1.0000 (0.9425)
2022-01-07 03:12:02,613 Val Step[0700/1563], Loss: 0.8966 (1.0108), Acc@1: 0.7812 (0.7749), Acc@5: 0.9688 (0.9396)
2022-01-07 03:12:03,907 Val Step[0750/1563], Loss: 0.9874 (1.0405), Acc@1: 0.8125 (0.7687), Acc@5: 0.9062 (0.9355)
2022-01-07 03:12:05,198 Val Step[0800/1563], Loss: 0.7170 (1.0765), Acc@1: 0.8125 (0.7598), Acc@5: 1.0000 (0.9309)
2022-01-07 03:12:06,510 Val Step[0850/1563], Loss: 1.1545 (1.1004), Acc@1: 0.6875 (0.7537), Acc@5: 0.9688 (0.9282)
2022-01-07 03:12:07,792 Val Step[0900/1563], Loss: 0.3189 (1.0987), Acc@1: 0.9375 (0.7554), Acc@5: 1.0000 (0.9277)
2022-01-07 03:12:09,180 Val Step[0950/1563], Loss: 1.2212 (1.1182), Acc@1: 0.7500 (0.7515), Acc@5: 0.9062 (0.9245)
2022-01-07 03:12:10,542 Val Step[1000/1563], Loss: 0.5592 (1.1414), Acc@1: 0.9688 (0.7449), Acc@5: 0.9688 (0.9215)
2022-01-07 03:12:11,817 Val Step[1050/1563], Loss: 0.2789 (1.1555), Acc@1: 0.9688 (0.7412), Acc@5: 1.0000 (0.9199)
2022-01-07 03:12:13,082 Val Step[1100/1563], Loss: 0.8083 (1.1685), Acc@1: 0.8125 (0.7389), Acc@5: 0.9688 (0.9180)
2022-01-07 03:12:14,332 Val Step[1150/1563], Loss: 1.2655 (1.1819), Acc@1: 0.7812 (0.7361), Acc@5: 0.8438 (0.9162)
2022-01-07 03:12:15,602 Val Step[1200/1563], Loss: 1.2242 (1.1957), Acc@1: 0.7500 (0.7324), Acc@5: 0.8438 (0.9139)
2022-01-07 03:12:16,875 Val Step[1250/1563], Loss: 0.7354 (1.2067), Acc@1: 0.8750 (0.7305), Acc@5: 0.9375 (0.9121)
2022-01-07 03:12:18,128 Val Step[1300/1563], Loss: 0.7346 (1.2147), Acc@1: 0.9062 (0.7289), Acc@5: 0.9375 (0.9112)
2022-01-07 03:12:19,400 Val Step[1350/1563], Loss: 1.8363 (1.2321), Acc@1: 0.5000 (0.7249), Acc@5: 0.8125 (0.9084)
2022-01-07 03:12:20,766 Val Step[1400/1563], Loss: 1.0852 (1.2380), Acc@1: 0.7188 (0.7236), Acc@5: 0.9062 (0.9077)
2022-01-07 03:12:22,024 Val Step[1450/1563], Loss: 1.3364 (1.2444), Acc@1: 0.7188 (0.7218), Acc@5: 0.9375 (0.9073)
2022-01-07 03:12:23,304 Val Step[1500/1563], Loss: 1.6048 (1.2342), Acc@1: 0.6562 (0.7243), Acc@5: 0.8750 (0.9087)
2022-01-07 03:12:24,575 Val Step[1550/1563], Loss: 0.9247 (1.2367), Acc@1: 0.8750 (0.7234), Acc@5: 0.9062 (0.9084)
2022-01-07 03:12:25,342 ----- Epoch[262/310], Validation Loss: 1.2352, Validation Acc@1: 0.7237, Validation Acc@5: 0.9086, time: 101.25
2022-01-07 03:12:25,342 ----- Epoch[262/310], Train Loss: 3.6033, Train Acc: 0.3871, time: 1571.59, Best Val(epoch262) Acc@1: 0.7237
2022-01-07 03:12:25,522 Max accuracy so far: 0.7237 at epoch_262
2022-01-07 03:12:25,522 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 03:12:25,522 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 03:12:25,627 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 03:12:26,054 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 03:12:26,054 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 03:12:26,141 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 03:12:26,142 Now training epoch 263. LR=0.000042
2022-01-07 03:13:39,846 Epoch[263/310], Step[0000/1251], Loss: 3.8541(3.8541), Acc: 0.4326(0.4326)
2022-01-07 03:14:38,072 Epoch[263/310], Step[0050/1251], Loss: 3.4919(3.6077), Acc: 0.3496(0.3680)
2022-01-07 03:15:37,289 Epoch[263/310], Step[0100/1251], Loss: 3.9443(3.6046), Acc: 0.3535(0.3836)
2022-01-07 03:16:36,561 Epoch[263/310], Step[0150/1251], Loss: 4.0576(3.6208), Acc: 0.2578(0.3757)
2022-01-07 03:17:36,010 Epoch[263/310], Step[0200/1251], Loss: 3.3015(3.6065), Acc: 0.4258(0.3774)
2022-01-07 03:18:35,402 Epoch[263/310], Step[0250/1251], Loss: 3.2899(3.6011), Acc: 0.4453(0.3802)
2022-01-07 03:19:35,682 Epoch[263/310], Step[0300/1251], Loss: 2.9777(3.6030), Acc: 0.4912(0.3811)
2022-01-07 03:20:35,467 Epoch[263/310], Step[0350/1251], Loss: 3.9616(3.5972), Acc: 0.4170(0.3828)
2022-01-07 03:21:34,160 Epoch[263/310], Step[0400/1251], Loss: 3.8043(3.5907), Acc: 0.3486(0.3843)
2022-01-07 03:22:31,668 Epoch[263/310], Step[0450/1251], Loss: 3.9261(3.5923), Acc: 0.3447(0.3825)
2022-01-07 03:23:29,738 Epoch[263/310], Step[0500/1251], Loss: 3.2015(3.5945), Acc: 0.2725(0.3816)
2022-01-07 03:24:27,293 Epoch[263/310], Step[0550/1251], Loss: 3.6154(3.5987), Acc: 0.4727(0.3825)
2022-01-07 03:25:26,856 Epoch[263/310], Step[0600/1251], Loss: 3.4042(3.5973), Acc: 0.1562(0.3832)
2022-01-07 03:26:26,470 Epoch[263/310], Step[0650/1251], Loss: 3.5483(3.5921), Acc: 0.4268(0.3830)
2022-01-07 03:27:25,039 Epoch[263/310], Step[0700/1251], Loss: 3.4546(3.5938), Acc: 0.1895(0.3812)
2022-01-07 03:28:24,820 Epoch[263/310], Step[0750/1251], Loss: 3.7025(3.5953), Acc: 0.4082(0.3817)
2022-01-07 03:29:25,050 Epoch[263/310], Step[0800/1251], Loss: 3.7035(3.5961), Acc: 0.2236(0.3826)
2022-01-07 03:30:25,275 Epoch[263/310], Step[0850/1251], Loss: 3.7843(3.5976), Acc: 0.4199(0.3824)
2022-01-07 03:31:25,709 Epoch[263/310], Step[0900/1251], Loss: 3.8237(3.5983), Acc: 0.5166(0.3830)
2022-01-07 03:32:25,124 Epoch[263/310], Step[0950/1251], Loss: 4.0510(3.6040), Acc: 0.3066(0.3814)
2022-01-07 03:33:25,794 Epoch[263/310], Step[1000/1251], Loss: 3.9654(3.6035), Acc: 0.3506(0.3817)
2022-01-07 03:34:26,811 Epoch[263/310], Step[1050/1251], Loss: 3.3233(3.6025), Acc: 0.5781(0.3809)
2022-01-07 03:35:27,537 Epoch[263/310], Step[1100/1251], Loss: 3.5353(3.6050), Acc: 0.4375(0.3802)
2022-01-07 03:36:28,520 Epoch[263/310], Step[1150/1251], Loss: 3.4828(3.6066), Acc: 0.3613(0.3804)
2022-01-07 03:37:28,662 Epoch[263/310], Step[1200/1251], Loss: 3.5681(3.6037), Acc: 0.5000(0.3804)
2022-01-07 03:38:29,231 Epoch[263/310], Step[1250/1251], Loss: 3.7584(3.6042), Acc: 0.1123(0.3798)
2022-01-07 03:38:30,804 ----- Epoch[263/310], Train Loss: 3.6042, Train Acc: 0.3798, time: 1564.66, Best Val(epoch262) Acc@1: 0.7237
2022-01-07 03:38:31,008 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 03:38:31,009 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 03:38:31,113 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 03:38:31,113 Now training epoch 264. LR=0.000040
2022-01-07 03:39:45,017 Epoch[264/310], Step[0000/1251], Loss: 3.7405(3.7405), Acc: 0.5000(0.5000)
2022-01-07 03:40:43,515 Epoch[264/310], Step[0050/1251], Loss: 4.0657(3.6301), Acc: 0.3740(0.4017)
2022-01-07 03:41:43,203 Epoch[264/310], Step[0100/1251], Loss: 3.3493(3.6284), Acc: 0.4297(0.3874)
2022-01-07 03:42:42,182 Epoch[264/310], Step[0150/1251], Loss: 3.7297(3.6202), Acc: 0.1963(0.3785)
2022-01-07 03:43:40,939 Epoch[264/310], Step[0200/1251], Loss: 3.9987(3.6216), Acc: 0.4102(0.3829)
2022-01-07 03:44:39,772 Epoch[264/310], Step[0250/1251], Loss: 3.8738(3.6308), Acc: 0.3174(0.3823)
2022-01-07 03:45:39,688 Epoch[264/310], Step[0300/1251], Loss: 2.8758(3.6229), Acc: 0.5078(0.3814)
2022-01-07 03:46:39,029 Epoch[264/310], Step[0350/1251], Loss: 2.8892(3.6191), Acc: 0.4678(0.3837)
2022-01-07 03:47:37,269 Epoch[264/310], Step[0400/1251], Loss: 3.0070(3.6156), Acc: 0.3730(0.3862)
2022-01-07 03:48:35,498 Epoch[264/310], Step[0450/1251], Loss: 3.6123(3.6165), Acc: 0.4102(0.3861)
2022-01-07 03:49:34,515 Epoch[264/310], Step[0500/1251], Loss: 3.5603(3.6119), Acc: 0.4219(0.3866)
2022-01-07 03:50:34,728 Epoch[264/310], Step[0550/1251], Loss: 3.7958(3.6117), Acc: 0.4736(0.3881)
2022-01-07 03:51:35,120 Epoch[264/310], Step[0600/1251], Loss: 4.0112(3.6054), Acc: 0.2100(0.3862)
2022-01-07 03:52:33,501 Epoch[264/310], Step[0650/1251], Loss: 3.8009(3.6091), Acc: 0.4600(0.3863)
2022-01-07 03:53:33,420 Epoch[264/310], Step[0700/1251], Loss: 3.7769(3.6101), Acc: 0.4209(0.3865)
2022-01-07 03:54:32,002 Epoch[264/310], Step[0750/1251], Loss: 3.9891(3.6116), Acc: 0.2051(0.3857)
2022-01-07 03:55:31,062 Epoch[264/310], Step[0800/1251], Loss: 3.5859(3.6085), Acc: 0.2822(0.3843)
2022-01-07 03:56:32,113 Epoch[264/310], Step[0850/1251], Loss: 3.9022(3.6076), Acc: 0.2666(0.3824)
2022-01-07 03:57:31,735 Epoch[264/310], Step[0900/1251], Loss: 3.6596(3.6061), Acc: 0.2861(0.3834)
2022-01-07 03:58:32,366 Epoch[264/310], Step[0950/1251], Loss: 3.6851(3.6087), Acc: 0.2363(0.3823)
2022-01-07 03:59:33,132 Epoch[264/310], Step[1000/1251], Loss: 3.8295(3.6095), Acc: 0.4043(0.3811)
2022-01-07 04:00:33,072 Epoch[264/310], Step[1050/1251], Loss: 3.9210(3.6081), Acc: 0.3447(0.3821)
2022-01-07 04:01:33,488 Epoch[264/310], Step[1100/1251], Loss: 3.7977(3.6096), Acc: 0.3750(0.3808)
2022-01-07 04:02:34,404 Epoch[264/310], Step[1150/1251], Loss: 3.6251(3.6077), Acc: 0.2510(0.3821)
2022-01-07 04:03:35,895 Epoch[264/310], Step[1200/1251], Loss: 3.2654(3.6033), Acc: 0.4141(0.3825)
2022-01-07 04:04:34,324 Epoch[264/310], Step[1250/1251], Loss: 3.5059(3.6037), Acc: 0.5176(0.3832)
2022-01-07 04:04:35,850 ----- Validation after Epoch: 264
2022-01-07 04:05:37,573 Val Step[0000/1563], Loss: 0.6305 (0.6305), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 04:05:38,866 Val Step[0050/1563], Loss: 2.1083 (0.7240), Acc@1: 0.4375 (0.8566), Acc@5: 0.8750 (0.9614)
2022-01-07 04:05:40,195 Val Step[0100/1563], Loss: 1.7569 (0.9697), Acc@1: 0.5625 (0.7865), Acc@5: 0.8438 (0.9400)
2022-01-07 04:05:41,583 Val Step[0150/1563], Loss: 0.4700 (0.9209), Acc@1: 0.8750 (0.8003), Acc@5: 1.0000 (0.9437)
2022-01-07 04:05:42,946 Val Step[0200/1563], Loss: 0.8664 (0.9279), Acc@1: 0.8125 (0.8013), Acc@5: 0.9375 (0.9426)
2022-01-07 04:05:44,331 Val Step[0250/1563], Loss: 0.4838 (0.8802), Acc@1: 0.9062 (0.8116), Acc@5: 1.0000 (0.9480)
2022-01-07 04:05:45,745 Val Step[0300/1563], Loss: 1.0315 (0.9371), Acc@1: 0.6875 (0.7950), Acc@5: 0.9688 (0.9434)
2022-01-07 04:05:47,062 Val Step[0350/1563], Loss: 0.8880 (0.9444), Acc@1: 0.7500 (0.7894), Acc@5: 0.9062 (0.9446)
2022-01-07 04:05:48,392 Val Step[0400/1563], Loss: 0.8960 (0.9502), Acc@1: 0.8125 (0.7844), Acc@5: 0.9688 (0.9451)
2022-01-07 04:05:49,761 Val Step[0450/1563], Loss: 0.8679 (0.9563), Acc@1: 0.7812 (0.7821), Acc@5: 1.0000 (0.9457)
2022-01-07 04:05:51,044 Val Step[0500/1563], Loss: 0.4380 (0.9483), Acc@1: 0.9375 (0.7838), Acc@5: 1.0000 (0.9469)
2022-01-07 04:05:52,440 Val Step[0550/1563], Loss: 0.7516 (0.9272), Acc@1: 0.8438 (0.7890), Acc@5: 0.9375 (0.9486)
2022-01-07 04:05:53,767 Val Step[0600/1563], Loss: 0.8019 (0.9362), Acc@1: 0.7812 (0.7870), Acc@5: 0.9375 (0.9479)
2022-01-07 04:05:55,061 Val Step[0650/1563], Loss: 0.5386 (0.9560), Acc@1: 0.9062 (0.7827), Acc@5: 1.0000 (0.9455)
2022-01-07 04:05:56,354 Val Step[0700/1563], Loss: 0.9698 (0.9847), Acc@1: 0.8438 (0.7756), Acc@5: 0.9375 (0.9423)
2022-01-07 04:05:57,639 Val Step[0750/1563], Loss: 1.1615 (1.0174), Acc@1: 0.8125 (0.7692), Acc@5: 0.9062 (0.9381)
2022-01-07 04:05:59,040 Val Step[0800/1563], Loss: 0.6528 (1.0542), Acc@1: 0.8125 (0.7600), Acc@5: 1.0000 (0.9339)
2022-01-07 04:06:00,315 Val Step[0850/1563], Loss: 1.4204 (1.0788), Acc@1: 0.5625 (0.7535), Acc@5: 0.9375 (0.9310)
2022-01-07 04:06:01,641 Val Step[0900/1563], Loss: 0.3171 (1.0780), Acc@1: 0.9688 (0.7551), Acc@5: 1.0000 (0.9303)
2022-01-07 04:06:03,002 Val Step[0950/1563], Loss: 1.2803 (1.0981), Acc@1: 0.7500 (0.7509), Acc@5: 0.9062 (0.9270)
2022-01-07 04:06:04,313 Val Step[1000/1563], Loss: 0.4998 (1.1211), Acc@1: 0.9375 (0.7446), Acc@5: 1.0000 (0.9238)
2022-01-07 04:06:05,591 Val Step[1050/1563], Loss: 0.3668 (1.1344), Acc@1: 0.9375 (0.7412), Acc@5: 1.0000 (0.9223)
2022-01-07 04:06:06,876 Val Step[1100/1563], Loss: 0.7430 (1.1479), Acc@1: 0.9062 (0.7387), Acc@5: 0.9375 (0.9204)
2022-01-07 04:06:08,134 Val Step[1150/1563], Loss: 1.2839 (1.1622), Acc@1: 0.7812 (0.7362), Acc@5: 0.8125 (0.9182)
2022-01-07 04:06:09,497 Val Step[1200/1563], Loss: 1.1445 (1.1753), Acc@1: 0.7812 (0.7328), Acc@5: 0.8438 (0.9160)
2022-01-07 04:06:10,880 Val Step[1250/1563], Loss: 0.7358 (1.1864), Acc@1: 0.8750 (0.7312), Acc@5: 0.9375 (0.9145)
2022-01-07 04:06:12,281 Val Step[1300/1563], Loss: 0.7029 (1.1948), Acc@1: 0.9062 (0.7299), Acc@5: 0.9375 (0.9134)
2022-01-07 04:06:13,678 Val Step[1350/1563], Loss: 1.7787 (1.2123), Acc@1: 0.5312 (0.7259), Acc@5: 0.8750 (0.9110)
2022-01-07 04:06:14,969 Val Step[1400/1563], Loss: 0.9085 (1.2176), Acc@1: 0.7500 (0.7246), Acc@5: 0.9688 (0.9104)
2022-01-07 04:06:16,232 Val Step[1450/1563], Loss: 1.4352 (1.2241), Acc@1: 0.6562 (0.7226), Acc@5: 0.9375 (0.9101)
2022-01-07 04:06:17,532 Val Step[1500/1563], Loss: 1.7754 (1.2139), Acc@1: 0.6250 (0.7250), Acc@5: 0.8125 (0.9114)
2022-01-07 04:06:18,932 Val Step[1550/1563], Loss: 0.9080 (1.2162), Acc@1: 0.8750 (0.7243), Acc@5: 0.9062 (0.9111)
2022-01-07 04:06:19,731 ----- Epoch[264/310], Validation Loss: 1.2147, Validation Acc@1: 0.7244, Validation Acc@5: 0.9113, time: 103.88
2022-01-07 04:06:19,732 ----- Epoch[264/310], Train Loss: 3.6037, Train Acc: 0.3832, time: 1564.73, Best Val(epoch264) Acc@1: 0.7244
2022-01-07 04:06:19,912 Max accuracy so far: 0.7244 at epoch_264
2022-01-07 04:06:19,912 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 04:06:19,913 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 04:06:20,017 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 04:06:20,442 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 04:06:20,443 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 04:06:20,530 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 04:06:20,530 Now training epoch 265. LR=0.000038
2022-01-07 04:07:34,927 Epoch[265/310], Step[0000/1251], Loss: 3.3244(3.3244), Acc: 0.5771(0.5771)
2022-01-07 04:08:34,848 Epoch[265/310], Step[0050/1251], Loss: 3.8737(3.5304), Acc: 0.3076(0.4176)
2022-01-07 04:09:34,948 Epoch[265/310], Step[0100/1251], Loss: 4.0221(3.5743), Acc: 0.3965(0.3977)
2022-01-07 04:10:35,068 Epoch[265/310], Step[0150/1251], Loss: 3.9128(3.6004), Acc: 0.2930(0.3910)
2022-01-07 04:11:35,837 Epoch[265/310], Step[0200/1251], Loss: 3.5017(3.6025), Acc: 0.2549(0.3902)
2022-01-07 04:12:36,690 Epoch[265/310], Step[0250/1251], Loss: 3.6811(3.5907), Acc: 0.3857(0.3920)
2022-01-07 04:13:36,907 Epoch[265/310], Step[0300/1251], Loss: 4.2899(3.5915), Acc: 0.2812(0.3872)
2022-01-07 04:14:34,545 Epoch[265/310], Step[0350/1251], Loss: 3.5681(3.5978), Acc: 0.4961(0.3841)
2022-01-07 04:15:33,333 Epoch[265/310], Step[0400/1251], Loss: 3.7664(3.5968), Acc: 0.4619(0.3863)
2022-01-07 04:16:32,754 Epoch[265/310], Step[0450/1251], Loss: 3.5790(3.5981), Acc: 0.4248(0.3859)
2022-01-07 04:17:33,153 Epoch[265/310], Step[0500/1251], Loss: 3.4474(3.5937), Acc: 0.5303(0.3831)
2022-01-07 04:18:33,164 Epoch[265/310], Step[0550/1251], Loss: 3.9115(3.5981), Acc: 0.4258(0.3845)
2022-01-07 04:19:34,030 Epoch[265/310], Step[0600/1251], Loss: 3.7493(3.6023), Acc: 0.4092(0.3841)
2022-01-07 04:20:34,575 Epoch[265/310], Step[0650/1251], Loss: 3.5730(3.6007), Acc: 0.5264(0.3852)
2022-01-07 04:21:34,617 Epoch[265/310], Step[0700/1251], Loss: 3.6714(3.5963), Acc: 0.3896(0.3853)
2022-01-07 04:22:33,857 Epoch[265/310], Step[0750/1251], Loss: 4.1358(3.5910), Acc: 0.2979(0.3851)
2022-01-07 04:23:32,149 Epoch[265/310], Step[0800/1251], Loss: 3.5900(3.5898), Acc: 0.2051(0.3845)
2022-01-07 04:24:30,836 Epoch[265/310], Step[0850/1251], Loss: 3.3060(3.5871), Acc: 0.2188(0.3830)
2022-01-07 04:25:30,762 Epoch[265/310], Step[0900/1251], Loss: 4.0055(3.5892), Acc: 0.2227(0.3828)
2022-01-07 04:26:32,262 Epoch[265/310], Step[0950/1251], Loss: 3.7032(3.5885), Acc: 0.5098(0.3813)
2022-01-07 04:27:33,400 Epoch[265/310], Step[1000/1251], Loss: 3.7914(3.5864), Acc: 0.0996(0.3794)
2022-01-07 04:28:32,248 Epoch[265/310], Step[1050/1251], Loss: 3.8070(3.5898), Acc: 0.4268(0.3790)
2022-01-07 04:29:32,359 Epoch[265/310], Step[1100/1251], Loss: 3.7099(3.5905), Acc: 0.2793(0.3784)
2022-01-07 04:30:33,467 Epoch[265/310], Step[1150/1251], Loss: 3.8781(3.5934), Acc: 0.4102(0.3783)
2022-01-07 04:31:33,751 Epoch[265/310], Step[1200/1251], Loss: 3.6591(3.5958), Acc: 0.3066(0.3783)
2022-01-07 04:32:34,214 Epoch[265/310], Step[1250/1251], Loss: 3.8136(3.5930), Acc: 0.3252(0.3789)
2022-01-07 04:32:35,680 ----- Epoch[265/310], Train Loss: 3.5930, Train Acc: 0.3789, time: 1575.15, Best Val(epoch264) Acc@1: 0.7244
2022-01-07 04:32:35,856 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 04:32:35,856 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 04:32:36,146 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 04:32:36,146 Now training epoch 266. LR=0.000036
2022-01-07 04:33:54,839 Epoch[266/310], Step[0000/1251], Loss: 3.3754(3.3754), Acc: 0.2988(0.2988)
2022-01-07 04:34:52,880 Epoch[266/310], Step[0050/1251], Loss: 3.1204(3.5743), Acc: 0.6270(0.3833)
2022-01-07 04:35:53,075 Epoch[266/310], Step[0100/1251], Loss: 3.8533(3.5400), Acc: 0.3486(0.3915)
2022-01-07 04:36:53,550 Epoch[266/310], Step[0150/1251], Loss: 3.9335(3.5635), Acc: 0.4658(0.3820)
2022-01-07 04:37:52,768 Epoch[266/310], Step[0200/1251], Loss: 3.6127(3.5502), Acc: 0.5811(0.3857)
2022-01-07 04:38:51,978 Epoch[266/310], Step[0250/1251], Loss: 3.6585(3.5668), Acc: 0.4209(0.3828)
2022-01-07 04:39:52,708 Epoch[266/310], Step[0300/1251], Loss: 4.1156(3.5742), Acc: 0.2090(0.3819)
2022-01-07 04:40:52,288 Epoch[266/310], Step[0350/1251], Loss: 4.0275(3.5763), Acc: 0.1289(0.3829)
2022-01-07 04:41:52,748 Epoch[266/310], Step[0400/1251], Loss: 3.9488(3.5791), Acc: 0.3311(0.3857)
2022-01-07 04:42:52,833 Epoch[266/310], Step[0450/1251], Loss: 3.4795(3.5877), Acc: 0.4102(0.3827)
2022-01-07 04:43:51,983 Epoch[266/310], Step[0500/1251], Loss: 3.6715(3.5845), Acc: 0.2998(0.3845)
2022-01-07 04:44:51,353 Epoch[266/310], Step[0550/1251], Loss: 3.4058(3.5874), Acc: 0.4199(0.3810)
2022-01-07 04:45:49,577 Epoch[266/310], Step[0600/1251], Loss: 3.2016(3.5799), Acc: 0.4336(0.3803)
2022-01-07 04:46:46,548 Epoch[266/310], Step[0650/1251], Loss: 3.8472(3.5836), Acc: 0.3008(0.3792)
2022-01-07 04:47:46,020 Epoch[266/310], Step[0700/1251], Loss: 3.5316(3.5847), Acc: 0.3965(0.3790)
2022-01-07 04:48:42,871 Epoch[266/310], Step[0750/1251], Loss: 3.6122(3.5830), Acc: 0.4512(0.3809)
2022-01-07 04:49:40,382 Epoch[266/310], Step[0800/1251], Loss: 3.6706(3.5857), Acc: 0.3818(0.3808)
2022-01-07 04:50:38,077 Epoch[266/310], Step[0850/1251], Loss: 3.8664(3.5880), Acc: 0.2432(0.3803)
2022-01-07 04:51:37,085 Epoch[266/310], Step[0900/1251], Loss: 3.7966(3.5872), Acc: 0.2207(0.3811)
2022-01-07 04:52:35,890 Epoch[266/310], Step[0950/1251], Loss: 3.2513(3.5844), Acc: 0.5752(0.3817)
2022-01-07 04:53:34,332 Epoch[266/310], Step[1000/1251], Loss: 3.1095(3.5865), Acc: 0.4453(0.3818)
2022-01-07 04:54:34,093 Epoch[266/310], Step[1050/1251], Loss: 3.5443(3.5866), Acc: 0.4336(0.3817)
2022-01-07 04:55:34,973 Epoch[266/310], Step[1100/1251], Loss: 3.5726(3.5849), Acc: 0.0977(0.3815)
2022-01-07 04:56:34,449 Epoch[266/310], Step[1150/1251], Loss: 3.5145(3.5886), Acc: 0.4473(0.3822)
2022-01-07 04:57:34,463 Epoch[266/310], Step[1200/1251], Loss: 3.7733(3.5855), Acc: 0.2891(0.3826)
2022-01-07 04:58:34,725 Epoch[266/310], Step[1250/1251], Loss: 3.7296(3.5832), Acc: 0.2021(0.3822)
2022-01-07 04:58:36,432 ----- Validation after Epoch: 266
2022-01-07 04:59:32,836 Val Step[0000/1563], Loss: 0.6627 (0.6627), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 04:59:34,128 Val Step[0050/1563], Loss: 2.1375 (0.7284), Acc@1: 0.4062 (0.8548), Acc@5: 0.8750 (0.9638)
2022-01-07 04:59:35,426 Val Step[0100/1563], Loss: 1.8349 (0.9861), Acc@1: 0.5000 (0.7871), Acc@5: 0.8125 (0.9415)
2022-01-07 04:59:36,710 Val Step[0150/1563], Loss: 0.3927 (0.9356), Acc@1: 0.9375 (0.7999), Acc@5: 1.0000 (0.9458)
2022-01-07 04:59:38,064 Val Step[0200/1563], Loss: 1.1037 (0.9479), Acc@1: 0.7500 (0.8005), Acc@5: 0.9062 (0.9437)
2022-01-07 04:59:39,327 Val Step[0250/1563], Loss: 0.5958 (0.8988), Acc@1: 0.9062 (0.8127), Acc@5: 1.0000 (0.9486)
2022-01-07 04:59:40,597 Val Step[0300/1563], Loss: 0.9925 (0.9582), Acc@1: 0.7188 (0.7963), Acc@5: 1.0000 (0.9435)
2022-01-07 04:59:41,870 Val Step[0350/1563], Loss: 0.8893 (0.9659), Acc@1: 0.8125 (0.7910), Acc@5: 0.9062 (0.9453)
2022-01-07 04:59:43,179 Val Step[0400/1563], Loss: 0.9152 (0.9726), Acc@1: 0.8438 (0.7861), Acc@5: 0.9688 (0.9455)
2022-01-07 04:59:44,529 Val Step[0450/1563], Loss: 0.9588 (0.9790), Acc@1: 0.7188 (0.7830), Acc@5: 1.0000 (0.9460)
2022-01-07 04:59:45,918 Val Step[0500/1563], Loss: 0.4454 (0.9701), Acc@1: 0.9062 (0.7855), Acc@5: 1.0000 (0.9471)
2022-01-07 04:59:47,325 Val Step[0550/1563], Loss: 0.6518 (0.9499), Acc@1: 0.8750 (0.7905), Acc@5: 1.0000 (0.9487)
2022-01-07 04:59:48,620 Val Step[0600/1563], Loss: 0.7797 (0.9581), Acc@1: 0.8125 (0.7895), Acc@5: 0.9375 (0.9476)
2022-01-07 04:59:49,901 Val Step[0650/1563], Loss: 0.5479 (0.9790), Acc@1: 0.9062 (0.7849), Acc@5: 1.0000 (0.9445)
2022-01-07 04:59:51,193 Val Step[0700/1563], Loss: 1.1063 (1.0073), Acc@1: 0.7812 (0.7777), Acc@5: 0.9375 (0.9409)
2022-01-07 04:59:52,499 Val Step[0750/1563], Loss: 1.2573 (1.0387), Acc@1: 0.8125 (0.7715), Acc@5: 0.8750 (0.9369)
2022-01-07 04:59:53,764 Val Step[0800/1563], Loss: 0.7837 (1.0750), Acc@1: 0.7812 (0.7630), Acc@5: 0.9688 (0.9318)
2022-01-07 04:59:55,040 Val Step[0850/1563], Loss: 1.2829 (1.0999), Acc@1: 0.6250 (0.7566), Acc@5: 0.9375 (0.9290)
2022-01-07 04:59:56,342 Val Step[0900/1563], Loss: 0.2762 (1.0987), Acc@1: 0.9375 (0.7582), Acc@5: 1.0000 (0.9286)
2022-01-07 04:59:57,739 Val Step[0950/1563], Loss: 1.1352 (1.1188), Acc@1: 0.7812 (0.7541), Acc@5: 0.9375 (0.9256)
2022-01-07 04:59:59,027 Val Step[1000/1563], Loss: 0.6006 (1.1418), Acc@1: 0.9688 (0.7485), Acc@5: 0.9688 (0.9224)
2022-01-07 05:00:00,367 Val Step[1050/1563], Loss: 0.3709 (1.1555), Acc@1: 0.9688 (0.7450), Acc@5: 1.0000 (0.9210)
2022-01-07 05:00:01,661 Val Step[1100/1563], Loss: 0.8640 (1.1692), Acc@1: 0.7500 (0.7420), Acc@5: 0.9688 (0.9191)
2022-01-07 05:00:03,028 Val Step[1150/1563], Loss: 1.3634 (1.1826), Acc@1: 0.7812 (0.7394), Acc@5: 0.7812 (0.9172)
2022-01-07 05:00:04,315 Val Step[1200/1563], Loss: 1.2507 (1.1974), Acc@1: 0.7812 (0.7357), Acc@5: 0.8438 (0.9148)
2022-01-07 05:00:05,582 Val Step[1250/1563], Loss: 0.7235 (1.2085), Acc@1: 0.8750 (0.7338), Acc@5: 0.9375 (0.9132)
2022-01-07 05:00:06,856 Val Step[1300/1563], Loss: 0.8031 (1.2169), Acc@1: 0.8750 (0.7324), Acc@5: 0.9375 (0.9120)
2022-01-07 05:00:08,195 Val Step[1350/1563], Loss: 1.6846 (1.2339), Acc@1: 0.5625 (0.7282), Acc@5: 0.9062 (0.9095)
2022-01-07 05:00:09,612 Val Step[1400/1563], Loss: 1.0353 (1.2401), Acc@1: 0.7812 (0.7266), Acc@5: 0.9375 (0.9087)
2022-01-07 05:00:11,034 Val Step[1450/1563], Loss: 1.5070 (1.2464), Acc@1: 0.7188 (0.7247), Acc@5: 0.9375 (0.9086)
2022-01-07 05:00:12,374 Val Step[1500/1563], Loss: 1.5787 (1.2362), Acc@1: 0.6875 (0.7270), Acc@5: 0.8750 (0.9099)
2022-01-07 05:00:13,756 Val Step[1550/1563], Loss: 0.8111 (1.2377), Acc@1: 0.8750 (0.7262), Acc@5: 0.9062 (0.9097)
2022-01-07 05:00:14,564 ----- Epoch[266/310], Validation Loss: 1.2359, Validation Acc@1: 0.7265, Validation Acc@5: 0.9099, time: 98.13
2022-01-07 05:00:14,564 ----- Epoch[266/310], Train Loss: 3.5832, Train Acc: 0.3822, time: 1560.28, Best Val(epoch266) Acc@1: 0.7265
2022-01-07 05:00:14,748 Max accuracy so far: 0.7265 at epoch_266
2022-01-07 05:00:14,748 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 05:00:14,748 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 05:00:14,857 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 05:00:15,256 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 05:00:15,257 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 05:00:15,585 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 05:00:15,585 Now training epoch 267. LR=0.000034
2022-01-07 05:01:30,115 Epoch[267/310], Step[0000/1251], Loss: 3.7522(3.7522), Acc: 0.3447(0.3447)
2022-01-07 05:02:29,378 Epoch[267/310], Step[0050/1251], Loss: 3.6304(3.6537), Acc: 0.4180(0.3634)
2022-01-07 05:03:28,559 Epoch[267/310], Step[0100/1251], Loss: 4.0226(3.6519), Acc: 0.3867(0.3740)
2022-01-07 05:04:28,690 Epoch[267/310], Step[0150/1251], Loss: 4.0571(3.6406), Acc: 0.3721(0.3718)
2022-01-07 05:05:28,784 Epoch[267/310], Step[0200/1251], Loss: 3.3923(3.6268), Acc: 0.4316(0.3750)
2022-01-07 05:06:29,495 Epoch[267/310], Step[0250/1251], Loss: 3.7517(3.6303), Acc: 0.3359(0.3789)
2022-01-07 05:07:29,422 Epoch[267/310], Step[0300/1251], Loss: 3.8336(3.6192), Acc: 0.4404(0.3845)
2022-01-07 05:08:29,399 Epoch[267/310], Step[0350/1251], Loss: 3.8471(3.6202), Acc: 0.1455(0.3835)
2022-01-07 05:09:29,236 Epoch[267/310], Step[0400/1251], Loss: 3.4060(3.6117), Acc: 0.2842(0.3847)
2022-01-07 05:10:29,210 Epoch[267/310], Step[0450/1251], Loss: 3.6414(3.6080), Acc: 0.3975(0.3829)
2022-01-07 05:11:29,622 Epoch[267/310], Step[0500/1251], Loss: 3.4837(3.6082), Acc: 0.4648(0.3808)
2022-01-07 05:12:30,405 Epoch[267/310], Step[0550/1251], Loss: 3.8556(3.6058), Acc: 0.2197(0.3792)
2022-01-07 05:13:30,816 Epoch[267/310], Step[0600/1251], Loss: 3.6212(3.6035), Acc: 0.3926(0.3799)
2022-01-07 05:14:30,282 Epoch[267/310], Step[0650/1251], Loss: 3.9599(3.6093), Acc: 0.4893(0.3803)
2022-01-07 05:15:29,511 Epoch[267/310], Step[0700/1251], Loss: 3.6521(3.6045), Acc: 0.5459(0.3804)
2022-01-07 05:16:28,935 Epoch[267/310], Step[0750/1251], Loss: 3.6444(3.5996), Acc: 0.4395(0.3829)
2022-01-07 05:17:28,809 Epoch[267/310], Step[0800/1251], Loss: 3.4914(3.5946), Acc: 0.5391(0.3842)
2022-01-07 05:18:28,319 Epoch[267/310], Step[0850/1251], Loss: 3.9280(3.5978), Acc: 0.3193(0.3837)
2022-01-07 05:19:28,297 Epoch[267/310], Step[0900/1251], Loss: 3.4647(3.5996), Acc: 0.2979(0.3828)
2022-01-07 05:20:28,293 Epoch[267/310], Step[0950/1251], Loss: 3.8988(3.6019), Acc: 0.4707(0.3826)
2022-01-07 05:21:29,137 Epoch[267/310], Step[1000/1251], Loss: 3.8042(3.6015), Acc: 0.3447(0.3819)
2022-01-07 05:22:29,306 Epoch[267/310], Step[1050/1251], Loss: 3.4536(3.6020), Acc: 0.3418(0.3820)
2022-01-07 05:23:30,017 Epoch[267/310], Step[1100/1251], Loss: 3.2229(3.6035), Acc: 0.5332(0.3824)
2022-01-07 05:24:28,955 Epoch[267/310], Step[1150/1251], Loss: 3.9114(3.6008), Acc: 0.3760(0.3822)
2022-01-07 05:25:28,779 Epoch[267/310], Step[1200/1251], Loss: 3.4590(3.5985), Acc: 0.4697(0.3822)
2022-01-07 05:26:27,668 Epoch[267/310], Step[1250/1251], Loss: 3.8150(3.5970), Acc: 0.2861(0.3827)
2022-01-07 05:26:29,785 ----- Epoch[267/310], Train Loss: 3.5970, Train Acc: 0.3827, time: 1574.20, Best Val(epoch266) Acc@1: 0.7265
2022-01-07 05:26:30,067 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 05:26:30,067 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 05:26:30,153 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 05:26:30,153 Now training epoch 268. LR=0.000033
2022-01-07 05:27:48,191 Epoch[268/310], Step[0000/1251], Loss: 3.9358(3.9358), Acc: 0.4590(0.4590)
2022-01-07 05:28:47,829 Epoch[268/310], Step[0050/1251], Loss: 3.7416(3.6470), Acc: 0.3486(0.3487)
2022-01-07 05:29:47,721 Epoch[268/310], Step[0100/1251], Loss: 4.2562(3.6287), Acc: 0.3242(0.3669)
2022-01-07 05:30:46,774 Epoch[268/310], Step[0150/1251], Loss: 3.8678(3.5824), Acc: 0.4512(0.3701)
2022-01-07 05:31:45,662 Epoch[268/310], Step[0200/1251], Loss: 2.8369(3.5691), Acc: 0.3047(0.3701)
2022-01-07 05:32:47,001 Epoch[268/310], Step[0250/1251], Loss: 3.7163(3.5662), Acc: 0.2871(0.3738)
2022-01-07 05:33:47,024 Epoch[268/310], Step[0300/1251], Loss: 3.5168(3.5701), Acc: 0.3643(0.3758)
2022-01-07 05:34:46,692 Epoch[268/310], Step[0350/1251], Loss: 3.5996(3.5802), Acc: 0.3955(0.3791)
2022-01-07 05:35:46,367 Epoch[268/310], Step[0400/1251], Loss: 3.0383(3.5747), Acc: 0.4863(0.3822)
2022-01-07 05:36:45,157 Epoch[268/310], Step[0450/1251], Loss: 3.2051(3.5701), Acc: 0.2451(0.3830)
2022-01-07 05:37:44,255 Epoch[268/310], Step[0500/1251], Loss: 4.0062(3.5732), Acc: 0.3203(0.3815)
2022-01-07 05:38:43,586 Epoch[268/310], Step[0550/1251], Loss: 3.0051(3.5690), Acc: 0.6113(0.3851)
2022-01-07 05:39:42,647 Epoch[268/310], Step[0600/1251], Loss: 2.9565(3.5670), Acc: 0.1670(0.3861)
2022-01-07 05:40:42,401 Epoch[268/310], Step[0650/1251], Loss: 4.1562(3.5680), Acc: 0.2549(0.3859)
2022-01-07 05:41:42,892 Epoch[268/310], Step[0700/1251], Loss: 3.4190(3.5715), Acc: 0.5664(0.3846)
2022-01-07 05:42:42,135 Epoch[268/310], Step[0750/1251], Loss: 3.6064(3.5722), Acc: 0.5479(0.3845)
2022-01-07 05:43:41,219 Epoch[268/310], Step[0800/1251], Loss: 3.3409(3.5763), Acc: 0.3975(0.3854)
2022-01-07 05:44:39,601 Epoch[268/310], Step[0850/1251], Loss: 3.8523(3.5827), Acc: 0.3037(0.3849)
2022-01-07 05:45:38,411 Epoch[268/310], Step[0900/1251], Loss: 3.5101(3.5863), Acc: 0.5049(0.3856)
2022-01-07 05:46:36,981 Epoch[268/310], Step[0950/1251], Loss: 4.1782(3.5819), Acc: 0.3652(0.3870)
2022-01-07 05:47:35,421 Epoch[268/310], Step[1000/1251], Loss: 3.1915(3.5855), Acc: 0.4775(0.3874)
2022-01-07 05:48:34,319 Epoch[268/310], Step[1050/1251], Loss: 3.8206(3.5876), Acc: 0.3301(0.3874)
2022-01-07 05:49:32,760 Epoch[268/310], Step[1100/1251], Loss: 3.5239(3.5884), Acc: 0.3350(0.3881)
2022-01-07 05:50:30,999 Epoch[268/310], Step[1150/1251], Loss: 3.7914(3.5888), Acc: 0.2168(0.3880)
2022-01-07 05:51:28,188 Epoch[268/310], Step[1200/1251], Loss: 3.8908(3.5882), Acc: 0.3994(0.3879)
2022-01-07 05:52:26,901 Epoch[268/310], Step[1250/1251], Loss: 3.3251(3.5879), Acc: 0.5508(0.3875)
2022-01-07 05:52:28,413 ----- Validation after Epoch: 268
2022-01-07 05:53:26,356 Val Step[0000/1563], Loss: 0.6574 (0.6574), Acc@1: 0.9688 (0.9688), Acc@5: 0.9688 (0.9688)
2022-01-07 05:53:27,863 Val Step[0050/1563], Loss: 2.2759 (0.7225), Acc@1: 0.4062 (0.8517), Acc@5: 0.8750 (0.9657)
2022-01-07 05:53:29,182 Val Step[0100/1563], Loss: 1.8688 (0.9890), Acc@1: 0.5000 (0.7788), Acc@5: 0.8125 (0.9387)
2022-01-07 05:53:30,574 Val Step[0150/1563], Loss: 0.4046 (0.9307), Acc@1: 0.9062 (0.7933), Acc@5: 1.0000 (0.9433)
2022-01-07 05:53:31,988 Val Step[0200/1563], Loss: 0.9463 (0.9371), Acc@1: 0.7812 (0.7948), Acc@5: 0.9688 (0.9420)
2022-01-07 05:53:33,391 Val Step[0250/1563], Loss: 0.5930 (0.8896), Acc@1: 0.9062 (0.8066), Acc@5: 1.0000 (0.9470)
2022-01-07 05:53:34,837 Val Step[0300/1563], Loss: 0.9914 (0.9477), Acc@1: 0.7500 (0.7908), Acc@5: 1.0000 (0.9422)
2022-01-07 05:53:36,183 Val Step[0350/1563], Loss: 0.9023 (0.9545), Acc@1: 0.7812 (0.7862), Acc@5: 0.9062 (0.9442)
2022-01-07 05:53:37,449 Val Step[0400/1563], Loss: 0.8584 (0.9619), Acc@1: 0.7500 (0.7820), Acc@5: 0.9688 (0.9444)
2022-01-07 05:53:38,730 Val Step[0450/1563], Loss: 0.8457 (0.9684), Acc@1: 0.7188 (0.7796), Acc@5: 1.0000 (0.9448)
2022-01-07 05:53:40,027 Val Step[0500/1563], Loss: 0.4640 (0.9591), Acc@1: 0.8438 (0.7823), Acc@5: 1.0000 (0.9461)
2022-01-07 05:53:41,411 Val Step[0550/1563], Loss: 0.7144 (0.9375), Acc@1: 0.8438 (0.7874), Acc@5: 0.9688 (0.9479)
2022-01-07 05:53:42,706 Val Step[0600/1563], Loss: 0.7937 (0.9444), Acc@1: 0.8125 (0.7865), Acc@5: 0.9375 (0.9474)
2022-01-07 05:53:43,974 Val Step[0650/1563], Loss: 0.5187 (0.9671), Acc@1: 0.9375 (0.7818), Acc@5: 1.0000 (0.9446)
2022-01-07 05:53:45,239 Val Step[0700/1563], Loss: 0.9676 (0.9940), Acc@1: 0.8125 (0.7756), Acc@5: 0.9375 (0.9415)
2022-01-07 05:53:46,519 Val Step[0750/1563], Loss: 1.0871 (1.0260), Acc@1: 0.8125 (0.7694), Acc@5: 0.9375 (0.9372)
2022-01-07 05:53:47,842 Val Step[0800/1563], Loss: 0.6082 (1.0618), Acc@1: 0.9062 (0.7608), Acc@5: 1.0000 (0.9326)
2022-01-07 05:53:49,137 Val Step[0850/1563], Loss: 1.2506 (1.0864), Acc@1: 0.6875 (0.7544), Acc@5: 0.9375 (0.9293)
2022-01-07 05:53:50,424 Val Step[0900/1563], Loss: 0.2404 (1.0853), Acc@1: 0.9688 (0.7559), Acc@5: 1.0000 (0.9289)
2022-01-07 05:53:51,872 Val Step[0950/1563], Loss: 1.2016 (1.1051), Acc@1: 0.7812 (0.7523), Acc@5: 0.9062 (0.9260)
2022-01-07 05:53:53,188 Val Step[1000/1563], Loss: 0.5513 (1.1274), Acc@1: 0.9688 (0.7468), Acc@5: 1.0000 (0.9230)
2022-01-07 05:53:54,521 Val Step[1050/1563], Loss: 0.3329 (1.1407), Acc@1: 0.9688 (0.7436), Acc@5: 1.0000 (0.9216)
2022-01-07 05:53:55,789 Val Step[1100/1563], Loss: 0.8134 (1.1533), Acc@1: 0.8125 (0.7411), Acc@5: 0.9688 (0.9196)
2022-01-07 05:53:57,061 Val Step[1150/1563], Loss: 1.1770 (1.1670), Acc@1: 0.7812 (0.7384), Acc@5: 0.8438 (0.9176)
2022-01-07 05:53:58,329 Val Step[1200/1563], Loss: 1.2572 (1.1819), Acc@1: 0.7812 (0.7347), Acc@5: 0.8438 (0.9152)
2022-01-07 05:53:59,622 Val Step[1250/1563], Loss: 0.7560 (1.1932), Acc@1: 0.8750 (0.7330), Acc@5: 0.9375 (0.9133)
2022-01-07 05:54:00,939 Val Step[1300/1563], Loss: 0.7886 (1.2016), Acc@1: 0.9062 (0.7313), Acc@5: 0.9375 (0.9123)
2022-01-07 05:54:02,246 Val Step[1350/1563], Loss: 1.7184 (1.2179), Acc@1: 0.5312 (0.7276), Acc@5: 0.9062 (0.9101)
2022-01-07 05:54:03,542 Val Step[1400/1563], Loss: 0.9381 (1.2240), Acc@1: 0.7812 (0.7264), Acc@5: 0.9375 (0.9094)
2022-01-07 05:54:04,842 Val Step[1450/1563], Loss: 1.3963 (1.2304), Acc@1: 0.6875 (0.7247), Acc@5: 0.9062 (0.9091)
2022-01-07 05:54:06,158 Val Step[1500/1563], Loss: 1.5888 (1.2197), Acc@1: 0.6250 (0.7272), Acc@5: 0.8750 (0.9104)
2022-01-07 05:54:07,445 Val Step[1550/1563], Loss: 0.8986 (1.2215), Acc@1: 0.8750 (0.7263), Acc@5: 0.9062 (0.9102)
2022-01-07 05:54:08,216 ----- Epoch[268/310], Validation Loss: 1.2200, Validation Acc@1: 0.7266, Validation Acc@5: 0.9105, time: 99.80
2022-01-07 05:54:08,216 ----- Epoch[268/310], Train Loss: 3.5879, Train Acc: 0.3875, time: 1558.26, Best Val(epoch268) Acc@1: 0.7266
2022-01-07 05:54:08,397 Max accuracy so far: 0.7266 at epoch_268
2022-01-07 05:54:08,397 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 05:54:08,397 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 05:54:08,501 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 05:54:09,077 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 05:54:09,078 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 05:54:09,127 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 05:54:09,128 Now training epoch 269. LR=0.000031
2022-01-07 05:55:22,959 Epoch[269/310], Step[0000/1251], Loss: 3.8073(3.8073), Acc: 0.3428(0.3428)
2022-01-07 05:56:22,186 Epoch[269/310], Step[0050/1251], Loss: 3.8239(3.5723), Acc: 0.4785(0.3754)
2022-01-07 05:57:20,889 Epoch[269/310], Step[0100/1251], Loss: 3.3957(3.6031), Acc: 0.1650(0.3751)
2022-01-07 05:58:19,755 Epoch[269/310], Step[0150/1251], Loss: 3.9209(3.5949), Acc: 0.0889(0.3785)
2022-01-07 05:59:19,602 Epoch[269/310], Step[0200/1251], Loss: 3.8338(3.5819), Acc: 0.3643(0.3792)
2022-01-07 06:00:18,598 Epoch[269/310], Step[0250/1251], Loss: 3.3521(3.5748), Acc: 0.5518(0.3814)
2022-01-07 06:01:18,648 Epoch[269/310], Step[0300/1251], Loss: 2.9670(3.5978), Acc: 0.4863(0.3804)
2022-01-07 06:02:18,617 Epoch[269/310], Step[0350/1251], Loss: 3.6872(3.5890), Acc: 0.4785(0.3794)
2022-01-07 06:03:18,688 Epoch[269/310], Step[0400/1251], Loss: 3.5879(3.5869), Acc: 0.3975(0.3830)
2022-01-07 06:04:17,532 Epoch[269/310], Step[0450/1251], Loss: 4.2115(3.5876), Acc: 0.3447(0.3833)
2022-01-07 06:05:16,422 Epoch[269/310], Step[0500/1251], Loss: 3.7176(3.5841), Acc: 0.4482(0.3820)
2022-01-07 06:06:14,694 Epoch[269/310], Step[0550/1251], Loss: 3.7344(3.5849), Acc: 0.3896(0.3852)
2022-01-07 06:07:14,365 Epoch[269/310], Step[0600/1251], Loss: 3.5025(3.5832), Acc: 0.4434(0.3858)
2022-01-07 06:08:13,973 Epoch[269/310], Step[0650/1251], Loss: 3.4767(3.5812), Acc: 0.4805(0.3880)
2022-01-07 06:09:13,549 Epoch[269/310], Step[0700/1251], Loss: 3.7435(3.5819), Acc: 0.1748(0.3868)
2022-01-07 06:10:13,277 Epoch[269/310], Step[0750/1251], Loss: 3.6496(3.5789), Acc: 0.4932(0.3864)
2022-01-07 06:11:12,772 Epoch[269/310], Step[0800/1251], Loss: 4.2363(3.5812), Acc: 0.1758(0.3851)
2022-01-07 06:12:12,999 Epoch[269/310], Step[0850/1251], Loss: 4.3766(3.5832), Acc: 0.2969(0.3835)
2022-01-07 06:13:11,999 Epoch[269/310], Step[0900/1251], Loss: 3.8931(3.5823), Acc: 0.4775(0.3847)
2022-01-07 06:14:11,469 Epoch[269/310], Step[0950/1251], Loss: 3.4907(3.5778), Acc: 0.5303(0.3852)
2022-01-07 06:15:10,829 Epoch[269/310], Step[1000/1251], Loss: 3.7225(3.5772), Acc: 0.3848(0.3850)
2022-01-07 06:16:10,699 Epoch[269/310], Step[1050/1251], Loss: 4.0526(3.5807), Acc: 0.3330(0.3842)
2022-01-07 06:17:11,266 Epoch[269/310], Step[1100/1251], Loss: 3.5402(3.5848), Acc: 0.5215(0.3832)
2022-01-07 06:18:10,856 Epoch[269/310], Step[1150/1251], Loss: 3.4107(3.5842), Acc: 0.2451(0.3849)
2022-01-07 06:19:11,072 Epoch[269/310], Step[1200/1251], Loss: 2.9761(3.5824), Acc: 0.5791(0.3849)
2022-01-07 06:20:11,057 Epoch[269/310], Step[1250/1251], Loss: 3.8975(3.5843), Acc: 0.3584(0.3844)
2022-01-07 06:20:12,577 ----- Epoch[269/310], Train Loss: 3.5843, Train Acc: 0.3844, time: 1563.45, Best Val(epoch268) Acc@1: 0.7266
2022-01-07 06:20:12,750 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 06:20:12,750 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 06:20:12,853 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 06:20:12,853 Now training epoch 270. LR=0.000029
2022-01-07 06:21:27,264 Epoch[270/310], Step[0000/1251], Loss: 3.2775(3.2775), Acc: 0.5947(0.5947)
2022-01-07 06:22:25,988 Epoch[270/310], Step[0050/1251], Loss: 3.4562(3.5557), Acc: 0.3906(0.4039)
2022-01-07 06:23:25,219 Epoch[270/310], Step[0100/1251], Loss: 3.4216(3.5631), Acc: 0.5957(0.3959)
2022-01-07 06:24:22,907 Epoch[270/310], Step[0150/1251], Loss: 3.5416(3.5445), Acc: 0.5420(0.3997)
2022-01-07 06:25:22,321 Epoch[270/310], Step[0200/1251], Loss: 2.7723(3.5452), Acc: 0.3076(0.3993)
2022-01-07 06:26:22,328 Epoch[270/310], Step[0250/1251], Loss: 3.6308(3.5410), Acc: 0.1689(0.3981)
2022-01-07 06:27:21,153 Epoch[270/310], Step[0300/1251], Loss: 3.4670(3.5435), Acc: 0.2852(0.3974)
2022-01-07 06:28:21,136 Epoch[270/310], Step[0350/1251], Loss: 3.4632(3.5554), Acc: 0.2246(0.3933)
2022-01-07 06:29:20,463 Epoch[270/310], Step[0400/1251], Loss: 3.7745(3.5570), Acc: 0.4873(0.3937)
2022-01-07 06:30:20,607 Epoch[270/310], Step[0450/1251], Loss: 3.3176(3.5623), Acc: 0.3945(0.3910)
2022-01-07 06:31:21,003 Epoch[270/310], Step[0500/1251], Loss: 3.5130(3.5628), Acc: 0.5273(0.3882)
2022-01-07 06:32:20,523 Epoch[270/310], Step[0550/1251], Loss: 3.0169(3.5646), Acc: 0.2725(0.3885)
2022-01-07 06:33:19,695 Epoch[270/310], Step[0600/1251], Loss: 3.4929(3.5660), Acc: 0.4297(0.3858)
2022-01-07 06:34:20,695 Epoch[270/310], Step[0650/1251], Loss: 4.0231(3.5679), Acc: 0.2383(0.3838)
2022-01-07 06:35:20,546 Epoch[270/310], Step[0700/1251], Loss: 3.0215(3.5657), Acc: 0.5566(0.3840)
2022-01-07 06:36:18,882 Epoch[270/310], Step[0750/1251], Loss: 3.5133(3.5659), Acc: 0.3877(0.3836)
2022-01-07 06:37:17,545 Epoch[270/310], Step[0800/1251], Loss: 3.3372(3.5653), Acc: 0.6094(0.3849)
2022-01-07 06:38:16,457 Epoch[270/310], Step[0850/1251], Loss: 3.2819(3.5663), Acc: 0.4521(0.3847)
2022-01-07 06:39:15,154 Epoch[270/310], Step[0900/1251], Loss: 3.2834(3.5676), Acc: 0.4814(0.3850)
2022-01-07 06:40:14,948 Epoch[270/310], Step[0950/1251], Loss: 3.7350(3.5705), Acc: 0.4014(0.3845)
2022-01-07 06:41:14,416 Epoch[270/310], Step[1000/1251], Loss: 3.6674(3.5691), Acc: 0.3096(0.3847)
2022-01-07 06:42:12,741 Epoch[270/310], Step[1050/1251], Loss: 3.5775(3.5710), Acc: 0.4697(0.3855)
2022-01-07 06:43:13,127 Epoch[270/310], Step[1100/1251], Loss: 3.5050(3.5709), Acc: 0.3408(0.3844)
2022-01-07 06:44:12,155 Epoch[270/310], Step[1150/1251], Loss: 3.5163(3.5694), Acc: 0.4131(0.3845)
2022-01-07 06:45:12,118 Epoch[270/310], Step[1200/1251], Loss: 3.2806(3.5690), Acc: 0.4443(0.3842)
2022-01-07 06:46:10,993 Epoch[270/310], Step[1250/1251], Loss: 3.1844(3.5653), Acc: 0.6035(0.3860)
2022-01-07 06:46:12,511 ----- Validation after Epoch: 270
2022-01-07 06:47:11,742 Val Step[0000/1563], Loss: 0.7085 (0.7085), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 06:47:13,150 Val Step[0050/1563], Loss: 2.2601 (0.7196), Acc@1: 0.4062 (0.8523), Acc@5: 0.8438 (0.9614)
2022-01-07 06:47:14,428 Val Step[0100/1563], Loss: 1.8811 (0.9800), Acc@1: 0.5625 (0.7837), Acc@5: 0.8125 (0.9372)
2022-01-07 06:47:15,864 Val Step[0150/1563], Loss: 0.4039 (0.9255), Acc@1: 0.9062 (0.7957), Acc@5: 1.0000 (0.9427)
2022-01-07 06:47:17,315 Val Step[0200/1563], Loss: 0.9329 (0.9318), Acc@1: 0.7812 (0.7979), Acc@5: 0.9375 (0.9423)
2022-01-07 06:47:18,726 Val Step[0250/1563], Loss: 0.5363 (0.8813), Acc@1: 0.9062 (0.8093), Acc@5: 1.0000 (0.9478)
2022-01-07 06:47:20,147 Val Step[0300/1563], Loss: 1.0727 (0.9410), Acc@1: 0.7188 (0.7947), Acc@5: 0.9375 (0.9437)
2022-01-07 06:47:21,566 Val Step[0350/1563], Loss: 1.0095 (0.9463), Acc@1: 0.7812 (0.7902), Acc@5: 0.9062 (0.9458)
2022-01-07 06:47:22,973 Val Step[0400/1563], Loss: 0.8674 (0.9560), Acc@1: 0.8438 (0.7845), Acc@5: 0.9688 (0.9455)
2022-01-07 06:47:24,400 Val Step[0450/1563], Loss: 1.0186 (0.9622), Acc@1: 0.7188 (0.7819), Acc@5: 1.0000 (0.9460)
2022-01-07 06:47:25,819 Val Step[0500/1563], Loss: 0.4089 (0.9535), Acc@1: 0.9062 (0.7843), Acc@5: 1.0000 (0.9469)
2022-01-07 06:47:27,354 Val Step[0550/1563], Loss: 0.6447 (0.9328), Acc@1: 0.8750 (0.7895), Acc@5: 0.9688 (0.9486)
2022-01-07 06:47:28,827 Val Step[0600/1563], Loss: 0.8117 (0.9404), Acc@1: 0.8125 (0.7887), Acc@5: 0.9375 (0.9478)
2022-01-07 06:47:30,369 Val Step[0650/1563], Loss: 0.5543 (0.9610), Acc@1: 0.9062 (0.7843), Acc@5: 1.0000 (0.9447)
2022-01-07 06:47:31,799 Val Step[0700/1563], Loss: 0.8835 (0.9889), Acc@1: 0.8438 (0.7775), Acc@5: 0.9375 (0.9415)
2022-01-07 06:47:33,254 Val Step[0750/1563], Loss: 1.1673 (1.0191), Acc@1: 0.7812 (0.7716), Acc@5: 0.9062 (0.9374)
2022-01-07 06:47:34,679 Val Step[0800/1563], Loss: 0.6290 (1.0558), Acc@1: 0.8125 (0.7622), Acc@5: 1.0000 (0.9331)
2022-01-07 06:47:36,126 Val Step[0850/1563], Loss: 1.1892 (1.0788), Acc@1: 0.6562 (0.7563), Acc@5: 0.9375 (0.9303)
2022-01-07 06:47:37,557 Val Step[0900/1563], Loss: 0.2656 (1.0781), Acc@1: 0.9688 (0.7579), Acc@5: 1.0000 (0.9299)
2022-01-07 06:47:39,058 Val Step[0950/1563], Loss: 1.1575 (1.0978), Acc@1: 0.7812 (0.7544), Acc@5: 0.9062 (0.9266)
2022-01-07 06:47:40,502 Val Step[1000/1563], Loss: 0.5238 (1.1204), Acc@1: 0.9688 (0.7486), Acc@5: 1.0000 (0.9237)
2022-01-07 06:47:41,935 Val Step[1050/1563], Loss: 0.3666 (1.1344), Acc@1: 0.9688 (0.7454), Acc@5: 0.9688 (0.9220)
2022-01-07 06:47:43,351 Val Step[1100/1563], Loss: 0.7440 (1.1477), Acc@1: 0.7812 (0.7428), Acc@5: 0.9375 (0.9202)
2022-01-07 06:47:44,786 Val Step[1150/1563], Loss: 1.3535 (1.1610), Acc@1: 0.7812 (0.7402), Acc@5: 0.8125 (0.9183)
2022-01-07 06:47:46,186 Val Step[1200/1563], Loss: 1.2218 (1.1746), Acc@1: 0.7812 (0.7369), Acc@5: 0.8438 (0.9160)
2022-01-07 06:47:47,589 Val Step[1250/1563], Loss: 0.6871 (1.1859), Acc@1: 0.8750 (0.7353), Acc@5: 0.9375 (0.9141)
2022-01-07 06:47:48,976 Val Step[1300/1563], Loss: 0.8176 (1.1943), Acc@1: 0.8438 (0.7337), Acc@5: 0.9375 (0.9130)
2022-01-07 06:47:50,371 Val Step[1350/1563], Loss: 1.6486 (1.2115), Acc@1: 0.5312 (0.7292), Acc@5: 0.8750 (0.9107)
2022-01-07 06:47:51,777 Val Step[1400/1563], Loss: 0.9763 (1.2174), Acc@1: 0.7500 (0.7278), Acc@5: 0.9375 (0.9100)
2022-01-07 06:47:53,229 Val Step[1450/1563], Loss: 1.3762 (1.2238), Acc@1: 0.6562 (0.7259), Acc@5: 0.9375 (0.9096)
2022-01-07 06:47:54,634 Val Step[1500/1563], Loss: 1.6608 (1.2141), Acc@1: 0.6562 (0.7282), Acc@5: 0.8750 (0.9109)
2022-01-07 06:47:56,040 Val Step[1550/1563], Loss: 0.8734 (1.2165), Acc@1: 0.8750 (0.7275), Acc@5: 0.9062 (0.9106)
2022-01-07 06:47:56,812 ----- Epoch[270/310], Validation Loss: 1.2149, Validation Acc@1: 0.7277, Validation Acc@5: 0.9109, time: 104.30
2022-01-07 06:47:56,812 ----- Epoch[270/310], Train Loss: 3.5653, Train Acc: 0.3860, time: 1559.65, Best Val(epoch270) Acc@1: 0.7277
2022-01-07 06:47:56,994 Max accuracy so far: 0.7277 at epoch_270
2022-01-07 06:47:56,994 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 06:47:56,994 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 06:47:57,098 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 06:47:57,229 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-270-Loss-3.583084675143186.pdparams
2022-01-07 06:47:57,230 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-270-Loss-3.583084675143186.pdopt
2022-01-07 06:47:57,271 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-270-Loss-3.583084675143186-EMA.pdparams
2022-01-07 06:47:57,272 Now training epoch 271. LR=0.000028
2022-01-07 06:49:07,835 Epoch[271/310], Step[0000/1251], Loss: 3.9039(3.9039), Acc: 0.3701(0.3701)
2022-01-07 06:50:07,464 Epoch[271/310], Step[0050/1251], Loss: 3.7345(3.6647), Acc: 0.1982(0.3718)
2022-01-07 06:51:08,069 Epoch[271/310], Step[0100/1251], Loss: 3.9742(3.6213), Acc: 0.3926(0.3820)
2022-01-07 06:52:06,140 Epoch[271/310], Step[0150/1251], Loss: 3.5954(3.6070), Acc: 0.3525(0.3898)
2022-01-07 06:53:06,563 Epoch[271/310], Step[0200/1251], Loss: 3.6159(3.5943), Acc: 0.5146(0.3833)
2022-01-07 06:54:06,310 Epoch[271/310], Step[0250/1251], Loss: 3.9970(3.5863), Acc: 0.3789(0.3857)
2022-01-07 06:55:05,722 Epoch[271/310], Step[0300/1251], Loss: 3.7823(3.5851), Acc: 0.4492(0.3852)
2022-01-07 06:56:05,266 Epoch[271/310], Step[0350/1251], Loss: 3.4595(3.5741), Acc: 0.2480(0.3840)
2022-01-07 06:57:05,729 Epoch[271/310], Step[0400/1251], Loss: 3.3333(3.5714), Acc: 0.3965(0.3841)
2022-01-07 06:58:05,261 Epoch[271/310], Step[0450/1251], Loss: 3.6428(3.5801), Acc: 0.2354(0.3833)
2022-01-07 06:59:03,653 Epoch[271/310], Step[0500/1251], Loss: 3.5337(3.5827), Acc: 0.5479(0.3845)
2022-01-07 07:00:02,954 Epoch[271/310], Step[0550/1251], Loss: 3.4651(3.5816), Acc: 0.5312(0.3841)
2022-01-07 07:01:01,663 Epoch[271/310], Step[0600/1251], Loss: 3.7072(3.5787), Acc: 0.5156(0.3862)
2022-01-07 07:02:01,298 Epoch[271/310], Step[0650/1251], Loss: 3.5382(3.5810), Acc: 0.5498(0.3847)
2022-01-07 07:03:00,798 Epoch[271/310], Step[0700/1251], Loss: 3.7151(3.5834), Acc: 0.2920(0.3845)
2022-01-07 07:04:01,192 Epoch[271/310], Step[0750/1251], Loss: 3.9230(3.5830), Acc: 0.4346(0.3835)
2022-01-07 07:05:00,395 Epoch[271/310], Step[0800/1251], Loss: 3.4650(3.5819), Acc: 0.3691(0.3849)
2022-01-07 07:06:00,548 Epoch[271/310], Step[0850/1251], Loss: 3.9482(3.5867), Acc: 0.4375(0.3837)
2022-01-07 07:06:59,875 Epoch[271/310], Step[0900/1251], Loss: 3.5682(3.5843), Acc: 0.5244(0.3845)
2022-01-07 07:07:59,445 Epoch[271/310], Step[0950/1251], Loss: 3.3972(3.5827), Acc: 0.4834(0.3859)
2022-01-07 07:08:58,626 Epoch[271/310], Step[1000/1251], Loss: 3.7170(3.5789), Acc: 0.2051(0.3850)
2022-01-07 07:09:58,158 Epoch[271/310], Step[1050/1251], Loss: 3.9803(3.5783), Acc: 0.2383(0.3852)
2022-01-07 07:10:58,414 Epoch[271/310], Step[1100/1251], Loss: 3.6461(3.5787), Acc: 0.1836(0.3844)
2022-01-07 07:11:57,827 Epoch[271/310], Step[1150/1251], Loss: 3.7777(3.5762), Acc: 0.3740(0.3835)
2022-01-07 07:12:57,804 Epoch[271/310], Step[1200/1251], Loss: 3.6044(3.5720), Acc: 0.4023(0.3836)
2022-01-07 07:13:56,465 Epoch[271/310], Step[1250/1251], Loss: 3.9808(3.5714), Acc: 0.4531(0.3840)
2022-01-07 07:13:58,107 ----- Epoch[271/310], Train Loss: 3.5714, Train Acc: 0.3840, time: 1560.83, Best Val(epoch270) Acc@1: 0.7277
2022-01-07 07:13:58,291 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 07:13:58,292 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 07:13:58,380 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 07:13:58,380 Now training epoch 272. LR=0.000026
2022-01-07 07:15:19,622 Epoch[272/310], Step[0000/1251], Loss: 3.3008(3.3008), Acc: 0.4316(0.4316)
2022-01-07 07:16:20,319 Epoch[272/310], Step[0050/1251], Loss: 3.2473(3.5684), Acc: 0.2607(0.3816)
2022-01-07 07:17:19,763 Epoch[272/310], Step[0100/1251], Loss: 3.7461(3.5775), Acc: 0.3154(0.3717)
2022-01-07 07:18:19,843 Epoch[272/310], Step[0150/1251], Loss: 3.8501(3.5747), Acc: 0.3145(0.3780)
2022-01-07 07:19:18,259 Epoch[272/310], Step[0200/1251], Loss: 3.6051(3.5816), Acc: 0.4404(0.3809)
2022-01-07 07:20:18,515 Epoch[272/310], Step[0250/1251], Loss: 3.0139(3.5655), Acc: 0.3154(0.3807)
2022-01-07 07:21:18,447 Epoch[272/310], Step[0300/1251], Loss: 3.6082(3.5528), Acc: 0.3623(0.3856)
2022-01-07 07:22:17,486 Epoch[272/310], Step[0350/1251], Loss: 3.7507(3.5468), Acc: 0.3760(0.3856)
2022-01-07 07:23:17,492 Epoch[272/310], Step[0400/1251], Loss: 3.5163(3.5538), Acc: 0.2100(0.3819)
2022-01-07 07:24:16,615 Epoch[272/310], Step[0450/1251], Loss: 4.0362(3.5605), Acc: 0.3721(0.3824)
2022-01-07 07:25:14,614 Epoch[272/310], Step[0500/1251], Loss: 3.7819(3.5688), Acc: 0.3896(0.3808)
2022-01-07 07:26:13,950 Epoch[272/310], Step[0550/1251], Loss: 3.4994(3.5681), Acc: 0.4316(0.3825)
2022-01-07 07:27:13,521 Epoch[272/310], Step[0600/1251], Loss: 3.5788(3.5694), Acc: 0.3955(0.3819)
2022-01-07 07:28:13,296 Epoch[272/310], Step[0650/1251], Loss: 3.9383(3.5676), Acc: 0.3223(0.3827)
2022-01-07 07:29:13,714 Epoch[272/310], Step[0700/1251], Loss: 3.3906(3.5724), Acc: 0.3623(0.3816)
2022-01-07 07:30:14,305 Epoch[272/310], Step[0750/1251], Loss: 3.5292(3.5716), Acc: 0.5645(0.3821)
2022-01-07 07:31:14,454 Epoch[272/310], Step[0800/1251], Loss: 3.7951(3.5676), Acc: 0.4102(0.3836)
2022-01-07 07:32:14,449 Epoch[272/310], Step[0850/1251], Loss: 3.4689(3.5641), Acc: 0.2363(0.3842)
2022-01-07 07:33:14,908 Epoch[272/310], Step[0900/1251], Loss: 3.9364(3.5694), Acc: 0.3506(0.3840)
2022-01-07 07:34:14,878 Epoch[272/310], Step[0950/1251], Loss: 3.5315(3.5714), Acc: 0.5312(0.3828)
2022-01-07 07:35:14,186 Epoch[272/310], Step[1000/1251], Loss: 3.2965(3.5703), Acc: 0.6084(0.3834)
2022-01-07 07:36:13,116 Epoch[272/310], Step[1050/1251], Loss: 3.8097(3.5715), Acc: 0.2705(0.3846)
2022-01-07 07:37:13,915 Epoch[272/310], Step[1100/1251], Loss: 3.2737(3.5696), Acc: 0.4814(0.3847)
2022-01-07 07:38:14,686 Epoch[272/310], Step[1150/1251], Loss: 3.5187(3.5731), Acc: 0.2441(0.3843)
2022-01-07 07:39:14,770 Epoch[272/310], Step[1200/1251], Loss: 3.9416(3.5743), Acc: 0.3730(0.3834)
2022-01-07 07:40:15,471 Epoch[272/310], Step[1250/1251], Loss: 3.7641(3.5788), Acc: 0.5088(0.3828)
2022-01-07 07:40:16,949 ----- Validation after Epoch: 272
2022-01-07 07:41:16,819 Val Step[0000/1563], Loss: 0.6786 (0.6786), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 07:41:18,192 Val Step[0050/1563], Loss: 2.3642 (0.7293), Acc@1: 0.4062 (0.8560), Acc@5: 0.8438 (0.9626)
2022-01-07 07:41:19,477 Val Step[0100/1563], Loss: 1.8746 (0.9826), Acc@1: 0.5312 (0.7837), Acc@5: 0.8438 (0.9384)
2022-01-07 07:41:20,953 Val Step[0150/1563], Loss: 0.3956 (0.9283), Acc@1: 0.9375 (0.7962), Acc@5: 1.0000 (0.9435)
2022-01-07 07:41:22,260 Val Step[0200/1563], Loss: 1.0413 (0.9332), Acc@1: 0.8125 (0.7990), Acc@5: 0.9375 (0.9423)
2022-01-07 07:41:23,541 Val Step[0250/1563], Loss: 0.5279 (0.8825), Acc@1: 0.9062 (0.8106), Acc@5: 1.0000 (0.9477)
2022-01-07 07:41:24,902 Val Step[0300/1563], Loss: 1.0436 (0.9388), Acc@1: 0.7188 (0.7957), Acc@5: 1.0000 (0.9434)
2022-01-07 07:41:26,200 Val Step[0350/1563], Loss: 0.9259 (0.9479), Acc@1: 0.7500 (0.7909), Acc@5: 0.9062 (0.9447)
2022-01-07 07:41:27,541 Val Step[0400/1563], Loss: 0.8853 (0.9570), Acc@1: 0.8438 (0.7851), Acc@5: 0.9688 (0.9445)
2022-01-07 07:41:28,866 Val Step[0450/1563], Loss: 0.8894 (0.9640), Acc@1: 0.7500 (0.7822), Acc@5: 1.0000 (0.9453)
2022-01-07 07:41:30,261 Val Step[0500/1563], Loss: 0.4396 (0.9548), Acc@1: 0.8750 (0.7849), Acc@5: 1.0000 (0.9465)
2022-01-07 07:41:31,769 Val Step[0550/1563], Loss: 0.7921 (0.9343), Acc@1: 0.8125 (0.7895), Acc@5: 0.9688 (0.9482)
2022-01-07 07:41:33,130 Val Step[0600/1563], Loss: 0.8005 (0.9425), Acc@1: 0.7812 (0.7881), Acc@5: 0.9375 (0.9477)
2022-01-07 07:41:34,496 Val Step[0650/1563], Loss: 0.4477 (0.9621), Acc@1: 0.9375 (0.7838), Acc@5: 1.0000 (0.9447)
2022-01-07 07:41:35,780 Val Step[0700/1563], Loss: 1.0209 (0.9896), Acc@1: 0.8125 (0.7767), Acc@5: 0.9688 (0.9416)
2022-01-07 07:41:37,057 Val Step[0750/1563], Loss: 1.3025 (1.0220), Acc@1: 0.7500 (0.7707), Acc@5: 0.9062 (0.9370)
2022-01-07 07:41:38,412 Val Step[0800/1563], Loss: 0.6349 (1.0579), Acc@1: 0.8750 (0.7622), Acc@5: 1.0000 (0.9326)
2022-01-07 07:41:39,681 Val Step[0850/1563], Loss: 1.2445 (1.0828), Acc@1: 0.6562 (0.7557), Acc@5: 0.9375 (0.9298)
2022-01-07 07:41:40,960 Val Step[0900/1563], Loss: 0.2524 (1.0816), Acc@1: 0.9688 (0.7572), Acc@5: 1.0000 (0.9294)
2022-01-07 07:41:42,424 Val Step[0950/1563], Loss: 1.2826 (1.1012), Acc@1: 0.7188 (0.7535), Acc@5: 0.8750 (0.9265)
2022-01-07 07:41:43,672 Val Step[1000/1563], Loss: 0.6360 (1.1235), Acc@1: 0.9375 (0.7478), Acc@5: 1.0000 (0.9238)
2022-01-07 07:41:44,927 Val Step[1050/1563], Loss: 0.4176 (1.1380), Acc@1: 0.9688 (0.7445), Acc@5: 0.9688 (0.9222)
2022-01-07 07:41:46,278 Val Step[1100/1563], Loss: 0.6920 (1.1507), Acc@1: 0.9062 (0.7420), Acc@5: 0.9375 (0.9203)
2022-01-07 07:41:47,607 Val Step[1150/1563], Loss: 1.3242 (1.1636), Acc@1: 0.7812 (0.7396), Acc@5: 0.8125 (0.9183)
2022-01-07 07:41:48,884 Val Step[1200/1563], Loss: 1.2750 (1.1769), Acc@1: 0.7500 (0.7365), Acc@5: 0.8438 (0.9160)
2022-01-07 07:41:50,159 Val Step[1250/1563], Loss: 0.7077 (1.1883), Acc@1: 0.8750 (0.7344), Acc@5: 0.9375 (0.9143)
2022-01-07 07:41:51,436 Val Step[1300/1563], Loss: 0.7822 (1.1965), Acc@1: 0.9062 (0.7327), Acc@5: 0.9375 (0.9132)
2022-01-07 07:41:52,754 Val Step[1350/1563], Loss: 1.7217 (1.2130), Acc@1: 0.5312 (0.7287), Acc@5: 0.8750 (0.9109)
2022-01-07 07:41:54,012 Val Step[1400/1563], Loss: 1.0436 (1.2191), Acc@1: 0.7500 (0.7273), Acc@5: 0.9375 (0.9102)
2022-01-07 07:41:55,262 Val Step[1450/1563], Loss: 1.3414 (1.2253), Acc@1: 0.6875 (0.7255), Acc@5: 0.9375 (0.9097)
2022-01-07 07:41:56,532 Val Step[1500/1563], Loss: 1.5899 (1.2149), Acc@1: 0.6250 (0.7279), Acc@5: 0.8750 (0.9111)
2022-01-07 07:41:57,790 Val Step[1550/1563], Loss: 0.8485 (1.2166), Acc@1: 0.8750 (0.7270), Acc@5: 0.9062 (0.9110)
2022-01-07 07:41:58,527 ----- Epoch[272/310], Validation Loss: 1.2150, Validation Acc@1: 0.7272, Validation Acc@5: 0.9112, time: 101.58
2022-01-07 07:41:58,528 ----- Epoch[272/310], Train Loss: 3.5788, Train Acc: 0.3828, time: 1578.57, Best Val(epoch270) Acc@1: 0.7277
2022-01-07 07:41:58,718 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 07:41:58,718 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 07:41:58,806 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 07:41:58,806 Now training epoch 273. LR=0.000025
2022-01-07 07:43:13,366 Epoch[273/310], Step[0000/1251], Loss: 3.7330(3.7330), Acc: 0.4229(0.4229)
2022-01-07 07:44:13,517 Epoch[273/310], Step[0050/1251], Loss: 3.3417(3.5284), Acc: 0.3740(0.3928)
2022-01-07 07:45:12,496 Epoch[273/310], Step[0100/1251], Loss: 3.6302(3.5688), Acc: 0.4062(0.3964)
2022-01-07 07:46:11,355 Epoch[273/310], Step[0150/1251], Loss: 3.7413(3.5628), Acc: 0.2891(0.3913)
2022-01-07 07:47:10,187 Epoch[273/310], Step[0200/1251], Loss: 3.4825(3.5749), Acc: 0.5098(0.3897)
2022-01-07 07:48:08,519 Epoch[273/310], Step[0250/1251], Loss: 3.4626(3.5665), Acc: 0.3818(0.3916)
2022-01-07 07:49:07,554 Epoch[273/310], Step[0300/1251], Loss: 3.5524(3.5720), Acc: 0.5107(0.3897)
2022-01-07 07:50:06,702 Epoch[273/310], Step[0350/1251], Loss: 3.2789(3.5671), Acc: 0.5908(0.3911)
2022-01-07 07:51:06,460 Epoch[273/310], Step[0400/1251], Loss: 3.5415(3.5653), Acc: 0.0879(0.3884)
2022-01-07 07:52:06,699 Epoch[273/310], Step[0450/1251], Loss: 3.3818(3.5685), Acc: 0.3252(0.3885)
2022-01-07 07:53:06,434 Epoch[273/310], Step[0500/1251], Loss: 3.3469(3.5717), Acc: 0.4648(0.3866)
2022-01-07 07:54:05,294 Epoch[273/310], Step[0550/1251], Loss: 3.7947(3.5730), Acc: 0.4102(0.3859)
2022-01-07 07:55:02,045 Epoch[273/310], Step[0600/1251], Loss: 3.4370(3.5714), Acc: 0.3096(0.3869)
2022-01-07 07:55:59,692 Epoch[273/310], Step[0650/1251], Loss: 3.7739(3.5765), Acc: 0.3232(0.3857)
2022-01-07 07:56:56,476 Epoch[273/310], Step[0700/1251], Loss: 3.7129(3.5711), Acc: 0.2686(0.3862)
2022-01-07 07:57:54,438 Epoch[273/310], Step[0750/1251], Loss: 3.5421(3.5709), Acc: 0.4180(0.3876)
2022-01-07 07:58:51,508 Epoch[273/310], Step[0800/1251], Loss: 3.6962(3.5716), Acc: 0.2148(0.3861)
2022-01-07 07:59:51,550 Epoch[273/310], Step[0850/1251], Loss: 3.5142(3.5713), Acc: 0.3926(0.3854)
2022-01-07 08:00:51,533 Epoch[273/310], Step[0900/1251], Loss: 2.9235(3.5672), Acc: 0.5498(0.3846)
2022-01-07 08:01:50,655 Epoch[273/310], Step[0950/1251], Loss: 3.2792(3.5662), Acc: 0.3574(0.3840)
2022-01-07 08:02:50,372 Epoch[273/310], Step[1000/1251], Loss: 3.6795(3.5623), Acc: 0.4551(0.3836)
2022-01-07 08:03:49,597 Epoch[273/310], Step[1050/1251], Loss: 3.4614(3.5632), Acc: 0.2822(0.3830)
2022-01-07 08:04:49,525 Epoch[273/310], Step[1100/1251], Loss: 3.7873(3.5661), Acc: 0.3750(0.3819)
2022-01-07 08:05:49,671 Epoch[273/310], Step[1150/1251], Loss: 3.5872(3.5637), Acc: 0.5938(0.3829)
2022-01-07 08:06:48,537 Epoch[273/310], Step[1200/1251], Loss: 3.6217(3.5655), Acc: 0.3740(0.3829)
2022-01-07 08:07:47,123 Epoch[273/310], Step[1250/1251], Loss: 3.0628(3.5667), Acc: 0.6328(0.3838)
2022-01-07 08:07:48,621 ----- Epoch[273/310], Train Loss: 3.5667, Train Acc: 0.3838, time: 1549.81, Best Val(epoch270) Acc@1: 0.7277
2022-01-07 08:07:48,790 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 08:07:48,791 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 08:07:49,061 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 08:07:49,061 Now training epoch 274. LR=0.000023
2022-01-07 08:09:08,474 Epoch[274/310], Step[0000/1251], Loss: 3.2483(3.2483), Acc: 0.4043(0.4043)
2022-01-07 08:10:07,744 Epoch[274/310], Step[0050/1251], Loss: 3.6263(3.5486), Acc: 0.3721(0.3801)
2022-01-07 08:11:07,317 Epoch[274/310], Step[0100/1251], Loss: 3.7799(3.5753), Acc: 0.5039(0.3906)
2022-01-07 08:12:04,736 Epoch[274/310], Step[0150/1251], Loss: 3.5971(3.5710), Acc: 0.4170(0.3899)
2022-01-07 08:13:02,832 Epoch[274/310], Step[0200/1251], Loss: 3.5597(3.5676), Acc: 0.5029(0.3928)
2022-01-07 08:14:01,379 Epoch[274/310], Step[0250/1251], Loss: 3.1808(3.5778), Acc: 0.2949(0.3888)
2022-01-07 08:15:00,147 Epoch[274/310], Step[0300/1251], Loss: 3.7639(3.5677), Acc: 0.4336(0.3922)
2022-01-07 08:15:59,737 Epoch[274/310], Step[0350/1251], Loss: 3.1340(3.5691), Acc: 0.3535(0.3918)
2022-01-07 08:16:59,867 Epoch[274/310], Step[0400/1251], Loss: 3.3929(3.5686), Acc: 0.4395(0.3864)
2022-01-07 08:17:59,923 Epoch[274/310], Step[0450/1251], Loss: 3.7037(3.5638), Acc: 0.2617(0.3875)
2022-01-07 08:18:59,717 Epoch[274/310], Step[0500/1251], Loss: 3.6842(3.5649), Acc: 0.3105(0.3878)
2022-01-07 08:19:58,789 Epoch[274/310], Step[0550/1251], Loss: 2.9273(3.5648), Acc: 0.5225(0.3874)
2022-01-07 08:20:58,596 Epoch[274/310], Step[0600/1251], Loss: 3.4983(3.5714), Acc: 0.4443(0.3858)
2022-01-07 08:21:58,405 Epoch[274/310], Step[0650/1251], Loss: 3.1912(3.5729), Acc: 0.2881(0.3854)
2022-01-07 08:22:58,495 Epoch[274/310], Step[0700/1251], Loss: 3.2744(3.5696), Acc: 0.1748(0.3850)
2022-01-07 08:23:58,680 Epoch[274/310], Step[0750/1251], Loss: 3.1027(3.5708), Acc: 0.4951(0.3854)
2022-01-07 08:24:58,233 Epoch[274/310], Step[0800/1251], Loss: 3.3940(3.5744), Acc: 0.5215(0.3846)
2022-01-07 08:25:58,176 Epoch[274/310], Step[0850/1251], Loss: 3.5266(3.5722), Acc: 0.4805(0.3852)
2022-01-07 08:26:58,686 Epoch[274/310], Step[0900/1251], Loss: 3.7802(3.5710), Acc: 0.1924(0.3843)
2022-01-07 08:27:59,296 Epoch[274/310], Step[0950/1251], Loss: 3.4645(3.5738), Acc: 0.4131(0.3851)
2022-01-07 08:28:59,127 Epoch[274/310], Step[1000/1251], Loss: 3.4248(3.5764), Acc: 0.4287(0.3852)
2022-01-07 08:29:59,658 Epoch[274/310], Step[1050/1251], Loss: 3.0879(3.5753), Acc: 0.4395(0.3836)
2022-01-07 08:30:58,919 Epoch[274/310], Step[1100/1251], Loss: 3.2951(3.5756), Acc: 0.5762(0.3849)
2022-01-07 08:31:59,767 Epoch[274/310], Step[1150/1251], Loss: 3.3563(3.5731), Acc: 0.3350(0.3846)
2022-01-07 08:33:00,245 Epoch[274/310], Step[1200/1251], Loss: 3.6973(3.5743), Acc: 0.3027(0.3838)
2022-01-07 08:34:00,513 Epoch[274/310], Step[1250/1251], Loss: 3.5713(3.5756), Acc: 0.3701(0.3835)
2022-01-07 08:34:01,972 ----- Validation after Epoch: 274
2022-01-07 08:35:03,613 Val Step[0000/1563], Loss: 0.6361 (0.6361), Acc@1: 0.9688 (0.9688), Acc@5: 0.9688 (0.9688)
2022-01-07 08:35:05,079 Val Step[0050/1563], Loss: 2.1514 (0.7185), Acc@1: 0.4375 (0.8603), Acc@5: 0.9062 (0.9638)
2022-01-07 08:35:06,476 Val Step[0100/1563], Loss: 1.8594 (0.9824), Acc@1: 0.5312 (0.7850), Acc@5: 0.8125 (0.9390)
2022-01-07 08:35:07,877 Val Step[0150/1563], Loss: 0.3776 (0.9296), Acc@1: 0.9375 (0.7993), Acc@5: 1.0000 (0.9437)
2022-01-07 08:35:09,357 Val Step[0200/1563], Loss: 1.0300 (0.9394), Acc@1: 0.7812 (0.8008), Acc@5: 0.9375 (0.9420)
2022-01-07 08:35:10,802 Val Step[0250/1563], Loss: 0.4588 (0.8923), Acc@1: 0.9375 (0.8124), Acc@5: 1.0000 (0.9472)
2022-01-07 08:35:12,213 Val Step[0300/1563], Loss: 1.1168 (0.9473), Acc@1: 0.6562 (0.7964), Acc@5: 1.0000 (0.9426)
2022-01-07 08:35:13,633 Val Step[0350/1563], Loss: 0.9434 (0.9537), Acc@1: 0.7812 (0.7917), Acc@5: 0.9062 (0.9441)
2022-01-07 08:35:15,016 Val Step[0400/1563], Loss: 0.9047 (0.9614), Acc@1: 0.8438 (0.7859), Acc@5: 0.9688 (0.9443)
2022-01-07 08:35:16,378 Val Step[0450/1563], Loss: 0.9476 (0.9675), Acc@1: 0.7188 (0.7833), Acc@5: 1.0000 (0.9453)
2022-01-07 08:35:17,777 Val Step[0500/1563], Loss: 0.4705 (0.9587), Acc@1: 0.8750 (0.7854), Acc@5: 1.0000 (0.9465)
2022-01-07 08:35:19,173 Val Step[0550/1563], Loss: 0.7013 (0.9385), Acc@1: 0.8438 (0.7904), Acc@5: 0.9688 (0.9480)
2022-01-07 08:35:20,486 Val Step[0600/1563], Loss: 0.7023 (0.9463), Acc@1: 0.8438 (0.7893), Acc@5: 0.9688 (0.9475)
2022-01-07 08:35:21,835 Val Step[0650/1563], Loss: 0.5330 (0.9666), Acc@1: 0.8750 (0.7849), Acc@5: 1.0000 (0.9450)
2022-01-07 08:35:23,125 Val Step[0700/1563], Loss: 0.8756 (0.9928), Acc@1: 0.8125 (0.7783), Acc@5: 0.9688 (0.9423)
2022-01-07 08:35:24,564 Val Step[0750/1563], Loss: 1.3259 (1.0245), Acc@1: 0.7812 (0.7721), Acc@5: 0.9062 (0.9380)
2022-01-07 08:35:25,824 Val Step[0800/1563], Loss: 0.6228 (1.0600), Acc@1: 0.9062 (0.7628), Acc@5: 1.0000 (0.9336)
2022-01-07 08:35:27,080 Val Step[0850/1563], Loss: 1.1660 (1.0830), Acc@1: 0.6562 (0.7571), Acc@5: 0.9375 (0.9306)
2022-01-07 08:35:28,392 Val Step[0900/1563], Loss: 0.2600 (1.0817), Acc@1: 0.9688 (0.7585), Acc@5: 1.0000 (0.9304)
2022-01-07 08:35:29,891 Val Step[0950/1563], Loss: 1.3260 (1.1014), Acc@1: 0.7188 (0.7547), Acc@5: 0.9062 (0.9271)
2022-01-07 08:35:31,264 Val Step[1000/1563], Loss: 0.6488 (1.1251), Acc@1: 0.9062 (0.7488), Acc@5: 0.9688 (0.9238)
2022-01-07 08:35:32,566 Val Step[1050/1563], Loss: 0.3319 (1.1382), Acc@1: 0.9688 (0.7459), Acc@5: 1.0000 (0.9222)
2022-01-07 08:35:33,867 Val Step[1100/1563], Loss: 0.7486 (1.1512), Acc@1: 0.8750 (0.7433), Acc@5: 1.0000 (0.9204)
2022-01-07 08:35:35,176 Val Step[1150/1563], Loss: 1.2976 (1.1644), Acc@1: 0.7812 (0.7407), Acc@5: 0.8125 (0.9186)
2022-01-07 08:35:36,466 Val Step[1200/1563], Loss: 1.2292 (1.1779), Acc@1: 0.7500 (0.7374), Acc@5: 0.8438 (0.9165)
2022-01-07 08:35:37,756 Val Step[1250/1563], Loss: 0.7376 (1.1893), Acc@1: 0.8750 (0.7353), Acc@5: 0.9375 (0.9148)
2022-01-07 08:35:39,082 Val Step[1300/1563], Loss: 0.8352 (1.1975), Acc@1: 0.8438 (0.7337), Acc@5: 0.9375 (0.9136)
2022-01-07 08:35:40,379 Val Step[1350/1563], Loss: 1.8266 (1.2140), Acc@1: 0.5312 (0.7297), Acc@5: 0.8125 (0.9112)
2022-01-07 08:35:41,671 Val Step[1400/1563], Loss: 0.9446 (1.2207), Acc@1: 0.7500 (0.7281), Acc@5: 0.9688 (0.9105)
2022-01-07 08:35:42,949 Val Step[1450/1563], Loss: 1.4255 (1.2264), Acc@1: 0.7188 (0.7263), Acc@5: 0.9375 (0.9103)
2022-01-07 08:35:44,241 Val Step[1500/1563], Loss: 1.7215 (1.2164), Acc@1: 0.6250 (0.7286), Acc@5: 0.8438 (0.9116)
2022-01-07 08:35:45,522 Val Step[1550/1563], Loss: 0.9113 (1.2192), Acc@1: 0.8750 (0.7275), Acc@5: 0.9062 (0.9113)
2022-01-07 08:35:46,295 ----- Epoch[274/310], Validation Loss: 1.2178, Validation Acc@1: 0.7278, Validation Acc@5: 0.9115, time: 104.32
2022-01-07 08:35:46,296 ----- Epoch[274/310], Train Loss: 3.5756, Train Acc: 0.3835, time: 1572.91, Best Val(epoch274) Acc@1: 0.7278
2022-01-07 08:35:46,478 Max accuracy so far: 0.7278 at epoch_274
2022-01-07 08:35:46,479 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 08:35:46,479 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 08:35:46,582 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 08:35:46,965 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 08:35:46,966 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 08:35:47,093 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 08:35:47,093 Now training epoch 275. LR=0.000022
2022-01-07 08:37:03,053 Epoch[275/310], Step[0000/1251], Loss: 3.2712(3.2712), Acc: 0.5566(0.5566)
2022-01-07 08:38:03,395 Epoch[275/310], Step[0050/1251], Loss: 3.1364(3.6011), Acc: 0.6143(0.3545)
2022-01-07 08:39:02,333 Epoch[275/310], Step[0100/1251], Loss: 2.9463(3.5221), Acc: 0.4717(0.3749)
2022-01-07 08:40:02,051 Epoch[275/310], Step[0150/1251], Loss: 2.9212(3.5099), Acc: 0.4443(0.3874)
2022-01-07 08:41:03,118 Epoch[275/310], Step[0200/1251], Loss: 3.6106(3.5091), Acc: 0.4189(0.3844)
2022-01-07 08:42:01,443 Epoch[275/310], Step[0250/1251], Loss: 3.0782(3.5082), Acc: 0.4639(0.3877)
2022-01-07 08:43:00,599 Epoch[275/310], Step[0300/1251], Loss: 3.2588(3.5210), Acc: 0.5830(0.3859)
2022-01-07 08:43:59,949 Epoch[275/310], Step[0350/1251], Loss: 3.4020(3.5210), Acc: 0.3223(0.3843)
2022-01-07 08:44:58,421 Epoch[275/310], Step[0400/1251], Loss: 3.1548(3.5328), Acc: 0.4395(0.3873)
2022-01-07 08:45:56,345 Epoch[275/310], Step[0450/1251], Loss: 3.7030(3.5374), Acc: 0.2510(0.3892)
2022-01-07 08:46:55,407 Epoch[275/310], Step[0500/1251], Loss: 3.8167(3.5397), Acc: 0.2832(0.3871)
2022-01-07 08:47:54,610 Epoch[275/310], Step[0550/1251], Loss: 3.8328(3.5416), Acc: 0.4277(0.3853)
2022-01-07 08:48:54,860 Epoch[275/310], Step[0600/1251], Loss: 3.5301(3.5441), Acc: 0.4746(0.3864)
2022-01-07 08:49:55,192 Epoch[275/310], Step[0650/1251], Loss: 3.4733(3.5473), Acc: 0.5078(0.3860)
2022-01-07 08:50:53,108 Epoch[275/310], Step[0700/1251], Loss: 3.2507(3.5459), Acc: 0.3359(0.3871)
2022-01-07 08:51:52,849 Epoch[275/310], Step[0750/1251], Loss: 3.3829(3.5476), Acc: 0.4883(0.3859)
2022-01-07 08:52:53,220 Epoch[275/310], Step[0800/1251], Loss: 3.9796(3.5452), Acc: 0.3936(0.3858)
2022-01-07 08:53:52,710 Epoch[275/310], Step[0850/1251], Loss: 3.3932(3.5473), Acc: 0.5020(0.3862)
2022-01-07 08:54:52,513 Epoch[275/310], Step[0900/1251], Loss: 2.6924(3.5492), Acc: 0.5234(0.3849)
2022-01-07 08:55:50,926 Epoch[275/310], Step[0950/1251], Loss: 3.6902(3.5498), Acc: 0.5342(0.3842)
2022-01-07 08:56:48,998 Epoch[275/310], Step[1000/1251], Loss: 4.2202(3.5503), Acc: 0.3633(0.3851)
2022-01-07 08:57:49,029 Epoch[275/310], Step[1050/1251], Loss: 3.2842(3.5510), Acc: 0.2119(0.3854)
2022-01-07 08:58:49,652 Epoch[275/310], Step[1100/1251], Loss: 3.1359(3.5520), Acc: 0.4395(0.3846)
2022-01-07 08:59:48,558 Epoch[275/310], Step[1150/1251], Loss: 4.0923(3.5525), Acc: 0.3389(0.3856)
2022-01-07 09:00:47,112 Epoch[275/310], Step[1200/1251], Loss: 4.2034(3.5533), Acc: 0.3643(0.3852)
2022-01-07 09:01:47,149 Epoch[275/310], Step[1250/1251], Loss: 3.8562(3.5544), Acc: 0.4736(0.3856)
2022-01-07 09:01:48,613 ----- Epoch[275/310], Train Loss: 3.5544, Train Acc: 0.3856, time: 1561.52, Best Val(epoch274) Acc@1: 0.7278
2022-01-07 09:01:48,790 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 09:01:48,790 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 09:01:48,898 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 09:01:48,899 Now training epoch 276. LR=0.000021
2022-01-07 09:03:06,454 Epoch[276/310], Step[0000/1251], Loss: 3.7233(3.7233), Acc: 0.4072(0.4072)
2022-01-07 09:04:06,768 Epoch[276/310], Step[0050/1251], Loss: 3.4424(3.5619), Acc: 0.3105(0.3498)
2022-01-07 09:05:06,820 Epoch[276/310], Step[0100/1251], Loss: 3.6816(3.5898), Acc: 0.4854(0.3632)
2022-01-07 09:06:05,876 Epoch[276/310], Step[0150/1251], Loss: 3.7933(3.5947), Acc: 0.1943(0.3738)
2022-01-07 09:07:03,675 Epoch[276/310], Step[0200/1251], Loss: 3.6642(3.5773), Acc: 0.4639(0.3745)
2022-01-07 09:08:03,537 Epoch[276/310], Step[0250/1251], Loss: 3.2817(3.5701), Acc: 0.5156(0.3791)
2022-01-07 09:09:02,347 Epoch[276/310], Step[0300/1251], Loss: 2.8750(3.5630), Acc: 0.3281(0.3884)
2022-01-07 09:10:02,751 Epoch[276/310], Step[0350/1251], Loss: 3.3812(3.5750), Acc: 0.3750(0.3813)
2022-01-07 09:11:01,673 Epoch[276/310], Step[0400/1251], Loss: 3.6827(3.5650), Acc: 0.2900(0.3843)
2022-01-07 09:12:01,033 Epoch[276/310], Step[0450/1251], Loss: 3.2267(3.5726), Acc: 0.2520(0.3840)
2022-01-07 09:13:00,450 Epoch[276/310], Step[0500/1251], Loss: 3.3414(3.5783), Acc: 0.4941(0.3819)
2022-01-07 09:14:00,069 Epoch[276/310], Step[0550/1251], Loss: 3.3167(3.5776), Acc: 0.2031(0.3826)
2022-01-07 09:15:00,979 Epoch[276/310], Step[0600/1251], Loss: 3.3265(3.5764), Acc: 0.4385(0.3809)
2022-01-07 09:16:01,366 Epoch[276/310], Step[0650/1251], Loss: 3.2527(3.5731), Acc: 0.5215(0.3814)
2022-01-07 09:17:02,633 Epoch[276/310], Step[0700/1251], Loss: 3.6092(3.5713), Acc: 0.3984(0.3807)
2022-01-07 09:18:03,471 Epoch[276/310], Step[0750/1251], Loss: 3.3569(3.5696), Acc: 0.5098(0.3795)
2022-01-07 09:19:04,248 Epoch[276/310], Step[0800/1251], Loss: 2.9467(3.5733), Acc: 0.5898(0.3786)
2022-01-07 09:20:03,566 Epoch[276/310], Step[0850/1251], Loss: 3.7067(3.5731), Acc: 0.5439(0.3812)
2022-01-07 09:21:03,777 Epoch[276/310], Step[0900/1251], Loss: 3.8665(3.5732), Acc: 0.2607(0.3820)
2022-01-07 09:22:03,511 Epoch[276/310], Step[0950/1251], Loss: 3.8438(3.5731), Acc: 0.3350(0.3813)
2022-01-07 09:23:03,345 Epoch[276/310], Step[1000/1251], Loss: 3.4626(3.5743), Acc: 0.2861(0.3810)
2022-01-07 09:24:03,016 Epoch[276/310], Step[1050/1251], Loss: 3.5318(3.5713), Acc: 0.4424(0.3812)
2022-01-07 09:25:01,550 Epoch[276/310], Step[1100/1251], Loss: 3.5217(3.5721), Acc: 0.4668(0.3818)
2022-01-07 09:26:00,483 Epoch[276/310], Step[1150/1251], Loss: 3.6405(3.5731), Acc: 0.2520(0.3808)
2022-01-07 09:26:59,184 Epoch[276/310], Step[1200/1251], Loss: 3.3387(3.5728), Acc: 0.1133(0.3815)
2022-01-07 09:27:58,898 Epoch[276/310], Step[1250/1251], Loss: 3.6766(3.5752), Acc: 0.4922(0.3821)
2022-01-07 09:28:00,390 ----- Validation after Epoch: 276
2022-01-07 09:29:06,393 Val Step[0000/1563], Loss: 0.6356 (0.6356), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 09:29:07,707 Val Step[0050/1563], Loss: 2.2548 (0.6999), Acc@1: 0.4062 (0.8603), Acc@5: 0.8750 (0.9626)
2022-01-07 09:29:08,970 Val Step[0100/1563], Loss: 1.8384 (0.9594), Acc@1: 0.5312 (0.7884), Acc@5: 0.8438 (0.9394)
2022-01-07 09:29:10,258 Val Step[0150/1563], Loss: 0.3647 (0.9104), Acc@1: 0.9375 (0.8003), Acc@5: 1.0000 (0.9445)
2022-01-07 09:29:11,534 Val Step[0200/1563], Loss: 0.9079 (0.9200), Acc@1: 0.7812 (0.8027), Acc@5: 0.9375 (0.9426)
2022-01-07 09:29:12,816 Val Step[0250/1563], Loss: 0.5779 (0.8718), Acc@1: 0.9062 (0.8137), Acc@5: 1.0000 (0.9481)
2022-01-07 09:29:14,191 Val Step[0300/1563], Loss: 0.9548 (0.9300), Acc@1: 0.7188 (0.7981), Acc@5: 1.0000 (0.9435)
2022-01-07 09:29:15,593 Val Step[0350/1563], Loss: 0.9355 (0.9374), Acc@1: 0.8125 (0.7933), Acc@5: 0.9062 (0.9449)
2022-01-07 09:29:16,956 Val Step[0400/1563], Loss: 0.8544 (0.9454), Acc@1: 0.8125 (0.7874), Acc@5: 0.9688 (0.9450)
2022-01-07 09:29:18,235 Val Step[0450/1563], Loss: 0.8341 (0.9514), Acc@1: 0.7812 (0.7846), Acc@5: 1.0000 (0.9458)
2022-01-07 09:29:19,510 Val Step[0500/1563], Loss: 0.4698 (0.9433), Acc@1: 0.8438 (0.7872), Acc@5: 1.0000 (0.9469)
2022-01-07 09:29:21,020 Val Step[0550/1563], Loss: 0.6775 (0.9221), Acc@1: 0.8750 (0.7922), Acc@5: 0.9688 (0.9485)
2022-01-07 09:29:22,425 Val Step[0600/1563], Loss: 0.8766 (0.9306), Acc@1: 0.7812 (0.7908), Acc@5: 0.9062 (0.9478)
2022-01-07 09:29:23,722 Val Step[0650/1563], Loss: 0.5610 (0.9499), Acc@1: 0.8750 (0.7868), Acc@5: 1.0000 (0.9452)
2022-01-07 09:29:24,990 Val Step[0700/1563], Loss: 0.8443 (0.9775), Acc@1: 0.8438 (0.7798), Acc@5: 0.9688 (0.9422)
2022-01-07 09:29:26,311 Val Step[0750/1563], Loss: 1.1363 (1.0098), Acc@1: 0.8125 (0.7734), Acc@5: 0.8750 (0.9380)
2022-01-07 09:29:27,606 Val Step[0800/1563], Loss: 0.6985 (1.0449), Acc@1: 0.8125 (0.7645), Acc@5: 1.0000 (0.9337)
2022-01-07 09:29:28,900 Val Step[0850/1563], Loss: 1.2712 (1.0684), Acc@1: 0.6875 (0.7584), Acc@5: 0.9688 (0.9310)
2022-01-07 09:29:30,261 Val Step[0900/1563], Loss: 0.2445 (1.0676), Acc@1: 0.9688 (0.7599), Acc@5: 1.0000 (0.9306)
2022-01-07 09:29:31,633 Val Step[0950/1563], Loss: 1.2845 (1.0864), Acc@1: 0.7500 (0.7562), Acc@5: 0.9062 (0.9275)
2022-01-07 09:29:32,992 Val Step[1000/1563], Loss: 0.5596 (1.1095), Acc@1: 0.9688 (0.7502), Acc@5: 1.0000 (0.9245)
2022-01-07 09:29:34,291 Val Step[1050/1563], Loss: 0.3145 (1.1237), Acc@1: 0.9688 (0.7467), Acc@5: 1.0000 (0.9230)
2022-01-07 09:29:35,582 Val Step[1100/1563], Loss: 0.7111 (1.1363), Acc@1: 0.8438 (0.7440), Acc@5: 0.9688 (0.9214)
2022-01-07 09:29:36,869 Val Step[1150/1563], Loss: 1.2550 (1.1491), Acc@1: 0.7812 (0.7414), Acc@5: 0.8125 (0.9194)
2022-01-07 09:29:38,179 Val Step[1200/1563], Loss: 1.1676 (1.1638), Acc@1: 0.7812 (0.7377), Acc@5: 0.8438 (0.9171)
2022-01-07 09:29:39,469 Val Step[1250/1563], Loss: 0.7606 (1.1748), Acc@1: 0.8750 (0.7357), Acc@5: 0.9375 (0.9153)
2022-01-07 09:29:40,755 Val Step[1300/1563], Loss: 0.7613 (1.1825), Acc@1: 0.9062 (0.7343), Acc@5: 0.9375 (0.9142)
2022-01-07 09:29:42,071 Val Step[1350/1563], Loss: 1.8837 (1.1985), Acc@1: 0.5000 (0.7306), Acc@5: 0.8438 (0.9120)
2022-01-07 09:29:43,450 Val Step[1400/1563], Loss: 1.0011 (1.2050), Acc@1: 0.7500 (0.7293), Acc@5: 0.9375 (0.9112)
2022-01-07 09:29:44,714 Val Step[1450/1563], Loss: 1.3015 (1.2108), Acc@1: 0.7188 (0.7273), Acc@5: 0.9375 (0.9110)
2022-01-07 09:29:46,014 Val Step[1500/1563], Loss: 1.6437 (1.2003), Acc@1: 0.6562 (0.7296), Acc@5: 0.8438 (0.9122)
2022-01-07 09:29:47,322 Val Step[1550/1563], Loss: 0.9022 (1.2029), Acc@1: 0.8750 (0.7287), Acc@5: 0.9062 (0.9119)
2022-01-07 09:29:48,085 ----- Epoch[276/310], Validation Loss: 1.2014, Validation Acc@1: 0.7291, Validation Acc@5: 0.9120, time: 107.69
2022-01-07 09:29:48,086 ----- Epoch[276/310], Train Loss: 3.5752, Train Acc: 0.3821, time: 1571.49, Best Val(epoch276) Acc@1: 0.7291
2022-01-07 09:29:48,316 Max accuracy so far: 0.7291 at epoch_276
2022-01-07 09:29:48,317 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 09:29:48,317 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 09:29:48,364 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 09:29:48,765 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 09:29:48,765 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 09:29:49,339 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 09:29:49,340 Now training epoch 277. LR=0.000019
2022-01-07 09:31:02,673 Epoch[277/310], Step[0000/1251], Loss: 3.1247(3.1247), Acc: 0.5410(0.5410)
2022-01-07 09:32:02,988 Epoch[277/310], Step[0050/1251], Loss: 3.7073(3.4693), Acc: 0.4824(0.4117)
2022-01-07 09:33:02,453 Epoch[277/310], Step[0100/1251], Loss: 3.5064(3.4942), Acc: 0.3789(0.3960)
2022-01-07 09:34:00,751 Epoch[277/310], Step[0150/1251], Loss: 4.1922(3.5063), Acc: 0.3555(0.4019)
2022-01-07 09:35:00,921 Epoch[277/310], Step[0200/1251], Loss: 3.5268(3.5274), Acc: 0.3643(0.3895)
2022-01-07 09:36:01,054 Epoch[277/310], Step[0250/1251], Loss: 2.7588(3.5249), Acc: 0.2695(0.3824)
2022-01-07 09:37:00,486 Epoch[277/310], Step[0300/1251], Loss: 3.8805(3.5349), Acc: 0.3643(0.3861)
2022-01-07 09:37:58,361 Epoch[277/310], Step[0350/1251], Loss: 3.2712(3.5410), Acc: 0.6025(0.3887)
2022-01-07 09:38:56,530 Epoch[277/310], Step[0400/1251], Loss: 3.6701(3.5337), Acc: 0.4209(0.3935)
2022-01-07 09:39:54,876 Epoch[277/310], Step[0450/1251], Loss: 3.3182(3.5384), Acc: 0.6104(0.3963)
2022-01-07 09:40:52,504 Epoch[277/310], Step[0500/1251], Loss: 3.8552(3.5336), Acc: 0.3350(0.3966)
2022-01-07 09:41:51,615 Epoch[277/310], Step[0550/1251], Loss: 3.3865(3.5367), Acc: 0.2754(0.3961)
2022-01-07 09:42:49,891 Epoch[277/310], Step[0600/1251], Loss: 3.7206(3.5388), Acc: 0.4609(0.3928)
2022-01-07 09:43:49,121 Epoch[277/310], Step[0650/1251], Loss: 3.9976(3.5401), Acc: 0.3770(0.3925)
2022-01-07 09:44:49,413 Epoch[277/310], Step[0700/1251], Loss: 3.3645(3.5405), Acc: 0.2021(0.3926)
2022-01-07 09:45:48,095 Epoch[277/310], Step[0750/1251], Loss: 3.6454(3.5338), Acc: 0.0400(0.3941)
2022-01-07 09:46:47,450 Epoch[277/310], Step[0800/1251], Loss: 3.4920(3.5336), Acc: 0.5312(0.3937)
2022-01-07 09:47:47,162 Epoch[277/310], Step[0850/1251], Loss: 3.7239(3.5329), Acc: 0.4688(0.3930)
2022-01-07 09:48:46,537 Epoch[277/310], Step[0900/1251], Loss: 3.7867(3.5378), Acc: 0.4395(0.3908)
2022-01-07 09:49:47,203 Epoch[277/310], Step[0950/1251], Loss: 2.9977(3.5378), Acc: 0.3916(0.3897)
2022-01-07 09:50:47,264 Epoch[277/310], Step[1000/1251], Loss: 3.0597(3.5369), Acc: 0.4766(0.3896)
2022-01-07 09:51:47,594 Epoch[277/310], Step[1050/1251], Loss: 3.9503(3.5347), Acc: 0.4658(0.3908)
2022-01-07 09:52:47,375 Epoch[277/310], Step[1100/1251], Loss: 3.6261(3.5399), Acc: 0.4482(0.3905)
2022-01-07 09:53:47,830 Epoch[277/310], Step[1150/1251], Loss: 3.6136(3.5423), Acc: 0.3730(0.3895)
2022-01-07 09:54:47,719 Epoch[277/310], Step[1200/1251], Loss: 3.5846(3.5423), Acc: 0.3887(0.3905)
2022-01-07 09:55:48,126 Epoch[277/310], Step[1250/1251], Loss: 3.4788(3.5437), Acc: 0.3818(0.3891)
2022-01-07 09:55:49,701 ----- Epoch[277/310], Train Loss: 3.5437, Train Acc: 0.3891, time: 1560.36, Best Val(epoch276) Acc@1: 0.7291
2022-01-07 09:55:49,927 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 09:55:49,927 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 09:55:49,979 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 09:55:49,979 Now training epoch 278. LR=0.000018
2022-01-07 09:57:06,318 Epoch[278/310], Step[0000/1251], Loss: 3.5379(3.5379), Acc: 0.2441(0.2441)
2022-01-07 09:58:05,280 Epoch[278/310], Step[0050/1251], Loss: 3.5682(3.6160), Acc: 0.3438(0.3715)
2022-01-07 09:59:04,391 Epoch[278/310], Step[0100/1251], Loss: 3.7476(3.5645), Acc: 0.5205(0.3989)
2022-01-07 10:00:03,199 Epoch[278/310], Step[0150/1251], Loss: 4.1675(3.5726), Acc: 0.3740(0.3866)
2022-01-07 10:01:03,245 Epoch[278/310], Step[0200/1251], Loss: 3.6958(3.5735), Acc: 0.5391(0.3847)
2022-01-07 10:02:03,051 Epoch[278/310], Step[0250/1251], Loss: 4.0290(3.5747), Acc: 0.3535(0.3892)
2022-01-07 10:03:02,994 Epoch[278/310], Step[0300/1251], Loss: 3.7353(3.5740), Acc: 0.3867(0.3905)
2022-01-07 10:04:04,163 Epoch[278/310], Step[0350/1251], Loss: 4.0522(3.5795), Acc: 0.2549(0.3865)
2022-01-07 10:05:03,745 Epoch[278/310], Step[0400/1251], Loss: 3.3319(3.5826), Acc: 0.5225(0.3881)
2022-01-07 10:06:00,873 Epoch[278/310], Step[0450/1251], Loss: 4.2126(3.5852), Acc: 0.3477(0.3899)
2022-01-07 10:06:59,677 Epoch[278/310], Step[0500/1251], Loss: 3.4174(3.5856), Acc: 0.5596(0.3885)
2022-01-07 10:07:56,898 Epoch[278/310], Step[0550/1251], Loss: 3.7802(3.5769), Acc: 0.4404(0.3903)
2022-01-07 10:08:56,438 Epoch[278/310], Step[0600/1251], Loss: 3.3124(3.5726), Acc: 0.2227(0.3909)
2022-01-07 10:09:55,092 Epoch[278/310], Step[0650/1251], Loss: 3.7612(3.5708), Acc: 0.1377(0.3913)
2022-01-07 10:10:53,928 Epoch[278/310], Step[0700/1251], Loss: 3.3811(3.5652), Acc: 0.5645(0.3927)
2022-01-07 10:11:53,998 Epoch[278/310], Step[0750/1251], Loss: 3.3876(3.5609), Acc: 0.4893(0.3921)
2022-01-07 10:12:53,304 Epoch[278/310], Step[0800/1251], Loss: 3.9855(3.5563), Acc: 0.2754(0.3923)
2022-01-07 10:13:53,806 Epoch[278/310], Step[0850/1251], Loss: 4.2807(3.5580), Acc: 0.2363(0.3911)
2022-01-07 10:14:53,718 Epoch[278/310], Step[0900/1251], Loss: 3.7385(3.5593), Acc: 0.4326(0.3902)
2022-01-07 10:15:54,172 Epoch[278/310], Step[0950/1251], Loss: 3.3702(3.5577), Acc: 0.5859(0.3901)
2022-01-07 10:16:53,918 Epoch[278/310], Step[1000/1251], Loss: 3.9492(3.5618), Acc: 0.4023(0.3895)
2022-01-07 10:17:52,661 Epoch[278/310], Step[1050/1251], Loss: 3.3064(3.5608), Acc: 0.2725(0.3880)
2022-01-07 10:18:52,927 Epoch[278/310], Step[1100/1251], Loss: 3.8381(3.5645), Acc: 0.5430(0.3877)
2022-01-07 10:19:52,313 Epoch[278/310], Step[1150/1251], Loss: 3.8110(3.5624), Acc: 0.3506(0.3869)
2022-01-07 10:20:52,281 Epoch[278/310], Step[1200/1251], Loss: 3.5149(3.5612), Acc: 0.5547(0.3862)
2022-01-07 10:21:52,072 Epoch[278/310], Step[1250/1251], Loss: 4.0346(3.5592), Acc: 0.3965(0.3857)
2022-01-07 10:21:53,564 ----- Validation after Epoch: 278
2022-01-07 10:22:57,915 Val Step[0000/1563], Loss: 0.6352 (0.6352), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 10:22:59,289 Val Step[0050/1563], Loss: 2.1585 (0.7074), Acc@1: 0.4062 (0.8554), Acc@5: 0.9062 (0.9645)
2022-01-07 10:23:00,595 Val Step[0100/1563], Loss: 1.8346 (0.9570), Acc@1: 0.5312 (0.7862), Acc@5: 0.8438 (0.9431)
2022-01-07 10:23:01,869 Val Step[0150/1563], Loss: 0.3915 (0.9118), Acc@1: 0.9062 (0.7990), Acc@5: 1.0000 (0.9466)
2022-01-07 10:23:03,136 Val Step[0200/1563], Loss: 1.0268 (0.9229), Acc@1: 0.7812 (0.8007), Acc@5: 0.9062 (0.9442)
2022-01-07 10:23:04,450 Val Step[0250/1563], Loss: 0.5616 (0.8776), Acc@1: 0.9062 (0.8114), Acc@5: 1.0000 (0.9487)
2022-01-07 10:23:05,857 Val Step[0300/1563], Loss: 1.0128 (0.9329), Acc@1: 0.6875 (0.7953), Acc@5: 1.0000 (0.9446)
2022-01-07 10:23:07,265 Val Step[0350/1563], Loss: 0.9083 (0.9406), Acc@1: 0.7812 (0.7907), Acc@5: 0.9062 (0.9456)
2022-01-07 10:23:08,701 Val Step[0400/1563], Loss: 0.7933 (0.9459), Acc@1: 0.8438 (0.7862), Acc@5: 0.9688 (0.9454)
2022-01-07 10:23:10,132 Val Step[0450/1563], Loss: 0.8842 (0.9532), Acc@1: 0.7500 (0.7837), Acc@5: 1.0000 (0.9458)
2022-01-07 10:23:11,590 Val Step[0500/1563], Loss: 0.4347 (0.9452), Acc@1: 0.9062 (0.7864), Acc@5: 1.0000 (0.9469)
2022-01-07 10:23:12,979 Val Step[0550/1563], Loss: 0.7152 (0.9243), Acc@1: 0.8438 (0.7915), Acc@5: 0.9688 (0.9487)
2022-01-07 10:23:14,252 Val Step[0600/1563], Loss: 0.7978 (0.9312), Acc@1: 0.8125 (0.7905), Acc@5: 0.9375 (0.9480)
2022-01-07 10:23:15,519 Val Step[0650/1563], Loss: 0.5607 (0.9499), Acc@1: 0.9062 (0.7865), Acc@5: 1.0000 (0.9455)
2022-01-07 10:23:16,984 Val Step[0700/1563], Loss: 0.9017 (0.9758), Acc@1: 0.8438 (0.7798), Acc@5: 0.9688 (0.9428)
2022-01-07 10:23:18,415 Val Step[0750/1563], Loss: 1.1410 (1.0072), Acc@1: 0.8125 (0.7733), Acc@5: 0.9062 (0.9388)
2022-01-07 10:23:19,832 Val Step[0800/1563], Loss: 0.6742 (1.0430), Acc@1: 0.8438 (0.7644), Acc@5: 1.0000 (0.9346)
2022-01-07 10:23:21,286 Val Step[0850/1563], Loss: 1.1668 (1.0673), Acc@1: 0.6250 (0.7583), Acc@5: 0.9688 (0.9318)
2022-01-07 10:23:22,704 Val Step[0900/1563], Loss: 0.2355 (1.0671), Acc@1: 0.9688 (0.7597), Acc@5: 1.0000 (0.9310)
2022-01-07 10:23:24,171 Val Step[0950/1563], Loss: 1.2965 (1.0870), Acc@1: 0.7188 (0.7558), Acc@5: 0.8750 (0.9279)
2022-01-07 10:23:25,644 Val Step[1000/1563], Loss: 0.5120 (1.1106), Acc@1: 0.9688 (0.7496), Acc@5: 1.0000 (0.9246)
2022-01-07 10:23:27,027 Val Step[1050/1563], Loss: 0.3367 (1.1240), Acc@1: 0.9688 (0.7464), Acc@5: 1.0000 (0.9233)
2022-01-07 10:23:28,308 Val Step[1100/1563], Loss: 0.6459 (1.1373), Acc@1: 0.9062 (0.7439), Acc@5: 0.9688 (0.9213)
2022-01-07 10:23:29,673 Val Step[1150/1563], Loss: 1.0832 (1.1514), Acc@1: 0.7812 (0.7409), Acc@5: 0.8438 (0.9194)
2022-01-07 10:23:30,997 Val Step[1200/1563], Loss: 1.1559 (1.1643), Acc@1: 0.7812 (0.7379), Acc@5: 0.8438 (0.9173)
2022-01-07 10:23:32,283 Val Step[1250/1563], Loss: 0.7478 (1.1758), Acc@1: 0.8750 (0.7357), Acc@5: 0.9375 (0.9156)
2022-01-07 10:23:33,576 Val Step[1300/1563], Loss: 0.7915 (1.1839), Acc@1: 0.9062 (0.7341), Acc@5: 0.9375 (0.9145)
2022-01-07 10:23:34,849 Val Step[1350/1563], Loss: 1.9028 (1.1997), Acc@1: 0.5312 (0.7303), Acc@5: 0.8125 (0.9121)
2022-01-07 10:23:36,172 Val Step[1400/1563], Loss: 0.9044 (1.2056), Acc@1: 0.7500 (0.7291), Acc@5: 0.9688 (0.9114)
2022-01-07 10:23:37,486 Val Step[1450/1563], Loss: 1.3797 (1.2117), Acc@1: 0.6875 (0.7273), Acc@5: 0.9375 (0.9111)
2022-01-07 10:23:38,773 Val Step[1500/1563], Loss: 1.5926 (1.2014), Acc@1: 0.6562 (0.7298), Acc@5: 0.8750 (0.9123)
2022-01-07 10:23:40,062 Val Step[1550/1563], Loss: 0.8812 (1.2039), Acc@1: 0.8750 (0.7289), Acc@5: 0.9062 (0.9120)
2022-01-07 10:23:40,850 ----- Epoch[278/310], Validation Loss: 1.2023, Validation Acc@1: 0.7292, Validation Acc@5: 0.9122, time: 107.28
2022-01-07 10:23:40,850 ----- Epoch[278/310], Train Loss: 3.5592, Train Acc: 0.3857, time: 1563.58, Best Val(epoch278) Acc@1: 0.7292
2022-01-07 10:23:41,036 Max accuracy so far: 0.7292 at epoch_278
2022-01-07 10:23:41,036 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 10:23:41,036 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 10:23:41,143 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 10:23:41,565 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 10:23:41,566 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 10:23:41,652 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 10:23:41,653 Now training epoch 279. LR=0.000017
2022-01-07 10:25:02,609 Epoch[279/310], Step[0000/1251], Loss: 3.2049(3.2049), Acc: 0.5732(0.5732)
2022-01-07 10:26:01,121 Epoch[279/310], Step[0050/1251], Loss: 3.6123(3.4864), Acc: 0.5137(0.3802)
2022-01-07 10:27:00,945 Epoch[279/310], Step[0100/1251], Loss: 3.8106(3.5003), Acc: 0.3887(0.3974)
2022-01-07 10:27:59,836 Epoch[279/310], Step[0150/1251], Loss: 4.0771(3.4941), Acc: 0.3652(0.3886)
2022-01-07 10:28:58,901 Epoch[279/310], Step[0200/1251], Loss: 3.4197(3.4996), Acc: 0.2061(0.3791)
2022-01-07 10:29:58,580 Epoch[279/310], Step[0250/1251], Loss: 3.5503(3.5123), Acc: 0.3594(0.3756)
2022-01-07 10:30:58,047 Epoch[279/310], Step[0300/1251], Loss: 3.1803(3.5231), Acc: 0.5654(0.3796)
2022-01-07 10:31:58,168 Epoch[279/310], Step[0350/1251], Loss: 3.4975(3.5289), Acc: 0.3369(0.3837)
2022-01-07 10:32:56,755 Epoch[279/310], Step[0400/1251], Loss: 3.4562(3.5353), Acc: 0.2334(0.3825)
2022-01-07 10:33:56,331 Epoch[279/310], Step[0450/1251], Loss: 3.5796(3.5364), Acc: 0.3760(0.3783)
2022-01-07 10:34:56,637 Epoch[279/310], Step[0500/1251], Loss: 3.8242(3.5401), Acc: 0.2979(0.3768)
2022-01-07 10:35:56,146 Epoch[279/310], Step[0550/1251], Loss: 4.0845(3.5491), Acc: 0.3799(0.3746)
2022-01-07 10:36:55,862 Epoch[279/310], Step[0600/1251], Loss: 3.4674(3.5480), Acc: 0.5215(0.3754)
2022-01-07 10:37:55,205 Epoch[279/310], Step[0650/1251], Loss: 3.5597(3.5461), Acc: 0.3750(0.3751)
2022-01-07 10:38:54,709 Epoch[279/310], Step[0700/1251], Loss: 3.5464(3.5459), Acc: 0.2139(0.3765)
2022-01-07 10:39:54,777 Epoch[279/310], Step[0750/1251], Loss: 3.1674(3.5488), Acc: 0.1885(0.3782)
2022-01-07 10:40:54,033 Epoch[279/310], Step[0800/1251], Loss: 3.4168(3.5456), Acc: 0.3154(0.3795)
2022-01-07 10:41:54,329 Epoch[279/310], Step[0850/1251], Loss: 3.2581(3.5447), Acc: 0.5459(0.3815)
2022-01-07 10:42:55,301 Epoch[279/310], Step[0900/1251], Loss: 3.7921(3.5466), Acc: 0.4707(0.3813)
2022-01-07 10:43:55,856 Epoch[279/310], Step[0950/1251], Loss: 3.5144(3.5443), Acc: 0.2656(0.3826)
2022-01-07 10:44:55,592 Epoch[279/310], Step[1000/1251], Loss: 3.6999(3.5453), Acc: 0.4805(0.3828)
2022-01-07 10:45:55,384 Epoch[279/310], Step[1050/1251], Loss: 3.5771(3.5450), Acc: 0.4482(0.3830)
2022-01-07 10:46:54,148 Epoch[279/310], Step[1100/1251], Loss: 2.9475(3.5448), Acc: 0.6348(0.3832)
2022-01-07 10:47:53,604 Epoch[279/310], Step[1150/1251], Loss: 3.4920(3.5443), Acc: 0.4824(0.3829)
2022-01-07 10:48:53,623 Epoch[279/310], Step[1200/1251], Loss: 3.6441(3.5440), Acc: 0.4189(0.3817)
2022-01-07 10:49:52,288 Epoch[279/310], Step[1250/1251], Loss: 3.5163(3.5413), Acc: 0.4746(0.3819)
2022-01-07 10:49:53,953 ----- Epoch[279/310], Train Loss: 3.5413, Train Acc: 0.3819, time: 1572.30, Best Val(epoch278) Acc@1: 0.7292
2022-01-07 10:49:54,125 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 10:49:54,126 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 10:49:54,231 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 10:49:54,231 Now training epoch 280. LR=0.000016
2022-01-07 10:51:20,424 Epoch[280/310], Step[0000/1251], Loss: 3.4108(3.4108), Acc: 0.2734(0.2734)
2022-01-07 10:52:20,012 Epoch[280/310], Step[0050/1251], Loss: 3.0971(3.5480), Acc: 0.5791(0.4053)
2022-01-07 10:53:19,928 Epoch[280/310], Step[0100/1251], Loss: 3.9595(3.5875), Acc: 0.1924(0.3915)
2022-01-07 10:54:18,100 Epoch[280/310], Step[0150/1251], Loss: 3.7563(3.6031), Acc: 0.3467(0.3884)
2022-01-07 10:55:18,818 Epoch[280/310], Step[0200/1251], Loss: 3.3440(3.5936), Acc: 0.4951(0.3826)
2022-01-07 10:56:18,767 Epoch[280/310], Step[0250/1251], Loss: 4.0395(3.5968), Acc: 0.3486(0.3858)
2022-01-07 10:57:17,936 Epoch[280/310], Step[0300/1251], Loss: 3.3795(3.5924), Acc: 0.3145(0.3881)
2022-01-07 10:58:15,524 Epoch[280/310], Step[0350/1251], Loss: 3.7340(3.5883), Acc: 0.4844(0.3857)
2022-01-07 10:59:14,747 Epoch[280/310], Step[0400/1251], Loss: 4.0678(3.5758), Acc: 0.2939(0.3853)
2022-01-07 11:00:14,470 Epoch[280/310], Step[0450/1251], Loss: 3.7617(3.5655), Acc: 0.4980(0.3876)
2022-01-07 11:01:13,103 Epoch[280/310], Step[0500/1251], Loss: 3.1662(3.5668), Acc: 0.3652(0.3885)
2022-01-07 11:02:13,445 Epoch[280/310], Step[0550/1251], Loss: 3.8650(3.5601), Acc: 0.1182(0.3885)
2022-01-07 11:03:12,919 Epoch[280/310], Step[0600/1251], Loss: 3.5258(3.5627), Acc: 0.0449(0.3876)
2022-01-07 11:04:13,929 Epoch[280/310], Step[0650/1251], Loss: 3.2806(3.5650), Acc: 0.4424(0.3854)
2022-01-07 11:05:13,554 Epoch[280/310], Step[0700/1251], Loss: 3.6146(3.5687), Acc: 0.4951(0.3855)
2022-01-07 11:06:14,079 Epoch[280/310], Step[0750/1251], Loss: 3.1966(3.5672), Acc: 0.5850(0.3862)
2022-01-07 11:07:12,624 Epoch[280/310], Step[0800/1251], Loss: 3.7321(3.5713), Acc: 0.2920(0.3857)
2022-01-07 11:08:12,320 Epoch[280/310], Step[0850/1251], Loss: 3.3235(3.5715), Acc: 0.5947(0.3868)
2022-01-07 11:09:12,005 Epoch[280/310], Step[0900/1251], Loss: 3.4266(3.5700), Acc: 0.5400(0.3878)
2022-01-07 11:10:11,351 Epoch[280/310], Step[0950/1251], Loss: 3.9783(3.5677), Acc: 0.4268(0.3894)
2022-01-07 11:11:12,629 Epoch[280/310], Step[1000/1251], Loss: 3.5275(3.5660), Acc: 0.3818(0.3887)
2022-01-07 11:12:14,061 Epoch[280/310], Step[1050/1251], Loss: 3.2877(3.5672), Acc: 0.5771(0.3885)
2022-01-07 11:13:14,490 Epoch[280/310], Step[1100/1251], Loss: 3.4212(3.5689), Acc: 0.5430(0.3877)
2022-01-07 11:14:14,739 Epoch[280/310], Step[1150/1251], Loss: 3.7855(3.5687), Acc: 0.2715(0.3873)
2022-01-07 11:15:14,883 Epoch[280/310], Step[1200/1251], Loss: 3.7328(3.5713), Acc: 0.4600(0.3876)
2022-01-07 11:16:14,932 Epoch[280/310], Step[1250/1251], Loss: 3.3551(3.5701), Acc: 0.2627(0.3891)
2022-01-07 11:16:16,586 ----- Validation after Epoch: 280
2022-01-07 11:17:21,566 Val Step[0000/1563], Loss: 0.6330 (0.6330), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 11:17:22,945 Val Step[0050/1563], Loss: 2.1776 (0.7210), Acc@1: 0.4062 (0.8609), Acc@5: 0.8750 (0.9608)
2022-01-07 11:17:24,274 Val Step[0100/1563], Loss: 1.9670 (0.9810), Acc@1: 0.5000 (0.7856), Acc@5: 0.8125 (0.9375)
2022-01-07 11:17:25,684 Val Step[0150/1563], Loss: 0.4043 (0.9287), Acc@1: 0.9375 (0.7999), Acc@5: 1.0000 (0.9412)
2022-01-07 11:17:27,082 Val Step[0200/1563], Loss: 0.9437 (0.9352), Acc@1: 0.8125 (0.8024), Acc@5: 0.9375 (0.9403)
2022-01-07 11:17:28,459 Val Step[0250/1563], Loss: 0.5355 (0.8853), Acc@1: 0.9062 (0.8137), Acc@5: 1.0000 (0.9460)
2022-01-07 11:17:29,887 Val Step[0300/1563], Loss: 0.9455 (0.9439), Acc@1: 0.7188 (0.7977), Acc@5: 1.0000 (0.9420)
2022-01-07 11:17:31,351 Val Step[0350/1563], Loss: 0.9114 (0.9513), Acc@1: 0.7812 (0.7925), Acc@5: 0.9062 (0.9435)
2022-01-07 11:17:32,814 Val Step[0400/1563], Loss: 0.8492 (0.9583), Acc@1: 0.8750 (0.7872), Acc@5: 0.9688 (0.9440)
2022-01-07 11:17:34,250 Val Step[0450/1563], Loss: 0.9673 (0.9659), Acc@1: 0.7500 (0.7844), Acc@5: 1.0000 (0.9448)
2022-01-07 11:17:35,671 Val Step[0500/1563], Loss: 0.4851 (0.9570), Acc@1: 0.8750 (0.7867), Acc@5: 1.0000 (0.9458)
2022-01-07 11:17:37,159 Val Step[0550/1563], Loss: 0.7756 (0.9369), Acc@1: 0.7812 (0.7915), Acc@5: 0.9688 (0.9477)
2022-01-07 11:17:38,589 Val Step[0600/1563], Loss: 0.7549 (0.9450), Acc@1: 0.8438 (0.7906), Acc@5: 0.9375 (0.9470)
2022-01-07 11:17:40,016 Val Step[0650/1563], Loss: 0.4416 (0.9636), Acc@1: 0.9688 (0.7866), Acc@5: 1.0000 (0.9445)
2022-01-07 11:17:41,422 Val Step[0700/1563], Loss: 0.9638 (0.9893), Acc@1: 0.8125 (0.7802), Acc@5: 0.9688 (0.9418)
2022-01-07 11:17:42,853 Val Step[0750/1563], Loss: 1.1999 (1.0215), Acc@1: 0.8125 (0.7735), Acc@5: 0.9062 (0.9374)
2022-01-07 11:17:44,298 Val Step[0800/1563], Loss: 0.7125 (1.0574), Acc@1: 0.7812 (0.7640), Acc@5: 1.0000 (0.9331)
2022-01-07 11:17:45,700 Val Step[0850/1563], Loss: 1.2337 (1.0818), Acc@1: 0.6250 (0.7577), Acc@5: 0.9375 (0.9300)
2022-01-07 11:17:47,094 Val Step[0900/1563], Loss: 0.2795 (1.0808), Acc@1: 0.9688 (0.7589), Acc@5: 1.0000 (0.9297)
2022-01-07 11:17:48,633 Val Step[0950/1563], Loss: 1.2236 (1.1004), Acc@1: 0.7812 (0.7553), Acc@5: 0.9062 (0.9265)
2022-01-07 11:17:50,053 Val Step[1000/1563], Loss: 0.5609 (1.1233), Acc@1: 0.9688 (0.7490), Acc@5: 0.9688 (0.9235)
2022-01-07 11:17:51,551 Val Step[1050/1563], Loss: 0.3618 (1.1367), Acc@1: 0.9688 (0.7457), Acc@5: 1.0000 (0.9222)
2022-01-07 11:17:52,963 Val Step[1100/1563], Loss: 0.7027 (1.1492), Acc@1: 0.9062 (0.7434), Acc@5: 0.9688 (0.9203)
2022-01-07 11:17:54,343 Val Step[1150/1563], Loss: 1.3056 (1.1626), Acc@1: 0.7812 (0.7406), Acc@5: 0.8125 (0.9184)
2022-01-07 11:17:55,757 Val Step[1200/1563], Loss: 1.2415 (1.1760), Acc@1: 0.7812 (0.7374), Acc@5: 0.8438 (0.9161)
2022-01-07 11:17:57,150 Val Step[1250/1563], Loss: 0.7359 (1.1867), Acc@1: 0.8750 (0.7352), Acc@5: 0.9375 (0.9145)
2022-01-07 11:17:58,544 Val Step[1300/1563], Loss: 0.8374 (1.1949), Acc@1: 0.9062 (0.7337), Acc@5: 0.9062 (0.9135)
2022-01-07 11:17:59,940 Val Step[1350/1563], Loss: 1.7440 (1.2108), Acc@1: 0.5312 (0.7299), Acc@5: 0.8750 (0.9112)
2022-01-07 11:18:01,362 Val Step[1400/1563], Loss: 1.0000 (1.2173), Acc@1: 0.7500 (0.7283), Acc@5: 0.9688 (0.9104)
2022-01-07 11:18:02,761 Val Step[1450/1563], Loss: 1.3921 (1.2230), Acc@1: 0.6875 (0.7266), Acc@5: 0.9375 (0.9101)
2022-01-07 11:18:04,143 Val Step[1500/1563], Loss: 1.5388 (1.2122), Acc@1: 0.6875 (0.7291), Acc@5: 0.8750 (0.9116)
2022-01-07 11:18:05,415 Val Step[1550/1563], Loss: 0.8931 (1.2144), Acc@1: 0.8750 (0.7284), Acc@5: 0.9062 (0.9113)
2022-01-07 11:18:06,152 ----- Epoch[280/310], Validation Loss: 1.2130, Validation Acc@1: 0.7286, Validation Acc@5: 0.9115, time: 109.56
2022-01-07 11:18:06,152 ----- Epoch[280/310], Train Loss: 3.5701, Train Acc: 0.3891, time: 1582.35, Best Val(epoch278) Acc@1: 0.7292
2022-01-07 11:18:06,313 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-280-Loss-3.579136450894826.pdparams
2022-01-07 11:18:06,314 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-280-Loss-3.579136450894826.pdopt
2022-01-07 11:18:06,354 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-280-Loss-3.579136450894826-EMA.pdparams
2022-01-07 11:18:06,354 Now training epoch 281. LR=0.000015
2022-01-07 11:19:28,731 Epoch[281/310], Step[0000/1251], Loss: 3.3719(3.3719), Acc: 0.0469(0.0469)
2022-01-07 11:20:27,885 Epoch[281/310], Step[0050/1251], Loss: 3.5874(3.5103), Acc: 0.4678(0.3923)
2022-01-07 11:21:27,583 Epoch[281/310], Step[0100/1251], Loss: 3.8871(3.5019), Acc: 0.1807(0.3914)
2022-01-07 11:22:26,520 Epoch[281/310], Step[0150/1251], Loss: 3.6496(3.5406), Acc: 0.5410(0.3945)
2022-01-07 11:23:27,668 Epoch[281/310], Step[0200/1251], Loss: 3.5623(3.5533), Acc: 0.4502(0.3840)
2022-01-07 11:24:27,633 Epoch[281/310], Step[0250/1251], Loss: 4.0188(3.5578), Acc: 0.2842(0.3811)
2022-01-07 11:25:27,880 Epoch[281/310], Step[0300/1251], Loss: 3.9322(3.5461), Acc: 0.2510(0.3861)
2022-01-07 11:26:27,983 Epoch[281/310], Step[0350/1251], Loss: 3.2865(3.5539), Acc: 0.4238(0.3839)
2022-01-07 11:27:28,206 Epoch[281/310], Step[0400/1251], Loss: 3.3707(3.5572), Acc: 0.3701(0.3794)
2022-01-07 11:28:28,284 Epoch[281/310], Step[0450/1251], Loss: 3.4063(3.5527), Acc: 0.3037(0.3816)
2022-01-07 11:29:27,235 Epoch[281/310], Step[0500/1251], Loss: 3.6142(3.5536), Acc: 0.5068(0.3823)
2022-01-07 11:30:25,701 Epoch[281/310], Step[0550/1251], Loss: 4.0660(3.5564), Acc: 0.3623(0.3843)
2022-01-07 11:31:24,603 Epoch[281/310], Step[0600/1251], Loss: 3.0279(3.5504), Acc: 0.4707(0.3819)
2022-01-07 11:32:25,135 Epoch[281/310], Step[0650/1251], Loss: 3.4229(3.5567), Acc: 0.2451(0.3831)
2022-01-07 11:33:26,593 Epoch[281/310], Step[0700/1251], Loss: 3.7701(3.5594), Acc: 0.1494(0.3819)
2022-01-07 11:34:26,934 Epoch[281/310], Step[0750/1251], Loss: 2.7522(3.5612), Acc: 0.6191(0.3826)
2022-01-07 11:35:27,156 Epoch[281/310], Step[0800/1251], Loss: 3.2878(3.5596), Acc: 0.5137(0.3832)
2022-01-07 11:36:27,745 Epoch[281/310], Step[0850/1251], Loss: 3.1433(3.5593), Acc: 0.4531(0.3840)
2022-01-07 11:37:25,420 Epoch[281/310], Step[0900/1251], Loss: 3.6760(3.5624), Acc: 0.1748(0.3840)
2022-01-07 11:38:23,647 Epoch[281/310], Step[0950/1251], Loss: 4.0000(3.5640), Acc: 0.3105(0.3855)
2022-01-07 11:39:22,759 Epoch[281/310], Step[1000/1251], Loss: 3.3661(3.5621), Acc: 0.5342(0.3861)
2022-01-07 11:40:21,981 Epoch[281/310], Step[1050/1251], Loss: 2.7679(3.5615), Acc: 0.4805(0.3875)
2022-01-07 11:41:20,191 Epoch[281/310], Step[1100/1251], Loss: 3.5094(3.5614), Acc: 0.5234(0.3871)
2022-01-07 11:42:19,574 Epoch[281/310], Step[1150/1251], Loss: 3.8428(3.5617), Acc: 0.4014(0.3861)
2022-01-07 11:43:19,641 Epoch[281/310], Step[1200/1251], Loss: 3.4458(3.5624), Acc: 0.5117(0.3851)
2022-01-07 11:44:19,827 Epoch[281/310], Step[1250/1251], Loss: 3.8450(3.5601), Acc: 0.1475(0.3863)
2022-01-07 11:44:21,298 ----- Epoch[281/310], Train Loss: 3.5601, Train Acc: 0.3863, time: 1574.94, Best Val(epoch278) Acc@1: 0.7292
2022-01-07 11:44:21,563 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 11:44:21,564 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 11:44:21,659 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 11:44:21,660 Now training epoch 282. LR=0.000014
2022-01-07 11:45:39,770 Epoch[282/310], Step[0000/1251], Loss: 3.6970(3.6970), Acc: 0.2178(0.2178)
2022-01-07 11:46:38,391 Epoch[282/310], Step[0050/1251], Loss: 3.3639(3.5280), Acc: 0.4902(0.3707)
2022-01-07 11:47:36,405 Epoch[282/310], Step[0100/1251], Loss: 3.2562(3.5295), Acc: 0.3125(0.3835)
2022-01-07 11:48:35,909 Epoch[282/310], Step[0150/1251], Loss: 3.9723(3.5483), Acc: 0.2715(0.3856)
2022-01-07 11:49:36,381 Epoch[282/310], Step[0200/1251], Loss: 3.2901(3.5407), Acc: 0.4111(0.3872)
2022-01-07 11:50:35,574 Epoch[282/310], Step[0250/1251], Loss: 3.4090(3.5423), Acc: 0.4131(0.3868)
2022-01-07 11:51:34,976 Epoch[282/310], Step[0300/1251], Loss: 3.9165(3.5476), Acc: 0.3818(0.3875)
2022-01-07 11:52:35,346 Epoch[282/310], Step[0350/1251], Loss: 3.3465(3.5578), Acc: 0.3252(0.3892)
2022-01-07 11:53:35,868 Epoch[282/310], Step[0400/1251], Loss: 3.4221(3.5557), Acc: 0.4199(0.3882)
2022-01-07 11:54:36,850 Epoch[282/310], Step[0450/1251], Loss: 3.4562(3.5545), Acc: 0.3838(0.3861)
2022-01-07 11:55:37,093 Epoch[282/310], Step[0500/1251], Loss: 3.5322(3.5604), Acc: 0.4678(0.3879)
2022-01-07 11:56:37,753 Epoch[282/310], Step[0550/1251], Loss: 2.7166(3.5593), Acc: 0.1670(0.3871)
2022-01-07 11:57:37,721 Epoch[282/310], Step[0600/1251], Loss: 3.5990(3.5581), Acc: 0.4365(0.3887)
2022-01-07 11:58:38,095 Epoch[282/310], Step[0650/1251], Loss: 3.7218(3.5650), Acc: 0.5352(0.3878)
2022-01-07 11:59:38,462 Epoch[282/310], Step[0700/1251], Loss: 3.0404(3.5629), Acc: 0.2959(0.3863)
2022-01-07 12:00:37,651 Epoch[282/310], Step[0750/1251], Loss: 4.1662(3.5609), Acc: 0.3672(0.3864)
2022-01-07 12:01:37,554 Epoch[282/310], Step[0800/1251], Loss: 3.4021(3.5608), Acc: 0.2451(0.3875)
2022-01-07 12:02:37,411 Epoch[282/310], Step[0850/1251], Loss: 3.8362(3.5625), Acc: 0.3281(0.3889)
2022-01-07 12:03:37,644 Epoch[282/310], Step[0900/1251], Loss: 4.1803(3.5659), Acc: 0.4131(0.3884)
2022-01-07 12:04:37,467 Epoch[282/310], Step[0950/1251], Loss: 3.5274(3.5648), Acc: 0.5537(0.3888)
2022-01-07 12:05:36,776 Epoch[282/310], Step[1000/1251], Loss: 3.9989(3.5658), Acc: 0.4053(0.3877)
2022-01-07 12:06:35,929 Epoch[282/310], Step[1050/1251], Loss: 2.7813(3.5612), Acc: 0.4844(0.3895)
2022-01-07 12:07:36,177 Epoch[282/310], Step[1100/1251], Loss: 2.8634(3.5571), Acc: 0.5146(0.3904)
2022-01-07 12:08:36,884 Epoch[282/310], Step[1150/1251], Loss: 3.3456(3.5567), Acc: 0.2041(0.3906)
2022-01-07 12:09:35,629 Epoch[282/310], Step[1200/1251], Loss: 3.8685(3.5602), Acc: 0.4746(0.3909)
2022-01-07 12:10:36,043 Epoch[282/310], Step[1250/1251], Loss: 3.2974(3.5607), Acc: 0.5986(0.3904)
2022-01-07 12:10:37,531 ----- Validation after Epoch: 282
2022-01-07 12:11:41,428 Val Step[0000/1563], Loss: 0.6437 (0.6437), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 12:11:42,819 Val Step[0050/1563], Loss: 2.1709 (0.7165), Acc@1: 0.4375 (0.8597), Acc@5: 0.9062 (0.9626)
2022-01-07 12:11:44,092 Val Step[0100/1563], Loss: 1.8273 (0.9775), Acc@1: 0.5312 (0.7893), Acc@5: 0.8438 (0.9378)
2022-01-07 12:11:45,435 Val Step[0150/1563], Loss: 0.4002 (0.9267), Acc@1: 0.9375 (0.7995), Acc@5: 1.0000 (0.9427)
2022-01-07 12:11:46,715 Val Step[0200/1563], Loss: 0.9871 (0.9347), Acc@1: 0.8125 (0.8019), Acc@5: 0.9375 (0.9415)
2022-01-07 12:11:48,020 Val Step[0250/1563], Loss: 0.5088 (0.8866), Acc@1: 0.9062 (0.8132), Acc@5: 1.0000 (0.9468)
2022-01-07 12:11:49,451 Val Step[0300/1563], Loss: 0.9873 (0.9423), Acc@1: 0.7500 (0.7975), Acc@5: 0.9688 (0.9432)
2022-01-07 12:11:50,717 Val Step[0350/1563], Loss: 0.8928 (0.9521), Acc@1: 0.8125 (0.7922), Acc@5: 0.9062 (0.9441)
2022-01-07 12:11:51,994 Val Step[0400/1563], Loss: 0.8285 (0.9593), Acc@1: 0.8750 (0.7870), Acc@5: 0.9688 (0.9444)
2022-01-07 12:11:53,267 Val Step[0450/1563], Loss: 0.8716 (0.9652), Acc@1: 0.7812 (0.7850), Acc@5: 1.0000 (0.9453)
2022-01-07 12:11:54,610 Val Step[0500/1563], Loss: 0.4451 (0.9566), Acc@1: 0.9062 (0.7874), Acc@5: 1.0000 (0.9468)
2022-01-07 12:11:56,030 Val Step[0550/1563], Loss: 0.7457 (0.9365), Acc@1: 0.8125 (0.7920), Acc@5: 0.9688 (0.9487)
2022-01-07 12:11:57,385 Val Step[0600/1563], Loss: 0.6884 (0.9442), Acc@1: 0.8750 (0.7909), Acc@5: 0.9375 (0.9481)
2022-01-07 12:11:58,698 Val Step[0650/1563], Loss: 0.5021 (0.9637), Acc@1: 0.9062 (0.7866), Acc@5: 1.0000 (0.9456)
2022-01-07 12:11:59,976 Val Step[0700/1563], Loss: 0.9567 (0.9901), Acc@1: 0.8125 (0.7803), Acc@5: 0.9688 (0.9424)
2022-01-07 12:12:01,250 Val Step[0750/1563], Loss: 1.2545 (1.0208), Acc@1: 0.8438 (0.7741), Acc@5: 0.9062 (0.9382)
2022-01-07 12:12:02,679 Val Step[0800/1563], Loss: 0.5862 (1.0564), Acc@1: 0.8750 (0.7651), Acc@5: 1.0000 (0.9338)
2022-01-07 12:12:04,097 Val Step[0850/1563], Loss: 1.1808 (1.0793), Acc@1: 0.6562 (0.7591), Acc@5: 0.9375 (0.9311)
2022-01-07 12:12:05,533 Val Step[0900/1563], Loss: 0.3049 (1.0790), Acc@1: 0.9688 (0.7604), Acc@5: 1.0000 (0.9307)
2022-01-07 12:12:06,997 Val Step[0950/1563], Loss: 1.2240 (1.0992), Acc@1: 0.7812 (0.7565), Acc@5: 0.9062 (0.9274)
2022-01-07 12:12:08,395 Val Step[1000/1563], Loss: 0.6026 (1.1221), Acc@1: 0.9375 (0.7508), Acc@5: 0.9688 (0.9242)
2022-01-07 12:12:09,793 Val Step[1050/1563], Loss: 0.3185 (1.1358), Acc@1: 0.9688 (0.7473), Acc@5: 1.0000 (0.9225)
2022-01-07 12:12:11,077 Val Step[1100/1563], Loss: 0.6747 (1.1488), Acc@1: 0.9062 (0.7447), Acc@5: 1.0000 (0.9209)
2022-01-07 12:12:12,468 Val Step[1150/1563], Loss: 1.2438 (1.1625), Acc@1: 0.7812 (0.7418), Acc@5: 0.8438 (0.9190)
2022-01-07 12:12:13,842 Val Step[1200/1563], Loss: 1.1694 (1.1757), Acc@1: 0.7812 (0.7385), Acc@5: 0.8438 (0.9170)
2022-01-07 12:12:15,096 Val Step[1250/1563], Loss: 0.7456 (1.1867), Acc@1: 0.8750 (0.7367), Acc@5: 0.9375 (0.9152)
2022-01-07 12:12:16,351 Val Step[1300/1563], Loss: 0.8358 (1.1954), Acc@1: 0.9062 (0.7348), Acc@5: 0.9375 (0.9141)
2022-01-07 12:12:17,594 Val Step[1350/1563], Loss: 1.6868 (1.2115), Acc@1: 0.5312 (0.7310), Acc@5: 0.8750 (0.9117)
2022-01-07 12:12:18,863 Val Step[1400/1563], Loss: 0.9527 (1.2179), Acc@1: 0.7500 (0.7295), Acc@5: 0.9688 (0.9109)
2022-01-07 12:12:20,187 Val Step[1450/1563], Loss: 1.4626 (1.2236), Acc@1: 0.6875 (0.7277), Acc@5: 0.9062 (0.9107)
2022-01-07 12:12:21,450 Val Step[1500/1563], Loss: 1.5454 (1.2137), Acc@1: 0.6562 (0.7300), Acc@5: 0.8750 (0.9120)
2022-01-07 12:12:22,713 Val Step[1550/1563], Loss: 0.8933 (1.2162), Acc@1: 0.8750 (0.7291), Acc@5: 0.9062 (0.9116)
2022-01-07 12:12:23,420 ----- Epoch[282/310], Validation Loss: 1.2148, Validation Acc@1: 0.7294, Validation Acc@5: 0.9118, time: 105.89
2022-01-07 12:12:23,421 ----- Epoch[282/310], Train Loss: 3.5607, Train Acc: 0.3904, time: 1575.87, Best Val(epoch282) Acc@1: 0.7294
2022-01-07 12:12:23,602 Max accuracy so far: 0.7294 at epoch_282
2022-01-07 12:12:23,602 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 12:12:23,602 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 12:12:23,801 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 12:12:24,113 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 12:12:24,113 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 12:12:24,222 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 12:12:24,223 Now training epoch 283. LR=0.000013
2022-01-07 12:13:47,041 Epoch[283/310], Step[0000/1251], Loss: 3.1385(3.1385), Acc: 0.4014(0.4014)
2022-01-07 12:14:46,888 Epoch[283/310], Step[0050/1251], Loss: 3.2150(3.4344), Acc: 0.2715(0.4051)
2022-01-07 12:15:46,322 Epoch[283/310], Step[0100/1251], Loss: 3.2372(3.4605), Acc: 0.2354(0.3945)
2022-01-07 12:16:46,130 Epoch[283/310], Step[0150/1251], Loss: 3.4318(3.4687), Acc: 0.1426(0.3970)
2022-01-07 12:17:44,976 Epoch[283/310], Step[0200/1251], Loss: 3.1397(3.4864), Acc: 0.2725(0.3965)
2022-01-07 12:18:44,451 Epoch[283/310], Step[0250/1251], Loss: 3.7885(3.4914), Acc: 0.4658(0.3928)
2022-01-07 12:19:44,075 Epoch[283/310], Step[0300/1251], Loss: 3.3222(3.5024), Acc: 0.4307(0.3927)
2022-01-07 12:20:43,997 Epoch[283/310], Step[0350/1251], Loss: 3.8494(3.5068), Acc: 0.3984(0.3938)
2022-01-07 12:21:42,582 Epoch[283/310], Step[0400/1251], Loss: 3.3184(3.5136), Acc: 0.4883(0.3921)
2022-01-07 12:22:42,817 Epoch[283/310], Step[0450/1251], Loss: 3.8174(3.5231), Acc: 0.2168(0.3919)
2022-01-07 12:23:42,768 Epoch[283/310], Step[0500/1251], Loss: 4.0759(3.5244), Acc: 0.2490(0.3953)
2022-01-07 12:24:42,959 Epoch[283/310], Step[0550/1251], Loss: 4.1106(3.5311), Acc: 0.3047(0.3925)
2022-01-07 12:25:42,339 Epoch[283/310], Step[0600/1251], Loss: 3.3135(3.5283), Acc: 0.4150(0.3936)
2022-01-07 12:26:40,903 Epoch[283/310], Step[0650/1251], Loss: 3.4017(3.5292), Acc: 0.5342(0.3978)
2022-01-07 12:27:39,834 Epoch[283/310], Step[0700/1251], Loss: 3.8330(3.5308), Acc: 0.4150(0.3973)
2022-01-07 12:28:39,993 Epoch[283/310], Step[0750/1251], Loss: 3.5510(3.5342), Acc: 0.2725(0.3962)
2022-01-07 12:29:39,681 Epoch[283/310], Step[0800/1251], Loss: 3.9628(3.5369), Acc: 0.3838(0.3955)
2022-01-07 12:30:39,593 Epoch[283/310], Step[0850/1251], Loss: 3.4541(3.5378), Acc: 0.2793(0.3948)
2022-01-07 12:31:40,190 Epoch[283/310], Step[0900/1251], Loss: 3.5829(3.5348), Acc: 0.4658(0.3944)
2022-01-07 12:32:40,741 Epoch[283/310], Step[0950/1251], Loss: 4.0117(3.5381), Acc: 0.2910(0.3945)
2022-01-07 12:33:41,999 Epoch[283/310], Step[1000/1251], Loss: 3.3078(3.5421), Acc: 0.2637(0.3937)
2022-01-07 12:34:42,342 Epoch[283/310], Step[1050/1251], Loss: 3.1500(3.5399), Acc: 0.5381(0.3948)
2022-01-07 12:35:43,341 Epoch[283/310], Step[1100/1251], Loss: 3.6058(3.5395), Acc: 0.4717(0.3942)
2022-01-07 12:36:44,430 Epoch[283/310], Step[1150/1251], Loss: 4.0031(3.5369), Acc: 0.2617(0.3938)
2022-01-07 12:37:44,226 Epoch[283/310], Step[1200/1251], Loss: 3.1879(3.5359), Acc: 0.3730(0.3926)
2022-01-07 12:38:44,381 Epoch[283/310], Step[1250/1251], Loss: 3.5728(3.5346), Acc: 0.4834(0.3920)
2022-01-07 12:38:45,873 ----- Epoch[283/310], Train Loss: 3.5346, Train Acc: 0.3920, time: 1581.65, Best Val(epoch282) Acc@1: 0.7294
2022-01-07 12:38:46,047 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 12:38:46,047 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 12:38:46,152 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 12:38:46,153 Now training epoch 284. LR=0.000012
2022-01-07 12:40:09,325 Epoch[284/310], Step[0000/1251], Loss: 3.7289(3.7289), Acc: 0.3125(0.3125)
2022-01-07 12:41:09,275 Epoch[284/310], Step[0050/1251], Loss: 3.4171(3.4720), Acc: 0.5586(0.4077)
2022-01-07 12:42:09,001 Epoch[284/310], Step[0100/1251], Loss: 3.9958(3.5239), Acc: 0.3125(0.3935)
2022-01-07 12:43:08,228 Epoch[284/310], Step[0150/1251], Loss: 3.5857(3.5626), Acc: 0.5410(0.3871)
2022-01-07 12:44:07,538 Epoch[284/310], Step[0200/1251], Loss: 3.2967(3.5705), Acc: 0.4053(0.3860)
2022-01-07 12:45:07,266 Epoch[284/310], Step[0250/1251], Loss: 3.7586(3.5572), Acc: 0.3662(0.3887)
2022-01-07 12:46:07,180 Epoch[284/310], Step[0300/1251], Loss: 3.7027(3.5608), Acc: 0.4502(0.3902)
2022-01-07 12:47:06,119 Epoch[284/310], Step[0350/1251], Loss: 3.3922(3.5640), Acc: 0.2559(0.3869)
2022-01-07 12:48:04,880 Epoch[284/310], Step[0400/1251], Loss: 3.6110(3.5558), Acc: 0.5664(0.3875)
2022-01-07 12:49:05,774 Epoch[284/310], Step[0450/1251], Loss: 3.1850(3.5562), Acc: 0.4131(0.3878)
2022-01-07 12:50:04,733 Epoch[284/310], Step[0500/1251], Loss: 3.3582(3.5526), Acc: 0.4775(0.3872)
2022-01-07 12:51:04,636 Epoch[284/310], Step[0550/1251], Loss: 3.5777(3.5546), Acc: 0.4473(0.3874)
2022-01-07 12:52:02,951 Epoch[284/310], Step[0600/1251], Loss: 3.8104(3.5557), Acc: 0.3535(0.3874)
2022-01-07 12:53:02,413 Epoch[284/310], Step[0650/1251], Loss: 3.3672(3.5554), Acc: 0.4922(0.3867)
2022-01-07 12:54:00,628 Epoch[284/310], Step[0700/1251], Loss: 3.0856(3.5519), Acc: 0.4902(0.3877)
2022-01-07 12:55:00,483 Epoch[284/310], Step[0750/1251], Loss: 3.4415(3.5542), Acc: 0.5674(0.3889)
2022-01-07 12:56:00,598 Epoch[284/310], Step[0800/1251], Loss: 3.2528(3.5556), Acc: 0.5068(0.3888)
2022-01-07 12:56:59,221 Epoch[284/310], Step[0850/1251], Loss: 3.9948(3.5547), Acc: 0.2305(0.3890)
2022-01-07 12:57:57,832 Epoch[284/310], Step[0900/1251], Loss: 3.2658(3.5603), Acc: 0.4023(0.3886)
2022-01-07 12:58:57,747 Epoch[284/310], Step[0950/1251], Loss: 3.5441(3.5598), Acc: 0.3369(0.3871)
2022-01-07 12:59:57,375 Epoch[284/310], Step[1000/1251], Loss: 3.9846(3.5605), Acc: 0.4531(0.3866)
2022-01-07 13:00:58,040 Epoch[284/310], Step[1050/1251], Loss: 3.9503(3.5637), Acc: 0.3477(0.3869)
2022-01-07 13:01:57,630 Epoch[284/310], Step[1100/1251], Loss: 3.0246(3.5605), Acc: 0.4561(0.3865)
2022-01-07 13:02:57,808 Epoch[284/310], Step[1150/1251], Loss: 3.3941(3.5576), Acc: 0.3701(0.3885)
2022-01-07 13:03:58,448 Epoch[284/310], Step[1200/1251], Loss: 3.3510(3.5548), Acc: 0.1436(0.3886)
2022-01-07 13:04:57,316 Epoch[284/310], Step[1250/1251], Loss: 3.2529(3.5545), Acc: 0.4189(0.3880)
2022-01-07 13:04:58,793 ----- Validation after Epoch: 284
2022-01-07 13:06:04,131 Val Step[0000/1563], Loss: 0.6409 (0.6409), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 13:06:05,430 Val Step[0050/1563], Loss: 2.2689 (0.7099), Acc@1: 0.3750 (0.8560), Acc@5: 0.8750 (0.9608)
2022-01-07 13:06:06,749 Val Step[0100/1563], Loss: 1.8721 (0.9607), Acc@1: 0.5000 (0.7884), Acc@5: 0.8438 (0.9403)
2022-01-07 13:06:08,119 Val Step[0150/1563], Loss: 0.3734 (0.9097), Acc@1: 0.9688 (0.8007), Acc@5: 1.0000 (0.9445)
2022-01-07 13:06:09,442 Val Step[0200/1563], Loss: 0.9515 (0.9170), Acc@1: 0.8125 (0.8032), Acc@5: 0.9375 (0.9434)
2022-01-07 13:06:10,715 Val Step[0250/1563], Loss: 0.4588 (0.8702), Acc@1: 0.9062 (0.8139), Acc@5: 1.0000 (0.9483)
2022-01-07 13:06:11,994 Val Step[0300/1563], Loss: 1.0255 (0.9263), Acc@1: 0.7500 (0.7978), Acc@5: 1.0000 (0.9444)
2022-01-07 13:06:13,269 Val Step[0350/1563], Loss: 0.9196 (0.9326), Acc@1: 0.8438 (0.7942), Acc@5: 0.9062 (0.9457)
2022-01-07 13:06:14,615 Val Step[0400/1563], Loss: 0.8468 (0.9411), Acc@1: 0.8125 (0.7882), Acc@5: 0.9688 (0.9455)
2022-01-07 13:06:15,878 Val Step[0450/1563], Loss: 0.8709 (0.9466), Acc@1: 0.7500 (0.7859), Acc@5: 1.0000 (0.9466)
2022-01-07 13:06:17,301 Val Step[0500/1563], Loss: 0.4529 (0.9384), Acc@1: 0.8750 (0.7884), Acc@5: 1.0000 (0.9477)
2022-01-07 13:06:18,678 Val Step[0550/1563], Loss: 0.7493 (0.9185), Acc@1: 0.8438 (0.7929), Acc@5: 0.9688 (0.9494)
2022-01-07 13:06:20,075 Val Step[0600/1563], Loss: 0.7247 (0.9264), Acc@1: 0.8438 (0.7915), Acc@5: 0.9375 (0.9487)
2022-01-07 13:06:21,357 Val Step[0650/1563], Loss: 0.5156 (0.9459), Acc@1: 0.9062 (0.7874), Acc@5: 1.0000 (0.9460)
2022-01-07 13:06:22,723 Val Step[0700/1563], Loss: 1.0548 (0.9726), Acc@1: 0.8125 (0.7809), Acc@5: 0.9688 (0.9428)
2022-01-07 13:06:24,114 Val Step[0750/1563], Loss: 1.1146 (1.0038), Acc@1: 0.8125 (0.7741), Acc@5: 0.9062 (0.9386)
2022-01-07 13:06:25,419 Val Step[0800/1563], Loss: 0.6318 (1.0395), Acc@1: 0.8750 (0.7654), Acc@5: 1.0000 (0.9342)
2022-01-07 13:06:26,710 Val Step[0850/1563], Loss: 1.1598 (1.0627), Acc@1: 0.6562 (0.7592), Acc@5: 0.9375 (0.9316)
2022-01-07 13:06:28,048 Val Step[0900/1563], Loss: 0.2450 (1.0625), Acc@1: 0.9688 (0.7603), Acc@5: 1.0000 (0.9313)
2022-01-07 13:06:29,533 Val Step[0950/1563], Loss: 1.2804 (1.0819), Acc@1: 0.7500 (0.7567), Acc@5: 0.8750 (0.9283)
2022-01-07 13:06:30,798 Val Step[1000/1563], Loss: 0.5769 (1.1044), Acc@1: 0.9375 (0.7508), Acc@5: 1.0000 (0.9254)
2022-01-07 13:06:32,090 Val Step[1050/1563], Loss: 0.3119 (1.1177), Acc@1: 0.9688 (0.7476), Acc@5: 1.0000 (0.9240)
2022-01-07 13:06:33,425 Val Step[1100/1563], Loss: 0.6390 (1.1310), Acc@1: 0.8750 (0.7451), Acc@5: 1.0000 (0.9223)
2022-01-07 13:06:34,775 Val Step[1150/1563], Loss: 1.2879 (1.1446), Acc@1: 0.7812 (0.7424), Acc@5: 0.8125 (0.9202)
2022-01-07 13:06:36,153 Val Step[1200/1563], Loss: 1.2513 (1.1578), Acc@1: 0.7812 (0.7395), Acc@5: 0.8438 (0.9179)
2022-01-07 13:06:37,460 Val Step[1250/1563], Loss: 0.6997 (1.1684), Acc@1: 0.8750 (0.7377), Acc@5: 0.9375 (0.9165)
2022-01-07 13:06:38,801 Val Step[1300/1563], Loss: 0.7883 (1.1767), Acc@1: 0.9062 (0.7361), Acc@5: 0.9375 (0.9154)
2022-01-07 13:06:40,195 Val Step[1350/1563], Loss: 1.6825 (1.1928), Acc@1: 0.5938 (0.7325), Acc@5: 0.8750 (0.9129)
2022-01-07 13:06:41,477 Val Step[1400/1563], Loss: 0.9445 (1.1994), Acc@1: 0.7500 (0.7310), Acc@5: 0.9375 (0.9122)
2022-01-07 13:06:42,753 Val Step[1450/1563], Loss: 1.2801 (1.2049), Acc@1: 0.7812 (0.7292), Acc@5: 0.9375 (0.9118)
2022-01-07 13:06:44,138 Val Step[1500/1563], Loss: 1.6598 (1.1948), Acc@1: 0.6250 (0.7315), Acc@5: 0.8750 (0.9131)
2022-01-07 13:06:45,430 Val Step[1550/1563], Loss: 0.8816 (1.1966), Acc@1: 0.8750 (0.7307), Acc@5: 0.9062 (0.9129)
2022-01-07 13:06:46,196 ----- Epoch[284/310], Validation Loss: 1.1951, Validation Acc@1: 0.7309, Validation Acc@5: 0.9131, time: 107.40
2022-01-07 13:06:46,196 ----- Epoch[284/310], Train Loss: 3.5545, Train Acc: 0.3880, time: 1572.64, Best Val(epoch284) Acc@1: 0.7309
2022-01-07 13:06:46,377 Max accuracy so far: 0.7309 at epoch_284
2022-01-07 13:06:46,378 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 13:06:46,378 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 13:06:46,713 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 13:06:46,872 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 13:06:46,873 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 13:06:47,006 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 13:06:47,007 Now training epoch 285. LR=0.000011
2022-01-07 13:08:06,741 Epoch[285/310], Step[0000/1251], Loss: 3.7105(3.7105), Acc: 0.3652(0.3652)
2022-01-07 13:09:06,393 Epoch[285/310], Step[0050/1251], Loss: 3.8313(3.5721), Acc: 0.4824(0.3807)
2022-01-07 13:10:05,676 Epoch[285/310], Step[0100/1251], Loss: 3.6774(3.5834), Acc: 0.4502(0.3892)
2022-01-07 13:11:05,078 Epoch[285/310], Step[0150/1251], Loss: 3.4283(3.5701), Acc: 0.1963(0.3846)
2022-01-07 13:12:04,015 Epoch[285/310], Step[0200/1251], Loss: 3.7085(3.5694), Acc: 0.3916(0.3767)
2022-01-07 13:13:03,246 Epoch[285/310], Step[0250/1251], Loss: 3.3841(3.5661), Acc: 0.2578(0.3761)
2022-01-07 13:14:02,555 Epoch[285/310], Step[0300/1251], Loss: 3.6960(3.5647), Acc: 0.2998(0.3778)
2022-01-07 13:15:00,421 Epoch[285/310], Step[0350/1251], Loss: 3.4329(3.5622), Acc: 0.5664(0.3821)
2022-01-07 13:15:59,722 Epoch[285/310], Step[0400/1251], Loss: 3.5916(3.5536), Acc: 0.3760(0.3834)
2022-01-07 13:16:58,990 Epoch[285/310], Step[0450/1251], Loss: 3.6616(3.5515), Acc: 0.4551(0.3827)
2022-01-07 13:17:57,904 Epoch[285/310], Step[0500/1251], Loss: 3.0447(3.5530), Acc: 0.4736(0.3851)
2022-01-07 13:18:57,860 Epoch[285/310], Step[0550/1251], Loss: 3.8339(3.5539), Acc: 0.3457(0.3850)
2022-01-07 13:19:57,408 Epoch[285/310], Step[0600/1251], Loss: 3.6918(3.5526), Acc: 0.3359(0.3861)
2022-01-07 13:20:57,311 Epoch[285/310], Step[0650/1251], Loss: 3.4714(3.5507), Acc: 0.5215(0.3858)
2022-01-07 13:21:56,623 Epoch[285/310], Step[0700/1251], Loss: 3.7592(3.5491), Acc: 0.2441(0.3864)
2022-01-07 13:22:57,158 Epoch[285/310], Step[0750/1251], Loss: 3.6740(3.5511), Acc: 0.2402(0.3830)
2022-01-07 13:23:55,733 Epoch[285/310], Step[0800/1251], Loss: 3.5143(3.5529), Acc: 0.4814(0.3810)
2022-01-07 13:24:55,188 Epoch[285/310], Step[0850/1251], Loss: 3.3396(3.5506), Acc: 0.5967(0.3822)
2022-01-07 13:25:55,746 Epoch[285/310], Step[0900/1251], Loss: 3.4450(3.5484), Acc: 0.4209(0.3829)
2022-01-07 13:26:55,760 Epoch[285/310], Step[0950/1251], Loss: 3.6059(3.5521), Acc: 0.4170(0.3836)
2022-01-07 13:27:54,748 Epoch[285/310], Step[1000/1251], Loss: 3.4980(3.5515), Acc: 0.4912(0.3840)
2022-01-07 13:28:53,831 Epoch[285/310], Step[1050/1251], Loss: 3.7661(3.5523), Acc: 0.4141(0.3844)
2022-01-07 13:29:53,685 Epoch[285/310], Step[1100/1251], Loss: 3.4155(3.5554), Acc: 0.3535(0.3837)
2022-01-07 13:30:51,601 Epoch[285/310], Step[1150/1251], Loss: 3.7082(3.5564), Acc: 0.4941(0.3839)
2022-01-07 13:31:50,369 Epoch[285/310], Step[1200/1251], Loss: 3.9715(3.5556), Acc: 0.5322(0.3846)
2022-01-07 13:32:47,857 Epoch[285/310], Step[1250/1251], Loss: 3.7907(3.5542), Acc: 0.2979(0.3865)
2022-01-07 13:32:49,380 ----- Epoch[285/310], Train Loss: 3.5542, Train Acc: 0.3865, time: 1562.37, Best Val(epoch284) Acc@1: 0.7309
2022-01-07 13:32:49,555 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 13:32:49,556 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 13:32:49,662 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 13:32:49,662 Now training epoch 286. LR=0.000010
2022-01-07 13:34:09,660 Epoch[286/310], Step[0000/1251], Loss: 3.1348(3.1348), Acc: 0.5918(0.5918)
2022-01-07 13:35:08,296 Epoch[286/310], Step[0050/1251], Loss: 3.8907(3.5161), Acc: 0.4395(0.3909)
2022-01-07 13:36:07,152 Epoch[286/310], Step[0100/1251], Loss: 3.7644(3.5237), Acc: 0.4600(0.3680)
2022-01-07 13:37:06,581 Epoch[286/310], Step[0150/1251], Loss: 3.5115(3.5308), Acc: 0.4541(0.3665)
2022-01-07 13:38:05,989 Epoch[286/310], Step[0200/1251], Loss: 3.8935(3.5548), Acc: 0.2959(0.3639)
2022-01-07 13:39:05,138 Epoch[286/310], Step[0250/1251], Loss: 3.4492(3.5479), Acc: 0.4014(0.3693)
2022-01-07 13:40:04,622 Epoch[286/310], Step[0300/1251], Loss: 4.2789(3.5439), Acc: 0.3730(0.3716)
2022-01-07 13:41:04,347 Epoch[286/310], Step[0350/1251], Loss: 3.7576(3.5453), Acc: 0.2891(0.3735)
2022-01-07 13:42:04,669 Epoch[286/310], Step[0400/1251], Loss: 3.3027(3.5511), Acc: 0.4746(0.3737)
2022-01-07 13:43:04,199 Epoch[286/310], Step[0450/1251], Loss: 3.4087(3.5555), Acc: 0.4521(0.3766)
2022-01-07 13:44:02,823 Epoch[286/310], Step[0500/1251], Loss: 3.8400(3.5509), Acc: 0.3047(0.3775)
2022-01-07 13:45:02,444 Epoch[286/310], Step[0550/1251], Loss: 3.1798(3.5531), Acc: 0.4473(0.3786)
2022-01-07 13:46:02,909 Epoch[286/310], Step[0600/1251], Loss: 4.0028(3.5535), Acc: 0.3789(0.3780)
2022-01-07 13:47:02,078 Epoch[286/310], Step[0650/1251], Loss: 3.2754(3.5533), Acc: 0.3340(0.3777)
2022-01-07 13:48:00,243 Epoch[286/310], Step[0700/1251], Loss: 3.4059(3.5525), Acc: 0.4717(0.3785)
2022-01-07 13:49:00,785 Epoch[286/310], Step[0750/1251], Loss: 3.9225(3.5510), Acc: 0.1299(0.3802)
2022-01-07 13:50:01,146 Epoch[286/310], Step[0800/1251], Loss: 3.7809(3.5523), Acc: 0.3154(0.3819)
2022-01-07 13:51:00,367 Epoch[286/310], Step[0850/1251], Loss: 3.3186(3.5521), Acc: 0.3594(0.3831)
2022-01-07 13:51:59,007 Epoch[286/310], Step[0900/1251], Loss: 3.5173(3.5484), Acc: 0.4551(0.3843)
2022-01-07 13:52:58,839 Epoch[286/310], Step[0950/1251], Loss: 3.1621(3.5430), Acc: 0.3994(0.3848)
2022-01-07 13:53:57,319 Epoch[286/310], Step[1000/1251], Loss: 3.5162(3.5434), Acc: 0.3652(0.3847)
2022-01-07 13:54:55,246 Epoch[286/310], Step[1050/1251], Loss: 4.0082(3.5446), Acc: 0.3223(0.3850)
2022-01-07 13:55:53,060 Epoch[286/310], Step[1100/1251], Loss: 3.2755(3.5438), Acc: 0.3379(0.3842)
2022-01-07 13:56:50,939 Epoch[286/310], Step[1150/1251], Loss: 3.6754(3.5444), Acc: 0.4141(0.3855)
2022-01-07 13:57:50,614 Epoch[286/310], Step[1200/1251], Loss: 3.5965(3.5443), Acc: 0.2949(0.3861)
2022-01-07 13:58:49,860 Epoch[286/310], Step[1250/1251], Loss: 3.7624(3.5444), Acc: 0.4785(0.3863)
2022-01-07 13:58:51,989 ----- Validation after Epoch: 286
2022-01-07 13:59:52,561 Val Step[0000/1563], Loss: 0.6219 (0.6219), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 13:59:54,085 Val Step[0050/1563], Loss: 2.2116 (0.7066), Acc@1: 0.4062 (0.8578), Acc@5: 0.8750 (0.9632)
2022-01-07 13:59:55,354 Val Step[0100/1563], Loss: 1.9123 (0.9645), Acc@1: 0.5000 (0.7850), Acc@5: 0.8438 (0.9403)
2022-01-07 13:59:56,705 Val Step[0150/1563], Loss: 0.3601 (0.9147), Acc@1: 0.9688 (0.7984), Acc@5: 1.0000 (0.9431)
2022-01-07 13:59:58,137 Val Step[0200/1563], Loss: 0.9246 (0.9204), Acc@1: 0.8125 (0.8007), Acc@5: 0.9375 (0.9425)
2022-01-07 13:59:59,542 Val Step[0250/1563], Loss: 0.4526 (0.8717), Acc@1: 0.9062 (0.8123), Acc@5: 1.0000 (0.9475)
2022-01-07 14:00:00,926 Val Step[0300/1563], Loss: 1.0234 (0.9298), Acc@1: 0.7188 (0.7971), Acc@5: 1.0000 (0.9432)
2022-01-07 14:00:02,355 Val Step[0350/1563], Loss: 0.8690 (0.9366), Acc@1: 0.7812 (0.7930), Acc@5: 0.9375 (0.9449)
2022-01-07 14:00:03,757 Val Step[0400/1563], Loss: 0.8221 (0.9444), Acc@1: 0.8438 (0.7876), Acc@5: 0.9688 (0.9452)
2022-01-07 14:00:05,167 Val Step[0450/1563], Loss: 0.8862 (0.9518), Acc@1: 0.7500 (0.7849), Acc@5: 1.0000 (0.9462)
2022-01-07 14:00:06,575 Val Step[0500/1563], Loss: 0.4212 (0.9440), Acc@1: 0.9375 (0.7875), Acc@5: 1.0000 (0.9475)
2022-01-07 14:00:08,153 Val Step[0550/1563], Loss: 0.7847 (0.9234), Acc@1: 0.8125 (0.7923), Acc@5: 0.9688 (0.9491)
2022-01-07 14:00:09,569 Val Step[0600/1563], Loss: 0.7874 (0.9319), Acc@1: 0.8125 (0.7907), Acc@5: 0.9375 (0.9482)
2022-01-07 14:00:10,978 Val Step[0650/1563], Loss: 0.4811 (0.9518), Acc@1: 0.9375 (0.7865), Acc@5: 1.0000 (0.9458)
2022-01-07 14:00:12,396 Val Step[0700/1563], Loss: 0.9549 (0.9786), Acc@1: 0.8438 (0.7803), Acc@5: 0.9688 (0.9424)
2022-01-07 14:00:13,815 Val Step[0750/1563], Loss: 1.1244 (1.0096), Acc@1: 0.8125 (0.7741), Acc@5: 0.9062 (0.9383)
2022-01-07 14:00:15,223 Val Step[0800/1563], Loss: 0.7197 (1.0458), Acc@1: 0.8125 (0.7650), Acc@5: 1.0000 (0.9337)
2022-01-07 14:00:16,620 Val Step[0850/1563], Loss: 1.1860 (1.0695), Acc@1: 0.6562 (0.7587), Acc@5: 0.9688 (0.9311)
2022-01-07 14:00:18,013 Val Step[0900/1563], Loss: 0.2575 (1.0690), Acc@1: 0.9688 (0.7601), Acc@5: 1.0000 (0.9308)
2022-01-07 14:00:19,479 Val Step[0950/1563], Loss: 1.2240 (1.0881), Acc@1: 0.7500 (0.7566), Acc@5: 0.9062 (0.9274)
2022-01-07 14:00:20,876 Val Step[1000/1563], Loss: 0.5904 (1.1104), Acc@1: 0.9375 (0.7509), Acc@5: 1.0000 (0.9244)
2022-01-07 14:00:22,266 Val Step[1050/1563], Loss: 0.3406 (1.1238), Acc@1: 0.9688 (0.7479), Acc@5: 1.0000 (0.9231)
2022-01-07 14:00:23,658 Val Step[1100/1563], Loss: 0.6838 (1.1370), Acc@1: 0.8750 (0.7453), Acc@5: 1.0000 (0.9212)
2022-01-07 14:00:25,037 Val Step[1150/1563], Loss: 1.1728 (1.1504), Acc@1: 0.7812 (0.7428), Acc@5: 0.8438 (0.9194)
2022-01-07 14:00:26,422 Val Step[1200/1563], Loss: 1.1914 (1.1634), Acc@1: 0.7500 (0.7394), Acc@5: 0.8438 (0.9172)
2022-01-07 14:00:27,723 Val Step[1250/1563], Loss: 0.7164 (1.1746), Acc@1: 0.8750 (0.7375), Acc@5: 0.9375 (0.9156)
2022-01-07 14:00:28,999 Val Step[1300/1563], Loss: 0.7729 (1.1827), Acc@1: 0.9062 (0.7360), Acc@5: 0.9375 (0.9147)
2022-01-07 14:00:30,416 Val Step[1350/1563], Loss: 1.7718 (1.1992), Acc@1: 0.5625 (0.7321), Acc@5: 0.8750 (0.9123)
2022-01-07 14:00:31,700 Val Step[1400/1563], Loss: 0.9500 (1.2057), Acc@1: 0.7500 (0.7304), Acc@5: 0.9375 (0.9116)
2022-01-07 14:00:33,004 Val Step[1450/1563], Loss: 1.3695 (1.2122), Acc@1: 0.7812 (0.7285), Acc@5: 0.9375 (0.9111)
2022-01-07 14:00:34,352 Val Step[1500/1563], Loss: 1.5973 (1.2022), Acc@1: 0.6875 (0.7308), Acc@5: 0.8750 (0.9124)
2022-01-07 14:00:35,622 Val Step[1550/1563], Loss: 0.8754 (1.2044), Acc@1: 0.8750 (0.7301), Acc@5: 0.9062 (0.9122)
2022-01-07 14:00:36,393 ----- Epoch[286/310], Validation Loss: 1.2028, Validation Acc@1: 0.7304, Validation Acc@5: 0.9124, time: 104.40
2022-01-07 14:00:36,393 ----- Epoch[286/310], Train Loss: 3.5444, Train Acc: 0.3863, time: 1562.32, Best Val(epoch284) Acc@1: 0.7309
2022-01-07 14:00:36,575 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 14:00:36,575 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 14:00:36,676 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 14:00:36,677 Now training epoch 287. LR=0.000010
2022-01-07 14:02:00,506 Epoch[287/310], Step[0000/1251], Loss: 3.5893(3.5893), Acc: 0.2812(0.2812)
2022-01-07 14:02:59,366 Epoch[287/310], Step[0050/1251], Loss: 3.0838(3.5468), Acc: 0.4697(0.3865)
2022-01-07 14:03:59,026 Epoch[287/310], Step[0100/1251], Loss: 3.7111(3.5309), Acc: 0.3965(0.3944)
2022-01-07 14:04:57,688 Epoch[287/310], Step[0150/1251], Loss: 3.5214(3.5218), Acc: 0.4404(0.3919)
2022-01-07 14:05:57,251 Epoch[287/310], Step[0200/1251], Loss: 3.2868(3.5220), Acc: 0.3008(0.3909)
2022-01-07 14:06:56,982 Epoch[287/310], Step[0250/1251], Loss: 3.4555(3.5258), Acc: 0.3926(0.3895)
2022-01-07 14:07:56,198 Epoch[287/310], Step[0300/1251], Loss: 4.2498(3.5316), Acc: 0.2539(0.3949)
2022-01-07 14:08:56,699 Epoch[287/310], Step[0350/1251], Loss: 3.2012(3.5353), Acc: 0.3262(0.3931)
2022-01-07 14:09:55,690 Epoch[287/310], Step[0400/1251], Loss: 4.3727(3.5394), Acc: 0.2959(0.3939)
2022-01-07 14:10:55,741 Epoch[287/310], Step[0450/1251], Loss: 3.8557(3.5445), Acc: 0.2705(0.3920)
2022-01-07 14:11:57,289 Epoch[287/310], Step[0500/1251], Loss: 3.3638(3.5464), Acc: 0.0977(0.3907)
2022-01-07 14:12:57,487 Epoch[287/310], Step[0550/1251], Loss: 4.0181(3.5509), Acc: 0.4150(0.3893)
2022-01-07 14:13:56,889 Epoch[287/310], Step[0600/1251], Loss: 3.4088(3.5512), Acc: 0.5205(0.3922)
2022-01-07 14:14:58,207 Epoch[287/310], Step[0650/1251], Loss: 3.8601(3.5551), Acc: 0.3740(0.3920)
2022-01-07 14:15:57,238 Epoch[287/310], Step[0700/1251], Loss: 3.5003(3.5512), Acc: 0.2891(0.3942)
2022-01-07 14:16:56,067 Epoch[287/310], Step[0750/1251], Loss: 3.6426(3.5517), Acc: 0.4092(0.3957)
2022-01-07 14:17:56,060 Epoch[287/310], Step[0800/1251], Loss: 3.5331(3.5524), Acc: 0.1543(0.3948)
2022-01-07 14:18:56,213 Epoch[287/310], Step[0850/1251], Loss: 3.5329(3.5506), Acc: 0.4482(0.3940)
2022-01-07 14:19:55,511 Epoch[287/310], Step[0900/1251], Loss: 3.5081(3.5514), Acc: 0.0664(0.3931)
2022-01-07 14:20:56,164 Epoch[287/310], Step[0950/1251], Loss: 3.1281(3.5534), Acc: 0.4688(0.3915)
2022-01-07 14:21:55,818 Epoch[287/310], Step[1000/1251], Loss: 3.6542(3.5542), Acc: 0.3232(0.3937)
2022-01-07 14:22:55,428 Epoch[287/310], Step[1050/1251], Loss: 3.6186(3.5537), Acc: 0.3760(0.3926)
2022-01-07 14:23:56,508 Epoch[287/310], Step[1100/1251], Loss: 3.5220(3.5521), Acc: 0.5205(0.3946)
2022-01-07 14:24:55,905 Epoch[287/310], Step[1150/1251], Loss: 3.2452(3.5516), Acc: 0.4844(0.3938)
2022-01-07 14:25:56,445 Epoch[287/310], Step[1200/1251], Loss: 3.2154(3.5511), Acc: 0.0264(0.3945)
2022-01-07 14:26:55,856 Epoch[287/310], Step[1250/1251], Loss: 3.4548(3.5496), Acc: 0.4766(0.3944)
2022-01-07 14:26:57,383 ----- Epoch[287/310], Train Loss: 3.5496, Train Acc: 0.3944, time: 1580.70, Best Val(epoch284) Acc@1: 0.7309
2022-01-07 14:26:57,558 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 14:26:57,558 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 14:26:57,661 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 14:26:57,662 Now training epoch 288. LR=0.000009
2022-01-07 14:28:18,601 Epoch[288/310], Step[0000/1251], Loss: 3.3788(3.3788), Acc: 0.4365(0.4365)
2022-01-07 14:29:17,146 Epoch[288/310], Step[0050/1251], Loss: 3.7632(3.6071), Acc: 0.1963(0.3752)
2022-01-07 14:30:15,719 Epoch[288/310], Step[0100/1251], Loss: 3.3538(3.5714), Acc: 0.3955(0.3928)
2022-01-07 14:31:15,062 Epoch[288/310], Step[0150/1251], Loss: 3.8903(3.5628), Acc: 0.3672(0.3951)
2022-01-07 14:32:15,200 Epoch[288/310], Step[0200/1251], Loss: 3.8071(3.5697), Acc: 0.2295(0.3889)
2022-01-07 14:33:14,432 Epoch[288/310], Step[0250/1251], Loss: 3.8012(3.5681), Acc: 0.5107(0.3923)
2022-01-07 14:34:13,824 Epoch[288/310], Step[0300/1251], Loss: 3.0376(3.5678), Acc: 0.3281(0.3905)
2022-01-07 14:35:11,913 Epoch[288/310], Step[0350/1251], Loss: 4.3812(3.5624), Acc: 0.2256(0.3920)
2022-01-07 14:36:11,945 Epoch[288/310], Step[0400/1251], Loss: 4.0059(3.5567), Acc: 0.2832(0.3915)
2022-01-07 14:37:11,398 Epoch[288/310], Step[0450/1251], Loss: 3.6638(3.5497), Acc: 0.3623(0.3898)
2022-01-07 14:38:11,696 Epoch[288/310], Step[0500/1251], Loss: 3.7924(3.5473), Acc: 0.3555(0.3893)
2022-01-07 14:39:09,984 Epoch[288/310], Step[0550/1251], Loss: 3.5669(3.5478), Acc: 0.3857(0.3882)
2022-01-07 14:40:09,319 Epoch[288/310], Step[0600/1251], Loss: 3.4211(3.5454), Acc: 0.2549(0.3890)
2022-01-07 14:41:09,486 Epoch[288/310], Step[0650/1251], Loss: 3.7584(3.5426), Acc: 0.3662(0.3897)
2022-01-07 14:42:10,769 Epoch[288/310], Step[0700/1251], Loss: 3.5139(3.5386), Acc: 0.2539(0.3884)
2022-01-07 14:43:11,064 Epoch[288/310], Step[0750/1251], Loss: 3.3579(3.5376), Acc: 0.3594(0.3894)
2022-01-07 14:44:10,563 Epoch[288/310], Step[0800/1251], Loss: 3.6655(3.5405), Acc: 0.4492(0.3884)
2022-01-07 14:45:08,734 Epoch[288/310], Step[0850/1251], Loss: 3.6318(3.5384), Acc: 0.4199(0.3892)
2022-01-07 14:46:09,134 Epoch[288/310], Step[0900/1251], Loss: 3.6702(3.5373), Acc: 0.4180(0.3883)
2022-01-07 14:47:08,774 Epoch[288/310], Step[0950/1251], Loss: 3.6891(3.5361), Acc: 0.5117(0.3889)
2022-01-07 14:48:08,559 Epoch[288/310], Step[1000/1251], Loss: 3.8351(3.5376), Acc: 0.4531(0.3893)
2022-01-07 14:49:09,150 Epoch[288/310], Step[1050/1251], Loss: 2.8172(3.5413), Acc: 0.6309(0.3895)
2022-01-07 14:50:09,179 Epoch[288/310], Step[1100/1251], Loss: 3.8452(3.5406), Acc: 0.2939(0.3897)
2022-01-07 14:51:10,315 Epoch[288/310], Step[1150/1251], Loss: 4.0365(3.5411), Acc: 0.4248(0.3881)
2022-01-07 14:52:09,726 Epoch[288/310], Step[1200/1251], Loss: 3.6572(3.5422), Acc: 0.4961(0.3877)
2022-01-07 14:53:10,278 Epoch[288/310], Step[1250/1251], Loss: 3.4802(3.5419), Acc: 0.5430(0.3879)
2022-01-07 14:53:11,797 ----- Validation after Epoch: 288
2022-01-07 14:54:15,185 Val Step[0000/1563], Loss: 0.6374 (0.6374), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 14:54:16,510 Val Step[0050/1563], Loss: 2.0775 (0.7070), Acc@1: 0.4062 (0.8597), Acc@5: 0.9062 (0.9620)
2022-01-07 14:54:17,793 Val Step[0100/1563], Loss: 1.9000 (0.9608), Acc@1: 0.5312 (0.7893), Acc@5: 0.8438 (0.9400)
2022-01-07 14:54:19,154 Val Step[0150/1563], Loss: 0.3631 (0.9083), Acc@1: 0.9688 (0.8015), Acc@5: 1.0000 (0.9441)
2022-01-07 14:54:20,552 Val Step[0200/1563], Loss: 0.9085 (0.9153), Acc@1: 0.8125 (0.8038), Acc@5: 0.9375 (0.9434)
2022-01-07 14:54:21,809 Val Step[0250/1563], Loss: 0.4677 (0.8682), Acc@1: 0.9062 (0.8147), Acc@5: 1.0000 (0.9487)
2022-01-07 14:54:23,190 Val Step[0300/1563], Loss: 1.0573 (0.9257), Acc@1: 0.6875 (0.7992), Acc@5: 1.0000 (0.9441)
2022-01-07 14:54:24,548 Val Step[0350/1563], Loss: 0.9174 (0.9340), Acc@1: 0.7812 (0.7951), Acc@5: 0.9062 (0.9458)
2022-01-07 14:54:25,787 Val Step[0400/1563], Loss: 0.8845 (0.9420), Acc@1: 0.8438 (0.7900), Acc@5: 0.9688 (0.9459)
2022-01-07 14:54:27,179 Val Step[0450/1563], Loss: 0.9490 (0.9490), Acc@1: 0.7188 (0.7873), Acc@5: 1.0000 (0.9466)
2022-01-07 14:54:28,501 Val Step[0500/1563], Loss: 0.4114 (0.9407), Acc@1: 0.9375 (0.7899), Acc@5: 1.0000 (0.9479)
2022-01-07 14:54:30,054 Val Step[0550/1563], Loss: 0.7601 (0.9201), Acc@1: 0.8125 (0.7940), Acc@5: 0.9688 (0.9494)
2022-01-07 14:54:31,337 Val Step[0600/1563], Loss: 0.7096 (0.9280), Acc@1: 0.8438 (0.7926), Acc@5: 0.9375 (0.9488)
2022-01-07 14:54:32,608 Val Step[0650/1563], Loss: 0.4927 (0.9479), Acc@1: 0.9062 (0.7884), Acc@5: 1.0000 (0.9459)
2022-01-07 14:54:33,929 Val Step[0700/1563], Loss: 1.0051 (0.9740), Acc@1: 0.7500 (0.7815), Acc@5: 0.9688 (0.9430)
2022-01-07 14:54:35,358 Val Step[0750/1563], Loss: 1.2913 (1.0047), Acc@1: 0.7812 (0.7748), Acc@5: 0.9062 (0.9392)
2022-01-07 14:54:36,724 Val Step[0800/1563], Loss: 0.6378 (1.0410), Acc@1: 0.8750 (0.7656), Acc@5: 1.0000 (0.9347)
2022-01-07 14:54:38,160 Val Step[0850/1563], Loss: 1.1604 (1.0647), Acc@1: 0.6250 (0.7593), Acc@5: 0.9375 (0.9319)
2022-01-07 14:54:39,619 Val Step[0900/1563], Loss: 0.2584 (1.0636), Acc@1: 0.9688 (0.7606), Acc@5: 1.0000 (0.9316)
2022-01-07 14:54:41,069 Val Step[0950/1563], Loss: 1.2722 (1.0827), Acc@1: 0.7812 (0.7569), Acc@5: 0.9062 (0.9284)
2022-01-07 14:54:42,455 Val Step[1000/1563], Loss: 0.5298 (1.1052), Acc@1: 0.9688 (0.7510), Acc@5: 0.9688 (0.9253)
2022-01-07 14:54:43,838 Val Step[1050/1563], Loss: 0.3032 (1.1188), Acc@1: 0.9688 (0.7474), Acc@5: 1.0000 (0.9238)
2022-01-07 14:54:45,241 Val Step[1100/1563], Loss: 0.7521 (1.1315), Acc@1: 0.8750 (0.7450), Acc@5: 1.0000 (0.9222)
2022-01-07 14:54:46,623 Val Step[1150/1563], Loss: 1.2852 (1.1453), Acc@1: 0.7812 (0.7422), Acc@5: 0.8125 (0.9203)
2022-01-07 14:54:48,060 Val Step[1200/1563], Loss: 1.1872 (1.1580), Acc@1: 0.7812 (0.7390), Acc@5: 0.8438 (0.9183)
2022-01-07 14:54:49,460 Val Step[1250/1563], Loss: 0.7328 (1.1692), Acc@1: 0.8750 (0.7371), Acc@5: 0.9375 (0.9166)
2022-01-07 14:54:50,866 Val Step[1300/1563], Loss: 0.7580 (1.1773), Acc@1: 0.8750 (0.7355), Acc@5: 0.9375 (0.9155)
2022-01-07 14:54:52,283 Val Step[1350/1563], Loss: 1.7365 (1.1938), Acc@1: 0.5625 (0.7316), Acc@5: 0.8750 (0.9131)
2022-01-07 14:54:53,828 Val Step[1400/1563], Loss: 0.9561 (1.2004), Acc@1: 0.7500 (0.7299), Acc@5: 0.9375 (0.9122)
2022-01-07 14:54:55,234 Val Step[1450/1563], Loss: 1.3597 (1.2067), Acc@1: 0.7500 (0.7281), Acc@5: 0.9375 (0.9118)
2022-01-07 14:54:56,657 Val Step[1500/1563], Loss: 1.5726 (1.1968), Acc@1: 0.6562 (0.7303), Acc@5: 0.8750 (0.9131)
2022-01-07 14:54:58,062 Val Step[1550/1563], Loss: 0.8395 (1.1988), Acc@1: 0.8750 (0.7295), Acc@5: 0.9062 (0.9129)
2022-01-07 14:54:58,834 ----- Epoch[288/310], Validation Loss: 1.1973, Validation Acc@1: 0.7298, Validation Acc@5: 0.9131, time: 107.03
2022-01-07 14:54:58,835 ----- Epoch[288/310], Train Loss: 3.5419, Train Acc: 0.3879, time: 1574.13, Best Val(epoch284) Acc@1: 0.7309
2022-01-07 14:54:59,023 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 14:54:59,023 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 14:54:59,129 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 14:54:59,129 Now training epoch 289. LR=0.000008
2022-01-07 14:56:18,984 Epoch[289/310], Step[0000/1251], Loss: 4.1517(4.1517), Acc: 0.3867(0.3867)
2022-01-07 14:57:17,857 Epoch[289/310], Step[0050/1251], Loss: 3.3727(3.5907), Acc: 0.4482(0.3846)
2022-01-07 14:58:16,998 Epoch[289/310], Step[0100/1251], Loss: 4.0482(3.5479), Acc: 0.3750(0.3976)
2022-01-07 14:59:15,950 Epoch[289/310], Step[0150/1251], Loss: 3.7695(3.5399), Acc: 0.1484(0.3932)
2022-01-07 15:00:14,726 Epoch[289/310], Step[0200/1251], Loss: 3.2641(3.5385), Acc: 0.4912(0.3975)
2022-01-07 15:01:13,657 Epoch[289/310], Step[0250/1251], Loss: 3.7095(3.5369), Acc: 0.4365(0.3971)
2022-01-07 15:02:14,213 Epoch[289/310], Step[0300/1251], Loss: 3.2832(3.5351), Acc: 0.6064(0.3958)
2022-01-07 15:03:14,084 Epoch[289/310], Step[0350/1251], Loss: 3.8954(3.5340), Acc: 0.3779(0.3962)
2022-01-07 15:04:14,531 Epoch[289/310], Step[0400/1251], Loss: 3.7763(3.5402), Acc: 0.4912(0.3958)
2022-01-07 15:05:14,904 Epoch[289/310], Step[0450/1251], Loss: 3.1722(3.5393), Acc: 0.2471(0.3960)
2022-01-07 15:06:14,021 Epoch[289/310], Step[0500/1251], Loss: 3.4422(3.5416), Acc: 0.5117(0.3972)
2022-01-07 15:07:14,175 Epoch[289/310], Step[0550/1251], Loss: 3.2925(3.5477), Acc: 0.5537(0.3946)
2022-01-07 15:08:13,627 Epoch[289/310], Step[0600/1251], Loss: 3.3328(3.5429), Acc: 0.5039(0.3934)
2022-01-07 15:09:12,705 Epoch[289/310], Step[0650/1251], Loss: 3.3063(3.5496), Acc: 0.3525(0.3927)
2022-01-07 15:10:13,968 Epoch[289/310], Step[0700/1251], Loss: 4.1593(3.5498), Acc: 0.3701(0.3916)
2022-01-07 15:11:15,439 Epoch[289/310], Step[0750/1251], Loss: 3.3795(3.5470), Acc: 0.5889(0.3900)
2022-01-07 15:12:15,306 Epoch[289/310], Step[0800/1251], Loss: 3.8070(3.5435), Acc: 0.3613(0.3902)
2022-01-07 15:13:16,211 Epoch[289/310], Step[0850/1251], Loss: 3.4720(3.5407), Acc: 0.4209(0.3903)
2022-01-07 15:14:16,793 Epoch[289/310], Step[0900/1251], Loss: 2.8343(3.5400), Acc: 0.3105(0.3891)
2022-01-07 15:15:16,938 Epoch[289/310], Step[0950/1251], Loss: 3.7266(3.5404), Acc: 0.4482(0.3895)
2022-01-07 15:16:17,979 Epoch[289/310], Step[1000/1251], Loss: 3.6943(3.5385), Acc: 0.2529(0.3890)
2022-01-07 15:17:18,251 Epoch[289/310], Step[1050/1251], Loss: 3.3422(3.5377), Acc: 0.5977(0.3880)
2022-01-07 15:18:18,315 Epoch[289/310], Step[1100/1251], Loss: 3.5105(3.5384), Acc: 0.3438(0.3877)
2022-01-07 15:19:18,297 Epoch[289/310], Step[1150/1251], Loss: 3.7268(3.5412), Acc: 0.3047(0.3871)
2022-01-07 15:20:18,196 Epoch[289/310], Step[1200/1251], Loss: 3.2423(3.5408), Acc: 0.5381(0.3865)
2022-01-07 15:21:18,421 Epoch[289/310], Step[1250/1251], Loss: 3.5068(3.5394), Acc: 0.3320(0.3866)
2022-01-07 15:21:20,042 ----- Epoch[289/310], Train Loss: 3.5394, Train Acc: 0.3866, time: 1580.91, Best Val(epoch284) Acc@1: 0.7309
2022-01-07 15:21:20,219 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 15:21:20,219 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 15:21:20,326 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 15:21:20,327 Now training epoch 290. LR=0.000008
2022-01-07 15:22:44,899 Epoch[290/310], Step[0000/1251], Loss: 3.6109(3.6109), Acc: 0.3750(0.3750)
2022-01-07 15:23:42,441 Epoch[290/310], Step[0050/1251], Loss: 3.4313(3.4606), Acc: 0.3652(0.3914)
2022-01-07 15:24:40,753 Epoch[290/310], Step[0100/1251], Loss: 3.6653(3.4875), Acc: 0.3857(0.4059)
2022-01-07 15:25:39,305 Epoch[290/310], Step[0150/1251], Loss: 3.7545(3.4882), Acc: 0.5195(0.3959)
2022-01-07 15:26:37,919 Epoch[290/310], Step[0200/1251], Loss: 3.6992(3.5106), Acc: 0.4717(0.3916)
2022-01-07 15:27:37,345 Epoch[290/310], Step[0250/1251], Loss: 3.6063(3.5230), Acc: 0.4639(0.3941)
2022-01-07 15:28:37,704 Epoch[290/310], Step[0300/1251], Loss: 3.3767(3.5264), Acc: 0.4873(0.3965)
2022-01-07 15:29:37,234 Epoch[290/310], Step[0350/1251], Loss: 3.4781(3.5379), Acc: 0.3828(0.3934)
2022-01-07 15:30:36,279 Epoch[290/310], Step[0400/1251], Loss: 3.1747(3.5355), Acc: 0.4385(0.3951)
2022-01-07 15:31:36,021 Epoch[290/310], Step[0450/1251], Loss: 3.6399(3.5337), Acc: 0.3896(0.3918)
2022-01-07 15:32:36,175 Epoch[290/310], Step[0500/1251], Loss: 3.6842(3.5374), Acc: 0.4658(0.3887)
2022-01-07 15:33:36,018 Epoch[290/310], Step[0550/1251], Loss: 4.0881(3.5413), Acc: 0.4111(0.3897)
2022-01-07 15:34:34,805 Epoch[290/310], Step[0600/1251], Loss: 4.3883(3.5433), Acc: 0.2227(0.3884)
2022-01-07 15:35:34,094 Epoch[290/310], Step[0650/1251], Loss: 3.3225(3.5408), Acc: 0.5674(0.3881)
2022-01-07 15:36:32,880 Epoch[290/310], Step[0700/1251], Loss: 3.6348(3.5395), Acc: 0.4766(0.3909)
2022-01-07 15:37:31,976 Epoch[290/310], Step[0750/1251], Loss: 3.7207(3.5427), Acc: 0.4375(0.3901)
2022-01-07 15:38:30,788 Epoch[290/310], Step[0800/1251], Loss: 3.9512(3.5448), Acc: 0.3193(0.3889)
2022-01-07 15:39:30,483 Epoch[290/310], Step[0850/1251], Loss: 3.5762(3.5491), Acc: 0.1904(0.3877)
2022-01-07 15:40:30,754 Epoch[290/310], Step[0900/1251], Loss: 3.3892(3.5512), Acc: 0.2334(0.3869)
2022-01-07 15:41:31,045 Epoch[290/310], Step[0950/1251], Loss: 3.6415(3.5548), Acc: 0.4648(0.3860)
2022-01-07 15:42:30,178 Epoch[290/310], Step[1000/1251], Loss: 3.5016(3.5556), Acc: 0.2588(0.3848)
2022-01-07 15:43:29,832 Epoch[290/310], Step[1050/1251], Loss: 2.9077(3.5546), Acc: 0.4736(0.3846)
2022-01-07 15:44:30,349 Epoch[290/310], Step[1100/1251], Loss: 3.2157(3.5555), Acc: 0.4492(0.3851)
2022-01-07 15:45:30,689 Epoch[290/310], Step[1150/1251], Loss: 3.3457(3.5549), Acc: 0.4365(0.3860)
2022-01-07 15:46:31,871 Epoch[290/310], Step[1200/1251], Loss: 3.7590(3.5518), Acc: 0.4326(0.3862)
2022-01-07 15:47:32,887 Epoch[290/310], Step[1250/1251], Loss: 3.7700(3.5511), Acc: 0.4619(0.3856)
2022-01-07 15:47:34,425 ----- Validation after Epoch: 290
2022-01-07 15:48:41,013 Val Step[0000/1563], Loss: 0.6560 (0.6560), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 15:48:42,353 Val Step[0050/1563], Loss: 2.1423 (0.7106), Acc@1: 0.4375 (0.8591), Acc@5: 0.8750 (0.9632)
2022-01-07 15:48:43,619 Val Step[0100/1563], Loss: 1.8388 (0.9615), Acc@1: 0.5312 (0.7884), Acc@5: 0.8125 (0.9425)
2022-01-07 15:48:44,957 Val Step[0150/1563], Loss: 0.3838 (0.9139), Acc@1: 0.9688 (0.8024), Acc@5: 1.0000 (0.9456)
2022-01-07 15:48:46,226 Val Step[0200/1563], Loss: 0.9171 (0.9203), Acc@1: 0.8125 (0.8044), Acc@5: 0.9375 (0.9439)
2022-01-07 15:48:47,490 Val Step[0250/1563], Loss: 0.4657 (0.8713), Acc@1: 0.9062 (0.8156), Acc@5: 1.0000 (0.9487)
2022-01-07 15:48:48,828 Val Step[0300/1563], Loss: 1.0446 (0.9290), Acc@1: 0.6875 (0.7997), Acc@5: 1.0000 (0.9445)
2022-01-07 15:48:50,096 Val Step[0350/1563], Loss: 0.8732 (0.9390), Acc@1: 0.7812 (0.7937), Acc@5: 0.9062 (0.9458)
2022-01-07 15:48:51,357 Val Step[0400/1563], Loss: 0.8783 (0.9470), Acc@1: 0.8125 (0.7881), Acc@5: 0.9688 (0.9461)
2022-01-07 15:48:52,739 Val Step[0450/1563], Loss: 0.9570 (0.9541), Acc@1: 0.7500 (0.7860), Acc@5: 1.0000 (0.9468)
2022-01-07 15:48:54,036 Val Step[0500/1563], Loss: 0.4569 (0.9461), Acc@1: 0.9062 (0.7887), Acc@5: 1.0000 (0.9477)
2022-01-07 15:48:55,457 Val Step[0550/1563], Loss: 0.7780 (0.9252), Acc@1: 0.7812 (0.7936), Acc@5: 0.9688 (0.9493)
2022-01-07 15:48:56,789 Val Step[0600/1563], Loss: 0.7530 (0.9333), Acc@1: 0.8438 (0.7924), Acc@5: 0.9375 (0.9488)
2022-01-07 15:48:58,160 Val Step[0650/1563], Loss: 0.5001 (0.9534), Acc@1: 0.8750 (0.7879), Acc@5: 1.0000 (0.9461)
2022-01-07 15:48:59,455 Val Step[0700/1563], Loss: 0.9871 (0.9806), Acc@1: 0.7812 (0.7811), Acc@5: 0.9688 (0.9430)
2022-01-07 15:49:00,871 Val Step[0750/1563], Loss: 1.1456 (1.0113), Acc@1: 0.8438 (0.7748), Acc@5: 0.9062 (0.9392)
2022-01-07 15:49:02,293 Val Step[0800/1563], Loss: 0.6085 (1.0472), Acc@1: 0.8750 (0.7656), Acc@5: 1.0000 (0.9348)
2022-01-07 15:49:03,551 Val Step[0850/1563], Loss: 1.1324 (1.0723), Acc@1: 0.6562 (0.7589), Acc@5: 0.9688 (0.9318)
2022-01-07 15:49:04,875 Val Step[0900/1563], Loss: 0.2410 (1.0714), Acc@1: 0.9688 (0.7602), Acc@5: 1.0000 (0.9315)
2022-01-07 15:49:06,311 Val Step[0950/1563], Loss: 1.1660 (1.0903), Acc@1: 0.7812 (0.7564), Acc@5: 0.9062 (0.9283)
2022-01-07 15:49:07,627 Val Step[1000/1563], Loss: 0.5481 (1.1126), Acc@1: 0.9375 (0.7506), Acc@5: 1.0000 (0.9253)
2022-01-07 15:49:08,896 Val Step[1050/1563], Loss: 0.3240 (1.1263), Acc@1: 0.9688 (0.7474), Acc@5: 1.0000 (0.9237)
2022-01-07 15:49:10,170 Val Step[1100/1563], Loss: 0.7306 (1.1392), Acc@1: 0.8750 (0.7448), Acc@5: 0.9688 (0.9219)
2022-01-07 15:49:11,454 Val Step[1150/1563], Loss: 1.2432 (1.1523), Acc@1: 0.7812 (0.7423), Acc@5: 0.8125 (0.9201)
2022-01-07 15:49:12,721 Val Step[1200/1563], Loss: 1.2113 (1.1656), Acc@1: 0.7812 (0.7392), Acc@5: 0.8438 (0.9178)
2022-01-07 15:49:13,973 Val Step[1250/1563], Loss: 0.7497 (1.1760), Acc@1: 0.8750 (0.7376), Acc@5: 0.9375 (0.9162)
2022-01-07 15:49:15,224 Val Step[1300/1563], Loss: 0.7939 (1.1842), Acc@1: 0.9062 (0.7359), Acc@5: 0.9375 (0.9151)
2022-01-07 15:49:16,467 Val Step[1350/1563], Loss: 1.7346 (1.2004), Acc@1: 0.5625 (0.7321), Acc@5: 0.8750 (0.9127)
2022-01-07 15:49:17,714 Val Step[1400/1563], Loss: 1.0136 (1.2070), Acc@1: 0.7500 (0.7308), Acc@5: 0.9375 (0.9118)
2022-01-07 15:49:19,015 Val Step[1450/1563], Loss: 1.4294 (1.2129), Acc@1: 0.7188 (0.7288), Acc@5: 0.9375 (0.9116)
2022-01-07 15:49:20,339 Val Step[1500/1563], Loss: 1.7082 (1.2030), Acc@1: 0.5938 (0.7310), Acc@5: 0.8438 (0.9130)
2022-01-07 15:49:21,618 Val Step[1550/1563], Loss: 0.8565 (1.2051), Acc@1: 0.8750 (0.7301), Acc@5: 0.9062 (0.9126)
2022-01-07 15:49:22,365 ----- Epoch[290/310], Validation Loss: 1.2035, Validation Acc@1: 0.7304, Validation Acc@5: 0.9128, time: 107.94
2022-01-07 15:49:22,365 ----- Epoch[290/310], Train Loss: 3.5511, Train Acc: 0.3856, time: 1574.09, Best Val(epoch284) Acc@1: 0.7309
2022-01-07 15:49:22,529 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-290-Loss-3.5432576660534365.pdparams
2022-01-07 15:49:22,530 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-290-Loss-3.5432576660534365.pdopt
2022-01-07 15:49:22,572 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-290-Loss-3.5432576660534365-EMA.pdparams
2022-01-07 15:49:22,572 Now training epoch 291. LR=0.000007
2022-01-07 15:50:42,874 Epoch[291/310], Step[0000/1251], Loss: 3.6459(3.6459), Acc: 0.4004(0.4004)
2022-01-07 15:51:42,309 Epoch[291/310], Step[0050/1251], Loss: 3.3141(3.6175), Acc: 0.5664(0.3873)
2022-01-07 15:52:40,900 Epoch[291/310], Step[0100/1251], Loss: 4.0088(3.5739), Acc: 0.4639(0.3897)
2022-01-07 15:53:40,086 Epoch[291/310], Step[0150/1251], Loss: 3.7869(3.5487), Acc: 0.3096(0.3966)
2022-01-07 15:54:39,295 Epoch[291/310], Step[0200/1251], Loss: 3.6192(3.5390), Acc: 0.2725(0.3983)
2022-01-07 15:55:38,228 Epoch[291/310], Step[0250/1251], Loss: 3.3721(3.5435), Acc: 0.3291(0.4022)
2022-01-07 15:56:38,344 Epoch[291/310], Step[0300/1251], Loss: 3.8306(3.5515), Acc: 0.3496(0.4011)
2022-01-07 15:57:38,111 Epoch[291/310], Step[0350/1251], Loss: 3.3870(3.5541), Acc: 0.5107(0.3996)
2022-01-07 15:58:38,191 Epoch[291/310], Step[0400/1251], Loss: 3.4309(3.5524), Acc: 0.5342(0.3980)
2022-01-07 15:59:38,603 Epoch[291/310], Step[0450/1251], Loss: 3.9191(3.5512), Acc: 0.4053(0.3990)
2022-01-07 16:00:37,309 Epoch[291/310], Step[0500/1251], Loss: 2.9797(3.5559), Acc: 0.4678(0.3998)
2022-01-07 16:01:37,067 Epoch[291/310], Step[0550/1251], Loss: 3.6934(3.5500), Acc: 0.4004(0.4001)
2022-01-07 16:02:36,994 Epoch[291/310], Step[0600/1251], Loss: 3.8911(3.5534), Acc: 0.3105(0.3989)
2022-01-07 16:03:37,308 Epoch[291/310], Step[0650/1251], Loss: 3.7767(3.5500), Acc: 0.3857(0.3996)
2022-01-07 16:04:38,207 Epoch[291/310], Step[0700/1251], Loss: 3.5766(3.5507), Acc: 0.2773(0.3994)
2022-01-07 16:05:39,085 Epoch[291/310], Step[0750/1251], Loss: 3.2865(3.5515), Acc: 0.2363(0.3996)
2022-01-07 16:06:38,997 Epoch[291/310], Step[0800/1251], Loss: 3.5599(3.5543), Acc: 0.3428(0.3989)
2022-01-07 16:07:38,611 Epoch[291/310], Step[0850/1251], Loss: 3.3925(3.5541), Acc: 0.3906(0.3989)
2022-01-07 16:08:39,094 Epoch[291/310], Step[0900/1251], Loss: 3.4933(3.5551), Acc: 0.4990(0.3979)
2022-01-07 16:09:39,167 Epoch[291/310], Step[0950/1251], Loss: 3.8297(3.5531), Acc: 0.4131(0.3977)
2022-01-07 16:10:39,840 Epoch[291/310], Step[1000/1251], Loss: 3.2130(3.5530), Acc: 0.3291(0.3955)
2022-01-07 16:11:39,866 Epoch[291/310], Step[1050/1251], Loss: 3.6116(3.5540), Acc: 0.3643(0.3960)
2022-01-07 16:12:39,220 Epoch[291/310], Step[1100/1251], Loss: 3.4620(3.5548), Acc: 0.5586(0.3959)
2022-01-07 16:13:38,189 Epoch[291/310], Step[1150/1251], Loss: 3.6032(3.5510), Acc: 0.4834(0.3976)
2022-01-07 16:14:37,116 Epoch[291/310], Step[1200/1251], Loss: 3.2505(3.5514), Acc: 0.5908(0.3983)
2022-01-07 16:15:36,241 Epoch[291/310], Step[1250/1251], Loss: 3.9415(3.5523), Acc: 0.1689(0.3985)
2022-01-07 16:15:38,451 ----- Epoch[291/310], Train Loss: 3.5523, Train Acc: 0.3985, time: 1575.88, Best Val(epoch284) Acc@1: 0.7309
2022-01-07 16:15:38,623 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 16:15:38,623 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 16:15:38,893 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 16:15:38,894 Now training epoch 292. LR=0.000007
2022-01-07 16:16:58,136 Epoch[292/310], Step[0000/1251], Loss: 3.1420(3.1420), Acc: 0.4307(0.4307)
2022-01-07 16:17:56,917 Epoch[292/310], Step[0050/1251], Loss: 3.8575(3.6061), Acc: 0.4492(0.3865)
2022-01-07 16:18:56,420 Epoch[292/310], Step[0100/1251], Loss: 3.7682(3.5897), Acc: 0.4619(0.3853)
2022-01-07 16:19:55,632 Epoch[292/310], Step[0150/1251], Loss: 3.2213(3.5706), Acc: 0.5850(0.3896)
2022-01-07 16:20:55,594 Epoch[292/310], Step[0200/1251], Loss: 3.7569(3.5872), Acc: 0.3242(0.3842)
2022-01-07 16:21:52,521 Epoch[292/310], Step[0250/1251], Loss: 3.3086(3.5789), Acc: 0.6172(0.3886)
2022-01-07 16:22:50,774 Epoch[292/310], Step[0300/1251], Loss: 3.5344(3.5836), Acc: 0.5693(0.3898)
2022-01-07 16:23:50,236 Epoch[292/310], Step[0350/1251], Loss: 3.1532(3.5727), Acc: 0.5801(0.3890)
2022-01-07 16:24:48,419 Epoch[292/310], Step[0400/1251], Loss: 3.6321(3.5693), Acc: 0.3340(0.3888)
2022-01-07 16:25:46,468 Epoch[292/310], Step[0450/1251], Loss: 3.2948(3.5620), Acc: 0.3535(0.3886)
2022-01-07 16:26:44,692 Epoch[292/310], Step[0500/1251], Loss: 3.8622(3.5547), Acc: 0.3691(0.3880)
2022-01-07 16:27:43,546 Epoch[292/310], Step[0550/1251], Loss: 3.4787(3.5539), Acc: 0.3193(0.3885)
2022-01-07 16:28:43,154 Epoch[292/310], Step[0600/1251], Loss: 3.7482(3.5604), Acc: 0.4629(0.3878)
2022-01-07 16:29:43,854 Epoch[292/310], Step[0650/1251], Loss: 3.6013(3.5610), Acc: 0.4307(0.3861)
2022-01-07 16:30:41,242 Epoch[292/310], Step[0700/1251], Loss: 3.1914(3.5624), Acc: 0.4414(0.3859)
2022-01-07 16:31:40,985 Epoch[292/310], Step[0750/1251], Loss: 3.3941(3.5640), Acc: 0.5820(0.3872)
2022-01-07 16:32:39,179 Epoch[292/310], Step[0800/1251], Loss: 3.8859(3.5611), Acc: 0.4717(0.3896)
2022-01-07 16:33:38,499 Epoch[292/310], Step[0850/1251], Loss: 3.9531(3.5563), Acc: 0.3047(0.3874)
2022-01-07 16:34:37,432 Epoch[292/310], Step[0900/1251], Loss: 3.3042(3.5572), Acc: 0.3945(0.3868)
2022-01-07 16:35:38,223 Epoch[292/310], Step[0950/1251], Loss: 3.5083(3.5554), Acc: 0.4629(0.3875)
2022-01-07 16:36:37,518 Epoch[292/310], Step[1000/1251], Loss: 3.0659(3.5574), Acc: 0.5518(0.3881)
2022-01-07 16:37:38,054 Epoch[292/310], Step[1050/1251], Loss: 3.7119(3.5575), Acc: 0.4238(0.3879)
2022-01-07 16:38:36,639 Epoch[292/310], Step[1100/1251], Loss: 3.6111(3.5568), Acc: 0.4990(0.3885)
2022-01-07 16:39:36,763 Epoch[292/310], Step[1150/1251], Loss: 3.5664(3.5574), Acc: 0.4795(0.3885)
2022-01-07 16:40:37,109 Epoch[292/310], Step[1200/1251], Loss: 3.6978(3.5553), Acc: 0.3574(0.3880)
2022-01-07 16:41:37,680 Epoch[292/310], Step[1250/1251], Loss: 3.6235(3.5560), Acc: 0.3359(0.3879)
2022-01-07 16:41:39,458 ----- Validation after Epoch: 292
2022-01-07 16:42:43,994 Val Step[0000/1563], Loss: 0.6498 (0.6498), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 16:42:45,433 Val Step[0050/1563], Loss: 2.1907 (0.7084), Acc@1: 0.4375 (0.8578), Acc@5: 0.8750 (0.9638)
2022-01-07 16:42:46,794 Val Step[0100/1563], Loss: 1.8263 (0.9651), Acc@1: 0.5312 (0.7862), Acc@5: 0.8750 (0.9421)
2022-01-07 16:42:48,249 Val Step[0150/1563], Loss: 0.4029 (0.9140), Acc@1: 0.9375 (0.7993), Acc@5: 1.0000 (0.9456)
2022-01-07 16:42:49,745 Val Step[0200/1563], Loss: 0.9404 (0.9199), Acc@1: 0.8125 (0.8022), Acc@5: 0.9375 (0.9448)
2022-01-07 16:42:51,163 Val Step[0250/1563], Loss: 0.5152 (0.8727), Acc@1: 0.9062 (0.8136), Acc@5: 1.0000 (0.9497)
2022-01-07 16:42:52,426 Val Step[0300/1563], Loss: 1.0165 (0.9313), Acc@1: 0.7188 (0.7982), Acc@5: 1.0000 (0.9454)
2022-01-07 16:42:53,694 Val Step[0350/1563], Loss: 0.8750 (0.9397), Acc@1: 0.8125 (0.7939), Acc@5: 0.9062 (0.9463)
2022-01-07 16:42:54,990 Val Step[0400/1563], Loss: 0.8469 (0.9476), Acc@1: 0.8438 (0.7883), Acc@5: 0.9688 (0.9465)
2022-01-07 16:42:56,298 Val Step[0450/1563], Loss: 0.8896 (0.9542), Acc@1: 0.7500 (0.7862), Acc@5: 1.0000 (0.9475)
2022-01-07 16:42:57,606 Val Step[0500/1563], Loss: 0.4395 (0.9464), Acc@1: 0.9375 (0.7884), Acc@5: 1.0000 (0.9484)
2022-01-07 16:42:58,984 Val Step[0550/1563], Loss: 0.7618 (0.9257), Acc@1: 0.8438 (0.7934), Acc@5: 0.9688 (0.9500)
2022-01-07 16:43:00,328 Val Step[0600/1563], Loss: 0.7520 (0.9333), Acc@1: 0.8438 (0.7925), Acc@5: 0.9375 (0.9494)
2022-01-07 16:43:01,602 Val Step[0650/1563], Loss: 0.5201 (0.9530), Acc@1: 0.9062 (0.7881), Acc@5: 1.0000 (0.9467)
2022-01-07 16:43:02,957 Val Step[0700/1563], Loss: 0.9392 (0.9793), Acc@1: 0.8125 (0.7817), Acc@5: 0.9688 (0.9437)
2022-01-07 16:43:04,228 Val Step[0750/1563], Loss: 1.1994 (1.0097), Acc@1: 0.8438 (0.7754), Acc@5: 0.9062 (0.9396)
2022-01-07 16:43:05,483 Val Step[0800/1563], Loss: 0.6377 (1.0450), Acc@1: 0.8750 (0.7667), Acc@5: 1.0000 (0.9352)
2022-01-07 16:43:06,756 Val Step[0850/1563], Loss: 1.2396 (1.0695), Acc@1: 0.6875 (0.7603), Acc@5: 0.9375 (0.9324)
2022-01-07 16:43:08,017 Val Step[0900/1563], Loss: 0.2777 (1.0694), Acc@1: 0.9688 (0.7615), Acc@5: 1.0000 (0.9319)
2022-01-07 16:43:09,382 Val Step[0950/1563], Loss: 1.2069 (1.0879), Acc@1: 0.7500 (0.7577), Acc@5: 0.9062 (0.9287)
2022-01-07 16:43:10,649 Val Step[1000/1563], Loss: 0.5668 (1.1107), Acc@1: 0.9688 (0.7517), Acc@5: 0.9688 (0.9255)
2022-01-07 16:43:11,920 Val Step[1050/1563], Loss: 0.3434 (1.1242), Acc@1: 0.9688 (0.7485), Acc@5: 1.0000 (0.9241)
2022-01-07 16:43:13,234 Val Step[1100/1563], Loss: 0.6823 (1.1367), Acc@1: 0.8750 (0.7458), Acc@5: 1.0000 (0.9223)
2022-01-07 16:43:14,664 Val Step[1150/1563], Loss: 1.2479 (1.1501), Acc@1: 0.7812 (0.7433), Acc@5: 0.8125 (0.9204)
2022-01-07 16:43:16,063 Val Step[1200/1563], Loss: 1.2336 (1.1634), Acc@1: 0.7812 (0.7402), Acc@5: 0.8438 (0.9182)
2022-01-07 16:43:17,362 Val Step[1250/1563], Loss: 0.7343 (1.1740), Acc@1: 0.8750 (0.7384), Acc@5: 0.9375 (0.9167)
2022-01-07 16:43:18,619 Val Step[1300/1563], Loss: 0.7984 (1.1823), Acc@1: 0.9062 (0.7367), Acc@5: 0.9375 (0.9156)
2022-01-07 16:43:19,898 Val Step[1350/1563], Loss: 1.8542 (1.1987), Acc@1: 0.5000 (0.7327), Acc@5: 0.8125 (0.9131)
2022-01-07 16:43:21,188 Val Step[1400/1563], Loss: 1.0041 (1.2051), Acc@1: 0.7500 (0.7312), Acc@5: 0.9375 (0.9123)
2022-01-07 16:43:22,448 Val Step[1450/1563], Loss: 1.4034 (1.2111), Acc@1: 0.7500 (0.7294), Acc@5: 0.9062 (0.9120)
2022-01-07 16:43:23,730 Val Step[1500/1563], Loss: 1.6368 (1.2013), Acc@1: 0.6875 (0.7317), Acc@5: 0.8438 (0.9133)
2022-01-07 16:43:24,995 Val Step[1550/1563], Loss: 0.8594 (1.2035), Acc@1: 0.8750 (0.7307), Acc@5: 0.9062 (0.9128)
2022-01-07 16:43:25,736 ----- Epoch[292/310], Validation Loss: 1.2020, Validation Acc@1: 0.7310, Validation Acc@5: 0.9131, time: 106.28
2022-01-07 16:43:25,737 ----- Epoch[292/310], Train Loss: 3.5560, Train Acc: 0.3879, time: 1560.56, Best Val(epoch292) Acc@1: 0.7310
2022-01-07 16:43:25,927 Max accuracy so far: 0.7310 at epoch_292
2022-01-07 16:43:25,928 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 16:43:25,928 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 16:43:26,019 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 16:43:26,452 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 16:43:26,452 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 16:43:26,539 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 16:43:26,539 Now training epoch 293. LR=0.000006
2022-01-07 16:44:44,105 Epoch[293/310], Step[0000/1251], Loss: 3.7430(3.7430), Acc: 0.3496(0.3496)
2022-01-07 16:45:42,533 Epoch[293/310], Step[0050/1251], Loss: 3.8593(3.5717), Acc: 0.3037(0.4092)
2022-01-07 16:46:40,332 Epoch[293/310], Step[0100/1251], Loss: 3.3259(3.5441), Acc: 0.3848(0.3974)
2022-01-07 16:47:39,752 Epoch[293/310], Step[0150/1251], Loss: 3.7405(3.5465), Acc: 0.4463(0.3986)
2022-01-07 16:48:39,496 Epoch[293/310], Step[0200/1251], Loss: 3.8076(3.5527), Acc: 0.2920(0.3957)
2022-01-07 16:49:39,128 Epoch[293/310], Step[0250/1251], Loss: 3.5815(3.5373), Acc: 0.4355(0.3975)
2022-01-07 16:50:37,415 Epoch[293/310], Step[0300/1251], Loss: 3.4161(3.5480), Acc: 0.2656(0.3970)
2022-01-07 16:51:37,247 Epoch[293/310], Step[0350/1251], Loss: 3.9868(3.5551), Acc: 0.3438(0.3908)
2022-01-07 16:52:36,519 Epoch[293/310], Step[0400/1251], Loss: 3.0791(3.5498), Acc: 0.6172(0.3899)
2022-01-07 16:53:36,513 Epoch[293/310], Step[0450/1251], Loss: 3.5405(3.5529), Acc: 0.3428(0.3875)
2022-01-07 16:54:35,935 Epoch[293/310], Step[0500/1251], Loss: 3.3991(3.5507), Acc: 0.3604(0.3843)
2022-01-07 16:55:34,266 Epoch[293/310], Step[0550/1251], Loss: 3.7844(3.5567), Acc: 0.3721(0.3839)
2022-01-07 16:56:33,143 Epoch[293/310], Step[0600/1251], Loss: 3.9014(3.5513), Acc: 0.4170(0.3840)
2022-01-07 16:57:30,816 Epoch[293/310], Step[0650/1251], Loss: 2.9861(3.5505), Acc: 0.4287(0.3847)
2022-01-07 16:58:30,153 Epoch[293/310], Step[0700/1251], Loss: 3.7898(3.5490), Acc: 0.3662(0.3850)
2022-01-07 16:59:29,178 Epoch[293/310], Step[0750/1251], Loss: 3.7151(3.5476), Acc: 0.3486(0.3847)
2022-01-07 17:00:29,268 Epoch[293/310], Step[0800/1251], Loss: 3.6106(3.5505), Acc: 0.3330(0.3826)
2022-01-07 17:01:29,279 Epoch[293/310], Step[0850/1251], Loss: 3.2389(3.5531), Acc: 0.4551(0.3820)
2022-01-07 17:02:29,652 Epoch[293/310], Step[0900/1251], Loss: 3.1485(3.5574), Acc: 0.4492(0.3809)
2022-01-07 17:03:29,547 Epoch[293/310], Step[0950/1251], Loss: 3.3809(3.5562), Acc: 0.5752(0.3812)
2022-01-07 17:04:29,487 Epoch[293/310], Step[1000/1251], Loss: 3.4084(3.5552), Acc: 0.0938(0.3818)
2022-01-07 17:05:29,150 Epoch[293/310], Step[1050/1251], Loss: 3.0909(3.5554), Acc: 0.2441(0.3821)
2022-01-07 17:06:29,270 Epoch[293/310], Step[1100/1251], Loss: 3.3916(3.5552), Acc: 0.5283(0.3825)
2022-01-07 17:07:28,608 Epoch[293/310], Step[1150/1251], Loss: 3.6715(3.5590), Acc: 0.4395(0.3822)
2022-01-07 17:08:28,558 Epoch[293/310], Step[1200/1251], Loss: 3.8449(3.5579), Acc: 0.3682(0.3822)
2022-01-07 17:09:26,960 Epoch[293/310], Step[1250/1251], Loss: 3.3405(3.5564), Acc: 0.2676(0.3822)
2022-01-07 17:09:28,429 ----- Epoch[293/310], Train Loss: 3.5564, Train Acc: 0.3822, time: 1561.89, Best Val(epoch292) Acc@1: 0.7310
2022-01-07 17:09:28,607 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 17:09:28,607 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 17:09:28,716 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 17:09:28,716 Now training epoch 294. LR=0.000006
2022-01-07 17:10:51,276 Epoch[294/310], Step[0000/1251], Loss: 3.4314(3.4314), Acc: 0.4180(0.4180)
2022-01-07 17:11:50,038 Epoch[294/310], Step[0050/1251], Loss: 4.0463(3.5814), Acc: 0.2871(0.4004)
2022-01-07 17:12:50,553 Epoch[294/310], Step[0100/1251], Loss: 2.7923(3.5494), Acc: 0.4844(0.3898)
2022-01-07 17:13:49,887 Epoch[294/310], Step[0150/1251], Loss: 3.5750(3.5367), Acc: 0.2500(0.3896)
2022-01-07 17:14:50,117 Epoch[294/310], Step[0200/1251], Loss: 3.5711(3.5483), Acc: 0.5039(0.3839)
2022-01-07 17:15:49,880 Epoch[294/310], Step[0250/1251], Loss: 3.0480(3.5384), Acc: 0.4688(0.3845)
2022-01-07 17:16:49,536 Epoch[294/310], Step[0300/1251], Loss: 3.7158(3.5389), Acc: 0.2822(0.3842)
2022-01-07 17:17:50,167 Epoch[294/310], Step[0350/1251], Loss: 4.1071(3.5344), Acc: 0.2822(0.3835)
2022-01-07 17:18:51,263 Epoch[294/310], Step[0400/1251], Loss: 3.9442(3.5365), Acc: 0.3516(0.3851)
2022-01-07 17:19:51,032 Epoch[294/310], Step[0450/1251], Loss: 3.2445(3.5254), Acc: 0.2676(0.3893)
2022-01-07 17:20:52,138 Epoch[294/310], Step[0500/1251], Loss: 3.7159(3.5321), Acc: 0.3867(0.3890)
2022-01-07 17:21:52,501 Epoch[294/310], Step[0550/1251], Loss: 3.4374(3.5337), Acc: 0.2490(0.3875)
2022-01-07 17:22:53,331 Epoch[294/310], Step[0600/1251], Loss: 3.2607(3.5367), Acc: 0.5195(0.3882)
2022-01-07 17:23:53,693 Epoch[294/310], Step[0650/1251], Loss: 3.7337(3.5428), Acc: 0.3887(0.3878)
2022-01-07 17:24:52,835 Epoch[294/310], Step[0700/1251], Loss: 3.8564(3.5453), Acc: 0.2432(0.3871)
2022-01-07 17:25:53,661 Epoch[294/310], Step[0750/1251], Loss: 3.6334(3.5424), Acc: 0.3516(0.3870)
2022-01-07 17:26:53,310 Epoch[294/310], Step[0800/1251], Loss: 3.2774(3.5413), Acc: 0.5605(0.3866)
2022-01-07 17:27:51,660 Epoch[294/310], Step[0850/1251], Loss: 3.4924(3.5356), Acc: 0.5068(0.3872)
2022-01-07 17:28:49,735 Epoch[294/310], Step[0900/1251], Loss: 3.3681(3.5347), Acc: 0.5273(0.3869)
2022-01-07 17:29:46,541 Epoch[294/310], Step[0950/1251], Loss: 3.6994(3.5385), Acc: 0.4482(0.3866)
2022-01-07 17:30:45,012 Epoch[294/310], Step[1000/1251], Loss: 3.9962(3.5371), Acc: 0.4561(0.3866)
2022-01-07 17:31:44,480 Epoch[294/310], Step[1050/1251], Loss: 3.4151(3.5365), Acc: 0.3037(0.3871)
2022-01-07 17:32:43,457 Epoch[294/310], Step[1100/1251], Loss: 3.2014(3.5359), Acc: 0.5791(0.3873)
2022-01-07 17:33:43,327 Epoch[294/310], Step[1150/1251], Loss: 4.0427(3.5391), Acc: 0.2744(0.3866)
2022-01-07 17:34:43,171 Epoch[294/310], Step[1200/1251], Loss: 3.4556(3.5380), Acc: 0.3574(0.3871)
2022-01-07 17:35:42,994 Epoch[294/310], Step[1250/1251], Loss: 3.3657(3.5383), Acc: 0.4092(0.3876)
2022-01-07 17:35:44,918 ----- Validation after Epoch: 294
2022-01-07 18:01:39,893 Val Step[0000/1563], Loss: 0.6609 (0.6609), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 18:01:41,419 Val Step[0050/1563], Loss: 2.1430 (0.7130), Acc@1: 0.4062 (0.8548), Acc@5: 0.8750 (0.9645)
2022-01-07 18:01:42,901 Val Step[0100/1563], Loss: 1.9277 (0.9710), Acc@1: 0.5000 (0.7853), Acc@5: 0.8438 (0.9403)
2022-01-07 18:01:44,389 Val Step[0150/1563], Loss: 0.3770 (0.9196), Acc@1: 0.9688 (0.7997), Acc@5: 1.0000 (0.9450)
2022-01-07 18:01:45,846 Val Step[0200/1563], Loss: 0.9127 (0.9260), Acc@1: 0.8125 (0.8021), Acc@5: 0.9375 (0.9434)
2022-01-07 18:01:47,275 Val Step[0250/1563], Loss: 0.4762 (0.8786), Acc@1: 0.9062 (0.8129), Acc@5: 1.0000 (0.9480)
2022-01-07 18:01:48,654 Val Step[0300/1563], Loss: 1.0913 (0.9349), Acc@1: 0.6875 (0.7973), Acc@5: 1.0000 (0.9438)
2022-01-07 18:01:50,060 Val Step[0350/1563], Loss: 0.9126 (0.9413), Acc@1: 0.7812 (0.7930), Acc@5: 0.9062 (0.9455)
2022-01-07 18:01:51,506 Val Step[0400/1563], Loss: 0.8858 (0.9491), Acc@1: 0.8125 (0.7869), Acc@5: 0.9688 (0.9456)
2022-01-07 18:01:53,131 Val Step[0450/1563], Loss: 0.8967 (0.9559), Acc@1: 0.7500 (0.7847), Acc@5: 1.0000 (0.9466)
2022-01-07 18:01:54,687 Val Step[0500/1563], Loss: 0.4152 (0.9471), Acc@1: 0.9375 (0.7874), Acc@5: 1.0000 (0.9478)
2022-01-07 18:01:56,243 Val Step[0550/1563], Loss: 0.7502 (0.9264), Acc@1: 0.8438 (0.7925), Acc@5: 0.9688 (0.9497)
2022-01-07 18:01:57,812 Val Step[0600/1563], Loss: 0.7625 (0.9344), Acc@1: 0.8438 (0.7911), Acc@5: 0.9375 (0.9489)
2022-01-07 18:01:59,238 Val Step[0650/1563], Loss: 0.5228 (0.9531), Acc@1: 0.9062 (0.7870), Acc@5: 1.0000 (0.9463)
2022-01-07 18:02:00,869 Val Step[0700/1563], Loss: 0.9414 (0.9792), Acc@1: 0.8125 (0.7803), Acc@5: 0.9688 (0.9437)
2022-01-07 18:02:02,313 Val Step[0750/1563], Loss: 1.2169 (1.0098), Acc@1: 0.8438 (0.7737), Acc@5: 0.9062 (0.9397)
2022-01-07 18:02:03,815 Val Step[0800/1563], Loss: 0.6234 (1.0447), Acc@1: 0.8750 (0.7650), Acc@5: 1.0000 (0.9352)
2022-01-07 18:02:05,390 Val Step[0850/1563], Loss: 1.2011 (1.0684), Acc@1: 0.6875 (0.7589), Acc@5: 0.9375 (0.9324)
2022-01-07 18:02:07,046 Val Step[0900/1563], Loss: 0.2590 (1.0677), Acc@1: 0.9688 (0.7602), Acc@5: 1.0000 (0.9320)
2022-01-07 18:02:08,569 Val Step[0950/1563], Loss: 1.2555 (1.0867), Acc@1: 0.7812 (0.7565), Acc@5: 0.8750 (0.9288)
2022-01-07 18:02:09,969 Val Step[1000/1563], Loss: 0.5548 (1.1092), Acc@1: 0.9688 (0.7506), Acc@5: 1.0000 (0.9258)
2022-01-07 18:02:11,574 Val Step[1050/1563], Loss: 0.3341 (1.1229), Acc@1: 0.9688 (0.7474), Acc@5: 1.0000 (0.9243)
2022-01-07 18:02:13,028 Val Step[1100/1563], Loss: 0.6888 (1.1362), Acc@1: 0.8750 (0.7449), Acc@5: 1.0000 (0.9227)
2022-01-07 18:02:14,536 Val Step[1150/1563], Loss: 1.2035 (1.1497), Acc@1: 0.7812 (0.7421), Acc@5: 0.8438 (0.9208)
2022-01-07 18:02:16,064 Val Step[1200/1563], Loss: 1.1989 (1.1626), Acc@1: 0.7812 (0.7389), Acc@5: 0.8438 (0.9187)
2022-01-07 18:02:17,699 Val Step[1250/1563], Loss: 0.7165 (1.1733), Acc@1: 0.8750 (0.7371), Acc@5: 0.9375 (0.9169)
2022-01-07 18:02:19,144 Val Step[1300/1563], Loss: 0.7811 (1.1811), Acc@1: 0.8750 (0.7356), Acc@5: 0.9375 (0.9158)
2022-01-07 18:02:20,549 Val Step[1350/1563], Loss: 1.6580 (1.1968), Acc@1: 0.5625 (0.7317), Acc@5: 0.8750 (0.9136)
2022-01-07 18:02:22,087 Val Step[1400/1563], Loss: 0.9865 (1.2030), Acc@1: 0.7500 (0.7303), Acc@5: 0.9375 (0.9129)
2022-01-07 18:02:23,537 Val Step[1450/1563], Loss: 1.3547 (1.2096), Acc@1: 0.7500 (0.7283), Acc@5: 0.9375 (0.9125)
2022-01-07 18:02:24,938 Val Step[1500/1563], Loss: 1.5217 (1.1994), Acc@1: 0.6875 (0.7308), Acc@5: 0.8750 (0.9138)
2022-01-07 18:02:26,454 Val Step[1550/1563], Loss: 0.8689 (1.2017), Acc@1: 0.8750 (0.7299), Acc@5: 0.9062 (0.9135)
2022-01-07 18:02:28,160 ----- Epoch[294/310], Validation Loss: 1.2002, Validation Acc@1: 0.7302, Validation Acc@5: 0.9137, time: 1603.24
2022-01-07 18:02:28,161 ----- Epoch[294/310], Train Loss: 3.5383, Train Acc: 0.3876, time: 1576.20, Best Val(epoch292) Acc@1: 0.7310
2022-01-07 18:02:28,376 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 18:02:28,377 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 18:02:28,620 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 18:02:28,620 Now training epoch 295. LR=0.000006
2022-01-07 18:04:08,398 Epoch[295/310], Step[0000/1251], Loss: 3.7807(3.7807), Acc: 0.4561(0.4561)
2022-01-07 18:05:12,540 Epoch[295/310], Step[0050/1251], Loss: 3.9425(3.5485), Acc: 0.4170(0.3819)
2022-01-07 18:06:15,583 Epoch[295/310], Step[0100/1251], Loss: 3.5101(3.5370), Acc: 0.3740(0.3954)
2022-01-07 18:07:20,087 Epoch[295/310], Step[0150/1251], Loss: 3.7127(3.5480), Acc: 0.1670(0.3880)
2022-01-07 18:08:24,234 Epoch[295/310], Step[0200/1251], Loss: 3.7286(3.5414), Acc: 0.3408(0.3861)
2022-01-07 18:09:28,303 Epoch[295/310], Step[0250/1251], Loss: 3.7658(3.5422), Acc: 0.3359(0.3873)
2022-01-07 18:10:31,956 Epoch[295/310], Step[0300/1251], Loss: 3.9873(3.5506), Acc: 0.2588(0.3864)
2022-01-07 18:11:36,323 Epoch[295/310], Step[0350/1251], Loss: 3.6284(3.5487), Acc: 0.4814(0.3816)
2022-01-07 18:12:39,882 Epoch[295/310], Step[0400/1251], Loss: 3.7496(3.5409), Acc: 0.4873(0.3804)
2022-01-07 18:13:42,633 Epoch[295/310], Step[0450/1251], Loss: 3.3921(3.5390), Acc: 0.5303(0.3800)
2022-01-07 18:14:43,909 Epoch[295/310], Step[0500/1251], Loss: 3.7122(3.5338), Acc: 0.1689(0.3829)
2022-01-07 18:15:47,649 Epoch[295/310], Step[0550/1251], Loss: 3.1327(3.5382), Acc: 0.5498(0.3826)
2022-01-07 18:16:50,278 Epoch[295/310], Step[0600/1251], Loss: 3.8635(3.5418), Acc: 0.3271(0.3859)
2022-01-07 18:17:52,304 Epoch[295/310], Step[0650/1251], Loss: 3.3927(3.5432), Acc: 0.4316(0.3860)
2022-01-07 18:18:54,004 Epoch[295/310], Step[0700/1251], Loss: 3.9841(3.5487), Acc: 0.3506(0.3843)
2022-01-07 18:19:56,231 Epoch[295/310], Step[0750/1251], Loss: 3.2564(3.5489), Acc: 0.5498(0.3845)
2022-01-07 18:20:58,880 Epoch[295/310], Step[0800/1251], Loss: 3.5584(3.5490), Acc: 0.5430(0.3840)
2022-01-07 18:22:01,306 Epoch[295/310], Step[0850/1251], Loss: 3.6559(3.5499), Acc: 0.2871(0.3816)
2022-01-07 18:23:05,081 Epoch[295/310], Step[0900/1251], Loss: 3.7238(3.5509), Acc: 0.4629(0.3807)
2022-01-07 18:24:08,684 Epoch[295/310], Step[0950/1251], Loss: 3.3730(3.5510), Acc: 0.5498(0.3819)
2022-01-07 18:25:12,647 Epoch[295/310], Step[1000/1251], Loss: 3.3183(3.5517), Acc: 0.2432(0.3837)
2022-01-07 18:26:15,705 Epoch[295/310], Step[1050/1251], Loss: 4.2663(3.5551), Acc: 0.3662(0.3840)
2022-01-07 18:27:19,046 Epoch[295/310], Step[1100/1251], Loss: 3.4717(3.5559), Acc: 0.3223(0.3850)
2022-01-07 18:28:19,601 Epoch[295/310], Step[1150/1251], Loss: 3.4464(3.5541), Acc: 0.2930(0.3861)
2022-01-07 18:29:20,247 Epoch[295/310], Step[1200/1251], Loss: 3.0751(3.5510), Acc: 0.6367(0.3863)
2022-01-07 18:30:21,466 Epoch[295/310], Step[1250/1251], Loss: 3.6141(3.5526), Acc: 0.4551(0.3852)
2022-01-07 18:30:24,125 ----- Epoch[295/310], Train Loss: 3.5526, Train Acc: 0.3852, time: 1675.50, Best Val(epoch292) Acc@1: 0.7310
2022-01-07 18:30:24,385 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 18:30:24,385 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 18:30:24,436 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 18:30:24,437 Now training epoch 296. LR=0.000005
2022-01-07 18:31:49,367 Epoch[296/310], Step[0000/1251], Loss: 3.9642(3.9642), Acc: 0.4053(0.4053)
2022-01-07 18:32:49,511 Epoch[296/310], Step[0050/1251], Loss: 3.3625(3.5888), Acc: 0.5928(0.3945)
2022-01-07 18:33:47,302 Epoch[296/310], Step[0100/1251], Loss: 3.3340(3.5646), Acc: 0.4893(0.4011)
2022-01-07 18:34:46,297 Epoch[296/310], Step[0150/1251], Loss: 3.9277(3.5606), Acc: 0.2705(0.4036)
2022-01-07 18:35:44,836 Epoch[296/310], Step[0200/1251], Loss: 3.3226(3.5586), Acc: 0.5947(0.3992)
2022-01-07 18:36:43,106 Epoch[296/310], Step[0250/1251], Loss: 3.5163(3.5640), Acc: 0.3564(0.3941)
2022-01-07 18:37:41,624 Epoch[296/310], Step[0300/1251], Loss: 3.6942(3.5554), Acc: 0.4414(0.3949)
2022-01-07 18:38:40,170 Epoch[296/310], Step[0350/1251], Loss: 3.4181(3.5560), Acc: 0.3379(0.3954)
2022-01-07 18:39:38,956 Epoch[296/310], Step[0400/1251], Loss: 2.7676(3.5494), Acc: 0.2822(0.3942)
2022-01-07 18:40:38,455 Epoch[296/310], Step[0450/1251], Loss: 3.2146(3.5436), Acc: 0.4365(0.3914)
2022-01-07 18:41:39,060 Epoch[296/310], Step[0500/1251], Loss: 3.6676(3.5456), Acc: 0.3213(0.3902)
2022-01-07 18:42:37,915 Epoch[296/310], Step[0550/1251], Loss: 3.1521(3.5422), Acc: 0.5898(0.3899)
2022-01-07 18:43:37,876 Epoch[296/310], Step[0600/1251], Loss: 3.5263(3.5395), Acc: 0.3467(0.3881)
2022-01-07 18:44:38,040 Epoch[296/310], Step[0650/1251], Loss: 3.5551(3.5380), Acc: 0.4121(0.3885)
2022-01-07 18:45:38,329 Epoch[296/310], Step[0700/1251], Loss: 3.2884(3.5381), Acc: 0.5430(0.3897)
2022-01-07 18:46:38,481 Epoch[296/310], Step[0750/1251], Loss: 3.9370(3.5345), Acc: 0.3936(0.3907)
2022-01-07 18:47:37,050 Epoch[296/310], Step[0800/1251], Loss: 4.0588(3.5389), Acc: 0.2773(0.3904)
2022-01-07 18:48:36,455 Epoch[296/310], Step[0850/1251], Loss: 3.3220(3.5388), Acc: 0.4658(0.3903)
2022-01-07 18:49:36,158 Epoch[296/310], Step[0900/1251], Loss: 3.4009(3.5398), Acc: 0.1855(0.3903)
2022-01-07 18:50:35,938 Epoch[296/310], Step[0950/1251], Loss: 3.4396(3.5371), Acc: 0.3008(0.3910)
2022-01-07 18:51:35,675 Epoch[296/310], Step[1000/1251], Loss: 3.6590(3.5392), Acc: 0.4570(0.3916)
2022-01-07 18:52:36,257 Epoch[296/310], Step[1050/1251], Loss: 3.4114(3.5407), Acc: 0.4961(0.3902)
2022-01-07 18:53:35,831 Epoch[296/310], Step[1100/1251], Loss: 4.3481(3.5449), Acc: 0.2480(0.3893)
2022-01-07 18:54:36,198 Epoch[296/310], Step[1150/1251], Loss: 3.3191(3.5469), Acc: 0.4658(0.3887)
2022-01-07 18:55:36,763 Epoch[296/310], Step[1200/1251], Loss: 3.5994(3.5454), Acc: 0.3584(0.3903)
2022-01-07 18:56:36,218 Epoch[296/310], Step[1250/1251], Loss: 3.6298(3.5424), Acc: 0.3887(0.3911)
2022-01-07 18:56:37,724 ----- Validation after Epoch: 296
2022-01-07 18:57:50,190 Val Step[0000/1563], Loss: 0.6733 (0.6733), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 18:57:51,504 Val Step[0050/1563], Loss: 2.2115 (0.7089), Acc@1: 0.4062 (0.8517), Acc@5: 0.8750 (0.9620)
2022-01-07 18:57:52,774 Val Step[0100/1563], Loss: 1.8496 (0.9646), Acc@1: 0.5000 (0.7812), Acc@5: 0.8438 (0.9384)
2022-01-07 18:57:54,043 Val Step[0150/1563], Loss: 0.3679 (0.9157), Acc@1: 0.9688 (0.7972), Acc@5: 1.0000 (0.9427)
2022-01-07 18:57:55,308 Val Step[0200/1563], Loss: 0.9276 (0.9235), Acc@1: 0.8125 (0.8007), Acc@5: 0.9375 (0.9415)
2022-01-07 18:57:56,583 Val Step[0250/1563], Loss: 0.4787 (0.8761), Acc@1: 0.9062 (0.8118), Acc@5: 1.0000 (0.9471)
2022-01-07 18:57:57,933 Val Step[0300/1563], Loss: 1.0801 (0.9321), Acc@1: 0.6875 (0.7957), Acc@5: 1.0000 (0.9432)
2022-01-07 18:57:59,298 Val Step[0350/1563], Loss: 0.9142 (0.9388), Acc@1: 0.7812 (0.7915), Acc@5: 0.9062 (0.9449)
2022-01-07 18:58:00,584 Val Step[0400/1563], Loss: 0.8481 (0.9465), Acc@1: 0.8125 (0.7862), Acc@5: 0.9688 (0.9447)
2022-01-07 18:58:01,963 Val Step[0450/1563], Loss: 0.9074 (0.9522), Acc@1: 0.7500 (0.7840), Acc@5: 1.0000 (0.9456)
2022-01-07 18:58:03,277 Val Step[0500/1563], Loss: 0.4107 (0.9441), Acc@1: 0.9062 (0.7863), Acc@5: 1.0000 (0.9467)
2022-01-07 18:58:04,747 Val Step[0550/1563], Loss: 0.7473 (0.9234), Acc@1: 0.8438 (0.7914), Acc@5: 0.9688 (0.9484)
2022-01-07 18:58:06,042 Val Step[0600/1563], Loss: 0.7505 (0.9312), Acc@1: 0.8438 (0.7902), Acc@5: 0.9375 (0.9477)
2022-01-07 18:58:07,311 Val Step[0650/1563], Loss: 0.5192 (0.9508), Acc@1: 0.8750 (0.7858), Acc@5: 1.0000 (0.9450)
2022-01-07 18:58:08,563 Val Step[0700/1563], Loss: 0.9807 (0.9771), Acc@1: 0.8125 (0.7794), Acc@5: 0.9688 (0.9421)
2022-01-07 18:58:09,853 Val Step[0750/1563], Loss: 1.2038 (1.0085), Acc@1: 0.8125 (0.7730), Acc@5: 0.9062 (0.9381)
2022-01-07 18:58:11,204 Val Step[0800/1563], Loss: 0.6071 (1.0437), Acc@1: 0.8750 (0.7645), Acc@5: 1.0000 (0.9338)
2022-01-07 18:58:12,469 Val Step[0850/1563], Loss: 1.1901 (1.0676), Acc@1: 0.6875 (0.7583), Acc@5: 0.9375 (0.9309)
2022-01-07 18:58:13,726 Val Step[0900/1563], Loss: 0.2517 (1.0662), Acc@1: 0.9688 (0.7598), Acc@5: 1.0000 (0.9306)
2022-01-07 18:58:15,134 Val Step[0950/1563], Loss: 1.2306 (1.0860), Acc@1: 0.7812 (0.7561), Acc@5: 0.8750 (0.9273)
2022-01-07 18:58:16,418 Val Step[1000/1563], Loss: 0.5913 (1.1087), Acc@1: 0.9375 (0.7503), Acc@5: 0.9688 (0.9243)
2022-01-07 18:58:17,810 Val Step[1050/1563], Loss: 0.3048 (1.1219), Acc@1: 0.9688 (0.7473), Acc@5: 1.0000 (0.9229)
2022-01-07 18:58:19,119 Val Step[1100/1563], Loss: 0.7338 (1.1351), Acc@1: 0.8438 (0.7449), Acc@5: 0.9375 (0.9210)
2022-01-07 18:58:20,415 Val Step[1150/1563], Loss: 1.2414 (1.1481), Acc@1: 0.7812 (0.7424), Acc@5: 0.8438 (0.9193)
2022-01-07 18:58:21,812 Val Step[1200/1563], Loss: 1.2265 (1.1612), Acc@1: 0.7500 (0.7393), Acc@5: 0.8438 (0.9170)
2022-01-07 18:58:23,071 Val Step[1250/1563], Loss: 0.7044 (1.1721), Acc@1: 0.8750 (0.7377), Acc@5: 0.9375 (0.9152)
2022-01-07 18:58:24,342 Val Step[1300/1563], Loss: 0.7719 (1.1802), Acc@1: 0.8750 (0.7360), Acc@5: 0.9375 (0.9141)
2022-01-07 18:58:25,584 Val Step[1350/1563], Loss: 1.7764 (1.1962), Acc@1: 0.5000 (0.7323), Acc@5: 0.8750 (0.9118)
2022-01-07 18:58:26,889 Val Step[1400/1563], Loss: 0.9526 (1.2023), Acc@1: 0.7500 (0.7310), Acc@5: 0.9375 (0.9111)
2022-01-07 18:58:28,271 Val Step[1450/1563], Loss: 1.3356 (1.2084), Acc@1: 0.7812 (0.7292), Acc@5: 0.9375 (0.9108)
2022-01-07 18:58:29,722 Val Step[1500/1563], Loss: 1.5949 (1.1984), Acc@1: 0.6562 (0.7316), Acc@5: 0.8750 (0.9122)
2022-01-07 18:58:31,145 Val Step[1550/1563], Loss: 0.8767 (1.2002), Acc@1: 0.8750 (0.7308), Acc@5: 0.9062 (0.9119)
2022-01-07 18:58:31,955 ----- Epoch[296/310], Validation Loss: 1.1985, Validation Acc@1: 0.7310, Validation Acc@5: 0.9122, time: 114.23
2022-01-07 18:58:31,955 ----- Epoch[296/310], Train Loss: 3.5424, Train Acc: 0.3911, time: 1573.28, Best Val(epoch296) Acc@1: 0.7310
2022-01-07 18:58:32,139 Max accuracy so far: 0.7310 at epoch_296
2022-01-07 18:58:32,140 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 18:58:32,140 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 18:58:32,569 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 18:58:32,722 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 18:58:32,723 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 18:58:32,830 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 18:58:32,831 Now training epoch 297. LR=0.000005
2022-01-07 19:00:01,827 Epoch[297/310], Step[0000/1251], Loss: 2.8336(2.8336), Acc: 0.6514(0.6514)
2022-01-07 19:00:59,956 Epoch[297/310], Step[0050/1251], Loss: 3.8639(3.5578), Acc: 0.3877(0.3849)
2022-01-07 19:01:59,091 Epoch[297/310], Step[0100/1251], Loss: 3.6664(3.5480), Acc: 0.2080(0.3769)
2022-01-07 19:02:57,307 Epoch[297/310], Step[0150/1251], Loss: 3.9178(3.5601), Acc: 0.3066(0.3790)
2022-01-07 19:03:57,245 Epoch[297/310], Step[0200/1251], Loss: 3.5410(3.5429), Acc: 0.5176(0.3839)
2022-01-07 19:04:56,517 Epoch[297/310], Step[0250/1251], Loss: 3.9024(3.5541), Acc: 0.2881(0.3775)
2022-01-07 19:05:56,439 Epoch[297/310], Step[0300/1251], Loss: 3.3273(3.5492), Acc: 0.5469(0.3805)
2022-01-07 19:06:56,849 Epoch[297/310], Step[0350/1251], Loss: 3.4933(3.5460), Acc: 0.4023(0.3772)
2022-01-07 19:07:57,167 Epoch[297/310], Step[0400/1251], Loss: 3.5716(3.5378), Acc: 0.4121(0.3756)
2022-01-07 19:08:58,088 Epoch[297/310], Step[0450/1251], Loss: 3.5367(3.5308), Acc: 0.3936(0.3749)
2022-01-07 19:09:58,501 Epoch[297/310], Step[0500/1251], Loss: 3.1971(3.5345), Acc: 0.5205(0.3771)
2022-01-07 19:10:59,619 Epoch[297/310], Step[0550/1251], Loss: 3.2867(3.5329), Acc: 0.2822(0.3767)
2022-01-07 19:12:00,123 Epoch[297/310], Step[0600/1251], Loss: 3.5195(3.5321), Acc: 0.5020(0.3775)
2022-01-07 19:13:00,189 Epoch[297/310], Step[0650/1251], Loss: 3.6774(3.5356), Acc: 0.3506(0.3794)
2022-01-07 19:14:00,807 Epoch[297/310], Step[0700/1251], Loss: 2.9717(3.5362), Acc: 0.6201(0.3815)
2022-01-07 19:15:01,716 Epoch[297/310], Step[0750/1251], Loss: 4.2084(3.5350), Acc: 0.2500(0.3824)
2022-01-07 19:16:00,526 Epoch[297/310], Step[0800/1251], Loss: 3.2574(3.5389), Acc: 0.5264(0.3831)
2022-01-07 19:16:59,633 Epoch[297/310], Step[0850/1251], Loss: 3.9462(3.5374), Acc: 0.4814(0.3833)
2022-01-07 19:17:59,037 Epoch[297/310], Step[0900/1251], Loss: 3.2901(3.5342), Acc: 0.1758(0.3831)
2022-01-07 19:18:57,760 Epoch[297/310], Step[0950/1251], Loss: 3.2855(3.5346), Acc: 0.3779(0.3844)
2022-01-07 19:19:57,448 Epoch[297/310], Step[1000/1251], Loss: 3.4290(3.5412), Acc: 0.4912(0.3838)
2022-01-07 19:20:56,634 Epoch[297/310], Step[1050/1251], Loss: 3.4599(3.5398), Acc: 0.4795(0.3847)
2022-01-07 19:21:56,698 Epoch[297/310], Step[1100/1251], Loss: 3.7761(3.5419), Acc: 0.3594(0.3859)
2022-01-07 19:22:56,708 Epoch[297/310], Step[1150/1251], Loss: 3.5165(3.5433), Acc: 0.5273(0.3866)
2022-01-07 19:23:56,414 Epoch[297/310], Step[1200/1251], Loss: 3.7073(3.5434), Acc: 0.3721(0.3868)
2022-01-07 19:24:54,894 Epoch[297/310], Step[1250/1251], Loss: 3.3372(3.5416), Acc: 0.4385(0.3876)
2022-01-07 19:24:56,453 ----- Epoch[297/310], Train Loss: 3.5416, Train Acc: 0.3876, time: 1583.62, Best Val(epoch296) Acc@1: 0.7310
2022-01-07 19:24:56,693 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 19:24:56,694 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 19:24:56,746 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 19:24:56,746 Now training epoch 298. LR=0.000005
2022-01-07 19:26:26,365 Epoch[298/310], Step[0000/1251], Loss: 3.4526(3.4526), Acc: 0.2705(0.2705)
2022-01-07 19:27:26,042 Epoch[298/310], Step[0050/1251], Loss: 3.4936(3.5269), Acc: 0.3857(0.4096)
2022-01-07 19:28:27,264 Epoch[298/310], Step[0100/1251], Loss: 3.0388(3.5341), Acc: 0.3770(0.4032)
2022-01-07 19:29:27,506 Epoch[298/310], Step[0150/1251], Loss: 3.0199(3.5400), Acc: 0.3516(0.4066)
2022-01-07 19:30:26,089 Epoch[298/310], Step[0200/1251], Loss: 4.0382(3.5339), Acc: 0.3477(0.4155)
2022-01-07 19:31:25,361 Epoch[298/310], Step[0250/1251], Loss: 3.3312(3.5243), Acc: 0.3721(0.4124)
2022-01-07 19:32:23,072 Epoch[298/310], Step[0300/1251], Loss: 3.8893(3.5195), Acc: 0.2764(0.4087)
2022-01-07 19:33:23,587 Epoch[298/310], Step[0350/1251], Loss: 3.7936(3.5281), Acc: 0.3945(0.4081)
2022-01-07 19:34:22,974 Epoch[298/310], Step[0400/1251], Loss: 3.4503(3.5261), Acc: 0.3936(0.4073)
2022-01-07 19:35:21,960 Epoch[298/310], Step[0450/1251], Loss: 3.5788(3.5248), Acc: 0.0840(0.4079)
2022-01-07 19:36:21,307 Epoch[298/310], Step[0500/1251], Loss: 3.0766(3.5271), Acc: 0.5752(0.4066)
2022-01-07 19:37:20,606 Epoch[298/310], Step[0550/1251], Loss: 3.6611(3.5272), Acc: 0.3291(0.4054)
2022-01-07 19:38:18,978 Epoch[298/310], Step[0600/1251], Loss: 3.9715(3.5258), Acc: 0.2227(0.4050)
2022-01-07 19:39:19,067 Epoch[298/310], Step[0650/1251], Loss: 3.6808(3.5313), Acc: 0.5430(0.4053)
2022-01-07 19:40:17,969 Epoch[298/310], Step[0700/1251], Loss: 4.0277(3.5321), Acc: 0.3340(0.4049)
2022-01-07 19:41:16,252 Epoch[298/310], Step[0750/1251], Loss: 3.8749(3.5346), Acc: 0.2773(0.4028)
2022-01-07 19:42:14,691 Epoch[298/310], Step[0800/1251], Loss: 3.6272(3.5366), Acc: 0.4150(0.4007)
2022-01-07 19:43:15,317 Epoch[298/310], Step[0850/1251], Loss: 3.1528(3.5397), Acc: 0.4736(0.3989)
2022-01-07 19:44:14,851 Epoch[298/310], Step[0900/1251], Loss: 3.1925(3.5419), Acc: 0.4297(0.3981)
2022-01-07 19:45:12,714 Epoch[298/310], Step[0950/1251], Loss: 3.6112(3.5389), Acc: 0.3477(0.3987)
2022-01-07 19:46:13,122 Epoch[298/310], Step[1000/1251], Loss: 3.1058(3.5385), Acc: 0.4473(0.3979)
2022-01-07 19:47:11,194 Epoch[298/310], Step[1050/1251], Loss: 2.9007(3.5376), Acc: 0.4062(0.3982)
2022-01-07 19:48:10,567 Epoch[298/310], Step[1100/1251], Loss: 3.8372(3.5390), Acc: 0.2021(0.3965)
2022-01-07 19:49:10,329 Epoch[298/310], Step[1150/1251], Loss: 3.4847(3.5386), Acc: 0.3838(0.3957)
2022-01-07 19:50:09,931 Epoch[298/310], Step[1200/1251], Loss: 3.7154(3.5414), Acc: 0.2812(0.3956)
2022-01-07 19:51:07,708 Epoch[298/310], Step[1250/1251], Loss: 3.8468(3.5436), Acc: 0.3760(0.3956)
2022-01-07 19:51:09,154 ----- Validation after Epoch: 298
2022-01-07 19:52:20,692 Val Step[0000/1563], Loss: 0.6715 (0.6715), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 19:52:22,090 Val Step[0050/1563], Loss: 2.1780 (0.7163), Acc@1: 0.4062 (0.8597), Acc@5: 0.8750 (0.9620)
2022-01-07 19:52:23,377 Val Step[0100/1563], Loss: 1.8954 (0.9732), Acc@1: 0.5000 (0.7887), Acc@5: 0.8125 (0.9387)
2022-01-07 19:52:24,655 Val Step[0150/1563], Loss: 0.3852 (0.9237), Acc@1: 0.9688 (0.8003), Acc@5: 1.0000 (0.9427)
2022-01-07 19:52:25,986 Val Step[0200/1563], Loss: 0.9274 (0.9294), Acc@1: 0.8125 (0.8012), Acc@5: 0.9375 (0.9420)
2022-01-07 19:52:27,250 Val Step[0250/1563], Loss: 0.4587 (0.8821), Acc@1: 0.9375 (0.8129), Acc@5: 1.0000 (0.9472)
2022-01-07 19:52:28,525 Val Step[0300/1563], Loss: 1.0380 (0.9389), Acc@1: 0.7188 (0.7977), Acc@5: 1.0000 (0.9434)
2022-01-07 19:52:29,877 Val Step[0350/1563], Loss: 0.8989 (0.9456), Acc@1: 0.7812 (0.7934), Acc@5: 0.9062 (0.9450)
2022-01-07 19:52:31,135 Val Step[0400/1563], Loss: 0.8583 (0.9528), Acc@1: 0.8125 (0.7875), Acc@5: 0.9688 (0.9451)
2022-01-07 19:52:32,448 Val Step[0450/1563], Loss: 0.9370 (0.9593), Acc@1: 0.7500 (0.7855), Acc@5: 1.0000 (0.9462)
2022-01-07 19:52:33,733 Val Step[0500/1563], Loss: 0.4228 (0.9506), Acc@1: 0.9375 (0.7880), Acc@5: 1.0000 (0.9474)
2022-01-07 19:52:35,109 Val Step[0550/1563], Loss: 0.7776 (0.9303), Acc@1: 0.7812 (0.7927), Acc@5: 0.9688 (0.9491)
2022-01-07 19:52:36,451 Val Step[0600/1563], Loss: 0.7746 (0.9382), Acc@1: 0.8125 (0.7916), Acc@5: 0.9375 (0.9483)
2022-01-07 19:52:37,724 Val Step[0650/1563], Loss: 0.4807 (0.9572), Acc@1: 0.9062 (0.7874), Acc@5: 1.0000 (0.9460)
2022-01-07 19:52:38,992 Val Step[0700/1563], Loss: 0.9467 (0.9835), Acc@1: 0.8438 (0.7811), Acc@5: 0.9688 (0.9432)
2022-01-07 19:52:40,266 Val Step[0750/1563], Loss: 1.1717 (1.0140), Acc@1: 0.8438 (0.7748), Acc@5: 0.9062 (0.9392)
2022-01-07 19:52:41,681 Val Step[0800/1563], Loss: 0.6820 (1.0490), Acc@1: 0.8750 (0.7660), Acc@5: 1.0000 (0.9347)
2022-01-07 19:52:43,105 Val Step[0850/1563], Loss: 1.1783 (1.0725), Acc@1: 0.6562 (0.7598), Acc@5: 0.9375 (0.9320)
2022-01-07 19:52:44,542 Val Step[0900/1563], Loss: 0.2718 (1.0714), Acc@1: 0.9688 (0.7609), Acc@5: 1.0000 (0.9317)
2022-01-07 19:52:46,025 Val Step[0950/1563], Loss: 1.2679 (1.0909), Acc@1: 0.7812 (0.7572), Acc@5: 0.8750 (0.9286)
2022-01-07 19:52:47,513 Val Step[1000/1563], Loss: 0.5714 (1.1132), Acc@1: 0.9375 (0.7512), Acc@5: 1.0000 (0.9256)
2022-01-07 19:52:48,935 Val Step[1050/1563], Loss: 0.3458 (1.1266), Acc@1: 0.9688 (0.7480), Acc@5: 1.0000 (0.9241)
2022-01-07 19:52:50,365 Val Step[1100/1563], Loss: 0.7060 (1.1395), Acc@1: 0.8438 (0.7455), Acc@5: 0.9688 (0.9223)
2022-01-07 19:52:52,003 Val Step[1150/1563], Loss: 1.2191 (1.1526), Acc@1: 0.7812 (0.7429), Acc@5: 0.8438 (0.9205)
2022-01-07 19:52:53,291 Val Step[1200/1563], Loss: 1.2388 (1.1654), Acc@1: 0.7500 (0.7395), Acc@5: 0.8438 (0.9184)
2022-01-07 19:52:54,566 Val Step[1250/1563], Loss: 0.7365 (1.1761), Acc@1: 0.8750 (0.7378), Acc@5: 0.9375 (0.9168)
2022-01-07 19:52:55,927 Val Step[1300/1563], Loss: 0.7863 (1.1842), Acc@1: 0.9062 (0.7361), Acc@5: 0.9375 (0.9159)
2022-01-07 19:52:57,215 Val Step[1350/1563], Loss: 1.6567 (1.2003), Acc@1: 0.5625 (0.7323), Acc@5: 0.8750 (0.9136)
2022-01-07 19:52:58,606 Val Step[1400/1563], Loss: 0.9489 (1.2064), Acc@1: 0.7500 (0.7309), Acc@5: 0.9375 (0.9127)
2022-01-07 19:52:59,892 Val Step[1450/1563], Loss: 1.3485 (1.2126), Acc@1: 0.7812 (0.7291), Acc@5: 0.9375 (0.9124)
2022-01-07 19:53:01,158 Val Step[1500/1563], Loss: 1.5824 (1.2027), Acc@1: 0.6562 (0.7314), Acc@5: 0.8750 (0.9136)
2022-01-07 19:53:02,427 Val Step[1550/1563], Loss: 0.8756 (1.2049), Acc@1: 0.8750 (0.7305), Acc@5: 0.9062 (0.9132)
2022-01-07 19:53:03,183 ----- Epoch[298/310], Validation Loss: 1.2033, Validation Acc@1: 0.7308, Validation Acc@5: 0.9135, time: 114.03
2022-01-07 19:53:03,183 ----- Epoch[298/310], Train Loss: 3.5436, Train Acc: 0.3956, time: 1572.40, Best Val(epoch296) Acc@1: 0.7310
2022-01-07 19:53:03,366 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 19:53:03,366 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 19:53:03,476 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 19:53:03,476 Now training epoch 299. LR=0.000005
2022-01-07 19:54:32,879 Epoch[299/310], Step[0000/1251], Loss: 3.6655(3.6655), Acc: 0.2266(0.2266)
2022-01-07 19:55:31,817 Epoch[299/310], Step[0050/1251], Loss: 3.5455(3.4944), Acc: 0.1172(0.4001)
2022-01-07 19:56:31,294 Epoch[299/310], Step[0100/1251], Loss: 3.7282(3.5083), Acc: 0.4102(0.3986)
2022-01-07 19:57:31,139 Epoch[299/310], Step[0150/1251], Loss: 3.4803(3.5134), Acc: 0.3623(0.3877)
2022-01-07 19:58:30,452 Epoch[299/310], Step[0200/1251], Loss: 3.0043(3.5156), Acc: 0.3369(0.3865)
2022-01-07 19:59:29,537 Epoch[299/310], Step[0250/1251], Loss: 3.4520(3.5170), Acc: 0.2910(0.3856)
2022-01-07 20:00:29,596 Epoch[299/310], Step[0300/1251], Loss: 4.0456(3.5098), Acc: 0.3955(0.3860)
2022-01-07 20:01:30,127 Epoch[299/310], Step[0350/1251], Loss: 3.9555(3.5161), Acc: 0.3447(0.3869)
2022-01-07 20:02:30,494 Epoch[299/310], Step[0400/1251], Loss: 3.4343(3.5146), Acc: 0.5928(0.3859)
2022-01-07 20:03:30,994 Epoch[299/310], Step[0450/1251], Loss: 3.5024(3.5076), Acc: 0.3838(0.3869)
2022-01-07 20:04:29,442 Epoch[299/310], Step[0500/1251], Loss: 3.9379(3.5103), Acc: 0.2324(0.3861)
2022-01-07 20:05:28,738 Epoch[299/310], Step[0550/1251], Loss: 3.6057(3.5094), Acc: 0.3906(0.3863)
2022-01-07 20:06:30,234 Epoch[299/310], Step[0600/1251], Loss: 3.7598(3.5143), Acc: 0.4355(0.3867)
2022-01-07 20:07:30,244 Epoch[299/310], Step[0650/1251], Loss: 3.5941(3.5163), Acc: 0.0557(0.3862)
2022-01-07 20:08:29,925 Epoch[299/310], Step[0700/1251], Loss: 3.6441(3.5172), Acc: 0.4473(0.3860)
2022-01-07 20:09:31,068 Epoch[299/310], Step[0750/1251], Loss: 2.9486(3.5153), Acc: 0.3486(0.3860)
2022-01-07 20:10:31,049 Epoch[299/310], Step[0800/1251], Loss: 3.1124(3.5168), Acc: 0.5850(0.3867)
2022-01-07 20:11:31,654 Epoch[299/310], Step[0850/1251], Loss: 3.8000(3.5197), Acc: 0.3662(0.3876)
2022-01-07 20:12:32,450 Epoch[299/310], Step[0900/1251], Loss: 3.4329(3.5216), Acc: 0.5303(0.3864)
2022-01-07 20:13:32,517 Epoch[299/310], Step[0950/1251], Loss: 3.4914(3.5227), Acc: 0.4854(0.3865)
2022-01-07 20:14:32,014 Epoch[299/310], Step[1000/1251], Loss: 3.8435(3.5216), Acc: 0.2129(0.3874)
2022-01-07 20:15:32,836 Epoch[299/310], Step[1050/1251], Loss: 4.0642(3.5208), Acc: 0.1680(0.3890)
2022-01-07 20:16:32,226 Epoch[299/310], Step[1100/1251], Loss: 3.4394(3.5217), Acc: 0.1553(0.3902)
2022-01-07 20:17:32,081 Epoch[299/310], Step[1150/1251], Loss: 3.5857(3.5220), Acc: 0.3496(0.3901)
2022-01-07 20:18:32,282 Epoch[299/310], Step[1200/1251], Loss: 3.3791(3.5219), Acc: 0.2930(0.3901)
2022-01-07 20:19:33,701 Epoch[299/310], Step[1250/1251], Loss: 3.5218(3.5240), Acc: 0.4619(0.3888)
2022-01-07 20:19:35,289 ----- Epoch[299/310], Train Loss: 3.5240, Train Acc: 0.3888, time: 1591.81, Best Val(epoch296) Acc@1: 0.7310
2022-01-07 20:19:35,468 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 20:19:35,659 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 20:19:35,709 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 20:19:35,710 Now training epoch 300. LR=0.000005
2022-01-07 20:25:28,030 Epoch[300/310], Step[0000/1251], Loss: 3.4611(3.4611), Acc: 0.4424(0.4424)
2022-01-07 20:26:26,315 Epoch[300/310], Step[0050/1251], Loss: 3.4564(3.5046), Acc: 0.4824(0.3949)
2022-01-07 20:27:26,000 Epoch[300/310], Step[0100/1251], Loss: 4.0388(3.5693), Acc: 0.1777(0.3712)
2022-01-07 20:28:25,829 Epoch[300/310], Step[0150/1251], Loss: 3.7531(3.5704), Acc: 0.2637(0.3786)
2022-01-07 20:29:27,381 Epoch[300/310], Step[0200/1251], Loss: 3.0467(3.5646), Acc: 0.4229(0.3824)
2022-01-07 20:30:28,230 Epoch[300/310], Step[0250/1251], Loss: 3.3900(3.5636), Acc: 0.4375(0.3750)
2022-01-07 20:31:28,417 Epoch[300/310], Step[0300/1251], Loss: 3.7502(3.5567), Acc: 0.4883(0.3763)
2022-01-07 20:32:28,956 Epoch[300/310], Step[0350/1251], Loss: 3.5842(3.5545), Acc: 0.3535(0.3790)
2022-01-07 20:33:28,841 Epoch[300/310], Step[0400/1251], Loss: 3.8488(3.5616), Acc: 0.4541(0.3797)
2022-01-07 20:34:30,273 Epoch[300/310], Step[0450/1251], Loss: 3.3757(3.5577), Acc: 0.6133(0.3807)
2022-01-07 20:35:30,955 Epoch[300/310], Step[0500/1251], Loss: 3.8437(3.5502), Acc: 0.3213(0.3837)
2022-01-07 20:36:32,372 Epoch[300/310], Step[0550/1251], Loss: 3.5986(3.5555), Acc: 0.5225(0.3830)
2022-01-07 20:37:32,716 Epoch[300/310], Step[0600/1251], Loss: 3.5351(3.5507), Acc: 0.2471(0.3820)
2022-01-07 20:38:33,529 Epoch[300/310], Step[0650/1251], Loss: 3.5798(3.5431), Acc: 0.3984(0.3847)
2022-01-07 20:39:34,200 Epoch[300/310], Step[0700/1251], Loss: 4.1143(3.5471), Acc: 0.3271(0.3848)
2022-01-07 20:40:35,268 Epoch[300/310], Step[0750/1251], Loss: 3.4741(3.5444), Acc: 0.5098(0.3846)
2022-01-07 20:41:35,545 Epoch[300/310], Step[0800/1251], Loss: 3.8847(3.5445), Acc: 0.3252(0.3846)
2022-01-07 20:42:36,140 Epoch[300/310], Step[0850/1251], Loss: 3.7223(3.5471), Acc: 0.3887(0.3843)
2022-01-07 20:43:37,625 Epoch[300/310], Step[0900/1251], Loss: 3.8669(3.5497), Acc: 0.3691(0.3849)
2022-01-07 20:44:38,764 Epoch[300/310], Step[0950/1251], Loss: 3.8351(3.5524), Acc: 0.4209(0.3843)
2022-01-07 20:45:39,069 Epoch[300/310], Step[1000/1251], Loss: 3.8229(3.5515), Acc: 0.0781(0.3856)
2022-01-07 20:46:39,841 Epoch[300/310], Step[1050/1251], Loss: 3.6309(3.5483), Acc: 0.1777(0.3859)
2022-01-07 20:47:41,354 Epoch[300/310], Step[1100/1251], Loss: 3.3993(3.5465), Acc: 0.5713(0.3868)
2022-01-07 20:48:40,386 Epoch[300/310], Step[1150/1251], Loss: 3.7592(3.5456), Acc: 0.3867(0.3868)
2022-01-07 20:49:40,997 Epoch[300/310], Step[1200/1251], Loss: 3.3039(3.5447), Acc: 0.5068(0.3873)
2022-01-07 20:50:41,714 Epoch[300/310], Step[1250/1251], Loss: 3.7807(3.5421), Acc: 0.2959(0.3876)
2022-01-07 20:50:43,312 ----- Validation after Epoch: 300
2022-01-07 20:51:57,745 Val Step[0000/1563], Loss: 0.6674 (0.6674), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2022-01-07 20:51:59,175 Val Step[0050/1563], Loss: 2.1665 (0.7098), Acc@1: 0.4062 (0.8591), Acc@5: 0.8750 (0.9614)
2022-01-07 20:52:00,438 Val Step[0100/1563], Loss: 1.8973 (0.9653), Acc@1: 0.5000 (0.7877), Acc@5: 0.8438 (0.9378)
2022-01-07 20:52:01,716 Val Step[0150/1563], Loss: 0.4075 (0.9152), Acc@1: 0.9375 (0.8001), Acc@5: 1.0000 (0.9437)
2022-01-07 20:52:02,984 Val Step[0200/1563], Loss: 0.9194 (0.9221), Acc@1: 0.8125 (0.8029), Acc@5: 0.9375 (0.9429)
2022-01-07 20:52:04,236 Val Step[0250/1563], Loss: 0.4589 (0.8742), Acc@1: 0.9375 (0.8139), Acc@5: 1.0000 (0.9481)
2022-01-07 20:52:05,570 Val Step[0300/1563], Loss: 1.0463 (0.9314), Acc@1: 0.7188 (0.7982), Acc@5: 1.0000 (0.9438)
2022-01-07 20:52:06,984 Val Step[0350/1563], Loss: 0.9118 (0.9384), Acc@1: 0.8125 (0.7940), Acc@5: 0.9062 (0.9454)
2022-01-07 20:52:08,390 Val Step[0400/1563], Loss: 0.8542 (0.9464), Acc@1: 0.8438 (0.7886), Acc@5: 0.9688 (0.9457)
2022-01-07 20:52:09,807 Val Step[0450/1563], Loss: 0.9416 (0.9537), Acc@1: 0.7500 (0.7862), Acc@5: 1.0000 (0.9464)
2022-01-07 20:52:11,248 Val Step[0500/1563], Loss: 0.4246 (0.9447), Acc@1: 0.9062 (0.7887), Acc@5: 1.0000 (0.9475)
2022-01-07 20:52:12,750 Val Step[0550/1563], Loss: 0.7691 (0.9241), Acc@1: 0.8438 (0.7933), Acc@5: 0.9688 (0.9494)
2022-01-07 20:52:14,195 Val Step[0600/1563], Loss: 0.7977 (0.9321), Acc@1: 0.8125 (0.7921), Acc@5: 0.9375 (0.9486)
2022-01-07 20:52:15,621 Val Step[0650/1563], Loss: 0.5040 (0.9517), Acc@1: 0.9062 (0.7881), Acc@5: 1.0000 (0.9461)
2022-01-07 20:52:17,058 Val Step[0700/1563], Loss: 0.9255 (0.9774), Acc@1: 0.8125 (0.7815), Acc@5: 0.9688 (0.9435)
2022-01-07 20:52:18,481 Val Step[0750/1563], Loss: 1.2050 (1.0080), Acc@1: 0.8438 (0.7751), Acc@5: 0.9062 (0.9395)
2022-01-07 20:52:19,904 Val Step[0800/1563], Loss: 0.6384 (1.0434), Acc@1: 0.8438 (0.7662), Acc@5: 1.0000 (0.9351)
2022-01-07 20:52:21,343 Val Step[0850/1563], Loss: 1.1889 (1.0675), Acc@1: 0.6562 (0.7600), Acc@5: 0.9688 (0.9324)
2022-01-07 20:52:22,817 Val Step[0900/1563], Loss: 0.2614 (1.0669), Acc@1: 0.9688 (0.7611), Acc@5: 1.0000 (0.9320)
2022-01-07 20:52:24,280 Val Step[0950/1563], Loss: 1.3189 (1.0862), Acc@1: 0.7812 (0.7573), Acc@5: 0.9062 (0.9289)
2022-01-07 20:52:25,683 Val Step[1000/1563], Loss: 0.5808 (1.1090), Acc@1: 0.9688 (0.7513), Acc@5: 0.9688 (0.9259)
2022-01-07 20:52:27,113 Val Step[1050/1563], Loss: 0.3055 (1.1222), Acc@1: 0.9688 (0.7480), Acc@5: 1.0000 (0.9244)
2022-01-07 20:52:28,553 Val Step[1100/1563], Loss: 0.7427 (1.1347), Acc@1: 0.8438 (0.7456), Acc@5: 0.9688 (0.9226)
2022-01-07 20:52:30,027 Val Step[1150/1563], Loss: 1.2004 (1.1477), Acc@1: 0.7812 (0.7432), Acc@5: 0.8438 (0.9208)
2022-01-07 20:52:31,632 Val Step[1200/1563], Loss: 1.2312 (1.1608), Acc@1: 0.7500 (0.7399), Acc@5: 0.8438 (0.9186)
2022-01-07 20:52:32,915 Val Step[1250/1563], Loss: 0.7302 (1.1717), Acc@1: 0.8750 (0.7381), Acc@5: 0.9375 (0.9171)
2022-01-07 20:52:34,188 Val Step[1300/1563], Loss: 0.7894 (1.1797), Acc@1: 0.8750 (0.7362), Acc@5: 0.9375 (0.9160)
2022-01-07 20:52:35,580 Val Step[1350/1563], Loss: 1.7478 (1.1961), Acc@1: 0.5625 (0.7322), Acc@5: 0.8750 (0.9137)
2022-01-07 20:52:36,996 Val Step[1400/1563], Loss: 0.9581 (1.2024), Acc@1: 0.7500 (0.7307), Acc@5: 0.9688 (0.9128)
2022-01-07 20:52:38,440 Val Step[1450/1563], Loss: 1.3523 (1.2085), Acc@1: 0.7812 (0.7289), Acc@5: 0.9375 (0.9126)
2022-01-07 20:52:39,882 Val Step[1500/1563], Loss: 1.5794 (1.1984), Acc@1: 0.6562 (0.7312), Acc@5: 0.8750 (0.9138)
2022-01-07 20:52:41,338 Val Step[1550/1563], Loss: 0.8696 (1.2004), Acc@1: 0.8750 (0.7304), Acc@5: 0.9062 (0.9135)
2022-01-07 20:52:42,141 ----- Epoch[300/310], Validation Loss: 1.1989, Validation Acc@1: 0.7307, Validation Acc@5: 0.9138, time: 118.83
2022-01-07 20:52:42,142 ----- Epoch[300/310], Train Loss: 3.5421, Train Acc: 0.3876, time: 1867.60, Best Val(epoch296) Acc@1: 0.7310
2022-01-07 20:52:42,303 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-300-Loss-3.5298042804312457.pdparams
2022-01-07 20:52:42,304 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-300-Loss-3.5298042804312457.pdopt
2022-01-07 20:52:42,345 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Epoch-300-Loss-3.5298042804312457-EMA.pdparams
2022-01-07 20:52:42,345 Now training epoch 301. LR=0.000005
2022-01-07 20:54:14,579 Epoch[301/310], Step[0000/1251], Loss: 3.3756(3.3756), Acc: 0.4863(0.4863)
2022-01-07 20:55:14,335 Epoch[301/310], Step[0050/1251], Loss: 3.3672(3.5469), Acc: 0.3936(0.3713)
2022-01-07 20:56:14,275 Epoch[301/310], Step[0100/1251], Loss: 3.7851(3.5476), Acc: 0.5078(0.3819)
2022-01-07 20:57:13,667 Epoch[301/310], Step[0150/1251], Loss: 3.1487(3.5515), Acc: 0.4541(0.3775)
2022-01-07 20:58:13,120 Epoch[301/310], Step[0200/1251], Loss: 4.0004(3.5394), Acc: 0.4023(0.3889)
2022-01-07 20:59:13,567 Epoch[301/310], Step[0250/1251], Loss: 3.1143(3.5408), Acc: 0.4434(0.3882)
2022-01-07 21:00:12,547 Epoch[301/310], Step[0300/1251], Loss: 3.1929(3.5398), Acc: 0.3457(0.3901)
2022-01-07 21:01:12,291 Epoch[301/310], Step[0350/1251], Loss: 3.0651(3.5396), Acc: 0.5986(0.3879)
2022-01-07 21:02:11,885 Epoch[301/310], Step[0400/1251], Loss: 3.5462(3.5546), Acc: 0.2451(0.3865)
2022-01-07 21:03:11,734 Epoch[301/310], Step[0450/1251], Loss: 3.0781(3.5518), Acc: 0.4951(0.3856)
2022-01-07 21:04:11,397 Epoch[301/310], Step[0500/1251], Loss: 3.2854(3.5512), Acc: 0.6064(0.3871)
2022-01-07 21:05:12,684 Epoch[301/310], Step[0550/1251], Loss: 3.0999(3.5491), Acc: 0.4834(0.3865)
2022-01-07 21:06:12,941 Epoch[301/310], Step[0600/1251], Loss: 3.3595(3.5474), Acc: 0.4141(0.3887)
2022-01-07 21:07:11,438 Epoch[301/310], Step[0650/1251], Loss: 3.2873(3.5510), Acc: 0.4600(0.3889)
2022-01-07 21:08:12,003 Epoch[301/310], Step[0700/1251], Loss: 3.2947(3.5509), Acc: 0.6191(0.3863)
2022-01-07 21:09:10,303 Epoch[301/310], Step[0750/1251], Loss: 3.4812(3.5487), Acc: 0.1357(0.3874)
2022-01-07 21:10:09,603 Epoch[301/310], Step[0800/1251], Loss: 2.8078(3.5491), Acc: 0.4844(0.3874)
2022-01-07 21:11:08,432 Epoch[301/310], Step[0850/1251], Loss: 3.6231(3.5509), Acc: 0.3193(0.3882)
2022-01-07 21:12:07,789 Epoch[301/310], Step[0900/1251], Loss: 3.6359(3.5485), Acc: 0.1650(0.3883)
2022-01-07 21:13:05,804 Epoch[301/310], Step[0950/1251], Loss: 3.1631(3.5455), Acc: 0.3369(0.3902)
2022-01-07 21:14:04,683 Epoch[301/310], Step[1000/1251], Loss: 2.9825(3.5470), Acc: 0.3828(0.3890)
2022-01-07 21:15:03,841 Epoch[301/310], Step[1050/1251], Loss: 4.1699(3.5516), Acc: 0.3496(0.3888)
2022-01-07 21:16:03,467 Epoch[301/310], Step[1100/1251], Loss: 3.4894(3.5496), Acc: 0.5215(0.3889)
2022-01-07 21:17:03,016 Epoch[301/310], Step[1150/1251], Loss: 3.3799(3.5473), Acc: 0.1699(0.3884)
2022-01-07 21:18:01,883 Epoch[301/310], Step[1200/1251], Loss: 3.3125(3.5456), Acc: 0.4580(0.3878)
2022-01-07 21:19:02,429 Epoch[301/310], Step[1250/1251], Loss: 3.9385(3.5429), Acc: 0.4092(0.3883)
2022-01-07 21:19:03,921 ----- Epoch[301/310], Train Loss: 3.5429, Train Acc: 0.3883, time: 1581.57, Best Val(epoch296) Acc@1: 0.7310
2022-01-07 21:19:04,098 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 21:19:04,098 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 21:19:04,208 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 21:19:04,208 Now training epoch 302. LR=0.000005
2022-01-07 21:20:29,004 Epoch[302/310], Step[0000/1251], Loss: 3.5030(3.5030), Acc: 0.4824(0.4824)
2022-01-07 21:21:28,379 Epoch[302/310], Step[0050/1251], Loss: 3.6609(3.5746), Acc: 0.2461(0.4026)
2022-01-07 21:22:28,328 Epoch[302/310], Step[0100/1251], Loss: 3.4833(3.5688), Acc: 0.5654(0.4089)
2022-01-07 21:23:27,391 Epoch[302/310], Step[0150/1251], Loss: 3.5265(3.5574), Acc: 0.4229(0.3983)
2022-01-07 21:24:27,083 Epoch[302/310], Step[0200/1251], Loss: 3.0844(3.5462), Acc: 0.4785(0.3974)
2022-01-07 21:25:25,922 Epoch[302/310], Step[0250/1251], Loss: 3.1572(3.5418), Acc: 0.5820(0.3957)
2022-01-07 21:26:26,231 Epoch[302/310], Step[0300/1251], Loss: 3.4920(3.5429), Acc: 0.3926(0.3934)
2022-01-07 21:27:25,431 Epoch[302/310], Step[0350/1251], Loss: 3.5869(3.5509), Acc: 0.4990(0.3902)
2022-01-07 21:28:25,469 Epoch[302/310], Step[0400/1251], Loss: 3.4382(3.5436), Acc: 0.3555(0.3910)
2022-01-07 21:29:25,470 Epoch[302/310], Step[0450/1251], Loss: 3.6424(3.5497), Acc: 0.5039(0.3912)
2022-01-07 21:30:25,181 Epoch[302/310], Step[0500/1251], Loss: 2.9574(3.5491), Acc: 0.5850(0.3939)
2022-01-07 21:31:25,892 Epoch[302/310], Step[0550/1251], Loss: 3.2999(3.5491), Acc: 0.3418(0.3919)
2022-01-07 21:32:26,394 Epoch[302/310], Step[0600/1251], Loss: 3.1572(3.5464), Acc: 0.4961(0.3936)
2022-01-07 21:33:25,933 Epoch[302/310], Step[0650/1251], Loss: 3.7990(3.5439), Acc: 0.4229(0.3934)
2022-01-07 21:34:26,451 Epoch[302/310], Step[0700/1251], Loss: 3.9720(3.5460), Acc: 0.4062(0.3928)
2022-01-07 21:35:26,230 Epoch[302/310], Step[0750/1251], Loss: 3.4941(3.5464), Acc: 0.4668(0.3953)
2022-01-07 21:36:24,783 Epoch[302/310], Step[0800/1251], Loss: 3.6424(3.5434), Acc: 0.3750(0.3959)
2022-01-07 21:37:22,264 Epoch[302/310], Step[0850/1251], Loss: 3.2821(3.5432), Acc: 0.5029(0.3957)
2022-01-07 21:38:19,150 Epoch[302/310], Step[0900/1251], Loss: 3.9416(3.5426), Acc: 0.3145(0.3948)
2022-01-07 21:39:19,136 Epoch[302/310], Step[0950/1251], Loss: 3.3857(3.5393), Acc: 0.2900(0.3958)
2022-01-07 21:40:17,393 Epoch[302/310], Step[1000/1251], Loss: 3.6655(3.5434), Acc: 0.2598(0.3948)
2022-01-07 21:41:16,175 Epoch[302/310], Step[1050/1251], Loss: 3.8607(3.5429), Acc: 0.3770(0.3941)
2022-01-07 21:42:15,455 Epoch[302/310], Step[1100/1251], Loss: 3.7212(3.5430), Acc: 0.4521(0.3938)
2022-01-07 21:43:15,293 Epoch[302/310], Step[1150/1251], Loss: 3.4537(3.5414), Acc: 0.3682(0.3931)
2022-01-07 21:44:15,943 Epoch[302/310], Step[1200/1251], Loss: 3.0951(3.5403), Acc: 0.3438(0.3927)
2022-01-07 21:45:15,271 Epoch[302/310], Step[1250/1251], Loss: 3.3853(3.5371), Acc: 0.5039(0.3933)
2022-01-07 21:45:16,786 ----- Validation after Epoch: 302
2022-01-07 21:46:26,928 Val Step[0000/1563], Loss: 0.6490 (0.6490), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2022-01-07 21:46:28,255 Val Step[0050/1563], Loss: 2.1575 (0.7042), Acc@1: 0.4062 (0.8609), Acc@5: 0.8750 (0.9638)
2022-01-07 21:46:29,572 Val Step[0100/1563], Loss: 1.9475 (0.9623), Acc@1: 0.5000 (0.7881), Acc@5: 0.8125 (0.9400)
2022-01-07 21:46:30,950 Val Step[0150/1563], Loss: 0.3860 (0.9144), Acc@1: 0.9688 (0.8003), Acc@5: 1.0000 (0.9437)
2022-01-07 21:46:32,240 Val Step[0200/1563], Loss: 0.9351 (0.9185), Acc@1: 0.8125 (0.8032), Acc@5: 0.9375 (0.9431)
2022-01-07 21:46:33,544 Val Step[0250/1563], Loss: 0.5114 (0.8715), Acc@1: 0.9062 (0.8147), Acc@5: 1.0000 (0.9486)
2022-01-07 21:46:34,859 Val Step[0300/1563], Loss: 1.0125 (0.9271), Acc@1: 0.7188 (0.7991), Acc@5: 1.0000 (0.9447)
2022-01-07 21:46:36,159 Val Step[0350/1563], Loss: 0.9364 (0.9342), Acc@1: 0.7812 (0.7946), Acc@5: 0.9062 (0.9461)
2022-01-07 21:46:37,494 Val Step[0400/1563], Loss: 0.8427 (0.9426), Acc@1: 0.8438 (0.7883), Acc@5: 0.9688 (0.9460)
2022-01-07 21:46:38,915 Val Step[0450/1563], Loss: 0.9779 (0.9493), Acc@1: 0.7188 (0.7860), Acc@5: 1.0000 (0.9467)
2022-01-07 21:46:40,240 Val Step[0500/1563], Loss: 0.4010 (0.9409), Acc@1: 0.9688 (0.7887), Acc@5: 1.0000 (0.9479)
2022-01-07 21:46:41,631 Val Step[0550/1563], Loss: 0.7247 (0.9210), Acc@1: 0.8438 (0.7933), Acc@5: 0.9688 (0.9495)
2022-01-07 21:46:42,923 Val Step[0600/1563], Loss: 0.7740 (0.9294), Acc@1: 0.8125 (0.7923), Acc@5: 0.9375 (0.9490)
2022-01-07 21:46:44,232 Val Step[0650/1563], Loss: 0.4971 (0.9492), Acc@1: 0.8750 (0.7882), Acc@5: 1.0000 (0.9464)
2022-01-07 21:46:45,499 Val Step[0700/1563], Loss: 0.9452 (0.9756), Acc@1: 0.8125 (0.7816), Acc@5: 0.9688 (0.9435)
2022-01-07 21:46:46,826 Val Step[0750/1563], Loss: 1.1442 (1.0065), Acc@1: 0.8438 (0.7753), Acc@5: 0.9062 (0.9395)
2022-01-07 21:46:48,159 Val Step[0800/1563], Loss: 0.6507 (1.0425), Acc@1: 0.8750 (0.7661), Acc@5: 1.0000 (0.9350)
2022-01-07 21:46:49,460 Val Step[0850/1563], Loss: 1.2251 (1.0670), Acc@1: 0.5938 (0.7597), Acc@5: 0.9375 (0.9322)
2022-01-07 21:46:50,739 Val Step[0900/1563], Loss: 0.2432 (1.0670), Acc@1: 0.9688 (0.7610), Acc@5: 1.0000 (0.9316)
2022-01-07 21:46:52,117 Val Step[0950/1563], Loss: 1.2403 (1.0871), Acc@1: 0.7812 (0.7571), Acc@5: 0.9062 (0.9283)
2022-01-07 21:46:53,411 Val Step[1000/1563], Loss: 0.5740 (1.1092), Acc@1: 0.9375 (0.7515), Acc@5: 1.0000 (0.9253)
2022-01-07 21:46:54,690 Val Step[1050/1563], Loss: 0.3131 (1.1224), Acc@1: 0.9688 (0.7484), Acc@5: 1.0000 (0.9239)
2022-01-07 21:46:55,963 Val Step[1100/1563], Loss: 0.7061 (1.1352), Acc@1: 0.8750 (0.7460), Acc@5: 0.9688 (0.9219)
2022-01-07 21:46:57,256 Val Step[1150/1563], Loss: 1.2021 (1.1481), Acc@1: 0.7812 (0.7434), Acc@5: 0.8438 (0.9200)
2022-01-07 21:46:58,582 Val Step[1200/1563], Loss: 1.2356 (1.1615), Acc@1: 0.7812 (0.7402), Acc@5: 0.8438 (0.9178)
2022-01-07 21:46:59,921 Val Step[1250/1563], Loss: 0.7441 (1.1719), Acc@1: 0.8750 (0.7385), Acc@5: 0.9375 (0.9163)
2022-01-07 21:47:01,240 Val Step[1300/1563], Loss: 0.7836 (1.1798), Acc@1: 0.9062 (0.7370), Acc@5: 0.9375 (0.9153)
2022-01-07 21:47:02,592 Val Step[1350/1563], Loss: 1.7209 (1.1959), Acc@1: 0.5312 (0.7328), Acc@5: 0.8750 (0.9131)
2022-01-07 21:47:03,863 Val Step[1400/1563], Loss: 0.9429 (1.2026), Acc@1: 0.7500 (0.7313), Acc@5: 0.9688 (0.9123)
2022-01-07 21:47:05,235 Val Step[1450/1563], Loss: 1.3221 (1.2085), Acc@1: 0.7500 (0.7295), Acc@5: 0.9375 (0.9120)
2022-01-07 21:47:06,564 Val Step[1500/1563], Loss: 1.6149 (1.1982), Acc@1: 0.6875 (0.7319), Acc@5: 0.8438 (0.9133)
2022-01-07 21:47:07,901 Val Step[1550/1563], Loss: 0.8604 (1.2001), Acc@1: 0.8750 (0.7310), Acc@5: 0.9062 (0.9130)
2022-01-07 21:47:08,664 ----- Epoch[302/310], Validation Loss: 1.1986, Validation Acc@1: 0.7313, Validation Acc@5: 0.9132, time: 111.88
2022-01-07 21:47:08,665 ----- Epoch[302/310], Train Loss: 3.5371, Train Acc: 0.3933, time: 1572.57, Best Val(epoch302) Acc@1: 0.7313
2022-01-07 21:47:08,849 Max accuracy so far: 0.7313 at epoch_302
2022-01-07 21:47:08,850 ----- Save BEST model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdparams
2022-01-07 21:47:08,850 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT.pdopt
2022-01-07 21:47:08,958 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/Best_PiT-EMA.pdparams
2022-01-07 21:47:09,343 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 21:47:09,343 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 21:47:09,564 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 21:47:09,564 Now training epoch 303. LR=0.000005
2022-01-07 21:48:38,297 Epoch[303/310], Step[0000/1251], Loss: 3.1211(3.1211), Acc: 0.2920(0.2920)
2022-01-07 21:49:38,258 Epoch[303/310], Step[0050/1251], Loss: 3.5659(3.5869), Acc: 0.5303(0.3648)
2022-01-07 21:50:38,537 Epoch[303/310], Step[0100/1251], Loss: 3.4196(3.5827), Acc: 0.2910(0.3643)
2022-01-07 21:51:38,183 Epoch[303/310], Step[0150/1251], Loss: 3.3603(3.5845), Acc: 0.5137(0.3728)
2022-01-07 21:52:38,428 Epoch[303/310], Step[0200/1251], Loss: 3.0668(3.5810), Acc: 0.5146(0.3808)
2022-01-07 21:53:38,210 Epoch[303/310], Step[0250/1251], Loss: 3.4067(3.5753), Acc: 0.1484(0.3823)
2022-01-07 21:54:38,206 Epoch[303/310], Step[0300/1251], Loss: 3.4461(3.5673), Acc: 0.2695(0.3864)
2022-01-07 21:55:36,895 Epoch[303/310], Step[0350/1251], Loss: 3.6221(3.5656), Acc: 0.2275(0.3871)
2022-01-07 21:56:35,677 Epoch[303/310], Step[0400/1251], Loss: 3.6659(3.5619), Acc: 0.2891(0.3903)
2022-01-07 21:57:34,523 Epoch[303/310], Step[0450/1251], Loss: 3.5360(3.5615), Acc: 0.2861(0.3914)
2022-01-07 21:58:34,096 Epoch[303/310], Step[0500/1251], Loss: 2.7598(3.5572), Acc: 0.2979(0.3911)
2022-01-07 21:59:33,303 Epoch[303/310], Step[0550/1251], Loss: 4.1805(3.5512), Acc: 0.3975(0.3923)
2022-01-07 22:00:30,927 Epoch[303/310], Step[0600/1251], Loss: 3.4773(3.5518), Acc: 0.2705(0.3935)
2022-01-07 22:01:29,381 Epoch[303/310], Step[0650/1251], Loss: 3.2273(3.5517), Acc: 0.6270(0.3937)
2022-01-07 22:02:28,078 Epoch[303/310], Step[0700/1251], Loss: 4.0230(3.5509), Acc: 0.2910(0.3931)
2022-01-07 22:03:28,317 Epoch[303/310], Step[0750/1251], Loss: 3.2567(3.5545), Acc: 0.2109(0.3903)
2022-01-07 22:04:29,015 Epoch[303/310], Step[0800/1251], Loss: 2.6568(3.5575), Acc: 0.6553(0.3899)
2022-01-07 22:05:29,580 Epoch[303/310], Step[0850/1251], Loss: 3.9678(3.5570), Acc: 0.3672(0.3889)
2022-01-07 22:06:29,475 Epoch[303/310], Step[0900/1251], Loss: 3.2481(3.5544), Acc: 0.2217(0.3890)
2022-01-07 22:07:28,951 Epoch[303/310], Step[0950/1251], Loss: 4.0021(3.5532), Acc: 0.4658(0.3892)
2022-01-07 22:08:29,366 Epoch[303/310], Step[1000/1251], Loss: 3.0947(3.5522), Acc: 0.3984(0.3910)
2022-01-07 22:09:29,742 Epoch[303/310], Step[1050/1251], Loss: 3.6113(3.5542), Acc: 0.3955(0.3890)
2022-01-07 22:10:30,010 Epoch[303/310], Step[1100/1251], Loss: 3.8396(3.5561), Acc: 0.3203(0.3879)
2022-01-07 22:11:28,709 Epoch[303/310], Step[1150/1251], Loss: 3.5048(3.5554), Acc: 0.4717(0.3894)
2022-01-07 22:12:29,049 Epoch[303/310], Step[1200/1251], Loss: 3.5118(3.5514), Acc: 0.2764(0.3901)
2022-01-07 22:13:28,849 Epoch[303/310], Step[1250/1251], Loss: 3.8750(3.5522), Acc: 0.4824(0.3903)
2022-01-07 22:13:30,391 ----- Epoch[303/310], Train Loss: 3.5522, Train Acc: 0.3903, time: 1580.82, Best Val(epoch302) Acc@1: 0.7313
2022-01-07 22:13:30,570 ----- Save model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdparams
2022-01-07 22:13:30,571 ----- Save optim: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest.pdopt
2022-01-07 22:13:30,679 ----- Save ema model: /root/paddlejob/workspace/output/train-20220106-10-32-46/PiT-Latest-EMA.pdparams
2022-01-07 22:13:30,679 Now training epoch 304. LR=0.000005
