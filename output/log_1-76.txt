2021-12-28 01:16:41,354 
AMP: False
AUG:
  AUTO_AUGMENT: None
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: ./Light_ILSVRC2012
  IMAGENET_MEAN: [0.485, 0.456, 0.406]
  IMAGENET_STD: [0.229, 0.224, 0.225]
  IMAGE_SIZE: 224
  NUM_WORKERS: 8
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.0
  DISTILL: False
  DROPOUT: 0.0
  DROP_PATH: 0.1
  NAME: pit_ti
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: None
  TRANS:
    BASE_DIMS: [32, 32, 32]
    DEPTH: [2, 6, 4]
    HEADS: [2, 4, 8]
    PATCH_SIZE: 16
    STRIDE: 8
  TYPE: PiT
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output/train-20211228-01-15-41
SAVE_FREQ: 10
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  COOLDOWN_EPOCHS: 10
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  DISTILLATION_ALPHA: 0.5
  DISTILLATION_TAU: 1.0
  DISTILLATION_TYPE: none
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 0
  LINEAR_SCALED_LR: None
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  MODEL_EMA: True
  MODEL_EMA_DECAY: 0.99996
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  TEACHER_MODEL: ./regnety_160
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
2021-12-28 01:16:41,354 ----- world_size = 4, local_rank = 0
2021-12-28 01:16:41,489 ----- Total # of train batch (single gpu): 1251
2021-12-28 01:16:41,489 ----- Total # of val batch (single gpu): 1563
2021-12-28 01:16:41,491 Start training from epoch 1.
2021-12-28 01:16:41,491 Now training epoch 1. LR=0.000050
2021-12-28 01:18:04,598 Epoch[001/310], Step[0000/1251], Loss: 7.0940(7.0940), Acc: 0.0010(0.0010)
2021-12-28 01:19:08,363 Epoch[001/310], Step[0050/1251], Loss: 6.9415(7.0088), Acc: 0.0020(0.0011)
2021-12-28 01:20:10,503 Epoch[001/310], Step[0100/1251], Loss: 6.9237(6.9746), Acc: 0.0000(0.0012)
2021-12-28 01:21:11,457 Epoch[001/310], Step[0150/1251], Loss: 6.9087(6.9556), Acc: 0.0020(0.0012)
2021-12-28 01:22:14,286 Epoch[001/310], Step[0200/1251], Loss: 6.9103(6.9440), Acc: 0.0020(0.0012)
2021-12-28 01:23:18,092 Epoch[001/310], Step[0250/1251], Loss: 6.9053(6.9359), Acc: 0.0029(0.0013)
2021-12-28 01:24:20,135 Epoch[001/310], Step[0300/1251], Loss: 6.8939(6.9298), Acc: 0.0039(0.0015)
2021-12-28 01:25:24,095 Epoch[001/310], Step[0350/1251], Loss: 6.9114(6.9249), Acc: 0.0020(0.0015)
2021-12-28 01:26:26,810 Epoch[001/310], Step[0400/1251], Loss: 6.8918(6.9205), Acc: 0.0029(0.0016)
2021-12-28 01:27:30,353 Epoch[001/310], Step[0450/1251], Loss: 6.8826(6.9166), Acc: 0.0020(0.0017)
2021-12-28 01:28:33,786 Epoch[001/310], Step[0500/1251], Loss: 6.8696(6.9127), Acc: 0.0049(0.0018)
2021-12-28 01:29:35,682 Epoch[001/310], Step[0550/1251], Loss: 6.8517(6.9087), Acc: 0.0029(0.0019)
2021-12-28 01:30:40,043 Epoch[001/310], Step[0600/1251], Loss: 6.8667(6.9045), Acc: 0.0000(0.0020)
2021-12-28 01:31:43,991 Epoch[001/310], Step[0650/1251], Loss: 6.8660(6.9000), Acc: 0.0039(0.0021)
2021-12-28 01:32:46,993 Epoch[001/310], Step[0700/1251], Loss: 6.8412(6.8954), Acc: 0.0098(0.0022)
2021-12-28 01:33:50,664 Epoch[001/310], Step[0750/1251], Loss: 6.8220(6.8906), Acc: 0.0010(0.0023)
2021-12-28 01:34:53,366 Epoch[001/310], Step[0800/1251], Loss: 6.8184(6.8856), Acc: 0.0059(0.0024)
2021-12-28 01:35:56,907 Epoch[001/310], Step[0850/1251], Loss: 6.8315(6.8799), Acc: 0.0010(0.0025)
2021-12-28 01:36:59,922 Epoch[001/310], Step[0900/1251], Loss: 6.8223(6.8743), Acc: 0.0049(0.0026)
2021-12-28 01:38:03,417 Epoch[001/310], Step[0950/1251], Loss: 6.7653(6.8687), Acc: 0.0088(0.0028)
2021-12-28 01:39:06,382 Epoch[001/310], Step[1000/1251], Loss: 6.7393(6.8631), Acc: 0.0020(0.0029)
2021-12-28 01:40:09,642 Epoch[001/310], Step[1050/1251], Loss: 6.7027(6.8576), Acc: 0.0098(0.0030)
2021-12-28 01:41:13,291 Epoch[001/310], Step[1100/1251], Loss: 6.7285(6.8519), Acc: 0.0059(0.0031)
2021-12-28 01:42:15,692 Epoch[001/310], Step[1150/1251], Loss: 6.7719(6.8464), Acc: 0.0088(0.0033)
2021-12-28 01:43:19,812 Epoch[001/310], Step[1200/1251], Loss: 6.7301(6.8411), Acc: 0.0088(0.0034)
2021-12-28 01:44:22,935 Epoch[001/310], Step[1250/1251], Loss: 6.6901(6.8359), Acc: 0.0068(0.0035)
2021-12-28 01:44:24,848 ----- Epoch[001/310], Train Loss: 6.8359, Train Acc: 0.0035, time: 1663.35
2021-12-28 01:44:24,848 Now training epoch 2. LR=0.000100
2021-12-28 01:45:41,090 Epoch[002/310], Step[0000/1251], Loss: 6.6706(6.6706), Acc: 0.0039(0.0039)
2021-12-28 01:46:42,212 Epoch[002/310], Step[0050/1251], Loss: 6.6461(6.7129), Acc: 0.0039(0.0070)
2021-12-28 01:47:42,999 Epoch[002/310], Step[0100/1251], Loss: 6.7165(6.7035), Acc: 0.0020(0.0067)
2021-12-28 01:48:44,362 Epoch[002/310], Step[0150/1251], Loss: 6.6357(6.6989), Acc: 0.0068(0.0068)
2021-12-28 01:49:46,698 Epoch[002/310], Step[0200/1251], Loss: 6.7086(6.6934), Acc: 0.0039(0.0070)
2021-12-28 01:50:50,094 Epoch[002/310], Step[0250/1251], Loss: 6.6614(6.6867), Acc: 0.0078(0.0073)
2021-12-28 01:51:50,687 Epoch[002/310], Step[0300/1251], Loss: 6.5899(6.6815), Acc: 0.0078(0.0076)
2021-12-28 01:52:53,891 Epoch[002/310], Step[0350/1251], Loss: 6.6899(6.6762), Acc: 0.0088(0.0079)
2021-12-28 01:53:56,844 Epoch[002/310], Step[0400/1251], Loss: 6.5864(6.6709), Acc: 0.0068(0.0081)
2021-12-28 01:55:00,218 Epoch[002/310], Step[0450/1251], Loss: 6.5619(6.6667), Acc: 0.0117(0.0082)
2021-12-28 01:56:03,437 Epoch[002/310], Step[0500/1251], Loss: 6.6267(6.6621), Acc: 0.0029(0.0083)
2021-12-28 01:57:05,172 Epoch[002/310], Step[0550/1251], Loss: 6.5709(6.6581), Acc: 0.0049(0.0084)
2021-12-28 01:58:07,259 Epoch[002/310], Step[0600/1251], Loss: 6.6521(6.6537), Acc: 0.0020(0.0086)
2021-12-28 01:59:10,790 Epoch[002/310], Step[0650/1251], Loss: 6.6175(6.6481), Acc: 0.0059(0.0087)
2021-12-28 02:00:11,775 Epoch[002/310], Step[0700/1251], Loss: 6.5885(6.6430), Acc: 0.0156(0.0089)
2021-12-28 02:01:15,222 Epoch[002/310], Step[0750/1251], Loss: 6.5930(6.6392), Acc: 0.0088(0.0090)
2021-12-28 02:02:17,228 Epoch[002/310], Step[0800/1251], Loss: 6.5865(6.6363), Acc: 0.0117(0.0092)
2021-12-28 02:03:20,822 Epoch[002/310], Step[0850/1251], Loss: 6.5710(6.6329), Acc: 0.0146(0.0093)
2021-12-28 02:04:23,056 Epoch[002/310], Step[0900/1251], Loss: 6.5508(6.6303), Acc: 0.0137(0.0094)
2021-12-28 02:05:24,450 Epoch[002/310], Step[0950/1251], Loss: 6.5499(6.6278), Acc: 0.0107(0.0095)
2021-12-28 02:06:27,162 Epoch[002/310], Step[1000/1251], Loss: 6.5868(6.6243), Acc: 0.0078(0.0097)
2021-12-28 02:07:31,261 Epoch[002/310], Step[1050/1251], Loss: 6.5730(6.6208), Acc: 0.0078(0.0097)
2021-12-28 02:08:35,254 Epoch[002/310], Step[1100/1251], Loss: 6.5274(6.6183), Acc: 0.0068(0.0099)
2021-12-28 02:09:38,729 Epoch[002/310], Step[1150/1251], Loss: 6.5782(6.6155), Acc: 0.0146(0.0100)
2021-12-28 02:10:42,967 Epoch[002/310], Step[1200/1251], Loss: 6.5596(6.6126), Acc: 0.0117(0.0101)
2021-12-28 02:11:44,045 Epoch[002/310], Step[1250/1251], Loss: 6.4667(6.6101), Acc: 0.0137(0.0103)
2021-12-28 02:11:45,984 ----- Validation after Epoch: 2
2021-12-28 02:12:47,828 Val Step[0000/1563], Loss: 6.1526 (6.1526), Acc@1: 0.0938 (0.0938), Acc@5: 0.1250 (0.1250)
2021-12-28 02:12:49,402 Val Step[0050/1563], Loss: 6.1569 (5.7906), Acc@1: 0.0000 (0.0386), Acc@5: 0.0000 (0.1544)
2021-12-28 02:12:50,843 Val Step[0100/1563], Loss: 5.5235 (5.8577), Acc@1: 0.0312 (0.0347), Acc@5: 0.1875 (0.1256)
2021-12-28 02:12:52,380 Val Step[0150/1563], Loss: 6.3391 (5.9068), Acc@1: 0.0000 (0.0294), Acc@5: 0.0625 (0.1136)
2021-12-28 02:12:53,880 Val Step[0200/1563], Loss: 6.4078 (5.8920), Acc@1: 0.0000 (0.0320), Acc@5: 0.0000 (0.1185)
2021-12-28 02:12:55,349 Val Step[0250/1563], Loss: 6.2010 (5.8724), Acc@1: 0.0000 (0.0335), Acc@5: 0.0312 (0.1282)
2021-12-28 02:12:56,809 Val Step[0300/1563], Loss: 6.0884 (5.9241), Acc@1: 0.0000 (0.0284), Acc@5: 0.0000 (0.1114)
2021-12-28 02:12:58,324 Val Step[0350/1563], Loss: 5.5859 (5.9384), Acc@1: 0.0625 (0.0259), Acc@5: 0.4375 (0.1048)
2021-12-28 02:12:59,779 Val Step[0400/1563], Loss: 5.4140 (5.9457), Acc@1: 0.0000 (0.0258), Acc@5: 0.0625 (0.1011)
2021-12-28 02:13:01,427 Val Step[0450/1563], Loss: 5.4875 (5.9498), Acc@1: 0.0000 (0.0245), Acc@5: 0.0938 (0.0974)
2021-12-28 02:13:02,988 Val Step[0500/1563], Loss: 5.0737 (5.9432), Acc@1: 0.0312 (0.0257), Acc@5: 0.5000 (0.1009)
2021-12-28 02:13:04,591 Val Step[0550/1563], Loss: 6.0668 (5.9316), Acc@1: 0.0312 (0.0283), Acc@5: 0.0625 (0.1070)
2021-12-28 02:13:06,188 Val Step[0600/1563], Loss: 5.2369 (5.9256), Acc@1: 0.1562 (0.0284), Acc@5: 0.4688 (0.1088)
2021-12-28 02:13:07,640 Val Step[0650/1563], Loss: 6.6271 (5.9238), Acc@1: 0.0000 (0.0311), Acc@5: 0.0312 (0.1136)
2021-12-28 02:13:09,123 Val Step[0700/1563], Loss: 6.6836 (5.9466), Acc@1: 0.0000 (0.0298), Acc@5: 0.0000 (0.1086)
2021-12-28 02:13:10,705 Val Step[0750/1563], Loss: 5.5981 (5.9514), Acc@1: 0.0312 (0.0301), Acc@5: 0.1875 (0.1094)
2021-12-28 02:13:12,082 Val Step[0800/1563], Loss: 5.6226 (5.9540), Acc@1: 0.1562 (0.0301), Acc@5: 0.3750 (0.1104)
2021-12-28 02:13:13,596 Val Step[0850/1563], Loss: 6.6143 (5.9702), Acc@1: 0.0000 (0.0294), Acc@5: 0.0000 (0.1077)
2021-12-28 02:13:15,144 Val Step[0900/1563], Loss: 5.9602 (5.9649), Acc@1: 0.0000 (0.0309), Acc@5: 0.0000 (0.1101)
2021-12-28 02:13:16,780 Val Step[0950/1563], Loss: 6.1074 (5.9744), Acc@1: 0.0312 (0.0313), Acc@5: 0.0938 (0.1091)
2021-12-28 02:13:18,296 Val Step[1000/1563], Loss: 4.8796 (5.9771), Acc@1: 0.3438 (0.0320), Acc@5: 0.6562 (0.1100)
2021-12-28 02:13:19,751 Val Step[1050/1563], Loss: 6.0258 (5.9772), Acc@1: 0.0000 (0.0319), Acc@5: 0.0000 (0.1104)
2021-12-28 02:13:21,302 Val Step[1100/1563], Loss: 6.3236 (5.9832), Acc@1: 0.0000 (0.0325), Acc@5: 0.0938 (0.1099)
2021-12-28 02:13:22,833 Val Step[1150/1563], Loss: 6.2608 (5.9910), Acc@1: 0.0312 (0.0315), Acc@5: 0.0625 (0.1080)
2021-12-28 02:13:24,343 Val Step[1200/1563], Loss: 6.0101 (5.9969), Acc@1: 0.1250 (0.0317), Acc@5: 0.2812 (0.1073)
2021-12-28 02:13:25,949 Val Step[1250/1563], Loss: 5.3095 (6.0005), Acc@1: 0.0625 (0.0317), Acc@5: 0.3125 (0.1071)
2021-12-28 02:13:27,417 Val Step[1300/1563], Loss: 5.9796 (6.0067), Acc@1: 0.0000 (0.0316), Acc@5: 0.0312 (0.1057)
2021-12-28 02:13:28,951 Val Step[1350/1563], Loss: 5.5858 (6.0123), Acc@1: 0.0000 (0.0310), Acc@5: 0.0312 (0.1039)
2021-12-28 02:13:30,465 Val Step[1400/1563], Loss: 5.8377 (6.0112), Acc@1: 0.0625 (0.0315), Acc@5: 0.1875 (0.1043)
2021-12-28 02:13:32,013 Val Step[1450/1563], Loss: 5.8420 (6.0050), Acc@1: 0.0000 (0.0322), Acc@5: 0.0000 (0.1056)
2021-12-28 02:13:33,460 Val Step[1500/1563], Loss: 6.0359 (5.9799), Acc@1: 0.0938 (0.0352), Acc@5: 0.1250 (0.1124)
2021-12-28 02:13:34,981 Val Step[1550/1563], Loss: 4.9260 (5.9561), Acc@1: 0.2812 (0.0390), Acc@5: 0.5625 (0.1196)
2021-12-28 02:13:35,838 ----- Epoch[002/310], Validation Loss: 5.9529, Validation Acc@1: 0.0398, Validation Acc@5: 0.1208, time: 109.85
2021-12-28 02:13:35,838 ----- Epoch[002/310], Train Loss: 6.6101, Train Acc: 0.0103, time: 1641.13, Best Val(epoch2) Acc@1: 0.0398
2021-12-28 02:13:36,019 Max accuracy so far: 0.0398 at epoch_2
2021-12-28 02:13:36,019 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 02:13:36,019 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 02:13:36,091 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 02:13:36,091 Now training epoch 3. LR=0.000150
2021-12-28 02:14:53,186 Epoch[003/310], Step[0000/1251], Loss: 6.5208(6.5208), Acc: 0.0078(0.0078)
2021-12-28 02:15:55,994 Epoch[003/310], Step[0050/1251], Loss: 6.5465(6.5358), Acc: 0.0088(0.0126)
2021-12-28 02:16:58,346 Epoch[003/310], Step[0100/1251], Loss: 6.4940(6.5424), Acc: 0.0166(0.0129)
2021-12-28 02:17:59,748 Epoch[003/310], Step[0150/1251], Loss: 6.5350(6.5424), Acc: 0.0195(0.0128)
2021-12-28 02:19:00,756 Epoch[003/310], Step[0200/1251], Loss: 6.4596(6.5453), Acc: 0.0205(0.0129)
2021-12-28 02:20:03,623 Epoch[003/310], Step[0250/1251], Loss: 6.4331(6.5404), Acc: 0.0342(0.0133)
2021-12-28 02:21:05,933 Epoch[003/310], Step[0300/1251], Loss: 6.5202(6.5360), Acc: 0.0156(0.0133)
2021-12-28 02:22:07,631 Epoch[003/310], Step[0350/1251], Loss: 6.5801(6.5345), Acc: 0.0176(0.0136)
2021-12-28 02:23:09,008 Epoch[003/310], Step[0400/1251], Loss: 6.5020(6.5320), Acc: 0.0205(0.0136)
2021-12-28 02:24:12,965 Epoch[003/310], Step[0450/1251], Loss: 6.4971(6.5285), Acc: 0.0195(0.0136)
2021-12-28 02:25:15,701 Epoch[003/310], Step[0500/1251], Loss: 6.5883(6.5258), Acc: 0.0039(0.0137)
2021-12-28 02:26:18,041 Epoch[003/310], Step[0550/1251], Loss: 6.3944(6.5245), Acc: 0.0234(0.0138)
2021-12-28 02:27:20,578 Epoch[003/310], Step[0600/1251], Loss: 6.4589(6.5219), Acc: 0.0195(0.0140)
2021-12-28 02:28:22,936 Epoch[003/310], Step[0650/1251], Loss: 6.6498(6.5198), Acc: 0.0146(0.0140)
2021-12-28 02:29:24,666 Epoch[003/310], Step[0700/1251], Loss: 6.4827(6.5182), Acc: 0.0010(0.0141)
2021-12-28 02:30:27,740 Epoch[003/310], Step[0750/1251], Loss: 6.4262(6.5171), Acc: 0.0107(0.0141)
2021-12-28 02:31:30,578 Epoch[003/310], Step[0800/1251], Loss: 6.5057(6.5151), Acc: 0.0166(0.0142)
2021-12-28 02:32:33,378 Epoch[003/310], Step[0850/1251], Loss: 6.5124(6.5135), Acc: 0.0127(0.0143)
2021-12-28 02:33:36,483 Epoch[003/310], Step[0900/1251], Loss: 6.5201(6.5115), Acc: 0.0127(0.0142)
2021-12-28 02:34:38,106 Epoch[003/310], Step[0950/1251], Loss: 6.4418(6.5091), Acc: 0.0059(0.0143)
2021-12-28 02:35:39,644 Epoch[003/310], Step[1000/1251], Loss: 6.5024(6.5066), Acc: 0.0176(0.0145)
2021-12-28 02:36:41,864 Epoch[003/310], Step[1050/1251], Loss: 6.4407(6.5039), Acc: 0.0146(0.0146)
2021-12-28 02:37:44,734 Epoch[003/310], Step[1100/1251], Loss: 6.4088(6.5017), Acc: 0.0137(0.0147)
2021-12-28 02:38:45,944 Epoch[003/310], Step[1150/1251], Loss: 6.5124(6.5012), Acc: 0.0205(0.0148)
2021-12-28 02:39:48,435 Epoch[003/310], Step[1200/1251], Loss: 6.3755(6.4985), Acc: 0.0146(0.0148)
2021-12-28 02:40:49,419 Epoch[003/310], Step[1250/1251], Loss: 6.4717(6.4974), Acc: 0.0137(0.0149)
2021-12-28 02:40:51,623 ----- Epoch[003/310], Train Loss: 6.4974, Train Acc: 0.0149, time: 1635.53, Best Val(epoch2) Acc@1: 0.0398
2021-12-28 02:40:51,623 Now training epoch 4. LR=0.000200
2021-12-28 02:42:12,677 Epoch[004/310], Step[0000/1251], Loss: 6.4337(6.4337), Acc: 0.0039(0.0039)
2021-12-28 02:43:14,505 Epoch[004/310], Step[0050/1251], Loss: 6.4698(6.4608), Acc: 0.0205(0.0175)
2021-12-28 02:44:14,824 Epoch[004/310], Step[0100/1251], Loss: 6.4222(6.4627), Acc: 0.0195(0.0181)
2021-12-28 02:45:16,376 Epoch[004/310], Step[0150/1251], Loss: 6.4593(6.4582), Acc: 0.0254(0.0185)
2021-12-28 02:46:18,569 Epoch[004/310], Step[0200/1251], Loss: 6.5867(6.4538), Acc: 0.0117(0.0182)
2021-12-28 02:47:19,847 Epoch[004/310], Step[0250/1251], Loss: 6.4943(6.4537), Acc: 0.0059(0.0177)
2021-12-28 02:48:22,048 Epoch[004/310], Step[0300/1251], Loss: 6.3752(6.4499), Acc: 0.0059(0.0181)
2021-12-28 02:49:23,861 Epoch[004/310], Step[0350/1251], Loss: 6.3365(6.4445), Acc: 0.0137(0.0183)
2021-12-28 02:50:25,157 Epoch[004/310], Step[0400/1251], Loss: 6.4265(6.4406), Acc: 0.0127(0.0185)
2021-12-28 02:51:27,093 Epoch[004/310], Step[0450/1251], Loss: 6.2875(6.4395), Acc: 0.0322(0.0185)
2021-12-28 02:52:28,893 Epoch[004/310], Step[0500/1251], Loss: 6.4583(6.4390), Acc: 0.0088(0.0186)
2021-12-28 02:53:31,676 Epoch[004/310], Step[0550/1251], Loss: 6.4258(6.4368), Acc: 0.0166(0.0188)
2021-12-28 02:54:35,388 Epoch[004/310], Step[0600/1251], Loss: 6.4289(6.4348), Acc: 0.0156(0.0189)
2021-12-28 02:55:37,051 Epoch[004/310], Step[0650/1251], Loss: 6.3990(6.4345), Acc: 0.0176(0.0189)
2021-12-28 02:56:41,069 Epoch[004/310], Step[0700/1251], Loss: 6.4094(6.4337), Acc: 0.0215(0.0189)
2021-12-28 02:57:44,303 Epoch[004/310], Step[0750/1251], Loss: 6.3095(6.4324), Acc: 0.0273(0.0189)
2021-12-28 02:58:47,879 Epoch[004/310], Step[0800/1251], Loss: 6.3775(6.4319), Acc: 0.0205(0.0189)
2021-12-28 02:59:50,598 Epoch[004/310], Step[0850/1251], Loss: 6.4454(6.4319), Acc: 0.0312(0.0189)
2021-12-28 03:00:54,205 Epoch[004/310], Step[0900/1251], Loss: 6.4107(6.4296), Acc: 0.0146(0.0191)
2021-12-28 03:01:57,258 Epoch[004/310], Step[0950/1251], Loss: 6.3756(6.4282), Acc: 0.0127(0.0192)
2021-12-28 03:02:59,066 Epoch[004/310], Step[1000/1251], Loss: 6.4438(6.4256), Acc: 0.0234(0.0193)
2021-12-28 03:04:01,330 Epoch[004/310], Step[1050/1251], Loss: 6.5477(6.4250), Acc: 0.0098(0.0194)
2021-12-28 03:05:04,406 Epoch[004/310], Step[1100/1251], Loss: 6.5235(6.4228), Acc: 0.0186(0.0195)
2021-12-28 03:06:05,574 Epoch[004/310], Step[1150/1251], Loss: 6.0741(6.4198), Acc: 0.0459(0.0197)
2021-12-28 03:07:08,039 Epoch[004/310], Step[1200/1251], Loss: 6.3781(6.4182), Acc: 0.0283(0.0199)
2021-12-28 03:08:10,408 Epoch[004/310], Step[1250/1251], Loss: 6.4743(6.4167), Acc: 0.0215(0.0199)
2021-12-28 03:08:12,337 ----- Validation after Epoch: 4
2021-12-28 03:09:11,773 Val Step[0000/1563], Loss: 5.5857 (5.5857), Acc@1: 0.0938 (0.0938), Acc@5: 0.2500 (0.2500)
2021-12-28 03:09:13,337 Val Step[0050/1563], Loss: 5.6522 (5.1074), Acc@1: 0.0000 (0.0974), Acc@5: 0.0938 (0.2966)
2021-12-28 03:09:14,807 Val Step[0100/1563], Loss: 4.8835 (5.3068), Acc@1: 0.0000 (0.0733), Acc@5: 0.2812 (0.2259)
2021-12-28 03:09:16,298 Val Step[0150/1563], Loss: 4.9888 (5.3062), Acc@1: 0.1562 (0.0791), Acc@5: 0.3438 (0.2281)
2021-12-28 03:09:17,768 Val Step[0200/1563], Loss: 5.9477 (5.3002), Acc@1: 0.0000 (0.0813), Acc@5: 0.0000 (0.2261)
2021-12-28 03:09:19,261 Val Step[0250/1563], Loss: 6.2631 (5.2610), Acc@1: 0.0000 (0.0864), Acc@5: 0.0000 (0.2446)
2021-12-28 03:09:20,797 Val Step[0300/1563], Loss: 6.0147 (5.3521), Acc@1: 0.0000 (0.0751), Acc@5: 0.0312 (0.2183)
2021-12-28 03:09:22,266 Val Step[0350/1563], Loss: 4.8924 (5.3735), Acc@1: 0.1875 (0.0715), Acc@5: 0.4062 (0.2123)
2021-12-28 03:09:23,703 Val Step[0400/1563], Loss: 4.8037 (5.3806), Acc@1: 0.0000 (0.0702), Acc@5: 0.1250 (0.2106)
2021-12-28 03:09:25,325 Val Step[0450/1563], Loss: 4.6478 (5.4105), Acc@1: 0.0625 (0.0655), Acc@5: 0.5312 (0.2022)
2021-12-28 03:09:26,861 Val Step[0500/1563], Loss: 4.1501 (5.3955), Acc@1: 0.2188 (0.0672), Acc@5: 0.5312 (0.2048)
2021-12-28 03:09:28,424 Val Step[0550/1563], Loss: 5.9484 (5.3908), Acc@1: 0.0312 (0.0707), Acc@5: 0.0625 (0.2072)
2021-12-28 03:09:30,122 Val Step[0600/1563], Loss: 4.1452 (5.3886), Acc@1: 0.3125 (0.0713), Acc@5: 0.6562 (0.2086)
2021-12-28 03:09:31,669 Val Step[0650/1563], Loss: 5.9684 (5.3911), Acc@1: 0.0312 (0.0747), Acc@5: 0.0625 (0.2114)
2021-12-28 03:09:33,328 Val Step[0700/1563], Loss: 6.6606 (5.4258), Acc@1: 0.0000 (0.0718), Acc@5: 0.0000 (0.2046)
2021-12-28 03:09:35,060 Val Step[0750/1563], Loss: 5.5674 (5.4416), Acc@1: 0.0000 (0.0707), Acc@5: 0.1562 (0.2031)
2021-12-28 03:09:36,582 Val Step[0800/1563], Loss: 5.1198 (5.4501), Acc@1: 0.2188 (0.0707), Acc@5: 0.5000 (0.2033)
2021-12-28 03:09:38,137 Val Step[0850/1563], Loss: 6.0277 (5.4708), Acc@1: 0.0312 (0.0693), Acc@5: 0.0625 (0.1998)
2021-12-28 03:09:39,700 Val Step[0900/1563], Loss: 5.5742 (5.4694), Acc@1: 0.0312 (0.0705), Acc@5: 0.1250 (0.2014)
2021-12-28 03:09:41,324 Val Step[0950/1563], Loss: 5.6599 (5.4824), Acc@1: 0.0625 (0.0701), Acc@5: 0.2812 (0.1989)
2021-12-28 03:09:42,840 Val Step[1000/1563], Loss: 4.0546 (5.4876), Acc@1: 0.4688 (0.0704), Acc@5: 0.7500 (0.1990)
2021-12-28 03:09:44,380 Val Step[1050/1563], Loss: 5.6944 (5.4926), Acc@1: 0.0000 (0.0701), Acc@5: 0.0000 (0.1986)
2021-12-28 03:09:45,883 Val Step[1100/1563], Loss: 6.5358 (5.5044), Acc@1: 0.0000 (0.0695), Acc@5: 0.0000 (0.1964)
2021-12-28 03:09:47,354 Val Step[1150/1563], Loss: 5.6395 (5.5186), Acc@1: 0.0625 (0.0681), Acc@5: 0.1875 (0.1925)
2021-12-28 03:09:48,793 Val Step[1200/1563], Loss: 4.7829 (5.5274), Acc@1: 0.4062 (0.0678), Acc@5: 0.5938 (0.1914)
2021-12-28 03:09:50,369 Val Step[1250/1563], Loss: 5.0767 (5.5373), Acc@1: 0.1250 (0.0674), Acc@5: 0.2812 (0.1897)
2021-12-28 03:09:51,778 Val Step[1300/1563], Loss: 5.4526 (5.5446), Acc@1: 0.0312 (0.0663), Acc@5: 0.1562 (0.1874)
2021-12-28 03:09:53,231 Val Step[1350/1563], Loss: 5.3696 (5.5563), Acc@1: 0.0000 (0.0653), Acc@5: 0.0000 (0.1848)
2021-12-28 03:09:54,680 Val Step[1400/1563], Loss: 5.6455 (5.5588), Acc@1: 0.0000 (0.0651), Acc@5: 0.1250 (0.1845)
2021-12-28 03:09:56,118 Val Step[1450/1563], Loss: 5.3385 (5.5540), Acc@1: 0.0000 (0.0660), Acc@5: 0.1875 (0.1854)
2021-12-28 03:09:57,637 Val Step[1500/1563], Loss: 5.4705 (5.5207), Acc@1: 0.0312 (0.0693), Acc@5: 0.1875 (0.1934)
2021-12-28 03:09:59,109 Val Step[1550/1563], Loss: 3.3275 (5.4918), Acc@1: 0.6250 (0.0746), Acc@5: 0.7812 (0.2006)
2021-12-28 03:09:59,950 ----- Epoch[004/310], Validation Loss: 5.4884, Validation Acc@1: 0.0754, Validation Acc@5: 0.2016, time: 107.61
2021-12-28 03:09:59,951 ----- Epoch[004/310], Train Loss: 6.4167, Train Acc: 0.0199, time: 1640.71, Best Val(epoch4) Acc@1: 0.0754
2021-12-28 03:10:00,172 Max accuracy so far: 0.0754 at epoch_4
2021-12-28 03:10:00,173 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 03:10:00,173 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 03:10:00,254 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 03:10:00,255 Now training epoch 5. LR=0.000250
2021-12-28 03:11:19,125 Epoch[005/310], Step[0000/1251], Loss: 6.2711(6.2711), Acc: 0.0342(0.0342)
2021-12-28 03:12:18,692 Epoch[005/310], Step[0050/1251], Loss: 6.5073(6.3789), Acc: 0.0146(0.0222)
2021-12-28 03:13:17,763 Epoch[005/310], Step[0100/1251], Loss: 6.4768(6.3824), Acc: 0.0215(0.0236)
2021-12-28 03:14:20,346 Epoch[005/310], Step[0150/1251], Loss: 6.3189(6.3667), Acc: 0.0332(0.0244)
2021-12-28 03:15:22,150 Epoch[005/310], Step[0200/1251], Loss: 6.4432(6.3697), Acc: 0.0195(0.0235)
2021-12-28 03:16:24,484 Epoch[005/310], Step[0250/1251], Loss: 6.4518(6.3725), Acc: 0.0312(0.0236)
2021-12-28 03:17:27,989 Epoch[005/310], Step[0300/1251], Loss: 6.4499(6.3703), Acc: 0.0127(0.0233)
2021-12-28 03:18:31,095 Epoch[005/310], Step[0350/1251], Loss: 6.3843(6.3693), Acc: 0.0264(0.0234)
2021-12-28 03:19:33,706 Epoch[005/310], Step[0400/1251], Loss: 6.4956(6.3681), Acc: 0.0146(0.0236)
2021-12-28 03:20:36,633 Epoch[005/310], Step[0450/1251], Loss: 6.1938(6.3684), Acc: 0.0400(0.0236)
2021-12-28 03:21:38,147 Epoch[005/310], Step[0500/1251], Loss: 6.4161(6.3650), Acc: 0.0273(0.0238)
2021-12-28 03:22:39,815 Epoch[005/310], Step[0550/1251], Loss: 6.3705(6.3635), Acc: 0.0068(0.0238)
2021-12-28 03:23:43,921 Epoch[005/310], Step[0600/1251], Loss: 6.2083(6.3632), Acc: 0.0186(0.0238)
2021-12-28 03:24:45,677 Epoch[005/310], Step[0650/1251], Loss: 6.4322(6.3622), Acc: 0.0303(0.0237)
2021-12-28 03:25:49,008 Epoch[005/310], Step[0700/1251], Loss: 6.2162(6.3600), Acc: 0.0312(0.0237)
2021-12-28 03:26:51,870 Epoch[005/310], Step[0750/1251], Loss: 6.4254(6.3571), Acc: 0.0234(0.0239)
2021-12-28 03:27:54,426 Epoch[005/310], Step[0800/1251], Loss: 6.3007(6.3534), Acc: 0.0449(0.0241)
2021-12-28 03:28:58,564 Epoch[005/310], Step[0850/1251], Loss: 6.5177(6.3515), Acc: 0.0146(0.0242)
2021-12-28 03:30:00,525 Epoch[005/310], Step[0900/1251], Loss: 6.2390(6.3502), Acc: 0.0195(0.0243)
2021-12-28 03:31:03,099 Epoch[005/310], Step[0950/1251], Loss: 5.9928(6.3486), Acc: 0.0469(0.0245)
2021-12-28 03:32:05,374 Epoch[005/310], Step[1000/1251], Loss: 6.5459(6.3474), Acc: 0.0127(0.0245)
2021-12-28 03:33:08,579 Epoch[005/310], Step[1050/1251], Loss: 6.3215(6.3449), Acc: 0.0244(0.0247)
2021-12-28 03:34:12,671 Epoch[005/310], Step[1100/1251], Loss: 6.2211(6.3434), Acc: 0.0146(0.0249)
2021-12-28 03:35:15,678 Epoch[005/310], Step[1150/1251], Loss: 6.4775(6.3409), Acc: 0.0156(0.0251)
2021-12-28 03:36:19,216 Epoch[005/310], Step[1200/1251], Loss: 6.2204(6.3393), Acc: 0.0332(0.0252)
2021-12-28 03:37:22,665 Epoch[005/310], Step[1250/1251], Loss: 6.4387(6.3376), Acc: 0.0244(0.0252)
2021-12-28 03:37:24,643 ----- Epoch[005/310], Train Loss: 6.3376, Train Acc: 0.0252, time: 1644.38, Best Val(epoch4) Acc@1: 0.0754
2021-12-28 03:37:24,643 Now training epoch 6. LR=0.000300
2021-12-28 03:38:43,521 Epoch[006/310], Step[0000/1251], Loss: 6.3832(6.3832), Acc: 0.0166(0.0166)
2021-12-28 03:39:46,185 Epoch[006/310], Step[0050/1251], Loss: 6.3771(6.3250), Acc: 0.0391(0.0277)
2021-12-28 03:40:48,606 Epoch[006/310], Step[0100/1251], Loss: 6.0324(6.3163), Acc: 0.0479(0.0279)
2021-12-28 03:41:51,480 Epoch[006/310], Step[0150/1251], Loss: 6.3904(6.3213), Acc: 0.0234(0.0280)
2021-12-28 03:42:53,903 Epoch[006/310], Step[0200/1251], Loss: 6.2318(6.3172), Acc: 0.0361(0.0273)
2021-12-28 03:43:57,297 Epoch[006/310], Step[0250/1251], Loss: 6.3587(6.3079), Acc: 0.0127(0.0275)
2021-12-28 03:45:00,054 Epoch[006/310], Step[0300/1251], Loss: 6.2288(6.3094), Acc: 0.0322(0.0276)
2021-12-28 03:46:02,448 Epoch[006/310], Step[0350/1251], Loss: 6.1735(6.3069), Acc: 0.0117(0.0279)
2021-12-28 03:47:02,187 Epoch[006/310], Step[0400/1251], Loss: 6.4323(6.3060), Acc: 0.0322(0.0280)
2021-12-28 03:48:01,871 Epoch[006/310], Step[0450/1251], Loss: 6.1138(6.3005), Acc: 0.0264(0.0285)
2021-12-28 03:49:04,047 Epoch[006/310], Step[0500/1251], Loss: 6.4421(6.2980), Acc: 0.0176(0.0285)
2021-12-28 03:50:06,262 Epoch[006/310], Step[0550/1251], Loss: 6.2870(6.2979), Acc: 0.0459(0.0285)
2021-12-28 03:51:08,180 Epoch[006/310], Step[0600/1251], Loss: 6.2377(6.2931), Acc: 0.0205(0.0286)
2021-12-28 03:52:10,823 Epoch[006/310], Step[0650/1251], Loss: 6.1608(6.2910), Acc: 0.0283(0.0287)
2021-12-28 03:53:14,364 Epoch[006/310], Step[0700/1251], Loss: 6.1621(6.2884), Acc: 0.0293(0.0288)
2021-12-28 03:54:17,734 Epoch[006/310], Step[0750/1251], Loss: 6.2115(6.2851), Acc: 0.0449(0.0291)
2021-12-28 03:55:17,120 Epoch[006/310], Step[0800/1251], Loss: 6.4529(6.2834), Acc: 0.0264(0.0294)
2021-12-28 03:56:19,067 Epoch[006/310], Step[0850/1251], Loss: 6.2526(6.2824), Acc: 0.0264(0.0295)
2021-12-28 03:57:22,218 Epoch[006/310], Step[0900/1251], Loss: 6.0972(6.2802), Acc: 0.0381(0.0296)
2021-12-28 03:58:25,537 Epoch[006/310], Step[0950/1251], Loss: 6.2266(6.2776), Acc: 0.0508(0.0298)
2021-12-28 03:59:28,743 Epoch[006/310], Step[1000/1251], Loss: 6.2157(6.2748), Acc: 0.0410(0.0298)
2021-12-28 04:00:32,699 Epoch[006/310], Step[1050/1251], Loss: 6.1232(6.2727), Acc: 0.0557(0.0298)
2021-12-28 04:01:35,307 Epoch[006/310], Step[1100/1251], Loss: 6.3269(6.2704), Acc: 0.0225(0.0299)
2021-12-28 04:02:36,923 Epoch[006/310], Step[1150/1251], Loss: 6.1745(6.2677), Acc: 0.0635(0.0301)
2021-12-28 04:03:39,400 Epoch[006/310], Step[1200/1251], Loss: 6.5095(6.2665), Acc: 0.0244(0.0303)
2021-12-28 04:04:42,437 Epoch[006/310], Step[1250/1251], Loss: 6.2276(6.2656), Acc: 0.0234(0.0305)
2021-12-28 04:04:44,585 ----- Validation after Epoch: 6
2021-12-28 04:06:01,772 Val Step[0000/1563], Loss: 4.5633 (4.5633), Acc@1: 0.4375 (0.4375), Acc@5: 0.6250 (0.6250)
2021-12-28 04:06:03,477 Val Step[0050/1563], Loss: 5.5109 (4.6850), Acc@1: 0.0000 (0.1789), Acc@5: 0.0312 (0.3732)
2021-12-28 04:06:05,020 Val Step[0100/1563], Loss: 4.4117 (4.8921), Acc@1: 0.0625 (0.1334), Acc@5: 0.5938 (0.3082)
2021-12-28 04:06:06,585 Val Step[0150/1563], Loss: 4.7017 (4.8700), Acc@1: 0.2188 (0.1364), Acc@5: 0.4688 (0.3181)
2021-12-28 04:06:08,130 Val Step[0200/1563], Loss: 5.3797 (4.8908), Acc@1: 0.0312 (0.1357), Acc@5: 0.0938 (0.3148)
2021-12-28 04:06:09,676 Val Step[0250/1563], Loss: 5.8486 (4.8120), Acc@1: 0.0000 (0.1460), Acc@5: 0.0625 (0.3379)
2021-12-28 04:06:11,192 Val Step[0300/1563], Loss: 5.5279 (4.9053), Acc@1: 0.0000 (0.1266), Acc@5: 0.0625 (0.3060)
2021-12-28 04:06:12,706 Val Step[0350/1563], Loss: 5.1189 (4.9267), Acc@1: 0.1250 (0.1188), Acc@5: 0.3438 (0.2953)
2021-12-28 04:06:14,252 Val Step[0400/1563], Loss: 4.8271 (4.9363), Acc@1: 0.0625 (0.1169), Acc@5: 0.2812 (0.2923)
2021-12-28 04:06:15,762 Val Step[0450/1563], Loss: 3.3774 (4.9552), Acc@1: 0.0625 (0.1111), Acc@5: 0.7812 (0.2864)
2021-12-28 04:06:17,317 Val Step[0500/1563], Loss: 3.9731 (4.9440), Acc@1: 0.2812 (0.1122), Acc@5: 0.5938 (0.2886)
2021-12-28 04:06:19,010 Val Step[0550/1563], Loss: 4.7111 (4.9255), Acc@1: 0.1250 (0.1178), Acc@5: 0.4062 (0.2949)
2021-12-28 04:06:20,609 Val Step[0600/1563], Loss: 3.6423 (4.9401), Acc@1: 0.3750 (0.1163), Acc@5: 0.7188 (0.2916)
2021-12-28 04:06:22,095 Val Step[0650/1563], Loss: 5.1332 (4.9398), Acc@1: 0.0625 (0.1186), Acc@5: 0.2812 (0.2934)
2021-12-28 04:06:23,555 Val Step[0700/1563], Loss: 6.3114 (4.9704), Acc@1: 0.0000 (0.1155), Acc@5: 0.0000 (0.2883)
2021-12-28 04:06:25,156 Val Step[0750/1563], Loss: 4.6611 (4.9849), Acc@1: 0.1562 (0.1143), Acc@5: 0.4375 (0.2853)
2021-12-28 04:06:26,760 Val Step[0800/1563], Loss: 4.8228 (4.9978), Acc@1: 0.3125 (0.1126), Acc@5: 0.4375 (0.2838)
2021-12-28 04:06:28,212 Val Step[0850/1563], Loss: 6.0093 (5.0180), Acc@1: 0.0000 (0.1100), Acc@5: 0.0625 (0.2785)
2021-12-28 04:06:29,836 Val Step[0900/1563], Loss: 4.6084 (5.0112), Acc@1: 0.1875 (0.1112), Acc@5: 0.4062 (0.2817)
2021-12-28 04:06:31,427 Val Step[0950/1563], Loss: 5.4470 (5.0249), Acc@1: 0.0938 (0.1106), Acc@5: 0.2188 (0.2789)
2021-12-28 04:06:33,013 Val Step[1000/1563], Loss: 3.0228 (5.0330), Acc@1: 0.6250 (0.1107), Acc@5: 0.8125 (0.2774)
2021-12-28 04:06:34,570 Val Step[1050/1563], Loss: 5.3903 (5.0369), Acc@1: 0.0000 (0.1093), Acc@5: 0.0312 (0.2764)
2021-12-28 04:06:36,146 Val Step[1100/1563], Loss: 6.0367 (5.0460), Acc@1: 0.0000 (0.1098), Acc@5: 0.0312 (0.2745)
2021-12-28 04:06:37,633 Val Step[1150/1563], Loss: 5.4603 (5.0609), Acc@1: 0.0312 (0.1077), Acc@5: 0.1875 (0.2706)
2021-12-28 04:06:39,095 Val Step[1200/1563], Loss: 4.5543 (5.0723), Acc@1: 0.3750 (0.1068), Acc@5: 0.5000 (0.2682)
2021-12-28 04:06:40,602 Val Step[1250/1563], Loss: 4.0460 (5.0803), Acc@1: 0.3750 (0.1067), Acc@5: 0.5938 (0.2669)
2021-12-28 04:06:41,997 Val Step[1300/1563], Loss: 5.3720 (5.0869), Acc@1: 0.0000 (0.1055), Acc@5: 0.0938 (0.2646)
2021-12-28 04:06:43,465 Val Step[1350/1563], Loss: 4.7858 (5.1007), Acc@1: 0.0000 (0.1035), Acc@5: 0.0938 (0.2610)
2021-12-28 04:06:44,953 Val Step[1400/1563], Loss: 5.0671 (5.1027), Acc@1: 0.0625 (0.1032), Acc@5: 0.2812 (0.2608)
2021-12-28 04:06:46,393 Val Step[1450/1563], Loss: 5.1176 (5.1004), Acc@1: 0.0312 (0.1038), Acc@5: 0.1562 (0.2613)
2021-12-28 04:06:48,037 Val Step[1500/1563], Loss: 5.3115 (5.0704), Acc@1: 0.0938 (0.1081), Acc@5: 0.2812 (0.2689)
2021-12-28 04:06:49,546 Val Step[1550/1563], Loss: 2.5734 (5.0449), Acc@1: 0.7500 (0.1126), Acc@5: 0.8438 (0.2755)
2021-12-28 04:06:50,395 ----- Epoch[006/310], Validation Loss: 5.0382, Validation Acc@1: 0.1141, Validation Acc@5: 0.2775, time: 125.81
2021-12-28 04:06:50,395 ----- Epoch[006/310], Train Loss: 6.2656, Train Acc: 0.0305, time: 1639.94, Best Val(epoch6) Acc@1: 0.1141
2021-12-28 04:06:50,607 Max accuracy so far: 0.1141 at epoch_6
2021-12-28 04:06:50,608 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 04:06:50,608 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 04:06:50,690 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 04:06:50,691 Now training epoch 7. LR=0.000350
2021-12-28 04:08:10,652 Epoch[007/310], Step[0000/1251], Loss: 6.1249(6.1249), Acc: 0.0371(0.0371)
2021-12-28 04:09:11,614 Epoch[007/310], Step[0050/1251], Loss: 6.2588(6.2178), Acc: 0.0166(0.0355)
2021-12-28 04:10:12,470 Epoch[007/310], Step[0100/1251], Loss: 6.2578(6.2173), Acc: 0.0400(0.0337)
2021-12-28 04:11:14,628 Epoch[007/310], Step[0150/1251], Loss: 6.1997(6.2183), Acc: 0.0479(0.0328)
2021-12-28 04:12:17,507 Epoch[007/310], Step[0200/1251], Loss: 6.1353(6.2176), Acc: 0.0254(0.0337)
2021-12-28 04:13:19,782 Epoch[007/310], Step[0250/1251], Loss: 6.3358(6.2147), Acc: 0.0303(0.0344)
2021-12-28 04:14:22,669 Epoch[007/310], Step[0300/1251], Loss: 6.1561(6.2097), Acc: 0.0293(0.0342)
2021-12-28 04:15:24,996 Epoch[007/310], Step[0350/1251], Loss: 6.3189(6.2076), Acc: 0.0205(0.0346)
2021-12-28 04:16:26,856 Epoch[007/310], Step[0400/1251], Loss: 6.0281(6.2121), Acc: 0.0322(0.0344)
2021-12-28 04:17:30,418 Epoch[007/310], Step[0450/1251], Loss: 6.0641(6.2103), Acc: 0.0293(0.0347)
2021-12-28 04:18:33,407 Epoch[007/310], Step[0500/1251], Loss: 6.3147(6.2107), Acc: 0.0322(0.0347)
2021-12-28 04:19:35,661 Epoch[007/310], Step[0550/1251], Loss: 6.1012(6.2088), Acc: 0.0391(0.0348)
2021-12-28 04:20:37,729 Epoch[007/310], Step[0600/1251], Loss: 6.4069(6.2091), Acc: 0.0215(0.0350)
2021-12-28 04:21:39,673 Epoch[007/310], Step[0650/1251], Loss: 6.3839(6.2051), Acc: 0.0195(0.0353)
2021-12-28 04:22:41,466 Epoch[007/310], Step[0700/1251], Loss: 6.2860(6.2024), Acc: 0.0205(0.0356)
2021-12-28 04:23:44,693 Epoch[007/310], Step[0750/1251], Loss: 6.3975(6.2006), Acc: 0.0410(0.0357)
2021-12-28 04:24:46,326 Epoch[007/310], Step[0800/1251], Loss: 6.0580(6.1996), Acc: 0.0273(0.0358)
2021-12-28 04:25:49,436 Epoch[007/310], Step[0850/1251], Loss: 6.3677(6.1981), Acc: 0.0293(0.0361)
2021-12-28 04:26:50,950 Epoch[007/310], Step[0900/1251], Loss: 6.0862(6.1959), Acc: 0.0244(0.0362)
2021-12-28 04:27:53,429 Epoch[007/310], Step[0950/1251], Loss: 6.1484(6.1939), Acc: 0.0547(0.0362)
2021-12-28 04:28:54,093 Epoch[007/310], Step[1000/1251], Loss: 5.9857(6.1934), Acc: 0.0391(0.0364)
2021-12-28 04:29:57,004 Epoch[007/310], Step[1050/1251], Loss: 6.1963(6.1923), Acc: 0.0430(0.0366)
2021-12-28 04:30:58,615 Epoch[007/310], Step[1100/1251], Loss: 6.3372(6.1920), Acc: 0.0186(0.0367)
2021-12-28 04:32:00,906 Epoch[007/310], Step[1150/1251], Loss: 6.3451(6.1915), Acc: 0.0420(0.0368)
2021-12-28 04:33:02,650 Epoch[007/310], Step[1200/1251], Loss: 6.0608(6.1892), Acc: 0.0420(0.0370)
2021-12-28 04:34:05,629 Epoch[007/310], Step[1250/1251], Loss: 6.3188(6.1875), Acc: 0.0303(0.0372)
2021-12-28 04:34:07,943 ----- Epoch[007/310], Train Loss: 6.1875, Train Acc: 0.0372, time: 1637.25, Best Val(epoch6) Acc@1: 0.1141
2021-12-28 04:34:07,943 Now training epoch 8. LR=0.000400
2021-12-28 04:35:29,412 Epoch[008/310], Step[0000/1251], Loss: 6.1598(6.1598), Acc: 0.0400(0.0400)
2021-12-28 04:36:29,589 Epoch[008/310], Step[0050/1251], Loss: 6.0677(6.1395), Acc: 0.0664(0.0439)
2021-12-28 04:37:31,980 Epoch[008/310], Step[0100/1251], Loss: 5.9164(6.1500), Acc: 0.0322(0.0408)
2021-12-28 04:38:35,089 Epoch[008/310], Step[0150/1251], Loss: 6.2306(6.1479), Acc: 0.0537(0.0402)
2021-12-28 04:39:37,239 Epoch[008/310], Step[0200/1251], Loss: 6.3538(6.1446), Acc: 0.0186(0.0398)
2021-12-28 04:40:41,226 Epoch[008/310], Step[0250/1251], Loss: 5.9127(6.1374), Acc: 0.0332(0.0398)
2021-12-28 04:41:44,920 Epoch[008/310], Step[0300/1251], Loss: 6.2269(6.1285), Acc: 0.0615(0.0408)
2021-12-28 04:42:48,458 Epoch[008/310], Step[0350/1251], Loss: 6.1453(6.1303), Acc: 0.0479(0.0408)
2021-12-28 04:43:51,587 Epoch[008/310], Step[0400/1251], Loss: 6.2626(6.1299), Acc: 0.0283(0.0410)
2021-12-28 04:44:54,894 Epoch[008/310], Step[0450/1251], Loss: 6.1011(6.1269), Acc: 0.0645(0.0418)
2021-12-28 04:45:57,388 Epoch[008/310], Step[0500/1251], Loss: 6.0233(6.1257), Acc: 0.0703(0.0418)
2021-12-28 04:47:00,433 Epoch[008/310], Step[0550/1251], Loss: 6.1314(6.1222), Acc: 0.0332(0.0417)
2021-12-28 04:48:02,484 Epoch[008/310], Step[0600/1251], Loss: 6.2690(6.1198), Acc: 0.0518(0.0423)
2021-12-28 04:49:06,106 Epoch[008/310], Step[0650/1251], Loss: 5.8832(6.1203), Acc: 0.0459(0.0426)
2021-12-28 04:50:10,161 Epoch[008/310], Step[0700/1251], Loss: 6.2126(6.1209), Acc: 0.0557(0.0429)
2021-12-28 04:51:13,393 Epoch[008/310], Step[0750/1251], Loss: 5.8652(6.1181), Acc: 0.0576(0.0430)
2021-12-28 04:52:16,707 Epoch[008/310], Step[0800/1251], Loss: 6.1133(6.1137), Acc: 0.0469(0.0433)
2021-12-28 04:53:19,773 Epoch[008/310], Step[0850/1251], Loss: 6.0166(6.1093), Acc: 0.0557(0.0437)
2021-12-28 04:54:23,694 Epoch[008/310], Step[0900/1251], Loss: 6.2069(6.1086), Acc: 0.0332(0.0437)
2021-12-28 04:55:27,836 Epoch[008/310], Step[0950/1251], Loss: 6.2659(6.1079), Acc: 0.0557(0.0437)
2021-12-28 04:56:29,508 Epoch[008/310], Step[1000/1251], Loss: 6.3163(6.1074), Acc: 0.0283(0.0437)
2021-12-28 04:57:33,031 Epoch[008/310], Step[1050/1251], Loss: 5.7493(6.1072), Acc: 0.0566(0.0436)
2021-12-28 04:58:35,480 Epoch[008/310], Step[1100/1251], Loss: 6.1479(6.1046), Acc: 0.0479(0.0439)
2021-12-28 04:59:38,360 Epoch[008/310], Step[1150/1251], Loss: 5.9699(6.1021), Acc: 0.0625(0.0440)
2021-12-28 05:00:41,408 Epoch[008/310], Step[1200/1251], Loss: 6.1207(6.1003), Acc: 0.0127(0.0439)
2021-12-28 05:01:43,995 Epoch[008/310], Step[1250/1251], Loss: 6.0955(6.0959), Acc: 0.0762(0.0444)
2021-12-28 05:01:46,154 ----- Validation after Epoch: 8
2021-12-28 05:02:53,459 Val Step[0000/1563], Loss: 4.2232 (4.2232), Acc@1: 0.3750 (0.3750), Acc@5: 0.5625 (0.5625)
2021-12-28 05:02:54,988 Val Step[0050/1563], Loss: 5.3471 (4.1724), Acc@1: 0.0625 (0.2279), Acc@5: 0.0625 (0.4761)
2021-12-28 05:02:56,508 Val Step[0100/1563], Loss: 4.2527 (4.5074), Acc@1: 0.2188 (0.1764), Acc@5: 0.5625 (0.3815)
2021-12-28 05:02:58,057 Val Step[0150/1563], Loss: 3.9361 (4.4241), Acc@1: 0.3750 (0.1898), Acc@5: 0.5625 (0.4000)
2021-12-28 05:02:59,505 Val Step[0200/1563], Loss: 5.2120 (4.4405), Acc@1: 0.0312 (0.1920), Acc@5: 0.1562 (0.3960)
2021-12-28 05:03:01,058 Val Step[0250/1563], Loss: 4.7525 (4.3457), Acc@1: 0.0625 (0.2044), Acc@5: 0.2500 (0.4137)
2021-12-28 05:03:02,625 Val Step[0300/1563], Loss: 4.8609 (4.3945), Acc@1: 0.0000 (0.1842), Acc@5: 0.1562 (0.3905)
2021-12-28 05:03:04,094 Val Step[0350/1563], Loss: 4.1755 (4.3781), Acc@1: 0.1250 (0.1805), Acc@5: 0.4062 (0.3918)
2021-12-28 05:03:05,644 Val Step[0400/1563], Loss: 4.3990 (4.3451), Acc@1: 0.0000 (0.1792), Acc@5: 0.0938 (0.3967)
2021-12-28 05:03:07,220 Val Step[0450/1563], Loss: 3.0670 (4.3572), Acc@1: 0.0312 (0.1728), Acc@5: 0.8125 (0.3907)
2021-12-28 05:03:08,766 Val Step[0500/1563], Loss: 3.4612 (4.3369), Acc@1: 0.3750 (0.1752), Acc@5: 0.6562 (0.3951)
2021-12-28 05:03:10,262 Val Step[0550/1563], Loss: 3.6726 (4.3301), Acc@1: 0.3750 (0.1790), Acc@5: 0.6250 (0.3983)
2021-12-28 05:03:11,877 Val Step[0600/1563], Loss: 2.9074 (4.3485), Acc@1: 0.5312 (0.1764), Acc@5: 0.7500 (0.3935)
2021-12-28 05:03:13,359 Val Step[0650/1563], Loss: 4.5197 (4.3566), Acc@1: 0.1562 (0.1783), Acc@5: 0.5000 (0.3939)
2021-12-28 05:03:14,815 Val Step[0700/1563], Loss: 5.8308 (4.4032), Acc@1: 0.0312 (0.1735), Acc@5: 0.1562 (0.3857)
2021-12-28 05:03:16,318 Val Step[0750/1563], Loss: 4.0523 (4.4388), Acc@1: 0.3438 (0.1699), Acc@5: 0.5625 (0.3787)
2021-12-28 05:03:17,860 Val Step[0800/1563], Loss: 4.4569 (4.4670), Acc@1: 0.3438 (0.1669), Acc@5: 0.5938 (0.3745)
2021-12-28 05:03:19,308 Val Step[0850/1563], Loss: 5.0408 (4.4992), Acc@1: 0.0625 (0.1642), Acc@5: 0.2188 (0.3681)
2021-12-28 05:03:20,829 Val Step[0900/1563], Loss: 4.3009 (4.5020), Acc@1: 0.2500 (0.1655), Acc@5: 0.5000 (0.3696)
2021-12-28 05:03:22,433 Val Step[0950/1563], Loss: 5.1388 (4.5251), Acc@1: 0.1562 (0.1642), Acc@5: 0.3438 (0.3654)
2021-12-28 05:03:23,908 Val Step[1000/1563], Loss: 2.8231 (4.5383), Acc@1: 0.5625 (0.1638), Acc@5: 0.8125 (0.3639)
2021-12-28 05:03:25,375 Val Step[1050/1563], Loss: 4.4427 (4.5463), Acc@1: 0.0938 (0.1622), Acc@5: 0.4062 (0.3630)
2021-12-28 05:03:26,948 Val Step[1100/1563], Loss: 5.9044 (4.5624), Acc@1: 0.0000 (0.1618), Acc@5: 0.0312 (0.3596)
2021-12-28 05:03:28,459 Val Step[1150/1563], Loss: 4.7928 (4.5845), Acc@1: 0.1250 (0.1595), Acc@5: 0.3750 (0.3555)
2021-12-28 05:03:29,995 Val Step[1200/1563], Loss: 3.0901 (4.6010), Acc@1: 0.5938 (0.1578), Acc@5: 0.6562 (0.3527)
2021-12-28 05:03:31,538 Val Step[1250/1563], Loss: 3.7739 (4.6168), Acc@1: 0.3438 (0.1571), Acc@5: 0.6250 (0.3494)
2021-12-28 05:03:33,038 Val Step[1300/1563], Loss: 4.5142 (4.6266), Acc@1: 0.0938 (0.1555), Acc@5: 0.4688 (0.3474)
2021-12-28 05:03:34,641 Val Step[1350/1563], Loss: 4.1959 (4.6451), Acc@1: 0.0625 (0.1530), Acc@5: 0.2188 (0.3425)
2021-12-28 05:03:36,200 Val Step[1400/1563], Loss: 4.9120 (4.6528), Acc@1: 0.0312 (0.1517), Acc@5: 0.2188 (0.3412)
2021-12-28 05:03:37,668 Val Step[1450/1563], Loss: 4.9033 (4.6534), Acc@1: 0.0000 (0.1524), Acc@5: 0.1562 (0.3411)
2021-12-28 05:03:39,117 Val Step[1500/1563], Loss: 4.6982 (4.6249), Acc@1: 0.1875 (0.1564), Acc@5: 0.4062 (0.3479)
2021-12-28 05:03:40,583 Val Step[1550/1563], Loss: 1.8617 (4.5963), Acc@1: 0.7500 (0.1622), Acc@5: 0.8750 (0.3541)
2021-12-28 05:03:41,442 ----- Epoch[008/310], Validation Loss: 4.5880, Validation Acc@1: 0.1640, Validation Acc@5: 0.3561, time: 115.29
2021-12-28 05:03:41,442 ----- Epoch[008/310], Train Loss: 6.0959, Train Acc: 0.0444, time: 1658.21, Best Val(epoch8) Acc@1: 0.1640
2021-12-28 05:03:41,647 Max accuracy so far: 0.1640 at epoch_8
2021-12-28 05:03:41,648 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 05:03:41,648 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 05:03:41,743 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 05:03:41,744 Now training epoch 9. LR=0.000450
2021-12-28 05:05:11,722 Epoch[009/310], Step[0000/1251], Loss: 6.1375(6.1375), Acc: 0.0537(0.0537)
2021-12-28 05:06:14,082 Epoch[009/310], Step[0050/1251], Loss: 6.4220(6.0726), Acc: 0.0264(0.0480)
2021-12-28 05:07:15,942 Epoch[009/310], Step[0100/1251], Loss: 5.9458(6.0661), Acc: 0.0430(0.0489)
2021-12-28 05:08:19,244 Epoch[009/310], Step[0150/1251], Loss: 6.0973(6.0586), Acc: 0.0615(0.0492)
2021-12-28 05:09:20,625 Epoch[009/310], Step[0200/1251], Loss: 5.9580(6.0590), Acc: 0.0791(0.0492)
2021-12-28 05:10:22,951 Epoch[009/310], Step[0250/1251], Loss: 5.9317(6.0513), Acc: 0.0605(0.0488)
2021-12-28 05:11:24,442 Epoch[009/310], Step[0300/1251], Loss: 5.8982(6.0513), Acc: 0.0566(0.0492)
2021-12-28 05:12:28,234 Epoch[009/310], Step[0350/1251], Loss: 6.0040(6.0509), Acc: 0.0273(0.0494)
2021-12-28 05:13:31,232 Epoch[009/310], Step[0400/1251], Loss: 5.9850(6.0504), Acc: 0.0693(0.0490)
2021-12-28 05:14:34,215 Epoch[009/310], Step[0450/1251], Loss: 5.7783(6.0447), Acc: 0.0850(0.0495)
2021-12-28 05:15:36,559 Epoch[009/310], Step[0500/1251], Loss: 5.9475(6.0422), Acc: 0.0684(0.0495)
2021-12-28 05:16:38,939 Epoch[009/310], Step[0550/1251], Loss: 5.7596(6.0385), Acc: 0.0684(0.0499)
2021-12-28 05:17:41,469 Epoch[009/310], Step[0600/1251], Loss: 6.0748(6.0374), Acc: 0.0566(0.0505)
2021-12-28 05:18:45,370 Epoch[009/310], Step[0650/1251], Loss: 6.2354(6.0360), Acc: 0.0293(0.0503)
2021-12-28 05:19:48,612 Epoch[009/310], Step[0700/1251], Loss: 5.7653(6.0344), Acc: 0.0928(0.0502)
2021-12-28 05:20:51,897 Epoch[009/310], Step[0750/1251], Loss: 5.9508(6.0323), Acc: 0.0615(0.0506)
2021-12-28 05:21:54,084 Epoch[009/310], Step[0800/1251], Loss: 5.9214(6.0305), Acc: 0.0762(0.0512)
2021-12-28 05:22:57,213 Epoch[009/310], Step[0850/1251], Loss: 5.8969(6.0296), Acc: 0.0264(0.0515)
2021-12-28 05:24:00,534 Epoch[009/310], Step[0900/1251], Loss: 6.0071(6.0262), Acc: 0.0684(0.0520)
2021-12-28 05:25:04,377 Epoch[009/310], Step[0950/1251], Loss: 5.9693(6.0216), Acc: 0.0352(0.0524)
2021-12-28 05:26:08,354 Epoch[009/310], Step[1000/1251], Loss: 5.9004(6.0211), Acc: 0.0420(0.0523)
2021-12-28 05:27:11,898 Epoch[009/310], Step[1050/1251], Loss: 6.0209(6.0180), Acc: 0.0342(0.0527)
2021-12-28 05:28:15,226 Epoch[009/310], Step[1100/1251], Loss: 6.0232(6.0151), Acc: 0.0576(0.0528)
2021-12-28 05:29:18,970 Epoch[009/310], Step[1150/1251], Loss: 5.7622(6.0125), Acc: 0.0244(0.0530)
2021-12-28 05:30:21,213 Epoch[009/310], Step[1200/1251], Loss: 6.0653(6.0093), Acc: 0.0322(0.0532)
2021-12-28 05:31:24,372 Epoch[009/310], Step[1250/1251], Loss: 6.1896(6.0062), Acc: 0.0166(0.0535)
2021-12-28 05:31:26,336 ----- Epoch[009/310], Train Loss: 6.0062, Train Acc: 0.0535, time: 1664.59, Best Val(epoch8) Acc@1: 0.1640
2021-12-28 05:31:26,336 Now training epoch 10. LR=0.000500
2021-12-28 05:32:45,897 Epoch[010/310], Step[0000/1251], Loss: 5.9057(5.9057), Acc: 0.0771(0.0771)
2021-12-28 05:33:44,684 Epoch[010/310], Step[0050/1251], Loss: 5.8278(6.0196), Acc: 0.0420(0.0511)
2021-12-28 05:34:46,429 Epoch[010/310], Step[0100/1251], Loss: 6.1851(6.0031), Acc: 0.0625(0.0541)
2021-12-28 05:35:49,353 Epoch[010/310], Step[0150/1251], Loss: 5.5877(5.9804), Acc: 0.0781(0.0562)
2021-12-28 05:36:52,636 Epoch[010/310], Step[0200/1251], Loss: 5.8691(5.9794), Acc: 0.0508(0.0551)
2021-12-28 05:37:55,469 Epoch[010/310], Step[0250/1251], Loss: 5.5436(5.9822), Acc: 0.0840(0.0550)
2021-12-28 05:38:57,420 Epoch[010/310], Step[0300/1251], Loss: 5.6745(5.9744), Acc: 0.0684(0.0554)
2021-12-28 05:40:00,564 Epoch[010/310], Step[0350/1251], Loss: 6.2661(5.9726), Acc: 0.0264(0.0562)
2021-12-28 05:41:02,936 Epoch[010/310], Step[0400/1251], Loss: 5.8947(5.9635), Acc: 0.1016(0.0570)
2021-12-28 05:42:05,370 Epoch[010/310], Step[0450/1251], Loss: 5.7852(5.9632), Acc: 0.1113(0.0574)
2021-12-28 05:43:07,686 Epoch[010/310], Step[0500/1251], Loss: 6.1793(5.9571), Acc: 0.0615(0.0582)
2021-12-28 05:44:09,071 Epoch[010/310], Step[0550/1251], Loss: 5.8962(5.9543), Acc: 0.0361(0.0588)
2021-12-28 05:45:11,855 Epoch[010/310], Step[0600/1251], Loss: 5.6894(5.9519), Acc: 0.0684(0.0592)
2021-12-28 05:46:15,616 Epoch[010/310], Step[0650/1251], Loss: 5.9433(5.9501), Acc: 0.0947(0.0591)
2021-12-28 05:47:17,403 Epoch[010/310], Step[0700/1251], Loss: 6.1404(5.9512), Acc: 0.0762(0.0590)
2021-12-28 05:48:20,781 Epoch[010/310], Step[0750/1251], Loss: 5.7930(5.9446), Acc: 0.0732(0.0591)
2021-12-28 05:49:22,759 Epoch[010/310], Step[0800/1251], Loss: 5.6043(5.9418), Acc: 0.0586(0.0595)
2021-12-28 05:50:25,374 Epoch[010/310], Step[0850/1251], Loss: 6.1905(5.9380), Acc: 0.0479(0.0599)
2021-12-28 05:51:28,053 Epoch[010/310], Step[0900/1251], Loss: 5.8473(5.9343), Acc: 0.0859(0.0603)
2021-12-28 05:52:32,409 Epoch[010/310], Step[0950/1251], Loss: 6.1441(5.9327), Acc: 0.0459(0.0604)
2021-12-28 05:53:35,948 Epoch[010/310], Step[1000/1251], Loss: 6.3020(5.9314), Acc: 0.0449(0.0603)
2021-12-28 05:54:39,337 Epoch[010/310], Step[1050/1251], Loss: 5.7227(5.9293), Acc: 0.0674(0.0603)
2021-12-28 05:55:43,189 Epoch[010/310], Step[1100/1251], Loss: 6.0977(5.9276), Acc: 0.0605(0.0603)
2021-12-28 05:56:46,178 Epoch[010/310], Step[1150/1251], Loss: 5.9378(5.9244), Acc: 0.0400(0.0605)
2021-12-28 05:57:48,697 Epoch[010/310], Step[1200/1251], Loss: 5.5351(5.9224), Acc: 0.1016(0.0611)
2021-12-28 05:58:51,712 Epoch[010/310], Step[1250/1251], Loss: 5.9723(5.9210), Acc: 0.0645(0.0613)
2021-12-28 05:58:53,745 ----- Validation after Epoch: 10
2021-12-28 05:59:56,848 Val Step[0000/1563], Loss: 3.3614 (3.3614), Acc@1: 0.5938 (0.5938), Acc@5: 0.6562 (0.6562)
2021-12-28 05:59:58,403 Val Step[0050/1563], Loss: 5.0017 (3.6205), Acc@1: 0.0312 (0.2947), Acc@5: 0.2500 (0.5729)
2021-12-28 05:59:59,976 Val Step[0100/1563], Loss: 3.7050 (3.9848), Acc@1: 0.3438 (0.2376), Acc@5: 0.6875 (0.4833)
2021-12-28 06:00:01,461 Val Step[0150/1563], Loss: 3.4145 (3.9175), Acc@1: 0.3125 (0.2554), Acc@5: 0.6875 (0.4977)
2021-12-28 06:00:03,024 Val Step[0200/1563], Loss: 4.3618 (3.9550), Acc@1: 0.1562 (0.2565), Acc@5: 0.4062 (0.4939)
2021-12-28 06:00:04,511 Val Step[0250/1563], Loss: 4.6667 (3.8546), Acc@1: 0.0938 (0.2743), Acc@5: 0.3750 (0.5110)
2021-12-28 06:00:06,026 Val Step[0300/1563], Loss: 4.5568 (3.9448), Acc@1: 0.0312 (0.2500), Acc@5: 0.2188 (0.4864)
2021-12-28 06:00:07,477 Val Step[0350/1563], Loss: 4.3282 (3.9784), Acc@1: 0.1562 (0.2382), Acc@5: 0.4062 (0.4767)
2021-12-28 06:00:08,922 Val Step[0400/1563], Loss: 4.1717 (3.9758), Acc@1: 0.1250 (0.2345), Acc@5: 0.4062 (0.4776)
2021-12-28 06:00:10,526 Val Step[0450/1563], Loss: 2.4690 (3.9968), Acc@1: 0.3125 (0.2271), Acc@5: 0.8438 (0.4728)
2021-12-28 06:00:12,007 Val Step[0500/1563], Loss: 2.6461 (3.9822), Acc@1: 0.5625 (0.2295), Acc@5: 0.7812 (0.4769)
2021-12-28 06:00:13,582 Val Step[0550/1563], Loss: 3.3437 (3.9610), Acc@1: 0.3750 (0.2349), Acc@5: 0.6875 (0.4815)
2021-12-28 06:00:15,207 Val Step[0600/1563], Loss: 3.0816 (3.9733), Acc@1: 0.4375 (0.2343), Acc@5: 0.7500 (0.4795)
2021-12-28 06:00:16,713 Val Step[0650/1563], Loss: 4.3881 (3.9699), Acc@1: 0.1875 (0.2360), Acc@5: 0.5000 (0.4796)
2021-12-28 06:00:18,187 Val Step[0700/1563], Loss: 5.5253 (4.0084), Acc@1: 0.0312 (0.2314), Acc@5: 0.1562 (0.4706)
2021-12-28 06:00:19,662 Val Step[0750/1563], Loss: 3.6820 (4.0355), Acc@1: 0.2812 (0.2278), Acc@5: 0.6562 (0.4635)
2021-12-28 06:00:21,267 Val Step[0800/1563], Loss: 4.2590 (4.0586), Acc@1: 0.2812 (0.2246), Acc@5: 0.5312 (0.4599)
2021-12-28 06:00:22,875 Val Step[0850/1563], Loss: 4.3357 (4.0815), Acc@1: 0.1875 (0.2214), Acc@5: 0.5312 (0.4561)
2021-12-28 06:00:24,521 Val Step[0900/1563], Loss: 3.0156 (4.0778), Acc@1: 0.4375 (0.2229), Acc@5: 0.6562 (0.4566)
2021-12-28 06:00:26,151 Val Step[0950/1563], Loss: 4.2880 (4.0977), Acc@1: 0.3125 (0.2213), Acc@5: 0.5625 (0.4525)
2021-12-28 06:00:27,737 Val Step[1000/1563], Loss: 2.4319 (4.1093), Acc@1: 0.6250 (0.2202), Acc@5: 0.8750 (0.4506)
2021-12-28 06:00:29,350 Val Step[1050/1563], Loss: 3.5920 (4.1148), Acc@1: 0.2188 (0.2182), Acc@5: 0.5625 (0.4490)
2021-12-28 06:00:30,950 Val Step[1100/1563], Loss: 4.9806 (4.1329), Acc@1: 0.0938 (0.2170), Acc@5: 0.2812 (0.4448)
2021-12-28 06:00:32,494 Val Step[1150/1563], Loss: 4.4535 (4.1544), Acc@1: 0.3125 (0.2138), Acc@5: 0.4375 (0.4397)
2021-12-28 06:00:34,084 Val Step[1200/1563], Loss: 3.5266 (4.1698), Acc@1: 0.5000 (0.2127), Acc@5: 0.5938 (0.4364)
2021-12-28 06:00:35,806 Val Step[1250/1563], Loss: 3.0643 (4.1868), Acc@1: 0.5625 (0.2111), Acc@5: 0.7812 (0.4325)
2021-12-28 06:00:37,399 Val Step[1300/1563], Loss: 3.9191 (4.1959), Acc@1: 0.1875 (0.2092), Acc@5: 0.5625 (0.4301)
2021-12-28 06:00:38,944 Val Step[1350/1563], Loss: 3.6659 (4.2122), Acc@1: 0.0625 (0.2066), Acc@5: 0.4688 (0.4263)
2021-12-28 06:00:40,444 Val Step[1400/1563], Loss: 3.5362 (4.2166), Acc@1: 0.3750 (0.2057), Acc@5: 0.6250 (0.4253)
2021-12-28 06:00:41,897 Val Step[1450/1563], Loss: 3.8765 (4.2127), Acc@1: 0.1562 (0.2068), Acc@5: 0.5312 (0.4257)
2021-12-28 06:00:43,291 Val Step[1500/1563], Loss: 4.0640 (4.1851), Acc@1: 0.2500 (0.2113), Acc@5: 0.5312 (0.4324)
2021-12-28 06:00:44,742 Val Step[1550/1563], Loss: 2.0297 (4.1610), Acc@1: 0.7500 (0.2158), Acc@5: 0.8125 (0.4368)
2021-12-28 06:00:45,656 ----- Epoch[010/310], Validation Loss: 4.1550, Validation Acc@1: 0.2171, Validation Acc@5: 0.4382, time: 111.91
2021-12-28 06:00:45,656 ----- Epoch[010/310], Train Loss: 5.9210, Train Acc: 0.0613, time: 1647.40, Best Val(epoch10) Acc@1: 0.2171
2021-12-28 06:00:45,854 Max accuracy so far: 0.2171 at epoch_10
2021-12-28 06:00:45,854 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 06:00:45,854 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 06:00:45,948 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 06:00:46,114 ----- Save model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-10-Loss-5.902993043263753.pdparams
2021-12-28 06:00:46,114 ----- Save optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-10-Loss-5.902993043263753.pdopt
2021-12-28 06:00:46,186 ----- Save ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-10-Loss-5.902993043263753-EMA.pdparams
2021-12-28 06:00:46,187 Now training epoch 11. LR=0.000550
2021-12-28 06:02:09,873 Epoch[011/310], Step[0000/1251], Loss: 5.8402(5.8402), Acc: 0.0859(0.0859)
2021-12-28 06:03:12,813 Epoch[011/310], Step[0050/1251], Loss: 5.7676(5.8775), Acc: 0.0947(0.0660)
2021-12-28 06:04:15,080 Epoch[011/310], Step[0100/1251], Loss: 5.7378(5.8863), Acc: 0.0771(0.0642)
2021-12-28 06:05:18,122 Epoch[011/310], Step[0150/1251], Loss: 6.3123(5.8754), Acc: 0.0156(0.0647)
2021-12-28 06:06:19,941 Epoch[011/310], Step[0200/1251], Loss: 5.9759(5.8723), Acc: 0.0537(0.0660)
2021-12-28 06:07:23,400 Epoch[011/310], Step[0250/1251], Loss: 5.7472(5.8702), Acc: 0.0391(0.0658)
2021-12-28 06:08:25,907 Epoch[011/310], Step[0300/1251], Loss: 6.2743(5.8620), Acc: 0.0488(0.0665)
2021-12-28 06:09:27,581 Epoch[011/310], Step[0350/1251], Loss: 5.3266(5.8579), Acc: 0.0967(0.0672)
2021-12-28 06:10:29,807 Epoch[011/310], Step[0400/1251], Loss: 5.7600(5.8611), Acc: 0.0537(0.0668)
2021-12-28 06:11:32,882 Epoch[011/310], Step[0450/1251], Loss: 6.1723(5.8570), Acc: 0.0420(0.0677)
2021-12-28 06:12:35,011 Epoch[011/310], Step[0500/1251], Loss: 6.1038(5.8551), Acc: 0.0703(0.0682)
2021-12-28 06:13:37,176 Epoch[011/310], Step[0550/1251], Loss: 5.8383(5.8546), Acc: 0.0869(0.0683)
2021-12-28 06:14:41,401 Epoch[011/310], Step[0600/1251], Loss: 5.7110(5.8537), Acc: 0.0557(0.0684)
2021-12-28 06:15:45,093 Epoch[011/310], Step[0650/1251], Loss: 6.0166(5.8511), Acc: 0.0742(0.0684)
2021-12-28 06:16:48,432 Epoch[011/310], Step[0700/1251], Loss: 5.8511(5.8470), Acc: 0.1064(0.0691)
2021-12-28 06:17:52,181 Epoch[011/310], Step[0750/1251], Loss: 5.5441(5.8464), Acc: 0.0811(0.0690)
2021-12-28 06:18:55,389 Epoch[011/310], Step[0800/1251], Loss: 5.3270(5.8432), Acc: 0.1406(0.0696)
2021-12-28 06:19:58,815 Epoch[011/310], Step[0850/1251], Loss: 5.6028(5.8402), Acc: 0.0742(0.0700)
2021-12-28 06:21:02,047 Epoch[011/310], Step[0900/1251], Loss: 5.7378(5.8377), Acc: 0.0605(0.0702)
2021-12-28 06:22:04,979 Epoch[011/310], Step[0950/1251], Loss: 5.9499(5.8371), Acc: 0.0410(0.0702)
2021-12-28 06:23:08,912 Epoch[011/310], Step[1000/1251], Loss: 5.8664(5.8348), Acc: 0.1055(0.0703)
2021-12-28 06:24:10,683 Epoch[011/310], Step[1050/1251], Loss: 5.6577(5.8299), Acc: 0.1104(0.0709)
2021-12-28 06:25:12,401 Epoch[011/310], Step[1100/1251], Loss: 6.1046(5.8230), Acc: 0.0596(0.0717)
2021-12-28 06:26:12,418 Epoch[011/310], Step[1150/1251], Loss: 5.7338(5.8200), Acc: 0.0771(0.0721)
2021-12-28 06:27:15,493 Epoch[011/310], Step[1200/1251], Loss: 5.9164(5.8185), Acc: 0.0752(0.0723)
2021-12-28 06:28:18,271 Epoch[011/310], Step[1250/1251], Loss: 6.1286(5.8177), Acc: 0.0693(0.0727)
2021-12-28 06:28:20,468 ----- Epoch[011/310], Train Loss: 5.8177, Train Acc: 0.0727, time: 1654.28, Best Val(epoch10) Acc@1: 0.2171
2021-12-28 06:28:20,468 Now training epoch 12. LR=0.000600
2021-12-28 06:29:38,665 Epoch[012/310], Step[0000/1251], Loss: 6.0570(6.0570), Acc: 0.0381(0.0381)
2021-12-28 06:30:41,314 Epoch[012/310], Step[0050/1251], Loss: 5.8181(5.7380), Acc: 0.1064(0.0820)
2021-12-28 06:31:43,372 Epoch[012/310], Step[0100/1251], Loss: 5.7663(5.7348), Acc: 0.0967(0.0798)
2021-12-28 06:32:46,346 Epoch[012/310], Step[0150/1251], Loss: 5.7197(5.7267), Acc: 0.0449(0.0813)
2021-12-28 06:33:48,556 Epoch[012/310], Step[0200/1251], Loss: 5.5393(5.7278), Acc: 0.0791(0.0801)
2021-12-28 06:34:51,709 Epoch[012/310], Step[0250/1251], Loss: 5.6361(5.7302), Acc: 0.0869(0.0803)
2021-12-28 06:35:54,997 Epoch[012/310], Step[0300/1251], Loss: 5.7490(5.7251), Acc: 0.1104(0.0806)
2021-12-28 06:36:56,802 Epoch[012/310], Step[0350/1251], Loss: 5.8216(5.7301), Acc: 0.0293(0.0810)
2021-12-28 06:37:59,556 Epoch[012/310], Step[0400/1251], Loss: 6.0363(5.7352), Acc: 0.0234(0.0801)
2021-12-28 06:39:02,422 Epoch[012/310], Step[0450/1251], Loss: 5.6102(5.7361), Acc: 0.0244(0.0798)
2021-12-28 06:40:03,979 Epoch[012/310], Step[0500/1251], Loss: 5.8202(5.7335), Acc: 0.0918(0.0802)
2021-12-28 06:41:06,250 Epoch[012/310], Step[0550/1251], Loss: 5.5210(5.7274), Acc: 0.1133(0.0805)
2021-12-28 06:42:08,431 Epoch[012/310], Step[0600/1251], Loss: 5.6166(5.7269), Acc: 0.0469(0.0811)
2021-12-28 06:43:11,050 Epoch[012/310], Step[0650/1251], Loss: 5.7082(5.7314), Acc: 0.0889(0.0812)
2021-12-28 06:44:14,543 Epoch[012/310], Step[0700/1251], Loss: 5.7577(5.7295), Acc: 0.0645(0.0816)
2021-12-28 06:45:17,407 Epoch[012/310], Step[0750/1251], Loss: 5.8784(5.7307), Acc: 0.0498(0.0810)
2021-12-28 06:46:20,037 Epoch[012/310], Step[0800/1251], Loss: 5.4949(5.7295), Acc: 0.0918(0.0810)
2021-12-28 06:47:21,101 Epoch[012/310], Step[0850/1251], Loss: 5.6947(5.7287), Acc: 0.0557(0.0811)
2021-12-28 06:48:24,420 Epoch[012/310], Step[0900/1251], Loss: 5.7077(5.7290), Acc: 0.1045(0.0813)
2021-12-28 06:49:26,528 Epoch[012/310], Step[0950/1251], Loss: 5.2144(5.7243), Acc: 0.0986(0.0816)
2021-12-28 06:50:29,110 Epoch[012/310], Step[1000/1251], Loss: 5.6991(5.7224), Acc: 0.1113(0.0820)
2021-12-28 06:51:32,578 Epoch[012/310], Step[1050/1251], Loss: 5.5236(5.7197), Acc: 0.1455(0.0823)
2021-12-28 06:52:36,548 Epoch[012/310], Step[1100/1251], Loss: 5.9494(5.7191), Acc: 0.1074(0.0824)
2021-12-28 06:53:40,458 Epoch[012/310], Step[1150/1251], Loss: 6.0036(5.7179), Acc: 0.0684(0.0825)
2021-12-28 06:54:44,039 Epoch[012/310], Step[1200/1251], Loss: 5.7296(5.7139), Acc: 0.1172(0.0829)
2021-12-28 06:55:48,005 Epoch[012/310], Step[1250/1251], Loss: 5.9216(5.7130), Acc: 0.0625(0.0831)
2021-12-28 06:55:50,042 ----- Validation after Epoch: 12
2021-12-28 06:56:49,210 Val Step[0000/1563], Loss: 2.2751 (2.2751), Acc@1: 0.6562 (0.6562), Acc@5: 0.7812 (0.7812)
2021-12-28 06:56:50,761 Val Step[0050/1563], Loss: 4.5798 (2.9692), Acc@1: 0.1250 (0.4062), Acc@5: 0.3438 (0.6906)
2021-12-28 06:56:52,295 Val Step[0100/1563], Loss: 3.5935 (3.4439), Acc@1: 0.2812 (0.3202), Acc@5: 0.5938 (0.5789)
2021-12-28 06:56:53,832 Val Step[0150/1563], Loss: 2.8282 (3.3259), Acc@1: 0.4688 (0.3413), Acc@5: 0.6562 (0.6024)
2021-12-28 06:56:55,284 Val Step[0200/1563], Loss: 4.0620 (3.3768), Acc@1: 0.0625 (0.3388), Acc@5: 0.3125 (0.5927)
2021-12-28 06:56:56,722 Val Step[0250/1563], Loss: 3.9279 (3.2616), Acc@1: 0.2500 (0.3576), Acc@5: 0.5000 (0.6129)
2021-12-28 06:56:58,351 Val Step[0300/1563], Loss: 4.0314 (3.3429), Acc@1: 0.0000 (0.3312), Acc@5: 0.3438 (0.5906)
2021-12-28 06:56:59,932 Val Step[0350/1563], Loss: 3.5568 (3.3600), Acc@1: 0.1875 (0.3226), Acc@5: 0.5625 (0.5882)
2021-12-28 06:57:01,529 Val Step[0400/1563], Loss: 3.4484 (3.3540), Acc@1: 0.0938 (0.3177), Acc@5: 0.6250 (0.5888)
2021-12-28 06:57:03,214 Val Step[0450/1563], Loss: 2.0452 (3.3638), Acc@1: 0.1250 (0.3114), Acc@5: 0.9375 (0.5892)
2021-12-28 06:57:04,769 Val Step[0500/1563], Loss: 2.0934 (3.3606), Acc@1: 0.6875 (0.3123), Acc@5: 0.8438 (0.5891)
2021-12-28 06:57:06,379 Val Step[0550/1563], Loss: 2.6090 (3.3342), Acc@1: 0.4688 (0.3201), Acc@5: 0.7500 (0.5932)
2021-12-28 06:57:08,025 Val Step[0600/1563], Loss: 2.5502 (3.3505), Acc@1: 0.5312 (0.3182), Acc@5: 0.8125 (0.5894)
2021-12-28 06:57:09,554 Val Step[0650/1563], Loss: 3.0434 (3.3669), Acc@1: 0.3750 (0.3175), Acc@5: 0.7188 (0.5866)
2021-12-28 06:57:11,049 Val Step[0700/1563], Loss: 5.0194 (3.4197), Acc@1: 0.0938 (0.3116), Acc@5: 0.2812 (0.5764)
2021-12-28 06:57:12,714 Val Step[0750/1563], Loss: 3.8146 (3.4640), Acc@1: 0.2188 (0.3059), Acc@5: 0.5000 (0.5672)
2021-12-28 06:57:14,246 Val Step[0800/1563], Loss: 3.8243 (3.5115), Acc@1: 0.3125 (0.2994), Acc@5: 0.6562 (0.5586)
2021-12-28 06:57:15,739 Val Step[0850/1563], Loss: 3.6224 (3.5470), Acc@1: 0.2812 (0.2952), Acc@5: 0.5000 (0.5513)
2021-12-28 06:57:17,227 Val Step[0900/1563], Loss: 1.9592 (3.5490), Acc@1: 0.7188 (0.2973), Acc@5: 0.8750 (0.5516)
2021-12-28 06:57:18,784 Val Step[0950/1563], Loss: 3.9040 (3.5790), Acc@1: 0.3125 (0.2946), Acc@5: 0.6250 (0.5461)
2021-12-28 06:57:20,310 Val Step[1000/1563], Loss: 2.2156 (3.6065), Acc@1: 0.6250 (0.2905), Acc@5: 0.7812 (0.5412)
2021-12-28 06:57:21,771 Val Step[1050/1563], Loss: 2.6299 (3.6245), Acc@1: 0.5625 (0.2868), Acc@5: 0.8125 (0.5374)
2021-12-28 06:57:23,276 Val Step[1100/1563], Loss: 4.6088 (3.6465), Acc@1: 0.1875 (0.2842), Acc@5: 0.3438 (0.5322)
2021-12-28 06:57:24,798 Val Step[1150/1563], Loss: 3.7726 (3.6724), Acc@1: 0.3750 (0.2803), Acc@5: 0.4688 (0.5271)
2021-12-28 06:57:26,361 Val Step[1200/1563], Loss: 3.0892 (3.6948), Acc@1: 0.5625 (0.2780), Acc@5: 0.6250 (0.5228)
2021-12-28 06:57:27,856 Val Step[1250/1563], Loss: 3.2957 (3.7205), Acc@1: 0.4062 (0.2746), Acc@5: 0.6250 (0.5173)
2021-12-28 06:57:29,332 Val Step[1300/1563], Loss: 4.7067 (3.7308), Acc@1: 0.1562 (0.2733), Acc@5: 0.2500 (0.5146)
2021-12-28 06:57:30,834 Val Step[1350/1563], Loss: 3.4855 (3.7562), Acc@1: 0.0938 (0.2690), Acc@5: 0.4688 (0.5094)
2021-12-28 06:57:32,327 Val Step[1400/1563], Loss: 3.6936 (3.7663), Acc@1: 0.3438 (0.2673), Acc@5: 0.6250 (0.5075)
2021-12-28 06:57:33,763 Val Step[1450/1563], Loss: 3.9730 (3.7684), Acc@1: 0.1250 (0.2672), Acc@5: 0.5938 (0.5066)
2021-12-28 06:57:35,389 Val Step[1500/1563], Loss: 4.1823 (3.7428), Acc@1: 0.1562 (0.2714), Acc@5: 0.5000 (0.5127)
2021-12-28 06:57:36,931 Val Step[1550/1563], Loss: 1.3357 (3.7194), Acc@1: 0.8438 (0.2759), Acc@5: 0.8750 (0.5167)
2021-12-28 06:57:37,838 ----- Epoch[012/310], Validation Loss: 3.7119, Validation Acc@1: 0.2777, Validation Acc@5: 0.5181, time: 107.79
2021-12-28 06:57:37,838 ----- Epoch[012/310], Train Loss: 5.7130, Train Acc: 0.0831, time: 1649.57, Best Val(epoch12) Acc@1: 0.2777
2021-12-28 06:57:38,049 Max accuracy so far: 0.2777 at epoch_12
2021-12-28 06:57:38,050 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 06:57:38,050 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 06:57:38,124 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 06:57:38,124 Now training epoch 13. LR=0.000650
2021-12-28 06:58:54,229 Epoch[013/310], Step[0000/1251], Loss: 5.5341(5.5341), Acc: 0.0918(0.0918)
2021-12-28 06:59:54,837 Epoch[013/310], Step[0050/1251], Loss: 5.4687(5.6141), Acc: 0.1006(0.0999)
2021-12-28 07:00:57,748 Epoch[013/310], Step[0100/1251], Loss: 5.7414(5.6487), Acc: 0.1260(0.0924)
2021-12-28 07:02:00,705 Epoch[013/310], Step[0150/1251], Loss: 5.4365(5.6701), Acc: 0.1523(0.0884)
2021-12-28 07:03:03,530 Epoch[013/310], Step[0200/1251], Loss: 5.6929(5.6722), Acc: 0.1182(0.0888)
2021-12-28 07:04:05,344 Epoch[013/310], Step[0250/1251], Loss: 5.8056(5.6752), Acc: 0.0781(0.0881)
2021-12-28 07:05:08,415 Epoch[013/310], Step[0300/1251], Loss: 5.6439(5.6797), Acc: 0.0420(0.0872)
2021-12-28 07:06:10,664 Epoch[013/310], Step[0350/1251], Loss: 5.7731(5.6783), Acc: 0.0820(0.0870)
2021-12-28 07:07:13,501 Epoch[013/310], Step[0400/1251], Loss: 5.8148(5.6739), Acc: 0.0762(0.0877)
2021-12-28 07:08:16,755 Epoch[013/310], Step[0450/1251], Loss: 5.1762(5.6707), Acc: 0.0986(0.0886)
2021-12-28 07:09:19,298 Epoch[013/310], Step[0500/1251], Loss: 6.1760(5.6609), Acc: 0.0420(0.0897)
2021-12-28 07:10:22,161 Epoch[013/310], Step[0550/1251], Loss: 6.1408(5.6653), Acc: 0.0674(0.0898)
2021-12-28 07:11:26,006 Epoch[013/310], Step[0600/1251], Loss: 5.6699(5.6625), Acc: 0.1084(0.0888)
2021-12-28 07:12:28,855 Epoch[013/310], Step[0650/1251], Loss: 5.4778(5.6606), Acc: 0.0254(0.0887)
2021-12-28 07:13:31,472 Epoch[013/310], Step[0700/1251], Loss: 5.2617(5.6608), Acc: 0.0996(0.0883)
2021-12-28 07:14:32,632 Epoch[013/310], Step[0750/1251], Loss: 5.5408(5.6597), Acc: 0.0488(0.0890)
2021-12-28 07:15:34,333 Epoch[013/310], Step[0800/1251], Loss: 5.1423(5.6569), Acc: 0.1865(0.0894)
2021-12-28 07:16:36,654 Epoch[013/310], Step[0850/1251], Loss: 5.6327(5.6529), Acc: 0.0879(0.0898)
2021-12-28 07:17:39,660 Epoch[013/310], Step[0900/1251], Loss: 5.3896(5.6519), Acc: 0.0986(0.0898)
2021-12-28 07:18:43,016 Epoch[013/310], Step[0950/1251], Loss: 5.7526(5.6491), Acc: 0.0713(0.0897)
2021-12-28 07:19:43,960 Epoch[013/310], Step[1000/1251], Loss: 5.9515(5.6457), Acc: 0.1133(0.0906)
2021-12-28 07:20:47,378 Epoch[013/310], Step[1050/1251], Loss: 5.6315(5.6461), Acc: 0.0977(0.0906)
2021-12-28 07:21:48,969 Epoch[013/310], Step[1100/1251], Loss: 5.6471(5.6443), Acc: 0.1270(0.0904)
2021-12-28 07:22:51,580 Epoch[013/310], Step[1150/1251], Loss: 5.2454(5.6449), Acc: 0.1504(0.0908)
2021-12-28 07:23:54,996 Epoch[013/310], Step[1200/1251], Loss: 5.3272(5.6404), Acc: 0.1738(0.0912)
2021-12-28 07:24:57,198 Epoch[013/310], Step[1250/1251], Loss: 5.7538(5.6371), Acc: 0.0557(0.0914)
2021-12-28 07:24:59,723 ----- Epoch[013/310], Train Loss: 5.6371, Train Acc: 0.0914, time: 1641.59, Best Val(epoch12) Acc@1: 0.2777
2021-12-28 07:24:59,723 Now training epoch 14. LR=0.000700
2021-12-28 07:26:18,015 Epoch[014/310], Step[0000/1251], Loss: 5.1568(5.1568), Acc: 0.0674(0.0674)
2021-12-28 07:27:20,746 Epoch[014/310], Step[0050/1251], Loss: 5.4195(5.5535), Acc: 0.1396(0.1049)
2021-12-28 07:28:22,525 Epoch[014/310], Step[0100/1251], Loss: 5.4277(5.5332), Acc: 0.0479(0.1008)
2021-12-28 07:29:24,088 Epoch[014/310], Step[0150/1251], Loss: 5.7859(5.5383), Acc: 0.0986(0.0988)
2021-12-28 07:30:25,624 Epoch[014/310], Step[0200/1251], Loss: 5.5962(5.5371), Acc: 0.0928(0.1006)
2021-12-28 07:31:28,045 Epoch[014/310], Step[0250/1251], Loss: 5.7579(5.5505), Acc: 0.0459(0.1002)
2021-12-28 07:32:30,149 Epoch[014/310], Step[0300/1251], Loss: 5.5318(5.5492), Acc: 0.0654(0.0989)
2021-12-28 07:33:32,917 Epoch[014/310], Step[0350/1251], Loss: 5.3217(5.5579), Acc: 0.1611(0.0987)
2021-12-28 07:34:36,805 Epoch[014/310], Step[0400/1251], Loss: 5.3984(5.5593), Acc: 0.0771(0.0987)
2021-12-28 07:35:38,611 Epoch[014/310], Step[0450/1251], Loss: 5.4874(5.5625), Acc: 0.1533(0.0989)
2021-12-28 07:36:40,433 Epoch[014/310], Step[0500/1251], Loss: 5.9691(5.5571), Acc: 0.0527(0.1004)
2021-12-28 07:37:43,129 Epoch[014/310], Step[0550/1251], Loss: 5.9723(5.5585), Acc: 0.0850(0.1005)
2021-12-28 07:38:46,662 Epoch[014/310], Step[0600/1251], Loss: 5.3409(5.5601), Acc: 0.0918(0.1009)
2021-12-28 07:39:50,418 Epoch[014/310], Step[0650/1251], Loss: 5.6691(5.5593), Acc: 0.0664(0.1009)
2021-12-28 07:40:53,981 Epoch[014/310], Step[0700/1251], Loss: 5.4780(5.5566), Acc: 0.1377(0.1009)
2021-12-28 07:41:56,762 Epoch[014/310], Step[0750/1251], Loss: 5.6194(5.5575), Acc: 0.0967(0.1012)
2021-12-28 07:43:00,920 Epoch[014/310], Step[0800/1251], Loss: 5.2731(5.5568), Acc: 0.0742(0.1006)
2021-12-28 07:44:01,699 Epoch[014/310], Step[0850/1251], Loss: 5.4809(5.5515), Acc: 0.1152(0.1006)
2021-12-28 07:45:05,461 Epoch[014/310], Step[0900/1251], Loss: 5.2640(5.5519), Acc: 0.1719(0.1006)
2021-12-28 07:46:08,962 Epoch[014/310], Step[0950/1251], Loss: 5.4471(5.5491), Acc: 0.1221(0.1006)
2021-12-28 07:47:12,786 Epoch[014/310], Step[1000/1251], Loss: 5.7125(5.5472), Acc: 0.0947(0.1008)
2021-12-28 07:48:16,148 Epoch[014/310], Step[1050/1251], Loss: 5.7761(5.5443), Acc: 0.1230(0.1013)
2021-12-28 07:49:18,663 Epoch[014/310], Step[1100/1251], Loss: 5.7787(5.5437), Acc: 0.1055(0.1013)
2021-12-28 07:50:22,249 Epoch[014/310], Step[1150/1251], Loss: 5.3068(5.5390), Acc: 0.1787(0.1018)
2021-12-28 07:51:24,646 Epoch[014/310], Step[1200/1251], Loss: 5.5040(5.5364), Acc: 0.1045(0.1021)
2021-12-28 07:52:28,075 Epoch[014/310], Step[1250/1251], Loss: 5.7638(5.5323), Acc: 0.1553(0.1028)
2021-12-28 07:52:30,051 ----- Validation after Epoch: 14
2021-12-28 07:53:29,809 Val Step[0000/1563], Loss: 2.1811 (2.1811), Acc@1: 0.7188 (0.7188), Acc@5: 0.8438 (0.8438)
2021-12-28 07:53:31,440 Val Step[0050/1563], Loss: 4.4849 (2.6487), Acc@1: 0.1250 (0.4792), Acc@5: 0.2812 (0.7439)
2021-12-28 07:53:32,987 Val Step[0100/1563], Loss: 3.6699 (3.1255), Acc@1: 0.2500 (0.3787), Acc@5: 0.5625 (0.6399)
2021-12-28 07:53:34,660 Val Step[0150/1563], Loss: 2.4119 (2.9714), Acc@1: 0.5938 (0.4048), Acc@5: 0.6250 (0.6612)
2021-12-28 07:53:36,157 Val Step[0200/1563], Loss: 2.8017 (3.0192), Acc@1: 0.3125 (0.3996), Acc@5: 0.7188 (0.6519)
2021-12-28 07:53:37,656 Val Step[0250/1563], Loss: 3.3411 (2.9078), Acc@1: 0.3438 (0.4147), Acc@5: 0.5625 (0.6712)
2021-12-28 07:53:39,139 Val Step[0300/1563], Loss: 3.4233 (2.9918), Acc@1: 0.1875 (0.3895), Acc@5: 0.5312 (0.6527)
2021-12-28 07:53:40,638 Val Step[0350/1563], Loss: 3.0630 (3.0171), Acc@1: 0.3438 (0.3790), Acc@5: 0.7188 (0.6489)
2021-12-28 07:53:42,131 Val Step[0400/1563], Loss: 3.2515 (3.0016), Acc@1: 0.0938 (0.3735), Acc@5: 0.6562 (0.6549)
2021-12-28 07:53:43,760 Val Step[0450/1563], Loss: 1.7834 (3.0129), Acc@1: 0.3125 (0.3678), Acc@5: 0.9062 (0.6529)
2021-12-28 07:53:45,314 Val Step[0500/1563], Loss: 1.4457 (3.0006), Acc@1: 0.7500 (0.3696), Acc@5: 0.9062 (0.6548)
2021-12-28 07:53:46,847 Val Step[0550/1563], Loss: 2.4378 (2.9789), Acc@1: 0.5000 (0.3770), Acc@5: 0.8125 (0.6572)
2021-12-28 07:53:48,456 Val Step[0600/1563], Loss: 1.8327 (2.9943), Acc@1: 0.6875 (0.3757), Acc@5: 0.8125 (0.6537)
2021-12-28 07:53:50,012 Val Step[0650/1563], Loss: 3.3495 (3.0118), Acc@1: 0.1875 (0.3735), Acc@5: 0.7500 (0.6498)
2021-12-28 07:53:51,460 Val Step[0700/1563], Loss: 4.7586 (3.0604), Acc@1: 0.1250 (0.3687), Acc@5: 0.4062 (0.6403)
2021-12-28 07:53:53,019 Val Step[0750/1563], Loss: 3.4697 (3.1058), Acc@1: 0.3125 (0.3627), Acc@5: 0.5625 (0.6311)
2021-12-28 07:53:54,459 Val Step[0800/1563], Loss: 3.4698 (3.1502), Acc@1: 0.3438 (0.3563), Acc@5: 0.6250 (0.6230)
2021-12-28 07:53:56,001 Val Step[0850/1563], Loss: 3.4147 (3.1871), Acc@1: 0.3438 (0.3525), Acc@5: 0.5625 (0.6156)
2021-12-28 07:53:57,615 Val Step[0900/1563], Loss: 1.8324 (3.1897), Acc@1: 0.6875 (0.3548), Acc@5: 0.8438 (0.6148)
2021-12-28 07:53:59,200 Val Step[0950/1563], Loss: 3.5437 (3.2271), Acc@1: 0.5000 (0.3497), Acc@5: 0.5312 (0.6070)
2021-12-28 07:54:00,626 Val Step[1000/1563], Loss: 1.5925 (3.2526), Acc@1: 0.7812 (0.3457), Acc@5: 0.9062 (0.6022)
2021-12-28 07:54:02,129 Val Step[1050/1563], Loss: 2.6973 (3.2688), Acc@1: 0.4375 (0.3424), Acc@5: 0.7812 (0.5997)
2021-12-28 07:54:03,622 Val Step[1100/1563], Loss: 3.8354 (3.2896), Acc@1: 0.2812 (0.3400), Acc@5: 0.5625 (0.5952)
2021-12-28 07:54:05,143 Val Step[1150/1563], Loss: 4.4016 (3.3171), Acc@1: 0.2500 (0.3362), Acc@5: 0.4375 (0.5898)
2021-12-28 07:54:06,732 Val Step[1200/1563], Loss: 2.8305 (3.3401), Acc@1: 0.4375 (0.3336), Acc@5: 0.5938 (0.5856)
2021-12-28 07:54:08,262 Val Step[1250/1563], Loss: 2.1695 (3.3640), Acc@1: 0.6562 (0.3306), Acc@5: 0.8438 (0.5810)
2021-12-28 07:54:09,759 Val Step[1300/1563], Loss: 3.3855 (3.3760), Acc@1: 0.2812 (0.3279), Acc@5: 0.5938 (0.5783)
2021-12-28 07:54:11,254 Val Step[1350/1563], Loss: 3.3267 (3.3990), Acc@1: 0.0625 (0.3235), Acc@5: 0.5000 (0.5736)
2021-12-28 07:54:12,789 Val Step[1400/1563], Loss: 3.1653 (3.4071), Acc@1: 0.3438 (0.3224), Acc@5: 0.7188 (0.5719)
2021-12-28 07:54:14,286 Val Step[1450/1563], Loss: 3.6773 (3.4070), Acc@1: 0.1250 (0.3226), Acc@5: 0.5938 (0.5717)
2021-12-28 07:54:15,833 Val Step[1500/1563], Loss: 3.9023 (3.3786), Acc@1: 0.1562 (0.3274), Acc@5: 0.5312 (0.5776)
2021-12-28 07:54:17,365 Val Step[1550/1563], Loss: 1.3219 (3.3645), Acc@1: 0.8750 (0.3300), Acc@5: 0.8750 (0.5798)
2021-12-28 07:54:18,243 ----- Epoch[014/310], Validation Loss: 3.3590, Validation Acc@1: 0.3315, Validation Acc@5: 0.5807, time: 108.19
2021-12-28 07:54:18,243 ----- Epoch[014/310], Train Loss: 5.5323, Train Acc: 0.1028, time: 1650.33, Best Val(epoch14) Acc@1: 0.3315
2021-12-28 07:54:18,449 Max accuracy so far: 0.3315 at epoch_14
2021-12-28 07:54:18,450 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 07:54:18,450 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 07:54:18,543 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 07:54:18,543 Now training epoch 15. LR=0.000750
2021-12-28 07:55:33,566 Epoch[015/310], Step[0000/1251], Loss: 5.3800(5.3800), Acc: 0.1641(0.1641)
2021-12-28 07:56:35,525 Epoch[015/310], Step[0050/1251], Loss: 5.5559(5.4395), Acc: 0.0889(0.1038)
2021-12-28 07:57:37,782 Epoch[015/310], Step[0100/1251], Loss: 4.8861(5.4635), Acc: 0.0938(0.1039)
2021-12-28 07:58:40,838 Epoch[015/310], Step[0150/1251], Loss: 5.4250(5.4564), Acc: 0.1641(0.1054)
2021-12-28 07:59:43,879 Epoch[015/310], Step[0200/1251], Loss: 5.4568(5.4654), Acc: 0.0732(0.1071)
2021-12-28 08:00:47,928 Epoch[015/310], Step[0250/1251], Loss: 5.6064(5.4700), Acc: 0.0625(0.1069)
2021-12-28 08:01:51,119 Epoch[015/310], Step[0300/1251], Loss: 5.2058(5.4650), Acc: 0.1221(0.1083)
2021-12-28 08:02:53,890 Epoch[015/310], Step[0350/1251], Loss: 5.1110(5.4630), Acc: 0.0713(0.1098)
2021-12-28 08:03:56,768 Epoch[015/310], Step[0400/1251], Loss: 5.3831(5.4696), Acc: 0.0625(0.1095)
2021-12-28 08:04:59,235 Epoch[015/310], Step[0450/1251], Loss: 5.6619(5.4666), Acc: 0.0381(0.1105)
2021-12-28 08:06:01,427 Epoch[015/310], Step[0500/1251], Loss: 5.2747(5.4600), Acc: 0.0303(0.1107)
2021-12-28 08:07:03,265 Epoch[015/310], Step[0550/1251], Loss: 4.9379(5.4610), Acc: 0.2090(0.1107)
2021-12-28 08:08:06,142 Epoch[015/310], Step[0600/1251], Loss: 5.9671(5.4691), Acc: 0.0859(0.1095)
2021-12-28 08:09:08,484 Epoch[015/310], Step[0650/1251], Loss: 5.4935(5.4691), Acc: 0.0869(0.1106)
2021-12-28 08:10:10,939 Epoch[015/310], Step[0700/1251], Loss: 5.4801(5.4697), Acc: 0.1025(0.1109)
2021-12-28 08:11:13,505 Epoch[015/310], Step[0750/1251], Loss: 5.3737(5.4705), Acc: 0.1602(0.1106)
2021-12-28 08:12:16,058 Epoch[015/310], Step[0800/1251], Loss: 5.3650(5.4678), Acc: 0.0664(0.1109)
2021-12-28 08:13:19,499 Epoch[015/310], Step[0850/1251], Loss: 5.4618(5.4674), Acc: 0.1074(0.1110)
2021-12-28 08:14:21,860 Epoch[015/310], Step[0900/1251], Loss: 5.3026(5.4687), Acc: 0.1650(0.1113)
2021-12-28 08:15:25,665 Epoch[015/310], Step[0950/1251], Loss: 5.2481(5.4679), Acc: 0.2080(0.1119)
2021-12-28 08:16:29,794 Epoch[015/310], Step[1000/1251], Loss: 5.1861(5.4648), Acc: 0.0996(0.1124)
2021-12-28 08:17:33,085 Epoch[015/310], Step[1050/1251], Loss: 5.4850(5.4625), Acc: 0.0693(0.1126)
2021-12-28 08:18:36,754 Epoch[015/310], Step[1100/1251], Loss: 5.6788(5.4615), Acc: 0.1182(0.1129)
2021-12-28 08:19:40,778 Epoch[015/310], Step[1150/1251], Loss: 5.5979(5.4572), Acc: 0.1514(0.1136)
2021-12-28 08:20:43,194 Epoch[015/310], Step[1200/1251], Loss: 5.3761(5.4550), Acc: 0.1543(0.1141)
2021-12-28 08:21:43,460 Epoch[015/310], Step[1250/1251], Loss: 5.7135(5.4543), Acc: 0.0791(0.1142)
2021-12-28 08:21:45,435 ----- Epoch[015/310], Train Loss: 5.4543, Train Acc: 0.1142, time: 1646.89, Best Val(epoch14) Acc@1: 0.3315
2021-12-28 08:21:45,435 Now training epoch 16. LR=0.000800
2021-12-28 08:23:05,870 Epoch[016/310], Step[0000/1251], Loss: 5.8383(5.8383), Acc: 0.1006(0.1006)
2021-12-28 08:24:07,610 Epoch[016/310], Step[0050/1251], Loss: 5.0874(5.4113), Acc: 0.1270(0.1180)
2021-12-28 08:25:09,863 Epoch[016/310], Step[0100/1251], Loss: 5.7592(5.4017), Acc: 0.1162(0.1213)
2021-12-28 08:26:13,640 Epoch[016/310], Step[0150/1251], Loss: 4.8757(5.4005), Acc: 0.1152(0.1228)
2021-12-28 08:27:16,440 Epoch[016/310], Step[0200/1251], Loss: 5.5050(5.4063), Acc: 0.1826(0.1214)
2021-12-28 08:28:19,075 Epoch[016/310], Step[0250/1251], Loss: 5.4394(5.4093), Acc: 0.1797(0.1219)
2021-12-28 08:29:22,554 Epoch[016/310], Step[0300/1251], Loss: 5.4271(5.4031), Acc: 0.1045(0.1220)
2021-12-28 08:30:26,531 Epoch[016/310], Step[0350/1251], Loss: 5.4102(5.4028), Acc: 0.0732(0.1217)
2021-12-28 08:31:30,393 Epoch[016/310], Step[0400/1251], Loss: 5.1642(5.3986), Acc: 0.1357(0.1211)
2021-12-28 08:32:33,407 Epoch[016/310], Step[0450/1251], Loss: 5.8684(5.4004), Acc: 0.0840(0.1203)
2021-12-28 08:33:35,735 Epoch[016/310], Step[0500/1251], Loss: 5.5560(5.4010), Acc: 0.0869(0.1214)
2021-12-28 08:34:37,941 Epoch[016/310], Step[0550/1251], Loss: 5.6167(5.3987), Acc: 0.1484(0.1214)
2021-12-28 08:35:39,878 Epoch[016/310], Step[0600/1251], Loss: 5.0835(5.3993), Acc: 0.1230(0.1213)
2021-12-28 08:36:40,508 Epoch[016/310], Step[0650/1251], Loss: 5.7317(5.3966), Acc: 0.1533(0.1220)
2021-12-28 08:37:43,284 Epoch[016/310], Step[0700/1251], Loss: 5.4023(5.3961), Acc: 0.1445(0.1220)
2021-12-28 08:38:45,005 Epoch[016/310], Step[0750/1251], Loss: 5.9730(5.3928), Acc: 0.0791(0.1229)
2021-12-28 08:39:47,753 Epoch[016/310], Step[0800/1251], Loss: 5.3877(5.3925), Acc: 0.1699(0.1232)
2021-12-28 08:40:51,512 Epoch[016/310], Step[0850/1251], Loss: 4.7347(5.3897), Acc: 0.1816(0.1236)
2021-12-28 08:41:55,012 Epoch[016/310], Step[0900/1251], Loss: 5.1141(5.3856), Acc: 0.1953(0.1240)
2021-12-28 08:42:56,953 Epoch[016/310], Step[0950/1251], Loss: 5.3257(5.3831), Acc: 0.1318(0.1242)
2021-12-28 08:44:00,254 Epoch[016/310], Step[1000/1251], Loss: 5.6917(5.3808), Acc: 0.1143(0.1245)
2021-12-28 08:45:03,940 Epoch[016/310], Step[1050/1251], Loss: 5.1012(5.3778), Acc: 0.0996(0.1247)
2021-12-28 08:46:06,148 Epoch[016/310], Step[1100/1251], Loss: 5.4246(5.3772), Acc: 0.1562(0.1247)
2021-12-28 08:47:11,087 Epoch[016/310], Step[1150/1251], Loss: 5.4267(5.3743), Acc: 0.0742(0.1251)
2021-12-28 08:48:14,133 Epoch[016/310], Step[1200/1251], Loss: 5.3510(5.3721), Acc: 0.0967(0.1256)
2021-12-28 08:49:15,612 Epoch[016/310], Step[1250/1251], Loss: 5.5410(5.3715), Acc: 0.1221(0.1257)
2021-12-28 08:49:17,559 ----- Validation after Epoch: 16
2021-12-28 08:50:14,087 Val Step[0000/1563], Loss: 2.0360 (2.0360), Acc@1: 0.7500 (0.7500), Acc@5: 0.8125 (0.8125)
2021-12-28 08:50:15,628 Val Step[0050/1563], Loss: 4.1025 (2.2220), Acc@1: 0.1562 (0.5509), Acc@5: 0.4062 (0.7923)
2021-12-28 08:50:17,122 Val Step[0100/1563], Loss: 3.3811 (2.7731), Acc@1: 0.1875 (0.4350), Acc@5: 0.6875 (0.6844)
2021-12-28 08:50:18,589 Val Step[0150/1563], Loss: 2.2938 (2.6053), Acc@1: 0.6562 (0.4743), Acc@5: 0.7812 (0.7127)
2021-12-28 08:50:20,227 Val Step[0200/1563], Loss: 2.4093 (2.6658), Acc@1: 0.3438 (0.4638), Acc@5: 0.8125 (0.7016)
2021-12-28 08:50:21,790 Val Step[0250/1563], Loss: 2.5843 (2.5700), Acc@1: 0.4062 (0.4777), Acc@5: 0.7812 (0.7214)
2021-12-28 08:50:23,268 Val Step[0300/1563], Loss: 2.5343 (2.6350), Acc@1: 0.4375 (0.4519), Acc@5: 0.9062 (0.7122)
2021-12-28 08:50:24,840 Val Step[0350/1563], Loss: 2.8143 (2.6376), Acc@1: 0.3750 (0.4464), Acc@5: 0.6875 (0.7152)
2021-12-28 08:50:26,342 Val Step[0400/1563], Loss: 3.1348 (2.6236), Acc@1: 0.1562 (0.4428), Acc@5: 0.6875 (0.7201)
2021-12-28 08:50:27,903 Val Step[0450/1563], Loss: 2.1188 (2.6463), Acc@1: 0.2812 (0.4356), Acc@5: 0.8438 (0.7169)
2021-12-28 08:50:29,469 Val Step[0500/1563], Loss: 1.4133 (2.6483), Acc@1: 0.7500 (0.4346), Acc@5: 0.9688 (0.7171)
2021-12-28 08:50:31,043 Val Step[0550/1563], Loss: 2.1658 (2.6270), Acc@1: 0.5312 (0.4422), Acc@5: 0.8125 (0.7198)
2021-12-28 08:50:32,709 Val Step[0600/1563], Loss: 1.8009 (2.6328), Acc@1: 0.7500 (0.4429), Acc@5: 0.8750 (0.7194)
2021-12-28 08:50:34,197 Val Step[0650/1563], Loss: 2.6033 (2.6601), Acc@1: 0.5312 (0.4392), Acc@5: 0.8125 (0.7148)
2021-12-28 08:50:35,806 Val Step[0700/1563], Loss: 4.7333 (2.7218), Acc@1: 0.1250 (0.4301), Acc@5: 0.3750 (0.7053)
2021-12-28 08:50:37,438 Val Step[0750/1563], Loss: 2.9790 (2.7806), Acc@1: 0.5000 (0.4213), Acc@5: 0.6562 (0.6927)
2021-12-28 08:50:39,054 Val Step[0800/1563], Loss: 3.1345 (2.8331), Acc@1: 0.3750 (0.4128), Acc@5: 0.7188 (0.6829)
2021-12-28 08:50:40,590 Val Step[0850/1563], Loss: 3.4384 (2.8732), Acc@1: 0.2812 (0.4072), Acc@5: 0.5312 (0.6756)
2021-12-28 08:50:42,217 Val Step[0900/1563], Loss: 1.5125 (2.8809), Acc@1: 0.7500 (0.4078), Acc@5: 0.8125 (0.6740)
2021-12-28 08:50:43,857 Val Step[0950/1563], Loss: 3.0851 (2.9201), Acc@1: 0.5000 (0.4028), Acc@5: 0.7812 (0.6668)
2021-12-28 08:50:45,419 Val Step[1000/1563], Loss: 1.4290 (2.9474), Acc@1: 0.7812 (0.3986), Acc@5: 0.9062 (0.6619)
2021-12-28 08:50:46,993 Val Step[1050/1563], Loss: 2.3221 (2.9685), Acc@1: 0.5312 (0.3945), Acc@5: 0.7500 (0.6587)
2021-12-28 08:50:48,557 Val Step[1100/1563], Loss: 3.4696 (2.9945), Acc@1: 0.4062 (0.3907), Acc@5: 0.5938 (0.6524)
2021-12-28 08:50:50,160 Val Step[1150/1563], Loss: 3.3387 (3.0268), Acc@1: 0.5000 (0.3863), Acc@5: 0.6562 (0.6460)
2021-12-28 08:50:51,752 Val Step[1200/1563], Loss: 2.1873 (3.0524), Acc@1: 0.5938 (0.3826), Acc@5: 0.7188 (0.6411)
2021-12-28 08:50:53,338 Val Step[1250/1563], Loss: 2.1212 (3.0747), Acc@1: 0.6562 (0.3802), Acc@5: 0.8438 (0.6371)
2021-12-28 08:50:54,843 Val Step[1300/1563], Loss: 3.2478 (3.0911), Acc@1: 0.2812 (0.3768), Acc@5: 0.5625 (0.6336)
2021-12-28 08:50:56,358 Val Step[1350/1563], Loss: 3.9684 (3.1223), Acc@1: 0.0312 (0.3713), Acc@5: 0.3438 (0.6275)
2021-12-28 08:50:57,824 Val Step[1400/1563], Loss: 3.8544 (3.1332), Acc@1: 0.2500 (0.3698), Acc@5: 0.5312 (0.6256)
2021-12-28 08:50:59,336 Val Step[1450/1563], Loss: 3.7140 (3.1357), Acc@1: 0.1562 (0.3692), Acc@5: 0.5625 (0.6252)
2021-12-28 08:51:00,821 Val Step[1500/1563], Loss: 2.8546 (3.1173), Acc@1: 0.3125 (0.3728), Acc@5: 0.8125 (0.6292)
2021-12-28 08:51:02,380 Val Step[1550/1563], Loss: 1.3854 (3.1030), Acc@1: 0.8125 (0.3755), Acc@5: 0.8750 (0.6313)
2021-12-28 08:51:03,462 ----- Epoch[016/310], Validation Loss: 3.1002, Validation Acc@1: 0.3763, Validation Acc@5: 0.6318, time: 105.90
2021-12-28 08:51:03,462 ----- Epoch[016/310], Train Loss: 5.3715, Train Acc: 0.1257, time: 1652.12, Best Val(epoch16) Acc@1: 0.3763
2021-12-28 08:51:03,670 Max accuracy so far: 0.3763 at epoch_16
2021-12-28 08:51:03,671 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 08:51:03,671 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 08:51:03,765 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 08:51:03,765 Now training epoch 17. LR=0.000850
2021-12-28 08:52:20,627 Epoch[017/310], Step[0000/1251], Loss: 5.1037(5.1037), Acc: 0.1641(0.1641)
2021-12-28 08:53:22,361 Epoch[017/310], Step[0050/1251], Loss: 5.0488(5.3625), Acc: 0.1494(0.1372)
2021-12-28 08:54:24,164 Epoch[017/310], Step[0100/1251], Loss: 5.7158(5.3392), Acc: 0.0391(0.1355)
2021-12-28 08:55:23,208 Epoch[017/310], Step[0150/1251], Loss: 5.2770(5.3111), Acc: 0.1445(0.1383)
2021-12-28 08:56:25,850 Epoch[017/310], Step[0200/1251], Loss: 5.4988(5.2967), Acc: 0.1221(0.1372)
2021-12-28 08:57:29,301 Epoch[017/310], Step[0250/1251], Loss: 5.3459(5.2982), Acc: 0.1826(0.1375)
2021-12-28 08:58:31,027 Epoch[017/310], Step[0300/1251], Loss: 5.1956(5.2979), Acc: 0.1309(0.1384)
2021-12-28 08:59:32,820 Epoch[017/310], Step[0350/1251], Loss: 5.1525(5.3041), Acc: 0.1279(0.1382)
2021-12-28 09:00:35,958 Epoch[017/310], Step[0400/1251], Loss: 4.9547(5.3045), Acc: 0.1396(0.1384)
2021-12-28 09:01:38,204 Epoch[017/310], Step[0450/1251], Loss: 5.2554(5.3111), Acc: 0.0938(0.1371)
2021-12-28 09:02:41,948 Epoch[017/310], Step[0500/1251], Loss: 5.2783(5.3150), Acc: 0.1162(0.1368)
2021-12-28 09:03:46,185 Epoch[017/310], Step[0550/1251], Loss: 5.1208(5.3122), Acc: 0.0605(0.1371)
2021-12-28 09:04:48,645 Epoch[017/310], Step[0600/1251], Loss: 4.8664(5.3077), Acc: 0.1660(0.1373)
2021-12-28 09:05:50,848 Epoch[017/310], Step[0650/1251], Loss: 5.1422(5.3115), Acc: 0.0303(0.1366)
2021-12-28 09:06:53,603 Epoch[017/310], Step[0700/1251], Loss: 5.7668(5.3138), Acc: 0.1123(0.1359)
2021-12-28 09:07:56,107 Epoch[017/310], Step[0750/1251], Loss: 5.3071(5.3120), Acc: 0.0898(0.1354)
2021-12-28 09:08:58,472 Epoch[017/310], Step[0800/1251], Loss: 5.2037(5.3082), Acc: 0.1953(0.1354)
2021-12-28 09:10:00,903 Epoch[017/310], Step[0850/1251], Loss: 5.0306(5.3087), Acc: 0.1113(0.1355)
2021-12-28 09:11:04,782 Epoch[017/310], Step[0900/1251], Loss: 5.7665(5.3116), Acc: 0.0752(0.1352)
2021-12-28 09:12:07,766 Epoch[017/310], Step[0950/1251], Loss: 5.4108(5.3086), Acc: 0.0420(0.1353)
2021-12-28 09:13:09,944 Epoch[017/310], Step[1000/1251], Loss: 5.5713(5.3071), Acc: 0.1885(0.1353)
2021-12-28 09:14:12,675 Epoch[017/310], Step[1050/1251], Loss: 5.2835(5.3029), Acc: 0.0928(0.1355)
2021-12-28 09:15:16,378 Epoch[017/310], Step[1100/1251], Loss: 5.5683(5.3015), Acc: 0.1240(0.1355)
2021-12-28 09:16:17,554 Epoch[017/310], Step[1150/1251], Loss: 5.1012(5.3015), Acc: 0.2109(0.1356)
2021-12-28 09:17:19,328 Epoch[017/310], Step[1200/1251], Loss: 5.7105(5.3003), Acc: 0.1367(0.1361)
2021-12-28 09:18:20,065 Epoch[017/310], Step[1250/1251], Loss: 5.5458(5.2991), Acc: 0.0918(0.1361)
2021-12-28 09:18:21,961 ----- Epoch[017/310], Train Loss: 5.2991, Train Acc: 0.1361, time: 1638.19, Best Val(epoch16) Acc@1: 0.3763
2021-12-28 09:18:21,962 Now training epoch 18. LR=0.000900
2021-12-28 09:19:43,879 Epoch[018/310], Step[0000/1251], Loss: 5.1961(5.1961), Acc: 0.2109(0.2109)
2021-12-28 09:20:46,487 Epoch[018/310], Step[0050/1251], Loss: 5.6486(5.2701), Acc: 0.1270(0.1269)
2021-12-28 09:21:47,927 Epoch[018/310], Step[0100/1251], Loss: 5.0782(5.3084), Acc: 0.0908(0.1299)
2021-12-28 09:22:50,987 Epoch[018/310], Step[0150/1251], Loss: 4.5768(5.2954), Acc: 0.1406(0.1317)
2021-12-28 09:23:54,720 Epoch[018/310], Step[0200/1251], Loss: 5.4717(5.2684), Acc: 0.0781(0.1358)
2021-12-28 09:24:57,494 Epoch[018/310], Step[0250/1251], Loss: 5.2406(5.2638), Acc: 0.1816(0.1384)
2021-12-28 09:26:00,744 Epoch[018/310], Step[0300/1251], Loss: 5.4910(5.2582), Acc: 0.1670(0.1402)
2021-12-28 09:27:02,627 Epoch[018/310], Step[0350/1251], Loss: 4.7432(5.2562), Acc: 0.0928(0.1403)
2021-12-28 09:28:04,871 Epoch[018/310], Step[0400/1251], Loss: 5.0113(5.2569), Acc: 0.0088(0.1393)
2021-12-28 09:29:07,069 Epoch[018/310], Step[0450/1251], Loss: 5.0892(5.2594), Acc: 0.1250(0.1398)
2021-12-28 09:30:09,962 Epoch[018/310], Step[0500/1251], Loss: 4.7892(5.2561), Acc: 0.1865(0.1405)
2021-12-28 09:31:12,945 Epoch[018/310], Step[0550/1251], Loss: 5.3265(5.2563), Acc: 0.1777(0.1404)
2021-12-28 09:32:16,871 Epoch[018/310], Step[0600/1251], Loss: 5.3082(5.2528), Acc: 0.1816(0.1404)
2021-12-28 09:33:19,405 Epoch[018/310], Step[0650/1251], Loss: 5.0805(5.2533), Acc: 0.1719(0.1409)
2021-12-28 09:34:21,748 Epoch[018/310], Step[0700/1251], Loss: 4.9975(5.2505), Acc: 0.1348(0.1418)
2021-12-28 09:35:23,773 Epoch[018/310], Step[0750/1251], Loss: 5.0408(5.2520), Acc: 0.1348(0.1416)
2021-12-28 09:36:27,201 Epoch[018/310], Step[0800/1251], Loss: 5.2822(5.2491), Acc: 0.2148(0.1418)
2021-12-28 09:37:28,845 Epoch[018/310], Step[0850/1251], Loss: 5.5053(5.2482), Acc: 0.1182(0.1421)
2021-12-28 09:38:32,897 Epoch[018/310], Step[0900/1251], Loss: 5.2855(5.2458), Acc: 0.0938(0.1414)
2021-12-28 09:39:34,959 Epoch[018/310], Step[0950/1251], Loss: 5.2137(5.2512), Acc: 0.1016(0.1405)
2021-12-28 09:40:37,765 Epoch[018/310], Step[1000/1251], Loss: 5.5302(5.2498), Acc: 0.0547(0.1408)
2021-12-28 09:41:40,714 Epoch[018/310], Step[1050/1251], Loss: 4.7811(5.2490), Acc: 0.2754(0.1406)
2021-12-28 09:42:42,844 Epoch[018/310], Step[1100/1251], Loss: 5.4148(5.2504), Acc: 0.1240(0.1408)
2021-12-28 09:43:45,742 Epoch[018/310], Step[1150/1251], Loss: 5.6055(5.2468), Acc: 0.0840(0.1409)
2021-12-28 09:44:48,335 Epoch[018/310], Step[1200/1251], Loss: 5.2502(5.2466), Acc: 0.1445(0.1412)
2021-12-28 09:45:50,596 Epoch[018/310], Step[1250/1251], Loss: 5.2813(5.2428), Acc: 0.1436(0.1418)
2021-12-28 09:45:52,578 ----- Validation after Epoch: 18
2021-12-28 09:46:48,507 Val Step[0000/1563], Loss: 1.5268 (1.5268), Acc@1: 0.7812 (0.7812), Acc@5: 0.8750 (0.8750)
2021-12-28 09:46:50,091 Val Step[0050/1563], Loss: 4.2408 (2.0612), Acc@1: 0.1875 (0.5790), Acc@5: 0.3750 (0.8107)
2021-12-28 09:46:51,676 Val Step[0100/1563], Loss: 2.8896 (2.5287), Acc@1: 0.4375 (0.4749), Acc@5: 0.7500 (0.7271)
2021-12-28 09:46:53,201 Val Step[0150/1563], Loss: 1.5638 (2.3973), Acc@1: 0.7500 (0.5052), Acc@5: 0.8438 (0.7448)
2021-12-28 09:46:54,809 Val Step[0200/1563], Loss: 2.2995 (2.4465), Acc@1: 0.3438 (0.4961), Acc@5: 0.8438 (0.7387)
2021-12-28 09:46:56,392 Val Step[0250/1563], Loss: 2.4219 (2.3564), Acc@1: 0.5000 (0.5105), Acc@5: 0.7188 (0.7560)
2021-12-28 09:46:57,887 Val Step[0300/1563], Loss: 2.5862 (2.4503), Acc@1: 0.4375 (0.4816), Acc@5: 0.6875 (0.7401)
2021-12-28 09:46:59,396 Val Step[0350/1563], Loss: 3.0197 (2.4731), Acc@1: 0.1562 (0.4718), Acc@5: 0.6562 (0.7370)
2021-12-28 09:47:00,944 Val Step[0400/1563], Loss: 2.2258 (2.4501), Acc@1: 0.5625 (0.4666), Acc@5: 0.8438 (0.7447)
2021-12-28 09:47:02,554 Val Step[0450/1563], Loss: 1.9279 (2.4384), Acc@1: 0.2500 (0.4655), Acc@5: 0.9062 (0.7497)
2021-12-28 09:47:04,140 Val Step[0500/1563], Loss: 1.7482 (2.4373), Acc@1: 0.6250 (0.4674), Acc@5: 0.8750 (0.7508)
2021-12-28 09:47:05,675 Val Step[0550/1563], Loss: 2.3747 (2.4036), Acc@1: 0.5312 (0.4771), Acc@5: 0.7812 (0.7561)
2021-12-28 09:47:07,297 Val Step[0600/1563], Loss: 1.6098 (2.3985), Acc@1: 0.7188 (0.4788), Acc@5: 0.8750 (0.7572)
2021-12-28 09:47:08,823 Val Step[0650/1563], Loss: 2.0081 (2.4212), Acc@1: 0.5938 (0.4764), Acc@5: 0.9375 (0.7530)
2021-12-28 09:47:10,260 Val Step[0700/1563], Loss: 4.1146 (2.4771), Acc@1: 0.2500 (0.4682), Acc@5: 0.4062 (0.7433)
2021-12-28 09:47:11,688 Val Step[0750/1563], Loss: 2.7859 (2.5315), Acc@1: 0.4062 (0.4595), Acc@5: 0.6562 (0.7325)
2021-12-28 09:47:13,111 Val Step[0800/1563], Loss: 3.2664 (2.5859), Acc@1: 0.4375 (0.4516), Acc@5: 0.6875 (0.7234)
2021-12-28 09:47:14,553 Val Step[0850/1563], Loss: 3.1566 (2.6324), Acc@1: 0.3438 (0.4449), Acc@5: 0.5312 (0.7149)
2021-12-28 09:47:16,035 Val Step[0900/1563], Loss: 1.6596 (2.6443), Acc@1: 0.7188 (0.4453), Acc@5: 0.8125 (0.7124)
2021-12-28 09:47:17,584 Val Step[0950/1563], Loss: 2.2636 (2.6785), Acc@1: 0.6562 (0.4404), Acc@5: 0.8125 (0.7059)
2021-12-28 09:47:18,886 Val Step[1000/1563], Loss: 1.2473 (2.7094), Acc@1: 0.7812 (0.4353), Acc@5: 0.9375 (0.7003)
2021-12-28 09:47:20,190 Val Step[1050/1563], Loss: 1.7723 (2.7280), Acc@1: 0.6562 (0.4324), Acc@5: 0.9062 (0.6973)
2021-12-28 09:47:21,630 Val Step[1100/1563], Loss: 3.1174 (2.7546), Acc@1: 0.5000 (0.4286), Acc@5: 0.6562 (0.6910)
2021-12-28 09:47:23,028 Val Step[1150/1563], Loss: 2.8792 (2.7847), Acc@1: 0.4375 (0.4236), Acc@5: 0.7188 (0.6854)
2021-12-28 09:47:24,398 Val Step[1200/1563], Loss: 1.9476 (2.8101), Acc@1: 0.6250 (0.4201), Acc@5: 0.7812 (0.6808)
2021-12-28 09:47:25,745 Val Step[1250/1563], Loss: 1.9234 (2.8368), Acc@1: 0.6875 (0.4170), Acc@5: 0.8438 (0.6759)
2021-12-28 09:47:27,059 Val Step[1300/1563], Loss: 2.4788 (2.8524), Acc@1: 0.5312 (0.4139), Acc@5: 0.7500 (0.6730)
2021-12-28 09:47:28,411 Val Step[1350/1563], Loss: 3.6395 (2.8794), Acc@1: 0.0312 (0.4084), Acc@5: 0.5000 (0.6675)
2021-12-28 09:47:29,737 Val Step[1400/1563], Loss: 2.6695 (2.8901), Acc@1: 0.4375 (0.4060), Acc@5: 0.7812 (0.6657)
2021-12-28 09:47:31,130 Val Step[1450/1563], Loss: 3.0831 (2.8941), Acc@1: 0.4688 (0.4055), Acc@5: 0.6875 (0.6652)
2021-12-28 09:47:32,701 Val Step[1500/1563], Loss: 2.7246 (2.8753), Acc@1: 0.3750 (0.4090), Acc@5: 0.7812 (0.6693)
2021-12-28 09:47:34,467 Val Step[1550/1563], Loss: 1.0971 (2.8621), Acc@1: 0.8438 (0.4116), Acc@5: 0.8750 (0.6713)
2021-12-28 09:47:35,530 ----- Epoch[018/310], Validation Loss: 2.8580, Validation Acc@1: 0.4127, Validation Acc@5: 0.6718, time: 102.95
2021-12-28 09:47:35,530 ----- Epoch[018/310], Train Loss: 5.2428, Train Acc: 0.1418, time: 1650.61, Best Val(epoch18) Acc@1: 0.4127
2021-12-28 09:47:35,745 Max accuracy so far: 0.4127 at epoch_18
2021-12-28 09:47:35,745 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 09:47:35,745 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 09:47:35,822 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 09:47:35,823 Now training epoch 19. LR=0.000950
2021-12-28 09:48:55,586 Epoch[019/310], Step[0000/1251], Loss: 5.4525(5.4525), Acc: 0.0693(0.0693)
2021-12-28 09:49:57,286 Epoch[019/310], Step[0050/1251], Loss: 4.8555(5.1774), Acc: 0.1240(0.1520)
2021-12-28 09:51:00,054 Epoch[019/310], Step[0100/1251], Loss: 4.3938(5.1803), Acc: 0.2139(0.1543)
2021-12-28 09:52:01,756 Epoch[019/310], Step[0150/1251], Loss: 5.0570(5.1707), Acc: 0.1221(0.1529)
2021-12-28 09:53:03,897 Epoch[019/310], Step[0200/1251], Loss: 5.4110(5.1724), Acc: 0.1582(0.1545)
2021-12-28 09:54:07,447 Epoch[019/310], Step[0250/1251], Loss: 5.0815(5.1820), Acc: 0.1348(0.1523)
2021-12-28 09:55:10,283 Epoch[019/310], Step[0300/1251], Loss: 5.2371(5.1877), Acc: 0.1387(0.1518)
2021-12-28 09:56:12,677 Epoch[019/310], Step[0350/1251], Loss: 4.8365(5.1805), Acc: 0.1084(0.1527)
2021-12-28 09:57:14,710 Epoch[019/310], Step[0400/1251], Loss: 5.3526(5.1839), Acc: 0.1631(0.1516)
2021-12-28 09:58:17,181 Epoch[019/310], Step[0450/1251], Loss: 4.9122(5.1809), Acc: 0.1602(0.1513)
2021-12-28 09:59:19,088 Epoch[019/310], Step[0500/1251], Loss: 5.1359(5.1841), Acc: 0.1982(0.1524)
2021-12-28 10:00:21,400 Epoch[019/310], Step[0550/1251], Loss: 5.7699(5.1838), Acc: 0.1025(0.1526)
2021-12-28 10:01:25,003 Epoch[019/310], Step[0600/1251], Loss: 5.6999(5.1817), Acc: 0.1201(0.1524)
2021-12-28 10:02:27,508 Epoch[019/310], Step[0650/1251], Loss: 4.8749(5.1806), Acc: 0.2305(0.1524)
2021-12-28 10:03:29,971 Epoch[019/310], Step[0700/1251], Loss: 4.9272(5.1794), Acc: 0.1201(0.1525)
2021-12-28 10:04:32,403 Epoch[019/310], Step[0750/1251], Loss: 5.2556(5.1810), Acc: 0.0850(0.1526)
2021-12-28 10:05:36,170 Epoch[019/310], Step[0800/1251], Loss: 4.9095(5.1815), Acc: 0.0117(0.1518)
2021-12-28 10:06:39,800 Epoch[019/310], Step[0850/1251], Loss: 5.6377(5.1778), Acc: 0.1084(0.1522)
2021-12-28 10:07:42,898 Epoch[019/310], Step[0900/1251], Loss: 5.3750(5.1716), Acc: 0.0752(0.1520)
2021-12-28 10:08:46,182 Epoch[019/310], Step[0950/1251], Loss: 5.7957(5.1727), Acc: 0.1162(0.1519)
2021-12-28 10:09:49,807 Epoch[019/310], Step[1000/1251], Loss: 5.1648(5.1706), Acc: 0.1660(0.1517)
2021-12-28 10:10:52,149 Epoch[019/310], Step[1050/1251], Loss: 5.3762(5.1712), Acc: 0.1719(0.1520)
2021-12-28 10:11:55,669 Epoch[019/310], Step[1100/1251], Loss: 5.2752(5.1729), Acc: 0.2012(0.1518)
2021-12-28 10:12:58,161 Epoch[019/310], Step[1150/1251], Loss: 5.3150(5.1707), Acc: 0.1699(0.1520)
2021-12-28 10:13:59,399 Epoch[019/310], Step[1200/1251], Loss: 4.9217(5.1692), Acc: 0.2256(0.1523)
2021-12-28 10:15:01,578 Epoch[019/310], Step[1250/1251], Loss: 5.5460(5.1685), Acc: 0.1377(0.1524)
2021-12-28 10:15:03,488 ----- Epoch[019/310], Train Loss: 5.1685, Train Acc: 0.1524, time: 1647.66, Best Val(epoch18) Acc@1: 0.4127
2021-12-28 10:15:03,488 Now training epoch 20. LR=0.000989
2021-12-28 10:16:28,014 Epoch[020/310], Step[0000/1251], Loss: 4.8232(4.8232), Acc: 0.2012(0.2012)
2021-12-28 10:17:30,327 Epoch[020/310], Step[0050/1251], Loss: 5.0354(5.0938), Acc: 0.1660(0.1621)
2021-12-28 10:18:32,575 Epoch[020/310], Step[0100/1251], Loss: 5.2030(5.1546), Acc: 0.2100(0.1557)
2021-12-28 10:19:34,026 Epoch[020/310], Step[0150/1251], Loss: 4.8789(5.1483), Acc: 0.2383(0.1549)
2021-12-28 10:20:36,029 Epoch[020/310], Step[0200/1251], Loss: 4.9492(5.1470), Acc: 0.2373(0.1572)
2021-12-28 10:21:40,079 Epoch[020/310], Step[0250/1251], Loss: 4.7567(5.1410), Acc: 0.2705(0.1601)
2021-12-28 10:22:43,064 Epoch[020/310], Step[0300/1251], Loss: 4.8628(5.1320), Acc: 0.1816(0.1605)
2021-12-28 10:23:45,330 Epoch[020/310], Step[0350/1251], Loss: 5.2628(5.1292), Acc: 0.1602(0.1593)
2021-12-28 10:24:47,888 Epoch[020/310], Step[0400/1251], Loss: 4.6515(5.1365), Acc: 0.1914(0.1591)
2021-12-28 10:25:51,234 Epoch[020/310], Step[0450/1251], Loss: 5.2496(5.1357), Acc: 0.1240(0.1573)
2021-12-28 10:26:55,277 Epoch[020/310], Step[0500/1251], Loss: 5.3338(5.1404), Acc: 0.1465(0.1577)
2021-12-28 10:27:56,969 Epoch[020/310], Step[0550/1251], Loss: 5.1894(5.1390), Acc: 0.2031(0.1586)
2021-12-28 10:28:59,428 Epoch[020/310], Step[0600/1251], Loss: 5.4165(5.1397), Acc: 0.1455(0.1583)
2021-12-28 10:30:02,959 Epoch[020/310], Step[0650/1251], Loss: 5.0425(5.1327), Acc: 0.0840(0.1584)
2021-12-28 10:31:07,076 Epoch[020/310], Step[0700/1251], Loss: 5.3043(5.1285), Acc: 0.1816(0.1580)
2021-12-28 10:32:09,360 Epoch[020/310], Step[0750/1251], Loss: 4.8467(5.1241), Acc: 0.2334(0.1576)
2021-12-28 10:33:12,873 Epoch[020/310], Step[0800/1251], Loss: 4.7188(5.1243), Acc: 0.0986(0.1576)
2021-12-28 10:34:16,320 Epoch[020/310], Step[0850/1251], Loss: 5.1548(5.1251), Acc: 0.1982(0.1573)
2021-12-28 10:35:19,931 Epoch[020/310], Step[0900/1251], Loss: 5.3781(5.1230), Acc: 0.1641(0.1581)
2021-12-28 10:36:22,164 Epoch[020/310], Step[0950/1251], Loss: 5.7699(5.1221), Acc: 0.1006(0.1576)
2021-12-28 10:37:24,065 Epoch[020/310], Step[1000/1251], Loss: 5.3758(5.1243), Acc: 0.0859(0.1577)
2021-12-28 10:38:27,473 Epoch[020/310], Step[1050/1251], Loss: 5.3547(5.1209), Acc: 0.1182(0.1581)
2021-12-28 10:39:31,061 Epoch[020/310], Step[1100/1251], Loss: 5.3091(5.1192), Acc: 0.1367(0.1584)
2021-12-28 10:40:34,393 Epoch[020/310], Step[1150/1251], Loss: 4.9686(5.1145), Acc: 0.2578(0.1588)
2021-12-28 10:41:38,500 Epoch[020/310], Step[1200/1251], Loss: 5.2136(5.1148), Acc: 0.0820(0.1591)
2021-12-28 10:42:41,263 Epoch[020/310], Step[1250/1251], Loss: 5.2403(5.1144), Acc: 0.1250(0.1590)
2021-12-28 10:42:43,574 ----- Validation after Epoch: 20
2021-12-28 10:43:46,391 Val Step[0000/1563], Loss: 1.6377 (1.6377), Acc@1: 0.8125 (0.8125), Acc@5: 0.8125 (0.8125)
2021-12-28 10:43:48,140 Val Step[0050/1563], Loss: 4.2400 (1.9656), Acc@1: 0.1250 (0.6017), Acc@5: 0.4062 (0.8217)
2021-12-28 10:43:49,694 Val Step[0100/1563], Loss: 2.7923 (2.4235), Acc@1: 0.3438 (0.4923), Acc@5: 0.8125 (0.7460)
2021-12-28 10:43:51,188 Val Step[0150/1563], Loss: 1.2736 (2.2700), Acc@1: 0.7500 (0.5294), Acc@5: 0.8750 (0.7657)
2021-12-28 10:43:52,739 Val Step[0200/1563], Loss: 1.9623 (2.3133), Acc@1: 0.4688 (0.5196), Acc@5: 0.8438 (0.7589)
2021-12-28 10:43:54,236 Val Step[0250/1563], Loss: 2.2547 (2.2222), Acc@1: 0.4375 (0.5327), Acc@5: 0.7812 (0.7735)
2021-12-28 10:43:55,692 Val Step[0300/1563], Loss: 2.5780 (2.2997), Acc@1: 0.5000 (0.5084), Acc@5: 0.7188 (0.7620)
2021-12-28 10:43:57,181 Val Step[0350/1563], Loss: 2.3945 (2.3094), Acc@1: 0.5312 (0.5012), Acc@5: 0.7500 (0.7638)
2021-12-28 10:43:58,803 Val Step[0400/1563], Loss: 2.4929 (2.3008), Acc@1: 0.5625 (0.4989), Acc@5: 0.8125 (0.7678)
2021-12-28 10:44:00,360 Val Step[0450/1563], Loss: 1.6283 (2.3191), Acc@1: 0.4688 (0.4918), Acc@5: 0.9375 (0.7652)
2021-12-28 10:44:01,953 Val Step[0500/1563], Loss: 1.1009 (2.3048), Acc@1: 0.7812 (0.4961), Acc@5: 0.9688 (0.7682)
2021-12-28 10:44:03,424 Val Step[0550/1563], Loss: 2.1098 (2.2740), Acc@1: 0.4688 (0.5032), Acc@5: 0.8125 (0.7722)
2021-12-28 10:44:05,040 Val Step[0600/1563], Loss: 1.5046 (2.2671), Acc@1: 0.7500 (0.5048), Acc@5: 0.8750 (0.7733)
2021-12-28 10:44:06,509 Val Step[0650/1563], Loss: 2.0953 (2.2945), Acc@1: 0.5938 (0.5014), Acc@5: 0.8438 (0.7686)
2021-12-28 10:44:08,038 Val Step[0700/1563], Loss: 3.0929 (2.3520), Acc@1: 0.4688 (0.4916), Acc@5: 0.6562 (0.7587)
2021-12-28 10:44:09,604 Val Step[0750/1563], Loss: 3.0467 (2.4052), Acc@1: 0.3750 (0.4815), Acc@5: 0.6562 (0.7483)
2021-12-28 10:44:11,204 Val Step[0800/1563], Loss: 2.5558 (2.4580), Acc@1: 0.6875 (0.4724), Acc@5: 0.7812 (0.7397)
2021-12-28 10:44:12,738 Val Step[0850/1563], Loss: 2.6531 (2.5001), Acc@1: 0.3750 (0.4659), Acc@5: 0.7188 (0.7333)
2021-12-28 10:44:14,349 Val Step[0900/1563], Loss: 1.4632 (2.5067), Acc@1: 0.7812 (0.4669), Acc@5: 0.8125 (0.7314)
2021-12-28 10:44:15,873 Val Step[0950/1563], Loss: 2.9471 (2.5399), Acc@1: 0.4688 (0.4620), Acc@5: 0.7500 (0.7253)
2021-12-28 10:44:17,406 Val Step[1000/1563], Loss: 1.3749 (2.5709), Acc@1: 0.7812 (0.4570), Acc@5: 0.9375 (0.7196)
2021-12-28 10:44:18,880 Val Step[1050/1563], Loss: 1.3615 (2.5859), Acc@1: 0.7812 (0.4544), Acc@5: 0.9375 (0.7174)
2021-12-28 10:44:20,374 Val Step[1100/1563], Loss: 2.8367 (2.6133), Acc@1: 0.4375 (0.4506), Acc@5: 0.7188 (0.7117)
2021-12-28 10:44:21,893 Val Step[1150/1563], Loss: 3.4854 (2.6367), Acc@1: 0.4062 (0.4474), Acc@5: 0.6250 (0.7071)
2021-12-28 10:44:23,469 Val Step[1200/1563], Loss: 2.1454 (2.6615), Acc@1: 0.5625 (0.4436), Acc@5: 0.7188 (0.7029)
2021-12-28 10:44:24,996 Val Step[1250/1563], Loss: 1.6777 (2.6844), Acc@1: 0.7500 (0.4411), Acc@5: 0.8438 (0.6986)
2021-12-28 10:44:26,422 Val Step[1300/1563], Loss: 2.5236 (2.7000), Acc@1: 0.5000 (0.4378), Acc@5: 0.7500 (0.6958)
2021-12-28 10:44:27,885 Val Step[1350/1563], Loss: 3.2362 (2.7283), Acc@1: 0.0625 (0.4324), Acc@5: 0.5625 (0.6908)
2021-12-28 10:44:29,420 Val Step[1400/1563], Loss: 2.5017 (2.7377), Acc@1: 0.6250 (0.4312), Acc@5: 0.7500 (0.6889)
2021-12-28 10:44:30,903 Val Step[1450/1563], Loss: 3.0028 (2.7396), Acc@1: 0.3750 (0.4308), Acc@5: 0.7500 (0.6889)
2021-12-28 10:44:32,454 Val Step[1500/1563], Loss: 3.3133 (2.7203), Acc@1: 0.2812 (0.4348), Acc@5: 0.6250 (0.6924)
2021-12-28 10:44:33,878 Val Step[1550/1563], Loss: 1.1518 (2.7091), Acc@1: 0.8750 (0.4371), Acc@5: 0.9062 (0.6941)
2021-12-28 10:44:34,776 ----- Epoch[020/310], Validation Loss: 2.7044, Validation Acc@1: 0.4384, Validation Acc@5: 0.6948, time: 111.20
2021-12-28 10:44:34,776 ----- Epoch[020/310], Train Loss: 5.1144, Train Acc: 0.1590, time: 1660.08, Best Val(epoch20) Acc@1: 0.4384
2021-12-28 10:44:34,988 Max accuracy so far: 0.4384 at epoch_20
2021-12-28 10:44:34,988 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 10:44:34,988 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 10:44:35,075 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 10:44:35,249 ----- Save model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-20-Loss-5.129591732192859.pdparams
2021-12-28 10:44:35,249 ----- Save optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-20-Loss-5.129591732192859.pdopt
2021-12-28 10:44:35,320 ----- Save ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-20-Loss-5.129591732192859-EMA.pdparams
2021-12-28 10:44:35,320 Now training epoch 21. LR=0.000988
2021-12-28 10:45:52,025 Epoch[021/310], Step[0000/1251], Loss: 4.9373(4.9373), Acc: 0.1816(0.1816)
2021-12-28 10:46:53,845 Epoch[021/310], Step[0050/1251], Loss: 4.8498(5.0515), Acc: 0.1016(0.1666)
2021-12-28 10:47:55,091 Epoch[021/310], Step[0100/1251], Loss: 5.2441(5.0343), Acc: 0.1797(0.1767)
2021-12-28 10:48:57,873 Epoch[021/310], Step[0150/1251], Loss: 5.0546(5.0511), Acc: 0.1436(0.1751)
2021-12-28 10:49:59,049 Epoch[021/310], Step[0200/1251], Loss: 5.3843(5.0683), Acc: 0.1992(0.1710)
2021-12-28 10:51:00,798 Epoch[021/310], Step[0250/1251], Loss: 5.2508(5.0799), Acc: 0.1934(0.1716)
2021-12-28 10:52:03,950 Epoch[021/310], Step[0300/1251], Loss: 4.9305(5.0708), Acc: 0.1484(0.1734)
2021-12-28 10:53:05,411 Epoch[021/310], Step[0350/1251], Loss: 5.4373(5.0709), Acc: 0.0439(0.1726)
2021-12-28 10:54:06,007 Epoch[021/310], Step[0400/1251], Loss: 5.1337(5.0686), Acc: 0.2334(0.1732)
2021-12-28 10:55:08,831 Epoch[021/310], Step[0450/1251], Loss: 5.3927(5.0674), Acc: 0.1426(0.1735)
2021-12-28 10:56:10,900 Epoch[021/310], Step[0500/1251], Loss: 5.0339(5.0698), Acc: 0.2002(0.1720)
2021-12-28 10:57:11,772 Epoch[021/310], Step[0550/1251], Loss: 5.2202(5.0605), Acc: 0.2256(0.1734)
2021-12-28 10:58:14,558 Epoch[021/310], Step[0600/1251], Loss: 5.3720(5.0583), Acc: 0.1279(0.1735)
2021-12-28 10:59:17,032 Epoch[021/310], Step[0650/1251], Loss: 5.1071(5.0570), Acc: 0.0322(0.1726)
2021-12-28 11:00:21,053 Epoch[021/310], Step[0700/1251], Loss: 5.2826(5.0581), Acc: 0.1523(0.1722)
2021-12-28 11:01:24,185 Epoch[021/310], Step[0750/1251], Loss: 5.0520(5.0581), Acc: 0.0605(0.1720)
2021-12-28 11:02:27,346 Epoch[021/310], Step[0800/1251], Loss: 5.2201(5.0559), Acc: 0.2051(0.1727)
2021-12-28 11:03:30,595 Epoch[021/310], Step[0850/1251], Loss: 5.1041(5.0552), Acc: 0.2012(0.1726)
2021-12-28 11:04:32,117 Epoch[021/310], Step[0900/1251], Loss: 5.5568(5.0477), Acc: 0.1689(0.1736)
2021-12-28 11:05:33,765 Epoch[021/310], Step[0950/1251], Loss: 4.7108(5.0457), Acc: 0.1602(0.1733)
2021-12-28 11:06:35,141 Epoch[021/310], Step[1000/1251], Loss: 4.9562(5.0445), Acc: 0.0771(0.1734)
2021-12-28 11:07:38,165 Epoch[021/310], Step[1050/1251], Loss: 4.9340(5.0443), Acc: 0.1816(0.1734)
2021-12-28 11:08:41,859 Epoch[021/310], Step[1100/1251], Loss: 5.1454(5.0437), Acc: 0.1846(0.1734)
2021-12-28 11:09:44,627 Epoch[021/310], Step[1150/1251], Loss: 5.7035(5.0449), Acc: 0.1426(0.1728)
2021-12-28 11:10:48,061 Epoch[021/310], Step[1200/1251], Loss: 5.3736(5.0435), Acc: 0.1240(0.1728)
2021-12-28 11:11:50,592 Epoch[021/310], Step[1250/1251], Loss: 4.9830(5.0411), Acc: 0.1650(0.1729)
2021-12-28 11:11:52,558 ----- Epoch[021/310], Train Loss: 5.0411, Train Acc: 0.1729, time: 1637.23, Best Val(epoch20) Acc@1: 0.4384
2021-12-28 11:11:52,558 Now training epoch 22. LR=0.000987
2021-12-28 11:13:13,755 Epoch[022/310], Step[0000/1251], Loss: 4.6714(4.6714), Acc: 0.1562(0.1562)
2021-12-28 11:14:14,930 Epoch[022/310], Step[0050/1251], Loss: 5.1081(4.8756), Acc: 0.2344(0.1768)
2021-12-28 11:15:16,750 Epoch[022/310], Step[0100/1251], Loss: 5.1197(4.9489), Acc: 0.2100(0.1793)
2021-12-28 11:16:18,824 Epoch[022/310], Step[0150/1251], Loss: 4.7031(4.9555), Acc: 0.0869(0.1810)
2021-12-28 11:17:20,921 Epoch[022/310], Step[0200/1251], Loss: 4.7676(4.9685), Acc: 0.1777(0.1763)
2021-12-28 11:18:22,538 Epoch[022/310], Step[0250/1251], Loss: 5.1832(4.9711), Acc: 0.0859(0.1781)
2021-12-28 11:19:25,296 Epoch[022/310], Step[0300/1251], Loss: 4.9539(4.9872), Acc: 0.1963(0.1797)
2021-12-28 11:20:26,850 Epoch[022/310], Step[0350/1251], Loss: 5.0745(4.9863), Acc: 0.1846(0.1801)
2021-12-28 11:21:29,824 Epoch[022/310], Step[0400/1251], Loss: 5.0447(4.9962), Acc: 0.0986(0.1784)
2021-12-28 11:22:32,149 Epoch[022/310], Step[0450/1251], Loss: 4.6335(5.0032), Acc: 0.2529(0.1782)
2021-12-28 11:23:34,704 Epoch[022/310], Step[0500/1251], Loss: 4.7244(4.9994), Acc: 0.2383(0.1781)
2021-12-28 11:24:37,575 Epoch[022/310], Step[0550/1251], Loss: 5.1659(5.0041), Acc: 0.1543(0.1782)
2021-12-28 11:25:39,762 Epoch[022/310], Step[0600/1251], Loss: 5.0824(5.0030), Acc: 0.1445(0.1777)
2021-12-28 11:26:43,507 Epoch[022/310], Step[0650/1251], Loss: 5.2648(4.9999), Acc: 0.1367(0.1783)
2021-12-28 11:27:46,303 Epoch[022/310], Step[0700/1251], Loss: 4.5187(4.9954), Acc: 0.0723(0.1782)
2021-12-28 11:28:48,064 Epoch[022/310], Step[0750/1251], Loss: 5.0921(4.9891), Acc: 0.1230(0.1780)
2021-12-28 11:29:49,977 Epoch[022/310], Step[0800/1251], Loss: 4.1556(4.9865), Acc: 0.1807(0.1783)
2021-12-28 11:30:53,468 Epoch[022/310], Step[0850/1251], Loss: 4.9792(4.9899), Acc: 0.1836(0.1779)
2021-12-28 11:31:56,084 Epoch[022/310], Step[0900/1251], Loss: 5.1733(4.9898), Acc: 0.1865(0.1780)
2021-12-28 11:32:58,981 Epoch[022/310], Step[0950/1251], Loss: 4.8067(4.9907), Acc: 0.2725(0.1775)
2021-12-28 11:34:01,328 Epoch[022/310], Step[1000/1251], Loss: 5.2903(4.9888), Acc: 0.1465(0.1779)
2021-12-28 11:35:03,331 Epoch[022/310], Step[1050/1251], Loss: 5.6069(4.9892), Acc: 0.1855(0.1779)
2021-12-28 11:36:06,102 Epoch[022/310], Step[1100/1251], Loss: 5.1900(4.9891), Acc: 0.1670(0.1784)
2021-12-28 11:37:08,797 Epoch[022/310], Step[1150/1251], Loss: 4.3443(4.9857), Acc: 0.3320(0.1786)
2021-12-28 11:38:11,199 Epoch[022/310], Step[1200/1251], Loss: 4.4272(4.9839), Acc: 0.1602(0.1794)
2021-12-28 11:39:14,479 Epoch[022/310], Step[1250/1251], Loss: 4.6659(4.9826), Acc: 0.1699(0.1792)
2021-12-28 11:39:16,382 ----- Validation after Epoch: 22
2021-12-28 11:40:18,306 Val Step[0000/1563], Loss: 1.1066 (1.1066), Acc@1: 0.8750 (0.8750), Acc@5: 0.9062 (0.9062)
2021-12-28 11:40:19,859 Val Step[0050/1563], Loss: 3.4615 (1.7847), Acc@1: 0.2812 (0.6354), Acc@5: 0.5312 (0.8438)
2021-12-28 11:40:21,361 Val Step[0100/1563], Loss: 3.5303 (2.2569), Acc@1: 0.0938 (0.5275), Acc@5: 0.6250 (0.7667)
2021-12-28 11:40:22,844 Val Step[0150/1563], Loss: 1.3680 (2.0888), Acc@1: 0.7500 (0.5606), Acc@5: 0.8750 (0.7908)
2021-12-28 11:40:24,385 Val Step[0200/1563], Loss: 1.8649 (2.1457), Acc@1: 0.6875 (0.5550), Acc@5: 0.8125 (0.7799)
2021-12-28 11:40:25,925 Val Step[0250/1563], Loss: 2.1423 (2.0452), Acc@1: 0.4688 (0.5740), Acc@5: 0.8438 (0.7974)
2021-12-28 11:40:27,401 Val Step[0300/1563], Loss: 2.5867 (2.0872), Acc@1: 0.2812 (0.5532), Acc@5: 0.8125 (0.7931)
2021-12-28 11:40:28,831 Val Step[0350/1563], Loss: 2.8490 (2.0905), Acc@1: 0.3125 (0.5470), Acc@5: 0.6875 (0.7973)
2021-12-28 11:40:30,285 Val Step[0400/1563], Loss: 2.4776 (2.0866), Acc@1: 0.4062 (0.5397), Acc@5: 0.7812 (0.8011)
2021-12-28 11:40:31,789 Val Step[0450/1563], Loss: 1.6628 (2.0902), Acc@1: 0.2500 (0.5364), Acc@5: 0.9688 (0.8029)
2021-12-28 11:40:33,301 Val Step[0500/1563], Loss: 0.9302 (2.0917), Acc@1: 0.8750 (0.5371), Acc@5: 0.9375 (0.8023)
2021-12-28 11:40:34,981 Val Step[0550/1563], Loss: 1.6787 (2.0669), Acc@1: 0.5625 (0.5440), Acc@5: 0.8438 (0.8053)
2021-12-28 11:40:36,629 Val Step[0600/1563], Loss: 1.7713 (2.0615), Acc@1: 0.6250 (0.5458), Acc@5: 0.8125 (0.8057)
2021-12-28 11:40:38,192 Val Step[0650/1563], Loss: 2.2146 (2.0859), Acc@1: 0.4375 (0.5421), Acc@5: 0.8750 (0.8021)
2021-12-28 11:40:39,694 Val Step[0700/1563], Loss: 2.9655 (2.1417), Acc@1: 0.4375 (0.5321), Acc@5: 0.6562 (0.7926)
2021-12-28 11:40:41,198 Val Step[0750/1563], Loss: 2.6984 (2.1896), Acc@1: 0.4688 (0.5238), Acc@5: 0.6562 (0.7841)
2021-12-28 11:40:42,744 Val Step[0800/1563], Loss: 2.9532 (2.2359), Acc@1: 0.3750 (0.5151), Acc@5: 0.7188 (0.7758)
2021-12-28 11:40:44,262 Val Step[0850/1563], Loss: 3.4873 (2.2784), Acc@1: 0.2812 (0.5089), Acc@5: 0.5000 (0.7678)
2021-12-28 11:40:45,782 Val Step[0900/1563], Loss: 1.0111 (2.2876), Acc@1: 0.8125 (0.5094), Acc@5: 0.9062 (0.7659)
2021-12-28 11:40:47,325 Val Step[0950/1563], Loss: 2.2508 (2.3209), Acc@1: 0.6250 (0.5041), Acc@5: 0.8125 (0.7599)
2021-12-28 11:40:48,813 Val Step[1000/1563], Loss: 1.5801 (2.3481), Acc@1: 0.7188 (0.4995), Acc@5: 0.9062 (0.7556)
2021-12-28 11:40:50,317 Val Step[1050/1563], Loss: 1.0651 (2.3652), Acc@1: 0.8438 (0.4966), Acc@5: 0.9688 (0.7528)
2021-12-28 11:40:51,784 Val Step[1100/1563], Loss: 2.7702 (2.3899), Acc@1: 0.5625 (0.4926), Acc@5: 0.7188 (0.7474)
2021-12-28 11:40:53,202 Val Step[1150/1563], Loss: 2.6078 (2.4182), Acc@1: 0.5625 (0.4884), Acc@5: 0.6875 (0.7417)
2021-12-28 11:40:54,716 Val Step[1200/1563], Loss: 2.4377 (2.4417), Acc@1: 0.5000 (0.4843), Acc@5: 0.7500 (0.7373)
2021-12-28 11:40:56,166 Val Step[1250/1563], Loss: 1.6942 (2.4667), Acc@1: 0.7500 (0.4811), Acc@5: 0.8438 (0.7329)
2021-12-28 11:40:57,639 Val Step[1300/1563], Loss: 2.4087 (2.4800), Acc@1: 0.5000 (0.4780), Acc@5: 0.7188 (0.7307)
2021-12-28 11:40:59,151 Val Step[1350/1563], Loss: 2.7110 (2.5039), Acc@1: 0.1250 (0.4737), Acc@5: 0.6562 (0.7264)
2021-12-28 11:41:00,755 Val Step[1400/1563], Loss: 2.3917 (2.5165), Acc@1: 0.4062 (0.4709), Acc@5: 0.7500 (0.7245)
2021-12-28 11:41:02,204 Val Step[1450/1563], Loss: 2.9248 (2.5197), Acc@1: 0.3438 (0.4708), Acc@5: 0.6562 (0.7240)
2021-12-28 11:41:03,698 Val Step[1500/1563], Loss: 2.2358 (2.5008), Acc@1: 0.5312 (0.4751), Acc@5: 0.8438 (0.7272)
2021-12-28 11:41:05,246 Val Step[1550/1563], Loss: 1.2822 (2.4938), Acc@1: 0.8438 (0.4763), Acc@5: 0.9062 (0.7284)
2021-12-28 11:41:06,220 ----- Epoch[022/310], Validation Loss: 2.4897, Validation Acc@1: 0.4772, Validation Acc@5: 0.7288, time: 109.84
2021-12-28 11:41:06,221 ----- Epoch[022/310], Train Loss: 4.9826, Train Acc: 0.1792, time: 1643.82, Best Val(epoch22) Acc@1: 0.4772
2021-12-28 11:41:06,426 Max accuracy so far: 0.4772 at epoch_22
2021-12-28 11:41:06,427 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 11:41:06,427 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 11:41:06,524 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 11:41:06,524 Now training epoch 23. LR=0.000986
2021-12-28 11:42:26,897 Epoch[023/310], Step[0000/1251], Loss: 4.8517(4.8517), Acc: 0.1777(0.1777)
2021-12-28 11:43:28,520 Epoch[023/310], Step[0050/1251], Loss: 5.2207(4.9732), Acc: 0.1836(0.1786)
2021-12-28 11:44:30,471 Epoch[023/310], Step[0100/1251], Loss: 4.6927(4.9508), Acc: 0.1055(0.1779)
2021-12-28 11:45:33,633 Epoch[023/310], Step[0150/1251], Loss: 4.9251(4.9519), Acc: 0.1533(0.1782)
2021-12-28 11:46:36,529 Epoch[023/310], Step[0200/1251], Loss: 5.4318(4.9550), Acc: 0.1006(0.1806)
2021-12-28 11:47:38,401 Epoch[023/310], Step[0250/1251], Loss: 5.1273(4.9520), Acc: 0.2188(0.1780)
2021-12-28 11:48:40,549 Epoch[023/310], Step[0300/1251], Loss: 5.0561(4.9552), Acc: 0.2383(0.1809)
2021-12-28 11:49:43,955 Epoch[023/310], Step[0350/1251], Loss: 4.4307(4.9442), Acc: 0.1230(0.1804)
2021-12-28 11:50:46,977 Epoch[023/310], Step[0400/1251], Loss: 5.2565(4.9383), Acc: 0.2090(0.1809)
2021-12-28 11:51:50,400 Epoch[023/310], Step[0450/1251], Loss: 4.7514(4.9368), Acc: 0.1953(0.1820)
2021-12-28 11:52:53,421 Epoch[023/310], Step[0500/1251], Loss: 4.7952(4.9364), Acc: 0.1807(0.1823)
2021-12-28 11:53:55,244 Epoch[023/310], Step[0550/1251], Loss: 4.8999(4.9402), Acc: 0.2480(0.1837)
2021-12-28 11:54:58,630 Epoch[023/310], Step[0600/1251], Loss: 5.0076(4.9393), Acc: 0.0996(0.1833)
2021-12-28 11:56:02,262 Epoch[023/310], Step[0650/1251], Loss: 5.1403(4.9385), Acc: 0.1016(0.1829)
2021-12-28 11:57:04,809 Epoch[023/310], Step[0700/1251], Loss: 4.7666(4.9367), Acc: 0.3037(0.1833)
2021-12-28 11:58:06,768 Epoch[023/310], Step[0750/1251], Loss: 4.6713(4.9346), Acc: 0.3037(0.1851)
2021-12-28 11:59:08,042 Epoch[023/310], Step[0800/1251], Loss: 4.8397(4.9345), Acc: 0.2646(0.1846)
2021-12-28 12:00:10,697 Epoch[023/310], Step[0850/1251], Loss: 4.7376(4.9325), Acc: 0.2324(0.1858)
2021-12-28 12:01:13,384 Epoch[023/310], Step[0900/1251], Loss: 5.1259(4.9273), Acc: 0.2090(0.1863)
2021-12-28 12:02:16,244 Epoch[023/310], Step[0950/1251], Loss: 4.5954(4.9260), Acc: 0.3096(0.1869)
2021-12-28 12:03:18,887 Epoch[023/310], Step[1000/1251], Loss: 4.4427(4.9253), Acc: 0.1875(0.1862)
2021-12-28 12:04:22,328 Epoch[023/310], Step[1050/1251], Loss: 4.8184(4.9276), Acc: 0.1426(0.1858)
2021-12-28 12:05:24,855 Epoch[023/310], Step[1100/1251], Loss: 5.0246(4.9306), Acc: 0.1709(0.1854)
2021-12-28 12:06:27,206 Epoch[023/310], Step[1150/1251], Loss: 4.1673(4.9290), Acc: 0.2822(0.1849)
2021-12-28 12:07:29,594 Epoch[023/310], Step[1200/1251], Loss: 5.4283(4.9308), Acc: 0.1416(0.1854)
2021-12-28 12:08:31,883 Epoch[023/310], Step[1250/1251], Loss: 4.8617(4.9311), Acc: 0.2646(0.1849)
2021-12-28 12:08:33,827 ----- Epoch[023/310], Train Loss: 4.9311, Train Acc: 0.1849, time: 1647.30, Best Val(epoch22) Acc@1: 0.4772
2021-12-28 12:08:33,827 Now training epoch 24. LR=0.000984
2021-12-28 12:09:57,385 Epoch[024/310], Step[0000/1251], Loss: 4.8214(4.8214), Acc: 0.1309(0.1309)
2021-12-28 12:10:59,876 Epoch[024/310], Step[0050/1251], Loss: 5.0370(4.9443), Acc: 0.2676(0.1935)
2021-12-28 12:12:02,487 Epoch[024/310], Step[0100/1251], Loss: 4.7677(4.9372), Acc: 0.2822(0.1829)
2021-12-28 12:13:04,868 Epoch[024/310], Step[0150/1251], Loss: 4.5585(4.8939), Acc: 0.2070(0.1905)
2021-12-28 12:14:07,188 Epoch[024/310], Step[0200/1251], Loss: 4.7071(4.8865), Acc: 0.1924(0.1935)
2021-12-28 12:15:11,141 Epoch[024/310], Step[0250/1251], Loss: 4.5295(4.8860), Acc: 0.0635(0.1906)
2021-12-28 12:16:13,486 Epoch[024/310], Step[0300/1251], Loss: 4.5016(4.8850), Acc: 0.2012(0.1894)
2021-12-28 12:17:16,503 Epoch[024/310], Step[0350/1251], Loss: 4.1184(4.8839), Acc: 0.2695(0.1905)
2021-12-28 12:18:20,948 Epoch[024/310], Step[0400/1251], Loss: 4.6483(4.8808), Acc: 0.2344(0.1927)
2021-12-28 12:19:25,046 Epoch[024/310], Step[0450/1251], Loss: 4.9407(4.8805), Acc: 0.1885(0.1936)
2021-12-28 12:20:27,864 Epoch[024/310], Step[0500/1251], Loss: 5.1062(4.8812), Acc: 0.2256(0.1934)
2021-12-28 12:21:29,980 Epoch[024/310], Step[0550/1251], Loss: 4.7172(4.8835), Acc: 0.2891(0.1926)
2021-12-28 12:22:33,878 Epoch[024/310], Step[0600/1251], Loss: 5.2160(4.8854), Acc: 0.1768(0.1925)
2021-12-28 12:23:35,692 Epoch[024/310], Step[0650/1251], Loss: 4.8169(4.8838), Acc: 0.1113(0.1928)
2021-12-28 12:24:39,592 Epoch[024/310], Step[0700/1251], Loss: 5.1110(4.8786), Acc: 0.1592(0.1929)
2021-12-28 12:25:43,681 Epoch[024/310], Step[0750/1251], Loss: 5.0490(4.8788), Acc: 0.1504(0.1933)
2021-12-28 12:26:46,860 Epoch[024/310], Step[0800/1251], Loss: 5.2008(4.8750), Acc: 0.2002(0.1931)
2021-12-28 12:27:48,825 Epoch[024/310], Step[0850/1251], Loss: 4.9411(4.8723), Acc: 0.2158(0.1930)
2021-12-28 12:28:52,394 Epoch[024/310], Step[0900/1251], Loss: 5.3473(4.8753), Acc: 0.2139(0.1924)
2021-12-28 12:29:52,946 Epoch[024/310], Step[0950/1251], Loss: 4.6030(4.8751), Acc: 0.2676(0.1920)
2021-12-28 12:30:54,553 Epoch[024/310], Step[1000/1251], Loss: 4.4199(4.8751), Acc: 0.3125(0.1921)
2021-12-28 12:31:57,997 Epoch[024/310], Step[1050/1251], Loss: 5.0354(4.8734), Acc: 0.0820(0.1917)
2021-12-28 12:33:00,391 Epoch[024/310], Step[1100/1251], Loss: 4.5944(4.8742), Acc: 0.3203(0.1920)
2021-12-28 12:34:01,627 Epoch[024/310], Step[1150/1251], Loss: 5.1286(4.8731), Acc: 0.1826(0.1931)
2021-12-28 12:35:03,842 Epoch[024/310], Step[1200/1251], Loss: 4.5492(4.8737), Acc: 0.2139(0.1932)
2021-12-28 12:36:05,875 Epoch[024/310], Step[1250/1251], Loss: 4.6067(4.8760), Acc: 0.2295(0.1929)
2021-12-28 12:36:07,978 ----- Validation after Epoch: 24
2021-12-28 12:37:09,312 Val Step[0000/1563], Loss: 1.1114 (1.1114), Acc@1: 0.8438 (0.8438), Acc@5: 0.9375 (0.9375)
2021-12-28 12:37:10,891 Val Step[0050/1563], Loss: 3.4367 (1.6066), Acc@1: 0.2812 (0.6636), Acc@5: 0.5625 (0.8676)
2021-12-28 12:37:12,338 Val Step[0100/1563], Loss: 2.6994 (1.9958), Acc@1: 0.4062 (0.5613), Acc@5: 0.7500 (0.8116)
2021-12-28 12:37:13,878 Val Step[0150/1563], Loss: 1.2073 (1.8933), Acc@1: 0.7500 (0.5935), Acc@5: 0.9062 (0.8241)
2021-12-28 12:37:15,350 Val Step[0200/1563], Loss: 1.9177 (1.9753), Acc@1: 0.5625 (0.5805), Acc@5: 0.8750 (0.8145)
2021-12-28 12:37:16,803 Val Step[0250/1563], Loss: 1.9149 (1.8931), Acc@1: 0.6562 (0.5980), Acc@5: 0.7812 (0.8268)
2021-12-28 12:37:18,238 Val Step[0300/1563], Loss: 2.5323 (1.9535), Acc@1: 0.4062 (0.5762), Acc@5: 0.7500 (0.8217)
2021-12-28 12:37:19,679 Val Step[0350/1563], Loss: 2.6546 (1.9636), Acc@1: 0.2188 (0.5687), Acc@5: 0.7812 (0.8243)
2021-12-28 12:37:21,187 Val Step[0400/1563], Loss: 1.9126 (1.9590), Acc@1: 0.6250 (0.5638), Acc@5: 0.8438 (0.8273)
2021-12-28 12:37:22,719 Val Step[0450/1563], Loss: 1.2978 (1.9595), Acc@1: 0.5938 (0.5615), Acc@5: 1.0000 (0.8291)
2021-12-28 12:37:24,299 Val Step[0500/1563], Loss: 1.2430 (1.9551), Acc@1: 0.6562 (0.5632), Acc@5: 0.9688 (0.8291)
2021-12-28 12:37:25,895 Val Step[0550/1563], Loss: 1.1837 (1.9352), Acc@1: 0.8125 (0.5695), Acc@5: 0.8750 (0.8312)
2021-12-28 12:37:27,566 Val Step[0600/1563], Loss: 1.7405 (1.9376), Acc@1: 0.6562 (0.5696), Acc@5: 0.9062 (0.8302)
2021-12-28 12:37:29,102 Val Step[0650/1563], Loss: 2.1089 (1.9663), Acc@1: 0.5000 (0.5646), Acc@5: 0.8750 (0.8255)
2021-12-28 12:37:30,570 Val Step[0700/1563], Loss: 2.8139 (2.0210), Acc@1: 0.5000 (0.5553), Acc@5: 0.6562 (0.8162)
2021-12-28 12:37:32,223 Val Step[0750/1563], Loss: 2.9378 (2.0748), Acc@1: 0.4375 (0.5455), Acc@5: 0.6250 (0.8067)
2021-12-28 12:37:33,740 Val Step[0800/1563], Loss: 2.8806 (2.1267), Acc@1: 0.5000 (0.5360), Acc@5: 0.7188 (0.7976)
2021-12-28 12:37:35,383 Val Step[0850/1563], Loss: 2.8107 (2.1666), Acc@1: 0.3125 (0.5291), Acc@5: 0.6875 (0.7907)
2021-12-28 12:37:36,987 Val Step[0900/1563], Loss: 0.8401 (2.1702), Acc@1: 0.8750 (0.5306), Acc@5: 0.9375 (0.7889)
2021-12-28 12:37:38,599 Val Step[0950/1563], Loss: 2.2508 (2.2031), Acc@1: 0.6875 (0.5253), Acc@5: 0.7812 (0.7825)
2021-12-28 12:37:40,057 Val Step[1000/1563], Loss: 1.2568 (2.2367), Acc@1: 0.8125 (0.5188), Acc@5: 0.9375 (0.7768)
2021-12-28 12:37:41,556 Val Step[1050/1563], Loss: 1.1828 (2.2552), Acc@1: 0.7500 (0.5158), Acc@5: 0.9375 (0.7738)
2021-12-28 12:37:43,080 Val Step[1100/1563], Loss: 2.4479 (2.2794), Acc@1: 0.6562 (0.5121), Acc@5: 0.7188 (0.7681)
2021-12-28 12:37:44,565 Val Step[1150/1563], Loss: 2.6135 (2.3068), Acc@1: 0.5625 (0.5080), Acc@5: 0.7500 (0.7629)
2021-12-28 12:37:46,071 Val Step[1200/1563], Loss: 2.3581 (2.3285), Acc@1: 0.5625 (0.5049), Acc@5: 0.7188 (0.7587)
2021-12-28 12:37:47,589 Val Step[1250/1563], Loss: 1.4830 (2.3539), Acc@1: 0.7812 (0.5016), Acc@5: 0.8750 (0.7544)
2021-12-28 12:37:49,144 Val Step[1300/1563], Loss: 2.2588 (2.3697), Acc@1: 0.5625 (0.4984), Acc@5: 0.7812 (0.7512)
2021-12-28 12:37:50,730 Val Step[1350/1563], Loss: 2.6893 (2.3953), Acc@1: 0.3125 (0.4940), Acc@5: 0.7500 (0.7467)
2021-12-28 12:37:52,282 Val Step[1400/1563], Loss: 1.9176 (2.4069), Acc@1: 0.5312 (0.4916), Acc@5: 0.9375 (0.7449)
2021-12-28 12:37:53,779 Val Step[1450/1563], Loss: 3.2503 (2.4122), Acc@1: 0.1875 (0.4906), Acc@5: 0.6875 (0.7440)
2021-12-28 12:37:55,334 Val Step[1500/1563], Loss: 3.1743 (2.3943), Acc@1: 0.2812 (0.4944), Acc@5: 0.6250 (0.7473)
2021-12-28 12:37:56,945 Val Step[1550/1563], Loss: 1.1126 (2.3854), Acc@1: 0.8438 (0.4968), Acc@5: 0.9062 (0.7489)
2021-12-28 12:37:57,845 ----- Epoch[024/310], Validation Loss: 2.3826, Validation Acc@1: 0.4975, Validation Acc@5: 0.7492, time: 109.86
2021-12-28 12:37:57,845 ----- Epoch[024/310], Train Loss: 4.8760, Train Acc: 0.1929, time: 1654.15, Best Val(epoch24) Acc@1: 0.4975
2021-12-28 12:37:58,058 Max accuracy so far: 0.4975 at epoch_24
2021-12-28 12:37:58,059 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 12:37:58,059 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 12:37:58,138 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 12:37:58,138 Now training epoch 25. LR=0.000983
2021-12-28 12:39:18,076 Epoch[025/310], Step[0000/1251], Loss: 4.9729(4.9729), Acc: 0.2412(0.2412)
2021-12-28 12:40:19,274 Epoch[025/310], Step[0050/1251], Loss: 4.9824(4.8710), Acc: 0.1533(0.1959)
2021-12-28 12:41:21,188 Epoch[025/310], Step[0100/1251], Loss: 5.1191(4.8829), Acc: 0.1523(0.1931)
2021-12-28 12:42:23,825 Epoch[025/310], Step[0150/1251], Loss: 5.2333(4.8866), Acc: 0.2012(0.1930)
2021-12-28 12:43:26,595 Epoch[025/310], Step[0200/1251], Loss: 3.9996(4.8826), Acc: 0.2412(0.1937)
2021-12-28 12:44:28,606 Epoch[025/310], Step[0250/1251], Loss: 4.5417(4.8877), Acc: 0.2998(0.1943)
2021-12-28 12:45:32,160 Epoch[025/310], Step[0300/1251], Loss: 4.9078(4.8828), Acc: 0.2588(0.1926)
2021-12-28 12:46:34,657 Epoch[025/310], Step[0350/1251], Loss: 4.7227(4.8791), Acc: 0.1279(0.1928)
2021-12-28 12:47:36,801 Epoch[025/310], Step[0400/1251], Loss: 5.0286(4.8781), Acc: 0.2471(0.1945)
2021-12-28 12:48:38,825 Epoch[025/310], Step[0450/1251], Loss: 4.6976(4.8856), Acc: 0.3076(0.1944)
2021-12-28 12:49:41,850 Epoch[025/310], Step[0500/1251], Loss: 4.9291(4.8803), Acc: 0.2080(0.1954)
2021-12-28 12:50:44,975 Epoch[025/310], Step[0550/1251], Loss: 4.8755(4.8748), Acc: 0.2607(0.1949)
2021-12-28 12:51:48,560 Epoch[025/310], Step[0600/1251], Loss: 5.0314(4.8701), Acc: 0.0928(0.1957)
2021-12-28 12:52:50,330 Epoch[025/310], Step[0650/1251], Loss: 5.4682(4.8651), Acc: 0.1641(0.1961)
2021-12-28 12:53:52,544 Epoch[025/310], Step[0700/1251], Loss: 4.6509(4.8649), Acc: 0.2041(0.1948)
2021-12-28 12:54:54,018 Epoch[025/310], Step[0750/1251], Loss: 4.9422(4.8636), Acc: 0.2500(0.1945)
2021-12-28 12:55:56,827 Epoch[025/310], Step[0800/1251], Loss: 5.5200(4.8681), Acc: 0.1416(0.1943)
2021-12-28 12:56:58,704 Epoch[025/310], Step[0850/1251], Loss: 4.6566(4.8688), Acc: 0.2158(0.1947)
2021-12-28 12:58:01,627 Epoch[025/310], Step[0900/1251], Loss: 4.6804(4.8673), Acc: 0.1240(0.1945)
2021-12-28 12:59:04,902 Epoch[025/310], Step[0950/1251], Loss: 4.6060(4.8646), Acc: 0.3086(0.1953)
2021-12-28 13:00:08,444 Epoch[025/310], Step[1000/1251], Loss: 5.5464(4.8609), Acc: 0.1533(0.1953)
2021-12-28 13:01:10,941 Epoch[025/310], Step[1050/1251], Loss: 4.5164(4.8605), Acc: 0.2480(0.1956)
2021-12-28 13:02:14,789 Epoch[025/310], Step[1100/1251], Loss: 4.7721(4.8611), Acc: 0.1201(0.1950)
2021-12-28 13:03:16,559 Epoch[025/310], Step[1150/1251], Loss: 5.2637(4.8634), Acc: 0.0869(0.1952)
2021-12-28 13:04:19,860 Epoch[025/310], Step[1200/1251], Loss: 4.4139(4.8608), Acc: 0.0693(0.1956)
2021-12-28 13:05:23,114 Epoch[025/310], Step[1250/1251], Loss: 4.0755(4.8628), Acc: 0.1924(0.1955)
2021-12-28 13:05:25,048 ----- Epoch[025/310], Train Loss: 4.8628, Train Acc: 0.1955, time: 1646.91, Best Val(epoch24) Acc@1: 0.4975
2021-12-28 13:05:25,048 Now training epoch 26. LR=0.000982
2021-12-28 13:06:50,922 Epoch[026/310], Step[0000/1251], Loss: 4.7124(4.7124), Acc: 0.2676(0.2676)
2021-12-28 13:07:53,511 Epoch[026/310], Step[0050/1251], Loss: 4.7954(4.8770), Acc: 0.1543(0.2051)
2021-12-28 13:08:54,943 Epoch[026/310], Step[0100/1251], Loss: 5.0094(4.8524), Acc: 0.2539(0.2024)
2021-12-28 13:09:55,799 Epoch[026/310], Step[0150/1251], Loss: 4.9302(4.8248), Acc: 0.1279(0.2047)
2021-12-28 13:10:58,035 Epoch[026/310], Step[0200/1251], Loss: 4.9254(4.8224), Acc: 0.0908(0.1989)
2021-12-28 13:12:00,871 Epoch[026/310], Step[0250/1251], Loss: 4.7798(4.8169), Acc: 0.2393(0.1990)
2021-12-28 13:13:03,367 Epoch[026/310], Step[0300/1251], Loss: 4.6122(4.8108), Acc: 0.2520(0.2009)
2021-12-28 13:14:06,377 Epoch[026/310], Step[0350/1251], Loss: 4.6742(4.8051), Acc: 0.2949(0.2022)
2021-12-28 13:15:09,309 Epoch[026/310], Step[0400/1251], Loss: 4.5162(4.8036), Acc: 0.3223(0.2020)
2021-12-28 13:16:12,350 Epoch[026/310], Step[0450/1251], Loss: 4.8506(4.8179), Acc: 0.2012(0.2021)
2021-12-28 13:17:15,917 Epoch[026/310], Step[0500/1251], Loss: 4.9374(4.8187), Acc: 0.2080(0.1991)
2021-12-28 13:18:19,002 Epoch[026/310], Step[0550/1251], Loss: 4.6717(4.8224), Acc: 0.2002(0.1989)
2021-12-28 13:19:22,383 Epoch[026/310], Step[0600/1251], Loss: 5.4031(4.8213), Acc: 0.1025(0.1992)
2021-12-28 13:20:24,908 Epoch[026/310], Step[0650/1251], Loss: 4.7315(4.8237), Acc: 0.0830(0.1988)
2021-12-28 13:21:26,322 Epoch[026/310], Step[0700/1251], Loss: 4.6825(4.8221), Acc: 0.0977(0.1989)
2021-12-28 13:22:29,157 Epoch[026/310], Step[0750/1251], Loss: 4.8858(4.8171), Acc: 0.1729(0.2002)
2021-12-28 13:23:32,122 Epoch[026/310], Step[0800/1251], Loss: 4.5197(4.8179), Acc: 0.3184(0.2002)
2021-12-28 13:24:35,038 Epoch[026/310], Step[0850/1251], Loss: 4.7235(4.8223), Acc: 0.2041(0.2001)
2021-12-28 13:25:38,196 Epoch[026/310], Step[0900/1251], Loss: 4.9894(4.8224), Acc: 0.2812(0.2004)
2021-12-28 13:26:41,280 Epoch[026/310], Step[0950/1251], Loss: 4.7736(4.8258), Acc: 0.1348(0.1996)
2021-12-28 13:27:44,698 Epoch[026/310], Step[1000/1251], Loss: 4.7451(4.8219), Acc: 0.2529(0.1992)
2021-12-28 13:28:47,741 Epoch[026/310], Step[1050/1251], Loss: 4.6085(4.8232), Acc: 0.2207(0.1995)
2021-12-28 13:29:51,357 Epoch[026/310], Step[1100/1251], Loss: 4.3936(4.8232), Acc: 0.2490(0.1990)
2021-12-28 13:30:55,602 Epoch[026/310], Step[1150/1251], Loss: 4.6032(4.8219), Acc: 0.1309(0.1996)
2021-12-28 13:31:58,942 Epoch[026/310], Step[1200/1251], Loss: 4.9799(4.8206), Acc: 0.0898(0.1999)
2021-12-28 13:33:00,579 Epoch[026/310], Step[1250/1251], Loss: 5.3714(4.8169), Acc: 0.2168(0.2011)
2021-12-28 13:33:02,556 ----- Validation after Epoch: 26
2021-12-28 13:34:06,235 Val Step[0000/1563], Loss: 1.2688 (1.2688), Acc@1: 0.8750 (0.8750), Acc@5: 0.8750 (0.8750)
2021-12-28 13:34:07,839 Val Step[0050/1563], Loss: 3.1431 (1.4825), Acc@1: 0.3438 (0.6893), Acc@5: 0.5625 (0.8775)
2021-12-28 13:34:09,291 Val Step[0100/1563], Loss: 2.2065 (1.9571), Acc@1: 0.4062 (0.5767), Acc@5: 0.8750 (0.8168)
2021-12-28 13:34:10,897 Val Step[0150/1563], Loss: 1.4646 (1.8365), Acc@1: 0.6562 (0.6051), Acc@5: 0.8750 (0.8301)
2021-12-28 13:34:12,392 Val Step[0200/1563], Loss: 1.5312 (1.8733), Acc@1: 0.5938 (0.5989), Acc@5: 0.9375 (0.8265)
2021-12-28 13:34:13,882 Val Step[0250/1563], Loss: 1.4359 (1.7984), Acc@1: 0.6875 (0.6135), Acc@5: 0.8750 (0.8370)
2021-12-28 13:34:15,435 Val Step[0300/1563], Loss: 2.2440 (1.8723), Acc@1: 0.5000 (0.5895), Acc@5: 0.7812 (0.8292)
2021-12-28 13:34:16,940 Val Step[0350/1563], Loss: 1.8604 (1.8894), Acc@1: 0.5938 (0.5809), Acc@5: 0.8438 (0.8312)
2021-12-28 13:34:18,450 Val Step[0400/1563], Loss: 2.0448 (1.8789), Acc@1: 0.5938 (0.5791), Acc@5: 0.8750 (0.8356)
2021-12-28 13:34:20,075 Val Step[0450/1563], Loss: 1.5509 (1.8759), Acc@1: 0.5000 (0.5775), Acc@5: 0.9375 (0.8377)
2021-12-28 13:34:21,666 Val Step[0500/1563], Loss: 0.6480 (1.8630), Acc@1: 0.9375 (0.5799), Acc@5: 0.9688 (0.8396)
2021-12-28 13:34:23,282 Val Step[0550/1563], Loss: 1.3201 (1.8443), Acc@1: 0.7500 (0.5856), Acc@5: 0.9062 (0.8416)
2021-12-28 13:34:24,817 Val Step[0600/1563], Loss: 1.0273 (1.8437), Acc@1: 0.8125 (0.5867), Acc@5: 0.9375 (0.8411)
2021-12-28 13:34:26,374 Val Step[0650/1563], Loss: 1.6917 (1.8700), Acc@1: 0.7500 (0.5834), Acc@5: 0.8750 (0.8356)
2021-12-28 13:34:27,895 Val Step[0700/1563], Loss: 2.4094 (1.9259), Acc@1: 0.4688 (0.5738), Acc@5: 0.7500 (0.8261)
2021-12-28 13:34:29,553 Val Step[0750/1563], Loss: 2.3495 (1.9781), Acc@1: 0.5625 (0.5649), Acc@5: 0.7500 (0.8171)
2021-12-28 13:34:31,106 Val Step[0800/1563], Loss: 2.2621 (2.0326), Acc@1: 0.5625 (0.5543), Acc@5: 0.8125 (0.8088)
2021-12-28 13:34:32,671 Val Step[0850/1563], Loss: 2.7012 (2.0721), Acc@1: 0.4375 (0.5475), Acc@5: 0.6250 (0.8021)
2021-12-28 13:34:34,399 Val Step[0900/1563], Loss: 1.1636 (2.0806), Acc@1: 0.8125 (0.5484), Acc@5: 0.9062 (0.8002)
2021-12-28 13:34:36,120 Val Step[0950/1563], Loss: 2.1669 (2.1158), Acc@1: 0.5625 (0.5424), Acc@5: 0.7188 (0.7939)
2021-12-28 13:34:37,649 Val Step[1000/1563], Loss: 1.1990 (2.1524), Acc@1: 0.8438 (0.5359), Acc@5: 0.9688 (0.7881)
2021-12-28 13:34:39,167 Val Step[1050/1563], Loss: 1.1592 (2.1718), Acc@1: 0.7812 (0.5318), Acc@5: 0.9688 (0.7854)
2021-12-28 13:34:40,748 Val Step[1100/1563], Loss: 2.3445 (2.1963), Acc@1: 0.5938 (0.5275), Acc@5: 0.6562 (0.7804)
2021-12-28 13:34:42,345 Val Step[1150/1563], Loss: 2.3954 (2.2246), Acc@1: 0.6562 (0.5227), Acc@5: 0.7500 (0.7754)
2021-12-28 13:34:43,832 Val Step[1200/1563], Loss: 2.3663 (2.2490), Acc@1: 0.5625 (0.5192), Acc@5: 0.7812 (0.7707)
2021-12-28 13:34:45,413 Val Step[1250/1563], Loss: 1.3751 (2.2736), Acc@1: 0.8125 (0.5158), Acc@5: 0.8750 (0.7659)
2021-12-28 13:34:46,937 Val Step[1300/1563], Loss: 2.3389 (2.2908), Acc@1: 0.5625 (0.5123), Acc@5: 0.6875 (0.7629)
2021-12-28 13:34:48,570 Val Step[1350/1563], Loss: 2.6676 (2.3166), Acc@1: 0.2188 (0.5071), Acc@5: 0.7812 (0.7587)
2021-12-28 13:34:50,089 Val Step[1400/1563], Loss: 1.8544 (2.3275), Acc@1: 0.6562 (0.5047), Acc@5: 0.9062 (0.7568)
2021-12-28 13:34:51,658 Val Step[1450/1563], Loss: 2.9866 (2.3317), Acc@1: 0.2812 (0.5043), Acc@5: 0.7500 (0.7564)
2021-12-28 13:34:53,240 Val Step[1500/1563], Loss: 2.7130 (2.3158), Acc@1: 0.4062 (0.5076), Acc@5: 0.7500 (0.7591)
2021-12-28 13:34:54,866 Val Step[1550/1563], Loss: 1.1294 (2.3059), Acc@1: 0.8750 (0.5099), Acc@5: 0.9062 (0.7604)
2021-12-28 13:34:55,821 ----- Epoch[026/310], Validation Loss: 2.3019, Validation Acc@1: 0.5108, Validation Acc@5: 0.7607, time: 113.26
2021-12-28 13:34:55,822 ----- Epoch[026/310], Train Loss: 4.8169, Train Acc: 0.2011, time: 1657.51, Best Val(epoch26) Acc@1: 0.5108
2021-12-28 13:34:56,033 Max accuracy so far: 0.5108 at epoch_26
2021-12-28 13:34:56,033 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 13:34:56,033 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 13:34:56,126 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 13:34:56,126 Now training epoch 27. LR=0.000980
2021-12-28 13:36:17,808 Epoch[027/310], Step[0000/1251], Loss: 4.4411(4.4411), Acc: 0.2861(0.2861)
2021-12-28 13:37:20,633 Epoch[027/310], Step[0050/1251], Loss: 5.3189(4.8300), Acc: 0.1875(0.2135)
2021-12-28 13:38:23,668 Epoch[027/310], Step[0100/1251], Loss: 4.9013(4.7986), Acc: 0.2041(0.2187)
2021-12-28 13:39:24,955 Epoch[027/310], Step[0150/1251], Loss: 4.1497(4.7981), Acc: 0.4014(0.2148)
2021-12-28 13:40:27,810 Epoch[027/310], Step[0200/1251], Loss: 4.9336(4.7979), Acc: 0.2578(0.2121)
2021-12-28 13:41:30,537 Epoch[027/310], Step[0250/1251], Loss: 5.2968(4.7900), Acc: 0.1826(0.2115)
2021-12-28 13:42:34,151 Epoch[027/310], Step[0300/1251], Loss: 5.4964(4.7923), Acc: 0.0938(0.2112)
2021-12-28 13:43:37,115 Epoch[027/310], Step[0350/1251], Loss: 4.7671(4.7970), Acc: 0.2100(0.2097)
2021-12-28 13:44:39,147 Epoch[027/310], Step[0400/1251], Loss: 5.0769(4.7891), Acc: 0.2051(0.2096)
2021-12-28 13:45:42,660 Epoch[027/310], Step[0450/1251], Loss: 4.9641(4.7854), Acc: 0.2939(0.2090)
2021-12-28 13:46:45,915 Epoch[027/310], Step[0500/1251], Loss: 5.2654(4.7879), Acc: 0.1533(0.2078)
2021-12-28 13:47:48,116 Epoch[027/310], Step[0550/1251], Loss: 4.4250(4.7906), Acc: 0.3350(0.2086)
2021-12-28 13:48:50,556 Epoch[027/310], Step[0600/1251], Loss: 4.9614(4.7924), Acc: 0.2412(0.2073)
2021-12-28 13:49:53,344 Epoch[027/310], Step[0650/1251], Loss: 4.8849(4.7908), Acc: 0.0352(0.2066)
2021-12-28 13:50:55,802 Epoch[027/310], Step[0700/1251], Loss: 4.5349(4.7916), Acc: 0.2666(0.2064)
2021-12-28 13:51:58,754 Epoch[027/310], Step[0750/1251], Loss: 4.6896(4.7923), Acc: 0.0889(0.2067)
2021-12-28 13:53:02,369 Epoch[027/310], Step[0800/1251], Loss: 5.0250(4.7963), Acc: 0.1611(0.2055)
2021-12-28 13:54:06,100 Epoch[027/310], Step[0850/1251], Loss: 4.9945(4.7957), Acc: 0.0742(0.2052)
2021-12-28 13:55:10,295 Epoch[027/310], Step[0900/1251], Loss: 4.9874(4.7946), Acc: 0.1494(0.2055)
2021-12-28 13:56:14,782 Epoch[027/310], Step[0950/1251], Loss: 5.0152(4.7925), Acc: 0.1826(0.2066)
2021-12-28 13:57:15,992 Epoch[027/310], Step[1000/1251], Loss: 4.6891(4.7912), Acc: 0.1055(0.2068)
2021-12-28 13:58:18,552 Epoch[027/310], Step[1050/1251], Loss: 4.3087(4.7918), Acc: 0.2539(0.2071)
2021-12-28 13:59:21,617 Epoch[027/310], Step[1100/1251], Loss: 4.9843(4.7916), Acc: 0.2920(0.2070)
2021-12-28 14:00:25,780 Epoch[027/310], Step[1150/1251], Loss: 4.1735(4.7874), Acc: 0.2871(0.2081)
2021-12-28 14:01:28,467 Epoch[027/310], Step[1200/1251], Loss: 5.0899(4.7846), Acc: 0.2178(0.2083)
2021-12-28 14:02:30,943 Epoch[027/310], Step[1250/1251], Loss: 4.8641(4.7841), Acc: 0.2373(0.2090)
2021-12-28 14:02:33,250 ----- Epoch[027/310], Train Loss: 4.7841, Train Acc: 0.2090, time: 1657.12, Best Val(epoch26) Acc@1: 0.5108
2021-12-28 14:02:33,250 Now training epoch 28. LR=0.000979
2021-12-28 14:03:57,524 Epoch[028/310], Step[0000/1251], Loss: 5.0082(5.0082), Acc: 0.0791(0.0791)
2021-12-28 14:05:00,687 Epoch[028/310], Step[0050/1251], Loss: 5.1723(4.7336), Acc: 0.0986(0.2080)
2021-12-28 14:06:02,477 Epoch[028/310], Step[0100/1251], Loss: 5.0460(4.7865), Acc: 0.1533(0.2070)
2021-12-28 14:07:03,716 Epoch[028/310], Step[0150/1251], Loss: 4.9813(4.7591), Acc: 0.1777(0.2062)
2021-12-28 14:08:06,727 Epoch[028/310], Step[0200/1251], Loss: 4.9776(4.7599), Acc: 0.1602(0.2056)
2021-12-28 14:09:10,791 Epoch[028/310], Step[0250/1251], Loss: 4.7951(4.7538), Acc: 0.2051(0.2094)
2021-12-28 14:10:14,450 Epoch[028/310], Step[0300/1251], Loss: 5.1362(4.7581), Acc: 0.2422(0.2096)
2021-12-28 14:11:18,415 Epoch[028/310], Step[0350/1251], Loss: 4.4821(4.7462), Acc: 0.2354(0.2100)
2021-12-28 14:12:21,847 Epoch[028/310], Step[0400/1251], Loss: 4.2199(4.7405), Acc: 0.3789(0.2107)
2021-12-28 14:13:25,055 Epoch[028/310], Step[0450/1251], Loss: 4.1905(4.7403), Acc: 0.2021(0.2104)
2021-12-28 14:14:29,065 Epoch[028/310], Step[0500/1251], Loss: 4.3304(4.7407), Acc: 0.2451(0.2109)
2021-12-28 14:15:31,435 Epoch[028/310], Step[0550/1251], Loss: 4.7823(4.7399), Acc: 0.2207(0.2122)
2021-12-28 14:16:33,649 Epoch[028/310], Step[0600/1251], Loss: 4.9301(4.7417), Acc: 0.1748(0.2127)
2021-12-28 14:17:36,367 Epoch[028/310], Step[0650/1251], Loss: 4.7310(4.7447), Acc: 0.1553(0.2132)
2021-12-28 14:18:38,434 Epoch[028/310], Step[0700/1251], Loss: 4.7279(4.7408), Acc: 0.3193(0.2121)
2021-12-28 14:19:42,541 Epoch[028/310], Step[0750/1251], Loss: 4.9849(4.7411), Acc: 0.1572(0.2118)
2021-12-28 14:20:44,631 Epoch[028/310], Step[0800/1251], Loss: 4.8153(4.7418), Acc: 0.2031(0.2126)
2021-12-28 14:21:47,464 Epoch[028/310], Step[0850/1251], Loss: 4.8472(4.7407), Acc: 0.2285(0.2130)
2021-12-28 14:22:49,802 Epoch[028/310], Step[0900/1251], Loss: 4.6244(4.7393), Acc: 0.1992(0.2129)
2021-12-28 14:23:52,766 Epoch[028/310], Step[0950/1251], Loss: 4.5098(4.7439), Acc: 0.2148(0.2124)
2021-12-28 14:24:55,165 Epoch[028/310], Step[1000/1251], Loss: 4.8338(4.7462), Acc: 0.2979(0.2125)
2021-12-28 14:25:58,004 Epoch[028/310], Step[1050/1251], Loss: 4.9783(4.7459), Acc: 0.1621(0.2128)
2021-12-28 14:26:59,883 Epoch[028/310], Step[1100/1251], Loss: 4.3837(4.7460), Acc: 0.2412(0.2129)
2021-12-28 14:28:00,387 Epoch[028/310], Step[1150/1251], Loss: 4.7035(4.7428), Acc: 0.2764(0.2129)
2021-12-28 14:29:02,296 Epoch[028/310], Step[1200/1251], Loss: 4.5722(4.7441), Acc: 0.3242(0.2129)
2021-12-28 14:30:04,829 Epoch[028/310], Step[1250/1251], Loss: 4.4228(4.7417), Acc: 0.2637(0.2137)
2021-12-28 14:30:06,909 ----- Validation after Epoch: 28
2021-12-28 14:31:10,338 Val Step[0000/1563], Loss: 0.8891 (0.8891), Acc@1: 0.8750 (0.8750), Acc@5: 0.9375 (0.9375)
2021-12-28 14:31:11,936 Val Step[0050/1563], Loss: 3.2121 (1.5109), Acc@1: 0.3438 (0.6924), Acc@5: 0.6250 (0.8750)
2021-12-28 14:31:13,505 Val Step[0100/1563], Loss: 3.0398 (1.9897), Acc@1: 0.2500 (0.5814), Acc@5: 0.7188 (0.8106)
2021-12-28 14:31:15,148 Val Step[0150/1563], Loss: 1.3823 (1.8515), Acc@1: 0.7500 (0.6087), Acc@5: 0.8750 (0.8317)
2021-12-28 14:31:16,742 Val Step[0200/1563], Loss: 1.8197 (1.8813), Acc@1: 0.5312 (0.6042), Acc@5: 0.8438 (0.8274)
2021-12-28 14:31:18,232 Val Step[0250/1563], Loss: 1.8822 (1.8004), Acc@1: 0.5938 (0.6216), Acc@5: 0.8438 (0.8398)
2021-12-28 14:31:19,716 Val Step[0300/1563], Loss: 2.4928 (1.8632), Acc@1: 0.4688 (0.6010), Acc@5: 0.7188 (0.8340)
2021-12-28 14:31:21,200 Val Step[0350/1563], Loss: 2.0189 (1.8603), Acc@1: 0.4375 (0.5946), Acc@5: 0.8438 (0.8376)
2021-12-28 14:31:22,743 Val Step[0400/1563], Loss: 1.9518 (1.8525), Acc@1: 0.5312 (0.5919), Acc@5: 0.8438 (0.8392)
2021-12-28 14:31:24,298 Val Step[0450/1563], Loss: 1.2809 (1.8450), Acc@1: 0.5625 (0.5901), Acc@5: 0.9688 (0.8424)
2021-12-28 14:31:25,889 Val Step[0500/1563], Loss: 1.4422 (1.8371), Acc@1: 0.5312 (0.5913), Acc@5: 0.9688 (0.8436)
2021-12-28 14:31:27,411 Val Step[0550/1563], Loss: 1.3888 (1.8062), Acc@1: 0.7500 (0.5991), Acc@5: 0.9062 (0.8478)
2021-12-28 14:31:29,035 Val Step[0600/1563], Loss: 1.1373 (1.8044), Acc@1: 0.7812 (0.6014), Acc@5: 0.9062 (0.8478)
2021-12-28 14:31:30,538 Val Step[0650/1563], Loss: 1.9604 (1.8306), Acc@1: 0.5625 (0.5983), Acc@5: 0.8438 (0.8437)
2021-12-28 14:31:32,017 Val Step[0700/1563], Loss: 2.3996 (1.8840), Acc@1: 0.5938 (0.5880), Acc@5: 0.7500 (0.8345)
2021-12-28 14:31:33,529 Val Step[0750/1563], Loss: 2.7335 (1.9316), Acc@1: 0.5625 (0.5806), Acc@5: 0.6875 (0.8263)
2021-12-28 14:31:35,144 Val Step[0800/1563], Loss: 2.7487 (1.9843), Acc@1: 0.4688 (0.5690), Acc@5: 0.6562 (0.8180)
2021-12-28 14:31:36,613 Val Step[0850/1563], Loss: 2.2951 (2.0219), Acc@1: 0.5000 (0.5625), Acc@5: 0.7812 (0.8122)
2021-12-28 14:31:38,153 Val Step[0900/1563], Loss: 1.0902 (2.0290), Acc@1: 0.8438 (0.5628), Acc@5: 0.8750 (0.8100)
2021-12-28 14:31:39,720 Val Step[0950/1563], Loss: 2.4864 (2.0621), Acc@1: 0.5938 (0.5571), Acc@5: 0.6875 (0.8041)
2021-12-28 14:31:41,227 Val Step[1000/1563], Loss: 0.8152 (2.0967), Acc@1: 0.8750 (0.5506), Acc@5: 0.9688 (0.7986)
2021-12-28 14:31:42,789 Val Step[1050/1563], Loss: 0.9505 (2.1147), Acc@1: 0.8750 (0.5475), Acc@5: 0.9688 (0.7958)
2021-12-28 14:31:44,340 Val Step[1100/1563], Loss: 1.9369 (2.1391), Acc@1: 0.6250 (0.5440), Acc@5: 0.8125 (0.7910)
2021-12-28 14:31:45,854 Val Step[1150/1563], Loss: 2.4982 (2.1664), Acc@1: 0.5938 (0.5391), Acc@5: 0.7188 (0.7857)
2021-12-28 14:31:47,336 Val Step[1200/1563], Loss: 2.3869 (2.1925), Acc@1: 0.6250 (0.5348), Acc@5: 0.7812 (0.7815)
2021-12-28 14:31:48,831 Val Step[1250/1563], Loss: 1.5194 (2.2138), Acc@1: 0.7500 (0.5324), Acc@5: 0.9062 (0.7777)
2021-12-28 14:31:50,330 Val Step[1300/1563], Loss: 2.1376 (2.2290), Acc@1: 0.6562 (0.5288), Acc@5: 0.7500 (0.7757)
2021-12-28 14:31:51,784 Val Step[1350/1563], Loss: 2.4434 (2.2543), Acc@1: 0.4375 (0.5248), Acc@5: 0.8438 (0.7714)
2021-12-28 14:31:53,251 Val Step[1400/1563], Loss: 2.3106 (2.2654), Acc@1: 0.5312 (0.5224), Acc@5: 0.7812 (0.7695)
2021-12-28 14:31:54,740 Val Step[1450/1563], Loss: 2.6650 (2.2722), Acc@1: 0.3438 (0.5212), Acc@5: 0.8125 (0.7684)
2021-12-28 14:31:56,276 Val Step[1500/1563], Loss: 2.8584 (2.2556), Acc@1: 0.2812 (0.5244), Acc@5: 0.7500 (0.7710)
2021-12-28 14:31:57,735 Val Step[1550/1563], Loss: 1.3343 (2.2471), Acc@1: 0.8125 (0.5266), Acc@5: 0.9062 (0.7723)
2021-12-28 14:31:58,576 ----- Epoch[028/310], Validation Loss: 2.2437, Validation Acc@1: 0.5273, Validation Acc@5: 0.7726, time: 111.66
2021-12-28 14:31:58,576 ----- Epoch[028/310], Train Loss: 4.7417, Train Acc: 0.2137, time: 1653.66, Best Val(epoch28) Acc@1: 0.5273
2021-12-28 14:31:58,799 Max accuracy so far: 0.5273 at epoch_28
2021-12-28 14:31:58,799 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 14:31:58,799 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 14:31:58,878 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 14:31:58,879 Now training epoch 29. LR=0.000977
2021-12-28 14:33:18,062 Epoch[029/310], Step[0000/1251], Loss: 5.0112(5.0112), Acc: 0.1768(0.1768)
2021-12-28 14:34:18,009 Epoch[029/310], Step[0050/1251], Loss: 4.1912(4.6771), Acc: 0.3730(0.2207)
2021-12-28 14:35:17,760 Epoch[029/310], Step[0100/1251], Loss: 5.2376(4.7215), Acc: 0.1104(0.2205)
2021-12-28 14:36:20,316 Epoch[029/310], Step[0150/1251], Loss: 4.9554(4.7427), Acc: 0.1582(0.2147)
2021-12-28 14:37:23,227 Epoch[029/310], Step[0200/1251], Loss: 5.0079(4.7523), Acc: 0.2539(0.2144)
2021-12-28 14:38:26,446 Epoch[029/310], Step[0250/1251], Loss: 4.9155(4.7539), Acc: 0.2236(0.2129)
2021-12-28 14:39:29,475 Epoch[029/310], Step[0300/1251], Loss: 4.7581(4.7388), Acc: 0.1865(0.2139)
2021-12-28 14:40:30,920 Epoch[029/310], Step[0350/1251], Loss: 4.4518(4.7380), Acc: 0.1719(0.2116)
2021-12-28 14:41:34,469 Epoch[029/310], Step[0400/1251], Loss: 4.6716(4.7388), Acc: 0.1426(0.2108)
2021-12-28 14:42:38,090 Epoch[029/310], Step[0450/1251], Loss: 4.4044(4.7342), Acc: 0.2119(0.2101)
2021-12-28 14:43:40,985 Epoch[029/310], Step[0500/1251], Loss: 4.6390(4.7313), Acc: 0.2656(0.2099)
2021-12-28 14:44:44,863 Epoch[029/310], Step[0550/1251], Loss: 4.5603(4.7296), Acc: 0.2158(0.2096)
2021-12-28 14:45:47,902 Epoch[029/310], Step[0600/1251], Loss: 4.0399(4.7259), Acc: 0.2617(0.2106)
2021-12-28 14:46:50,387 Epoch[029/310], Step[0650/1251], Loss: 4.7821(4.7199), Acc: 0.2646(0.2117)
2021-12-28 14:47:54,236 Epoch[029/310], Step[0700/1251], Loss: 4.5362(4.7199), Acc: 0.3682(0.2122)
2021-12-28 14:48:56,789 Epoch[029/310], Step[0750/1251], Loss: 4.3527(4.7153), Acc: 0.2979(0.2120)
2021-12-28 14:49:59,247 Epoch[029/310], Step[0800/1251], Loss: 5.0040(4.7173), Acc: 0.2861(0.2116)
2021-12-28 14:51:01,896 Epoch[029/310], Step[0850/1251], Loss: 4.6421(4.7178), Acc: 0.1729(0.2124)
2021-12-28 14:52:04,467 Epoch[029/310], Step[0900/1251], Loss: 4.6873(4.7126), Acc: 0.2012(0.2132)
2021-12-28 14:53:07,707 Epoch[029/310], Step[0950/1251], Loss: 4.3605(4.7133), Acc: 0.3506(0.2133)
2021-12-28 14:54:10,819 Epoch[029/310], Step[1000/1251], Loss: 4.8615(4.7102), Acc: 0.1572(0.2142)
2021-12-28 14:55:13,772 Epoch[029/310], Step[1050/1251], Loss: 4.2984(4.7109), Acc: 0.1426(0.2144)
2021-12-28 14:56:16,610 Epoch[029/310], Step[1100/1251], Loss: 5.2494(4.7109), Acc: 0.1611(0.2143)
2021-12-28 14:57:19,808 Epoch[029/310], Step[1150/1251], Loss: 4.9373(4.7090), Acc: 0.1826(0.2142)
2021-12-28 14:58:21,556 Epoch[029/310], Step[1200/1251], Loss: 5.0688(4.7107), Acc: 0.1465(0.2143)
2021-12-28 14:59:24,257 Epoch[029/310], Step[1250/1251], Loss: 4.8185(4.7082), Acc: 0.2705(0.2148)
2021-12-28 14:59:26,679 ----- Epoch[029/310], Train Loss: 4.7082, Train Acc: 0.2148, time: 1647.80, Best Val(epoch28) Acc@1: 0.5273
2021-12-28 14:59:26,679 Now training epoch 30. LR=0.000976
2021-12-28 15:00:46,179 Epoch[030/310], Step[0000/1251], Loss: 4.4758(4.4758), Acc: 0.2051(0.2051)
2021-12-28 15:01:48,803 Epoch[030/310], Step[0050/1251], Loss: 4.1806(4.6833), Acc: 0.2598(0.2130)
2021-12-28 15:02:50,546 Epoch[030/310], Step[0100/1251], Loss: 5.3881(4.6820), Acc: 0.1240(0.2172)
2021-12-28 15:03:52,272 Epoch[030/310], Step[0150/1251], Loss: 4.6399(4.6934), Acc: 0.1719(0.2153)
2021-12-28 15:04:54,525 Epoch[030/310], Step[0200/1251], Loss: 4.5878(4.6772), Acc: 0.1631(0.2149)
2021-12-28 15:05:56,859 Epoch[030/310], Step[0250/1251], Loss: 4.9499(4.6889), Acc: 0.1182(0.2173)
2021-12-28 15:06:59,398 Epoch[030/310], Step[0300/1251], Loss: 4.7854(4.6896), Acc: 0.3037(0.2185)
2021-12-28 15:08:02,001 Epoch[030/310], Step[0350/1251], Loss: 4.3437(4.6852), Acc: 0.0781(0.2191)
2021-12-28 15:09:04,493 Epoch[030/310], Step[0400/1251], Loss: 4.7620(4.6878), Acc: 0.1416(0.2206)
2021-12-28 15:10:07,265 Epoch[030/310], Step[0450/1251], Loss: 4.0000(4.6845), Acc: 0.2754(0.2202)
2021-12-28 15:11:09,309 Epoch[030/310], Step[0500/1251], Loss: 4.4402(4.6888), Acc: 0.2686(0.2206)
2021-12-28 15:12:11,235 Epoch[030/310], Step[0550/1251], Loss: 4.1407(4.6874), Acc: 0.2939(0.2197)
2021-12-28 15:13:14,428 Epoch[030/310], Step[0600/1251], Loss: 4.6229(4.6854), Acc: 0.2715(0.2188)
2021-12-28 15:14:17,144 Epoch[030/310], Step[0650/1251], Loss: 4.7167(4.6861), Acc: 0.1738(0.2182)
2021-12-28 15:15:20,391 Epoch[030/310], Step[0700/1251], Loss: 5.0188(4.6900), Acc: 0.3096(0.2183)
2021-12-28 15:16:23,839 Epoch[030/310], Step[0750/1251], Loss: 4.9010(4.6925), Acc: 0.1982(0.2177)
2021-12-28 15:17:26,871 Epoch[030/310], Step[0800/1251], Loss: 4.6175(4.6925), Acc: 0.2598(0.2170)
2021-12-28 15:18:29,423 Epoch[030/310], Step[0850/1251], Loss: 4.8430(4.6934), Acc: 0.1543(0.2172)
2021-12-28 15:19:31,041 Epoch[030/310], Step[0900/1251], Loss: 4.8501(4.6950), Acc: 0.1660(0.2172)
2021-12-28 15:20:32,241 Epoch[030/310], Step[0950/1251], Loss: 4.3768(4.6900), Acc: 0.3467(0.2182)
2021-12-28 15:21:33,516 Epoch[030/310], Step[1000/1251], Loss: 5.2658(4.6912), Acc: 0.1387(0.2182)
2021-12-28 15:22:36,794 Epoch[030/310], Step[1050/1251], Loss: 4.4866(4.6885), Acc: 0.3389(0.2191)
2021-12-28 15:23:39,525 Epoch[030/310], Step[1100/1251], Loss: 5.1983(4.6869), Acc: 0.2314(0.2189)
2021-12-28 15:24:42,128 Epoch[030/310], Step[1150/1251], Loss: 4.3946(4.6890), Acc: 0.2529(0.2185)
2021-12-28 15:25:44,622 Epoch[030/310], Step[1200/1251], Loss: 5.0890(4.6912), Acc: 0.2256(0.2185)
2021-12-28 15:26:48,015 Epoch[030/310], Step[1250/1251], Loss: 4.7674(4.6882), Acc: 0.2568(0.2193)
2021-12-28 15:26:50,049 ----- Validation after Epoch: 30
2021-12-28 15:27:59,406 Val Step[0000/1563], Loss: 0.9843 (0.9843), Acc@1: 0.8750 (0.8750), Acc@5: 0.9375 (0.9375)
2021-12-28 15:28:01,044 Val Step[0050/1563], Loss: 3.2286 (1.4382), Acc@1: 0.3438 (0.7089), Acc@5: 0.5938 (0.8787)
2021-12-28 15:28:02,597 Val Step[0100/1563], Loss: 2.2914 (1.8617), Acc@1: 0.4688 (0.5968), Acc@5: 0.8125 (0.8224)
2021-12-28 15:28:04,183 Val Step[0150/1563], Loss: 1.0879 (1.7250), Acc@1: 0.7812 (0.6267), Acc@5: 0.9062 (0.8425)
2021-12-28 15:28:05,690 Val Step[0200/1563], Loss: 2.2205 (1.7504), Acc@1: 0.3750 (0.6266), Acc@5: 0.7812 (0.8394)
2021-12-28 15:28:07,162 Val Step[0250/1563], Loss: 1.5495 (1.6801), Acc@1: 0.6250 (0.6421), Acc@5: 0.9375 (0.8521)
2021-12-28 15:28:08,660 Val Step[0300/1563], Loss: 2.2742 (1.7483), Acc@1: 0.4375 (0.6194), Acc@5: 0.7188 (0.8450)
2021-12-28 15:28:10,145 Val Step[0350/1563], Loss: 2.2252 (1.7536), Acc@1: 0.3750 (0.6128), Acc@5: 0.8125 (0.8494)
2021-12-28 15:28:11,582 Val Step[0400/1563], Loss: 1.8638 (1.7476), Acc@1: 0.5312 (0.6097), Acc@5: 0.8750 (0.8524)
2021-12-28 15:28:13,102 Val Step[0450/1563], Loss: 1.3787 (1.7344), Acc@1: 0.5938 (0.6088), Acc@5: 0.9375 (0.8562)
2021-12-28 15:28:14,660 Val Step[0500/1563], Loss: 0.7279 (1.7268), Acc@1: 0.7812 (0.6104), Acc@5: 0.9688 (0.8563)
2021-12-28 15:28:16,263 Val Step[0550/1563], Loss: 1.2416 (1.6875), Acc@1: 0.7188 (0.6201), Acc@5: 0.9375 (0.8607)
2021-12-28 15:28:17,869 Val Step[0600/1563], Loss: 1.2317 (1.6885), Acc@1: 0.7188 (0.6211), Acc@5: 0.9062 (0.8605)
2021-12-28 15:28:19,313 Val Step[0650/1563], Loss: 1.2179 (1.7213), Acc@1: 0.7812 (0.6148), Acc@5: 0.9375 (0.8556)
2021-12-28 15:28:20,857 Val Step[0700/1563], Loss: 2.1753 (1.7734), Acc@1: 0.5938 (0.6052), Acc@5: 0.7500 (0.8470)
2021-12-28 15:28:22,382 Val Step[0750/1563], Loss: 2.8170 (1.8249), Acc@1: 0.4375 (0.5952), Acc@5: 0.6562 (0.8385)
2021-12-28 15:28:23,862 Val Step[0800/1563], Loss: 2.5122 (1.8743), Acc@1: 0.5938 (0.5854), Acc@5: 0.7500 (0.8301)
2021-12-28 15:28:25,358 Val Step[0850/1563], Loss: 2.4080 (1.9115), Acc@1: 0.5000 (0.5793), Acc@5: 0.7188 (0.8242)
2021-12-28 15:28:26,955 Val Step[0900/1563], Loss: 0.7589 (1.9165), Acc@1: 0.8438 (0.5805), Acc@5: 0.9375 (0.8229)
2021-12-28 15:28:28,553 Val Step[0950/1563], Loss: 2.4786 (1.9511), Acc@1: 0.5000 (0.5745), Acc@5: 0.6875 (0.8174)
2021-12-28 15:28:30,111 Val Step[1000/1563], Loss: 0.9477 (1.9847), Acc@1: 0.8438 (0.5683), Acc@5: 0.9375 (0.8122)
2021-12-28 15:28:31,657 Val Step[1050/1563], Loss: 1.0000 (2.0028), Acc@1: 0.8438 (0.5647), Acc@5: 0.9688 (0.8097)
2021-12-28 15:28:33,241 Val Step[1100/1563], Loss: 2.4151 (2.0271), Acc@1: 0.5625 (0.5596), Acc@5: 0.6875 (0.8049)
2021-12-28 15:28:34,794 Val Step[1150/1563], Loss: 2.1729 (2.0534), Acc@1: 0.6250 (0.5550), Acc@5: 0.7500 (0.8000)
2021-12-28 15:28:36,250 Val Step[1200/1563], Loss: 2.1619 (2.0748), Acc@1: 0.6250 (0.5512), Acc@5: 0.7812 (0.7963)
2021-12-28 15:28:37,742 Val Step[1250/1563], Loss: 2.1450 (2.1017), Acc@1: 0.6562 (0.5471), Acc@5: 0.8438 (0.7917)
2021-12-28 15:28:39,271 Val Step[1300/1563], Loss: 1.9046 (2.1178), Acc@1: 0.6875 (0.5437), Acc@5: 0.8125 (0.7889)
2021-12-28 15:28:40,707 Val Step[1350/1563], Loss: 2.4818 (2.1441), Acc@1: 0.4062 (0.5388), Acc@5: 0.8438 (0.7844)
2021-12-28 15:28:42,258 Val Step[1400/1563], Loss: 1.9699 (2.1547), Acc@1: 0.4688 (0.5367), Acc@5: 0.8125 (0.7828)
2021-12-28 15:28:43,658 Val Step[1450/1563], Loss: 2.8321 (2.1612), Acc@1: 0.3438 (0.5355), Acc@5: 0.7500 (0.7821)
2021-12-28 15:28:45,169 Val Step[1500/1563], Loss: 2.6071 (2.1450), Acc@1: 0.4688 (0.5385), Acc@5: 0.7812 (0.7847)
2021-12-28 15:28:46,681 Val Step[1550/1563], Loss: 1.1917 (2.1356), Acc@1: 0.8438 (0.5402), Acc@5: 0.9062 (0.7860)
2021-12-28 15:28:47,569 ----- Epoch[030/310], Validation Loss: 2.1324, Validation Acc@1: 0.5410, Validation Acc@5: 0.7862, time: 117.52
2021-12-28 15:28:47,569 ----- Epoch[030/310], Train Loss: 4.6882, Train Acc: 0.2193, time: 1643.37, Best Val(epoch30) Acc@1: 0.5410
2021-12-28 15:28:47,824 Max accuracy so far: 0.5410 at epoch_30
2021-12-28 15:28:47,825 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 15:28:47,825 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 15:28:47,906 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 15:28:48,076 ----- Save model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-30-Loss-4.680917646482789.pdparams
2021-12-28 15:28:48,076 ----- Save optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-30-Loss-4.680917646482789.pdopt
2021-12-28 15:28:48,143 ----- Save ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-30-Loss-4.680917646482789-EMA.pdparams
2021-12-28 15:28:48,144 Now training epoch 31. LR=0.000974
2021-12-28 15:30:17,197 Epoch[031/310], Step[0000/1251], Loss: 4.7504(4.7504), Acc: 0.2510(0.2510)
2021-12-28 15:31:19,661 Epoch[031/310], Step[0050/1251], Loss: 4.5308(4.5941), Acc: 0.1885(0.2232)
2021-12-28 15:32:21,872 Epoch[031/310], Step[0100/1251], Loss: 4.4221(4.6380), Acc: 0.2676(0.2233)
2021-12-28 15:33:24,287 Epoch[031/310], Step[0150/1251], Loss: 4.2161(4.6554), Acc: 0.1621(0.2272)
2021-12-28 15:34:27,377 Epoch[031/310], Step[0200/1251], Loss: 4.1505(4.6542), Acc: 0.2764(0.2309)
2021-12-28 15:35:29,057 Epoch[031/310], Step[0250/1251], Loss: 4.4997(4.6516), Acc: 0.2266(0.2277)
2021-12-28 15:36:32,728 Epoch[031/310], Step[0300/1251], Loss: 4.2279(4.6521), Acc: 0.2793(0.2284)
2021-12-28 15:37:35,848 Epoch[031/310], Step[0350/1251], Loss: 4.4472(4.6549), Acc: 0.2832(0.2292)
2021-12-28 15:38:37,514 Epoch[031/310], Step[0400/1251], Loss: 4.4200(4.6435), Acc: 0.2617(0.2304)
2021-12-28 15:39:40,073 Epoch[031/310], Step[0450/1251], Loss: 4.5655(4.6394), Acc: 0.2295(0.2297)
2021-12-28 15:40:43,028 Epoch[031/310], Step[0500/1251], Loss: 4.6200(4.6301), Acc: 0.2676(0.2299)
2021-12-28 15:41:45,827 Epoch[031/310], Step[0550/1251], Loss: 4.3593(4.6275), Acc: 0.3076(0.2306)
2021-12-28 15:42:48,950 Epoch[031/310], Step[0600/1251], Loss: 4.2172(4.6359), Acc: 0.3574(0.2296)
2021-12-28 15:43:51,871 Epoch[031/310], Step[0650/1251], Loss: 4.4910(4.6377), Acc: 0.2051(0.2298)
2021-12-28 15:44:54,092 Epoch[031/310], Step[0700/1251], Loss: 4.6423(4.6459), Acc: 0.2539(0.2293)
2021-12-28 15:45:56,422 Epoch[031/310], Step[0750/1251], Loss: 4.5289(4.6564), Acc: 0.3584(0.2291)
2021-12-28 15:46:58,463 Epoch[031/310], Step[0800/1251], Loss: 4.7616(4.6575), Acc: 0.1572(0.2290)
2021-12-28 15:47:59,878 Epoch[031/310], Step[0850/1251], Loss: 4.8902(4.6568), Acc: 0.2588(0.2289)
2021-12-28 15:49:01,432 Epoch[031/310], Step[0900/1251], Loss: 4.7116(4.6560), Acc: 0.1768(0.2298)
2021-12-28 15:50:02,354 Epoch[031/310], Step[0950/1251], Loss: 4.7285(4.6592), Acc: 0.3008(0.2306)
2021-12-28 15:51:04,815 Epoch[031/310], Step[1000/1251], Loss: 4.5303(4.6586), Acc: 0.3486(0.2314)
2021-12-28 15:52:08,505 Epoch[031/310], Step[1050/1251], Loss: 4.7972(4.6561), Acc: 0.1768(0.2304)
2021-12-28 15:53:12,371 Epoch[031/310], Step[1100/1251], Loss: 4.9183(4.6567), Acc: 0.2158(0.2301)
2021-12-28 15:54:16,448 Epoch[031/310], Step[1150/1251], Loss: 4.4516(4.6540), Acc: 0.3662(0.2303)
2021-12-28 15:55:18,895 Epoch[031/310], Step[1200/1251], Loss: 5.0091(4.6556), Acc: 0.2148(0.2306)
2021-12-28 15:56:22,488 Epoch[031/310], Step[1250/1251], Loss: 4.8287(4.6553), Acc: 0.1816(0.2306)
2021-12-28 15:56:24,844 ----- Epoch[031/310], Train Loss: 4.6553, Train Acc: 0.2306, time: 1656.70, Best Val(epoch30) Acc@1: 0.5410
2021-12-28 15:56:24,844 Now training epoch 32. LR=0.000972
2021-12-28 15:57:52,144 Epoch[032/310], Step[0000/1251], Loss: 3.5499(3.5499), Acc: 0.2383(0.2383)
2021-12-28 15:58:55,597 Epoch[032/310], Step[0050/1251], Loss: 4.3120(4.5288), Acc: 0.1709(0.2342)
2021-12-28 15:59:57,976 Epoch[032/310], Step[0100/1251], Loss: 5.2575(4.5972), Acc: 0.2217(0.2342)
2021-12-28 16:00:59,158 Epoch[032/310], Step[0150/1251], Loss: 4.2109(4.6016), Acc: 0.1924(0.2317)
2021-12-28 16:02:01,010 Epoch[032/310], Step[0200/1251], Loss: 4.9870(4.6249), Acc: 0.2490(0.2291)
2021-12-28 16:03:02,720 Epoch[032/310], Step[0250/1251], Loss: 4.8990(4.6263), Acc: 0.1025(0.2305)
2021-12-28 16:04:05,788 Epoch[032/310], Step[0300/1251], Loss: 4.0606(4.6283), Acc: 0.3896(0.2302)
2021-12-28 16:05:09,657 Epoch[032/310], Step[0350/1251], Loss: 4.4867(4.6325), Acc: 0.2656(0.2291)
2021-12-28 16:06:11,996 Epoch[032/310], Step[0400/1251], Loss: 5.0229(4.6452), Acc: 0.1553(0.2286)
2021-12-28 16:07:15,662 Epoch[032/310], Step[0450/1251], Loss: 5.2084(4.6456), Acc: 0.1426(0.2290)
2021-12-28 16:08:18,080 Epoch[032/310], Step[0500/1251], Loss: 4.3260(4.6485), Acc: 0.2295(0.2304)
2021-12-28 16:09:20,562 Epoch[032/310], Step[0550/1251], Loss: 3.9657(4.6465), Acc: 0.0049(0.2295)
2021-12-28 16:10:24,472 Epoch[032/310], Step[0600/1251], Loss: 4.3841(4.6531), Acc: 0.1621(0.2290)
2021-12-28 16:11:28,480 Epoch[032/310], Step[0650/1251], Loss: 4.3170(4.6501), Acc: 0.3447(0.2287)
2021-12-28 16:12:31,298 Epoch[032/310], Step[0700/1251], Loss: 5.0435(4.6535), Acc: 0.0967(0.2290)
2021-12-28 16:13:35,001 Epoch[032/310], Step[0750/1251], Loss: 4.4487(4.6584), Acc: 0.3418(0.2276)
2021-12-28 16:14:35,698 Epoch[032/310], Step[0800/1251], Loss: 4.4305(4.6579), Acc: 0.3623(0.2285)
2021-12-28 16:15:38,223 Epoch[032/310], Step[0850/1251], Loss: 4.1691(4.6610), Acc: 0.2783(0.2286)
2021-12-28 16:16:40,239 Epoch[032/310], Step[0900/1251], Loss: 4.7435(4.6613), Acc: 0.2178(0.2291)
2021-12-28 16:17:42,059 Epoch[032/310], Step[0950/1251], Loss: 4.5055(4.6584), Acc: 0.2334(0.2299)
2021-12-28 16:18:46,162 Epoch[032/310], Step[1000/1251], Loss: 4.2797(4.6577), Acc: 0.3096(0.2295)
2021-12-28 16:19:47,665 Epoch[032/310], Step[1050/1251], Loss: 4.8019(4.6575), Acc: 0.1162(0.2301)
2021-12-28 16:20:50,544 Epoch[032/310], Step[1100/1251], Loss: 4.7691(4.6530), Acc: 0.2979(0.2307)
2021-12-28 16:21:53,001 Epoch[032/310], Step[1150/1251], Loss: 4.3320(4.6513), Acc: 0.3320(0.2307)
2021-12-28 16:22:56,131 Epoch[032/310], Step[1200/1251], Loss: 5.0126(4.6523), Acc: 0.1875(0.2310)
2021-12-28 16:23:57,705 Epoch[032/310], Step[1250/1251], Loss: 4.4705(4.6487), Acc: 0.3682(0.2314)
2021-12-28 16:24:00,382 ----- Validation after Epoch: 32
2021-12-28 16:25:07,307 Val Step[0000/1563], Loss: 1.0611 (1.0611), Acc@1: 0.8438 (0.8438), Acc@5: 0.9375 (0.9375)
2021-12-28 16:25:08,972 Val Step[0050/1563], Loss: 3.4530 (1.3919), Acc@1: 0.2812 (0.7132), Acc@5: 0.5625 (0.8934)
2021-12-28 16:25:10,641 Val Step[0100/1563], Loss: 2.2035 (1.8087), Acc@1: 0.3750 (0.6086), Acc@5: 0.8438 (0.8388)
2021-12-28 16:25:12,284 Val Step[0150/1563], Loss: 0.9502 (1.6710), Acc@1: 0.8125 (0.6405), Acc@5: 0.9375 (0.8568)
2021-12-28 16:25:13,926 Val Step[0200/1563], Loss: 1.5709 (1.7085), Acc@1: 0.7188 (0.6374), Acc@5: 0.8750 (0.8518)
2021-12-28 16:25:15,492 Val Step[0250/1563], Loss: 1.3316 (1.6367), Acc@1: 0.7188 (0.6515), Acc@5: 0.9062 (0.8616)
2021-12-28 16:25:17,043 Val Step[0300/1563], Loss: 1.8394 (1.7033), Acc@1: 0.6250 (0.6282), Acc@5: 0.8125 (0.8531)
2021-12-28 16:25:18,613 Val Step[0350/1563], Loss: 2.0442 (1.7024), Acc@1: 0.4375 (0.6222), Acc@5: 0.8438 (0.8567)
2021-12-28 16:25:20,168 Val Step[0400/1563], Loss: 1.5282 (1.6920), Acc@1: 0.5938 (0.6208), Acc@5: 0.9688 (0.8610)
2021-12-28 16:25:21,773 Val Step[0450/1563], Loss: 1.3457 (1.6941), Acc@1: 0.5938 (0.6186), Acc@5: 0.9688 (0.8611)
2021-12-28 16:25:23,406 Val Step[0500/1563], Loss: 0.7252 (1.6952), Acc@1: 0.8750 (0.6191), Acc@5: 0.9688 (0.8616)
2021-12-28 16:25:24,988 Val Step[0550/1563], Loss: 1.2840 (1.6644), Acc@1: 0.7188 (0.6274), Acc@5: 0.9375 (0.8655)
2021-12-28 16:25:26,730 Val Step[0600/1563], Loss: 1.7054 (1.6647), Acc@1: 0.5938 (0.6285), Acc@5: 0.9062 (0.8653)
2021-12-28 16:25:28,232 Val Step[0650/1563], Loss: 1.4268 (1.6936), Acc@1: 0.8125 (0.6235), Acc@5: 0.9062 (0.8608)
2021-12-28 16:25:29,847 Val Step[0700/1563], Loss: 2.7463 (1.7458), Acc@1: 0.4688 (0.6133), Acc@5: 0.7188 (0.8520)
2021-12-28 16:25:31,424 Val Step[0750/1563], Loss: 2.3646 (1.7936), Acc@1: 0.5938 (0.6036), Acc@5: 0.7500 (0.8440)
2021-12-28 16:25:32,888 Val Step[0800/1563], Loss: 2.0782 (1.8445), Acc@1: 0.6250 (0.5942), Acc@5: 0.8750 (0.8359)
2021-12-28 16:25:34,389 Val Step[0850/1563], Loss: 2.2681 (1.8791), Acc@1: 0.4688 (0.5877), Acc@5: 0.7500 (0.8308)
2021-12-28 16:25:36,077 Val Step[0900/1563], Loss: 0.7672 (1.8850), Acc@1: 0.8750 (0.5887), Acc@5: 0.9375 (0.8291)
2021-12-28 16:25:37,692 Val Step[0950/1563], Loss: 2.1860 (1.9166), Acc@1: 0.5312 (0.5831), Acc@5: 0.7812 (0.8239)
2021-12-28 16:25:39,189 Val Step[1000/1563], Loss: 0.9920 (1.9456), Acc@1: 0.8438 (0.5778), Acc@5: 0.9375 (0.8197)
2021-12-28 16:25:40,949 Val Step[1050/1563], Loss: 0.7727 (1.9620), Acc@1: 0.8438 (0.5751), Acc@5: 0.9688 (0.8176)
2021-12-28 16:25:42,437 Val Step[1100/1563], Loss: 1.7928 (1.9886), Acc@1: 0.6250 (0.5702), Acc@5: 0.8125 (0.8124)
2021-12-28 16:25:43,924 Val Step[1150/1563], Loss: 1.7679 (2.0129), Acc@1: 0.6875 (0.5654), Acc@5: 0.7500 (0.8088)
2021-12-28 16:25:45,399 Val Step[1200/1563], Loss: 2.2843 (2.0342), Acc@1: 0.6250 (0.5624), Acc@5: 0.7812 (0.8049)
2021-12-28 16:25:46,977 Val Step[1250/1563], Loss: 1.3091 (2.0569), Acc@1: 0.7500 (0.5594), Acc@5: 0.9062 (0.8010)
2021-12-28 16:25:48,436 Val Step[1300/1563], Loss: 2.1438 (2.0691), Acc@1: 0.5312 (0.5563), Acc@5: 0.7812 (0.7992)
2021-12-28 16:25:49,847 Val Step[1350/1563], Loss: 2.6559 (2.0922), Acc@1: 0.2500 (0.5510), Acc@5: 0.7812 (0.7953)
2021-12-28 16:25:51,276 Val Step[1400/1563], Loss: 1.5715 (2.1036), Acc@1: 0.7188 (0.5486), Acc@5: 0.8750 (0.7932)
2021-12-28 16:25:52,741 Val Step[1450/1563], Loss: 2.5545 (2.1114), Acc@1: 0.3438 (0.5469), Acc@5: 0.8125 (0.7922)
2021-12-28 16:25:54,219 Val Step[1500/1563], Loss: 2.8343 (2.0976), Acc@1: 0.3750 (0.5494), Acc@5: 0.6875 (0.7944)
2021-12-28 16:25:55,675 Val Step[1550/1563], Loss: 1.1725 (2.0913), Acc@1: 0.8750 (0.5505), Acc@5: 0.9062 (0.7951)
2021-12-28 16:25:56,568 ----- Epoch[032/310], Validation Loss: 2.0886, Validation Acc@1: 0.5513, Validation Acc@5: 0.7953, time: 116.18
2021-12-28 16:25:56,568 ----- Epoch[032/310], Train Loss: 4.6487, Train Acc: 0.2314, time: 1655.53, Best Val(epoch32) Acc@1: 0.5513
2021-12-28 16:25:56,768 Max accuracy so far: 0.5513 at epoch_32
2021-12-28 16:25:56,769 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 16:25:56,769 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 16:25:56,862 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 16:25:56,862 Now training epoch 33. LR=0.000971
2021-12-28 16:27:15,941 Epoch[033/310], Step[0000/1251], Loss: 4.5970(4.5970), Acc: 0.3057(0.3057)
2021-12-28 16:28:18,765 Epoch[033/310], Step[0050/1251], Loss: 4.8405(4.5722), Acc: 0.1465(0.2530)
2021-12-28 16:29:20,862 Epoch[033/310], Step[0100/1251], Loss: 4.6069(4.5982), Acc: 0.3369(0.2451)
2021-12-28 16:30:22,803 Epoch[033/310], Step[0150/1251], Loss: 4.0154(4.6151), Acc: 0.2783(0.2410)
2021-12-28 16:31:24,980 Epoch[033/310], Step[0200/1251], Loss: 4.6524(4.6227), Acc: 0.1709(0.2355)
2021-12-28 16:32:27,101 Epoch[033/310], Step[0250/1251], Loss: 4.7737(4.6156), Acc: 0.1318(0.2321)
2021-12-28 16:33:27,766 Epoch[033/310], Step[0300/1251], Loss: 4.3097(4.6215), Acc: 0.2891(0.2341)
2021-12-28 16:34:30,879 Epoch[033/310], Step[0350/1251], Loss: 4.8875(4.6258), Acc: 0.2949(0.2336)
2021-12-28 16:35:33,564 Epoch[033/310], Step[0400/1251], Loss: 3.8917(4.6219), Acc: 0.1064(0.2333)
2021-12-28 16:36:35,539 Epoch[033/310], Step[0450/1251], Loss: 5.0112(4.6308), Acc: 0.2002(0.2318)
2021-12-28 16:37:38,370 Epoch[033/310], Step[0500/1251], Loss: 4.3085(4.6378), Acc: 0.2715(0.2313)
2021-12-28 16:38:40,614 Epoch[033/310], Step[0550/1251], Loss: 4.5871(4.6334), Acc: 0.1963(0.2324)
2021-12-28 16:39:42,943 Epoch[033/310], Step[0600/1251], Loss: 4.2710(4.6308), Acc: 0.2334(0.2348)
2021-12-28 16:40:45,911 Epoch[033/310], Step[0650/1251], Loss: 4.2754(4.6318), Acc: 0.0586(0.2338)
2021-12-28 16:41:50,207 Epoch[033/310], Step[0700/1251], Loss: 4.6774(4.6331), Acc: 0.2236(0.2345)
2021-12-28 16:42:53,605 Epoch[033/310], Step[0750/1251], Loss: 4.6304(4.6264), Acc: 0.3516(0.2349)
2021-12-28 16:43:56,695 Epoch[033/310], Step[0800/1251], Loss: 4.4445(4.6249), Acc: 0.1201(0.2338)
2021-12-28 16:44:59,430 Epoch[033/310], Step[0850/1251], Loss: 5.0177(4.6247), Acc: 0.1309(0.2332)
2021-12-28 16:46:03,792 Epoch[033/310], Step[0900/1251], Loss: 4.7594(4.6269), Acc: 0.1992(0.2331)
2021-12-28 16:47:06,509 Epoch[033/310], Step[0950/1251], Loss: 4.7644(4.6256), Acc: 0.2412(0.2333)
2021-12-28 16:48:09,167 Epoch[033/310], Step[1000/1251], Loss: 4.5260(4.6258), Acc: 0.3369(0.2328)
2021-12-28 16:49:12,134 Epoch[033/310], Step[1050/1251], Loss: 4.6795(4.6268), Acc: 0.2666(0.2328)
2021-12-28 16:50:14,865 Epoch[033/310], Step[1100/1251], Loss: 4.7538(4.6273), Acc: 0.2705(0.2326)
2021-12-28 16:51:16,933 Epoch[033/310], Step[1150/1251], Loss: 4.9424(4.6262), Acc: 0.1777(0.2329)
2021-12-28 16:52:18,210 Epoch[033/310], Step[1200/1251], Loss: 5.0134(4.6287), Acc: 0.2695(0.2326)
2021-12-28 16:53:20,404 Epoch[033/310], Step[1250/1251], Loss: 4.4424(4.6300), Acc: 0.3672(0.2326)
2021-12-28 16:53:22,619 ----- Epoch[033/310], Train Loss: 4.6300, Train Acc: 0.2326, time: 1645.75, Best Val(epoch32) Acc@1: 0.5513
2021-12-28 16:53:22,619 Now training epoch 34. LR=0.000969
2021-12-28 16:54:38,836 Epoch[034/310], Step[0000/1251], Loss: 5.0943(5.0943), Acc: 0.1035(0.1035)
2021-12-28 16:55:41,285 Epoch[034/310], Step[0050/1251], Loss: 4.6567(4.6289), Acc: 0.2129(0.2351)
2021-12-28 16:56:44,073 Epoch[034/310], Step[0100/1251], Loss: 5.0730(4.5909), Acc: 0.3047(0.2320)
2021-12-28 16:57:46,534 Epoch[034/310], Step[0150/1251], Loss: 4.9527(4.5840), Acc: 0.2734(0.2348)
2021-12-28 16:58:48,147 Epoch[034/310], Step[0200/1251], Loss: 4.7444(4.5875), Acc: 0.2383(0.2363)
2021-12-28 16:59:49,626 Epoch[034/310], Step[0250/1251], Loss: 4.7155(4.5767), Acc: 0.2930(0.2389)
2021-12-28 17:00:50,524 Epoch[034/310], Step[0300/1251], Loss: 4.5519(4.5756), Acc: 0.1475(0.2399)
2021-12-28 17:01:53,906 Epoch[034/310], Step[0350/1251], Loss: 4.6646(4.5739), Acc: 0.1377(0.2407)
2021-12-28 17:02:54,922 Epoch[034/310], Step[0400/1251], Loss: 4.2906(4.5717), Acc: 0.1348(0.2408)
2021-12-28 17:03:57,636 Epoch[034/310], Step[0450/1251], Loss: 4.4282(4.5699), Acc: 0.1846(0.2400)
2021-12-28 17:05:00,920 Epoch[034/310], Step[0500/1251], Loss: 4.9244(4.5729), Acc: 0.2002(0.2391)
2021-12-28 17:06:05,228 Epoch[034/310], Step[0550/1251], Loss: 4.9354(4.5766), Acc: 0.1826(0.2390)
2021-12-28 17:07:07,422 Epoch[034/310], Step[0600/1251], Loss: 4.4608(4.5801), Acc: 0.2324(0.2385)
2021-12-28 17:08:11,326 Epoch[034/310], Step[0650/1251], Loss: 3.9525(4.5813), Acc: 0.0039(0.2371)
2021-12-28 17:09:14,314 Epoch[034/310], Step[0700/1251], Loss: 4.7190(4.5897), Acc: 0.2285(0.2364)
2021-12-28 17:10:17,165 Epoch[034/310], Step[0750/1251], Loss: 4.4540(4.5921), Acc: 0.0146(0.2363)
2021-12-28 17:11:20,381 Epoch[034/310], Step[0800/1251], Loss: 4.6660(4.5909), Acc: 0.2305(0.2366)
2021-12-28 17:12:23,313 Epoch[034/310], Step[0850/1251], Loss: 4.5986(4.5929), Acc: 0.3125(0.2367)
2021-12-28 17:13:25,775 Epoch[034/310], Step[0900/1251], Loss: 4.7461(4.5905), Acc: 0.3350(0.2379)
2021-12-28 17:14:29,184 Epoch[034/310], Step[0950/1251], Loss: 5.0634(4.5898), Acc: 0.2891(0.2375)
2021-12-28 17:15:30,364 Epoch[034/310], Step[1000/1251], Loss: 4.7795(4.5883), Acc: 0.2764(0.2376)
2021-12-28 17:16:32,645 Epoch[034/310], Step[1050/1251], Loss: 4.8232(4.5882), Acc: 0.2979(0.2377)
2021-12-28 17:17:34,312 Epoch[034/310], Step[1100/1251], Loss: 3.8591(4.5868), Acc: 0.3164(0.2372)
2021-12-28 17:18:35,946 Epoch[034/310], Step[1150/1251], Loss: 3.7136(4.5875), Acc: 0.2256(0.2372)
2021-12-28 17:19:39,452 Epoch[034/310], Step[1200/1251], Loss: 4.8212(4.5903), Acc: 0.1182(0.2364)
2021-12-28 17:20:41,922 Epoch[034/310], Step[1250/1251], Loss: 4.7119(4.5909), Acc: 0.2725(0.2364)
2021-12-28 17:20:43,846 ----- Validation after Epoch: 34
2021-12-28 17:21:44,155 Val Step[0000/1563], Loss: 1.0837 (1.0837), Acc@1: 0.8438 (0.8438), Acc@5: 0.9062 (0.9062)
2021-12-28 17:21:45,704 Val Step[0050/1563], Loss: 3.5010 (1.4044), Acc@1: 0.2500 (0.6985), Acc@5: 0.5938 (0.8860)
2021-12-28 17:21:47,278 Val Step[0100/1563], Loss: 2.4390 (1.7786), Acc@1: 0.2812 (0.6089), Acc@5: 0.8125 (0.8369)
2021-12-28 17:21:48,772 Val Step[0150/1563], Loss: 0.9707 (1.6621), Acc@1: 0.8125 (0.6397), Acc@5: 0.8750 (0.8531)
2021-12-28 17:21:50,295 Val Step[0200/1563], Loss: 1.4605 (1.6966), Acc@1: 0.7188 (0.6342), Acc@5: 0.9375 (0.8498)
2021-12-28 17:21:51,822 Val Step[0250/1563], Loss: 1.6541 (1.6120), Acc@1: 0.5625 (0.6541), Acc@5: 0.8125 (0.8623)
2021-12-28 17:21:53,358 Val Step[0300/1563], Loss: 1.7796 (1.6867), Acc@1: 0.5625 (0.6287), Acc@5: 0.9062 (0.8558)
2021-12-28 17:21:54,858 Val Step[0350/1563], Loss: 1.9720 (1.6931), Acc@1: 0.4688 (0.6230), Acc@5: 0.8438 (0.8586)
2021-12-28 17:21:56,317 Val Step[0400/1563], Loss: 1.6940 (1.6820), Acc@1: 0.6250 (0.6213), Acc@5: 0.9375 (0.8626)
2021-12-28 17:21:57,831 Val Step[0450/1563], Loss: 1.2792 (1.6785), Acc@1: 0.5312 (0.6192), Acc@5: 1.0000 (0.8646)
2021-12-28 17:21:59,346 Val Step[0500/1563], Loss: 0.9813 (1.6692), Acc@1: 0.7812 (0.6223), Acc@5: 0.9375 (0.8655)
2021-12-28 17:22:00,966 Val Step[0550/1563], Loss: 1.1142 (1.6458), Acc@1: 0.7500 (0.6299), Acc@5: 0.9062 (0.8681)
2021-12-28 17:22:02,580 Val Step[0600/1563], Loss: 1.0624 (1.6426), Acc@1: 0.7812 (0.6312), Acc@5: 0.9062 (0.8682)
2021-12-28 17:22:04,064 Val Step[0650/1563], Loss: 1.3015 (1.6649), Acc@1: 0.8125 (0.6270), Acc@5: 0.9375 (0.8644)
2021-12-28 17:22:05,578 Val Step[0700/1563], Loss: 2.1054 (1.7138), Acc@1: 0.6250 (0.6177), Acc@5: 0.8438 (0.8562)
2021-12-28 17:22:07,171 Val Step[0750/1563], Loss: 2.1118 (1.7595), Acc@1: 0.5000 (0.6095), Acc@5: 0.7500 (0.8482)
2021-12-28 17:22:08,698 Val Step[0800/1563], Loss: 2.7313 (1.8070), Acc@1: 0.5000 (0.5992), Acc@5: 0.6250 (0.8407)
2021-12-28 17:22:10,240 Val Step[0850/1563], Loss: 2.3640 (1.8441), Acc@1: 0.5000 (0.5925), Acc@5: 0.7500 (0.8350)
2021-12-28 17:22:11,746 Val Step[0900/1563], Loss: 0.6843 (1.8502), Acc@1: 0.8438 (0.5931), Acc@5: 0.9062 (0.8329)
2021-12-28 17:22:13,400 Val Step[0950/1563], Loss: 2.0203 (1.8819), Acc@1: 0.6250 (0.5870), Acc@5: 0.7812 (0.8273)
2021-12-28 17:22:14,943 Val Step[1000/1563], Loss: 1.0042 (1.9085), Acc@1: 0.8125 (0.5814), Acc@5: 0.9375 (0.8232)
2021-12-28 17:22:16,498 Val Step[1050/1563], Loss: 0.7917 (1.9228), Acc@1: 0.8750 (0.5789), Acc@5: 0.9688 (0.8218)
2021-12-28 17:22:18,048 Val Step[1100/1563], Loss: 1.9312 (1.9443), Acc@1: 0.6562 (0.5750), Acc@5: 0.7188 (0.8175)
2021-12-28 17:22:19,654 Val Step[1150/1563], Loss: 2.0079 (1.9680), Acc@1: 0.6562 (0.5709), Acc@5: 0.7500 (0.8130)
2021-12-28 17:22:21,115 Val Step[1200/1563], Loss: 2.5365 (1.9870), Acc@1: 0.5312 (0.5671), Acc@5: 0.7188 (0.8097)
2021-12-28 17:22:22,648 Val Step[1250/1563], Loss: 1.3520 (2.0065), Acc@1: 0.7812 (0.5641), Acc@5: 0.8750 (0.8062)
2021-12-28 17:22:24,151 Val Step[1300/1563], Loss: 1.8869 (2.0194), Acc@1: 0.6875 (0.5613), Acc@5: 0.7812 (0.8048)
2021-12-28 17:22:25,661 Val Step[1350/1563], Loss: 3.0637 (2.0404), Acc@1: 0.1250 (0.5569), Acc@5: 0.6875 (0.8014)
2021-12-28 17:22:27,221 Val Step[1400/1563], Loss: 1.9487 (2.0496), Acc@1: 0.5938 (0.5557), Acc@5: 0.8438 (0.8000)
2021-12-28 17:22:28,688 Val Step[1450/1563], Loss: 2.3185 (2.0564), Acc@1: 0.3750 (0.5546), Acc@5: 0.8438 (0.7991)
2021-12-28 17:22:30,169 Val Step[1500/1563], Loss: 2.6891 (2.0409), Acc@1: 0.3750 (0.5580), Acc@5: 0.7500 (0.8013)
2021-12-28 17:22:31,723 Val Step[1550/1563], Loss: 1.0933 (2.0350), Acc@1: 0.8750 (0.5592), Acc@5: 0.9062 (0.8021)
2021-12-28 17:22:32,580 ----- Epoch[034/310], Validation Loss: 2.0317, Validation Acc@1: 0.5598, Validation Acc@5: 0.8025, time: 108.73
2021-12-28 17:22:32,581 ----- Epoch[034/310], Train Loss: 4.5909, Train Acc: 0.2364, time: 1641.22, Best Val(epoch34) Acc@1: 0.5598
2021-12-28 17:22:32,777 Max accuracy so far: 0.5598 at epoch_34
2021-12-28 17:22:32,778 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 17:22:32,778 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 17:22:32,867 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 17:22:32,867 Now training epoch 35. LR=0.000967
2021-12-28 17:23:51,899 Epoch[035/310], Step[0000/1251], Loss: 4.2946(4.2946), Acc: 0.3477(0.3477)
2021-12-28 17:24:53,798 Epoch[035/310], Step[0050/1251], Loss: 4.4988(4.4707), Acc: 0.3574(0.2459)
2021-12-28 17:25:56,051 Epoch[035/310], Step[0100/1251], Loss: 4.7155(4.4964), Acc: 0.2910(0.2479)
2021-12-28 17:26:58,534 Epoch[035/310], Step[0150/1251], Loss: 4.5857(4.5508), Acc: 0.1309(0.2420)
2021-12-28 17:27:59,935 Epoch[035/310], Step[0200/1251], Loss: 4.0762(4.5507), Acc: 0.3535(0.2454)
2021-12-28 17:29:02,260 Epoch[035/310], Step[0250/1251], Loss: 5.3123(4.5601), Acc: 0.1357(0.2391)
2021-12-28 17:30:03,637 Epoch[035/310], Step[0300/1251], Loss: 4.4996(4.5622), Acc: 0.2783(0.2375)
2021-12-28 17:31:06,421 Epoch[035/310], Step[0350/1251], Loss: 4.7794(4.5639), Acc: 0.2744(0.2370)
2021-12-28 17:32:09,107 Epoch[035/310], Step[0400/1251], Loss: 4.8341(4.5636), Acc: 0.2842(0.2390)
2021-12-28 17:33:12,094 Epoch[035/310], Step[0450/1251], Loss: 4.4282(4.5647), Acc: 0.1348(0.2378)
2021-12-28 17:34:14,672 Epoch[035/310], Step[0500/1251], Loss: 4.9843(4.5742), Acc: 0.2695(0.2366)
2021-12-28 17:35:16,763 Epoch[035/310], Step[0550/1251], Loss: 4.7017(4.5768), Acc: 0.1367(0.2361)
2021-12-28 17:36:19,445 Epoch[035/310], Step[0600/1251], Loss: 4.5967(4.5778), Acc: 0.3486(0.2373)
2021-12-28 17:37:20,157 Epoch[035/310], Step[0650/1251], Loss: 4.8384(4.5777), Acc: 0.1885(0.2378)
2021-12-28 17:38:23,750 Epoch[035/310], Step[0700/1251], Loss: 4.8569(4.5783), Acc: 0.2783(0.2372)
2021-12-28 17:39:25,872 Epoch[035/310], Step[0750/1251], Loss: 4.8573(4.5874), Acc: 0.1768(0.2367)
2021-12-28 17:40:27,402 Epoch[035/310], Step[0800/1251], Loss: 4.9277(4.5866), Acc: 0.2344(0.2365)
2021-12-28 17:41:31,131 Epoch[035/310], Step[0850/1251], Loss: 5.0495(4.5827), Acc: 0.1426(0.2372)
2021-12-28 17:42:34,616 Epoch[035/310], Step[0900/1251], Loss: 5.0818(4.5834), Acc: 0.2041(0.2373)
2021-12-28 17:43:38,203 Epoch[035/310], Step[0950/1251], Loss: 4.9574(4.5840), Acc: 0.2676(0.2370)
2021-12-28 17:44:38,763 Epoch[035/310], Step[1000/1251], Loss: 4.2634(4.5837), Acc: 0.3516(0.2370)
2021-12-28 17:45:40,895 Epoch[035/310], Step[1050/1251], Loss: 4.4340(4.5842), Acc: 0.1240(0.2362)
2021-12-28 17:46:43,921 Epoch[035/310], Step[1100/1251], Loss: 4.3200(4.5858), Acc: 0.1172(0.2361)
2021-12-28 17:47:45,906 Epoch[035/310], Step[1150/1251], Loss: 4.8363(4.5862), Acc: 0.2686(0.2365)
2021-12-28 17:48:48,318 Epoch[035/310], Step[1200/1251], Loss: 5.3058(4.5872), Acc: 0.1533(0.2363)
2021-12-28 17:49:49,760 Epoch[035/310], Step[1250/1251], Loss: 4.1479(4.5870), Acc: 0.3828(0.2361)
2021-12-28 17:49:51,712 ----- Epoch[035/310], Train Loss: 4.5870, Train Acc: 0.2361, time: 1638.84, Best Val(epoch34) Acc@1: 0.5598
2021-12-28 17:49:51,712 Now training epoch 36. LR=0.000965
2021-12-28 17:51:17,632 Epoch[036/310], Step[0000/1251], Loss: 5.0282(5.0282), Acc: 0.1729(0.1729)
2021-12-28 17:52:20,993 Epoch[036/310], Step[0050/1251], Loss: 4.7613(4.5989), Acc: 0.1934(0.2472)
2021-12-28 17:53:23,943 Epoch[036/310], Step[0100/1251], Loss: 4.4287(4.5768), Acc: 0.2998(0.2455)
2021-12-28 17:54:25,947 Epoch[036/310], Step[0150/1251], Loss: 4.9554(4.5871), Acc: 0.1387(0.2440)
2021-12-28 17:55:26,671 Epoch[036/310], Step[0200/1251], Loss: 4.5010(4.5900), Acc: 0.3096(0.2449)
2021-12-28 17:56:29,332 Epoch[036/310], Step[0250/1251], Loss: 4.3569(4.5936), Acc: 0.3770(0.2398)
2021-12-28 17:57:31,626 Epoch[036/310], Step[0300/1251], Loss: 4.6072(4.6011), Acc: 0.2969(0.2406)
2021-12-28 17:58:32,581 Epoch[036/310], Step[0350/1251], Loss: 4.1589(4.5996), Acc: 0.2676(0.2375)
2021-12-28 17:59:36,652 Epoch[036/310], Step[0400/1251], Loss: 4.6901(4.5999), Acc: 0.2969(0.2367)
2021-12-28 18:00:40,099 Epoch[036/310], Step[0450/1251], Loss: 4.4977(4.5962), Acc: 0.2363(0.2366)
2021-12-28 18:01:43,503 Epoch[036/310], Step[0500/1251], Loss: 4.2040(4.5946), Acc: 0.2725(0.2376)
2021-12-28 18:02:46,222 Epoch[036/310], Step[0550/1251], Loss: 4.0297(4.5933), Acc: 0.4141(0.2374)
2021-12-28 18:03:50,432 Epoch[036/310], Step[0600/1251], Loss: 4.6289(4.5941), Acc: 0.2754(0.2377)
2021-12-28 18:04:54,131 Epoch[036/310], Step[0650/1251], Loss: 5.0335(4.5969), Acc: 0.2422(0.2375)
2021-12-28 18:05:55,373 Epoch[036/310], Step[0700/1251], Loss: 4.7228(4.5932), Acc: 0.2607(0.2383)
2021-12-28 18:06:57,967 Epoch[036/310], Step[0750/1251], Loss: 4.2802(4.5884), Acc: 0.3535(0.2380)
2021-12-28 18:08:00,461 Epoch[036/310], Step[0800/1251], Loss: 4.2043(4.5890), Acc: 0.3789(0.2365)
2021-12-28 18:09:02,384 Epoch[036/310], Step[0850/1251], Loss: 5.1192(4.5869), Acc: 0.1895(0.2365)
2021-12-28 18:10:05,414 Epoch[036/310], Step[0900/1251], Loss: 3.6826(4.5843), Acc: 0.3271(0.2363)
2021-12-28 18:11:09,416 Epoch[036/310], Step[0950/1251], Loss: 4.7219(4.5842), Acc: 0.2715(0.2371)
2021-12-28 18:12:10,467 Epoch[036/310], Step[1000/1251], Loss: 4.7240(4.5869), Acc: 0.3291(0.2373)
2021-12-28 18:13:13,524 Epoch[036/310], Step[1050/1251], Loss: 3.9763(4.5849), Acc: 0.2314(0.2371)
2021-12-28 18:14:16,562 Epoch[036/310], Step[1100/1251], Loss: 5.0447(4.5809), Acc: 0.2051(0.2372)
2021-12-28 18:15:20,241 Epoch[036/310], Step[1150/1251], Loss: 4.6513(4.5764), Acc: 0.2637(0.2376)
2021-12-28 18:16:22,748 Epoch[036/310], Step[1200/1251], Loss: 4.4755(4.5752), Acc: 0.1504(0.2379)
2021-12-28 18:17:25,422 Epoch[036/310], Step[1250/1251], Loss: 4.6542(4.5737), Acc: 0.3242(0.2378)
2021-12-28 18:17:27,381 ----- Validation after Epoch: 36
2021-12-28 18:18:32,155 Val Step[0000/1563], Loss: 1.0995 (1.0995), Acc@1: 0.8438 (0.8438), Acc@5: 0.9062 (0.9062)
2021-12-28 18:18:33,781 Val Step[0050/1563], Loss: 2.9876 (1.3365), Acc@1: 0.3438 (0.7200), Acc@5: 0.6250 (0.8903)
2021-12-28 18:18:35,312 Val Step[0100/1563], Loss: 2.4167 (1.7451), Acc@1: 0.4375 (0.6204), Acc@5: 0.7812 (0.8366)
2021-12-28 18:18:36,843 Val Step[0150/1563], Loss: 0.9739 (1.6404), Acc@1: 0.8125 (0.6492), Acc@5: 0.9062 (0.8495)
2021-12-28 18:18:38,329 Val Step[0200/1563], Loss: 1.6269 (1.6673), Acc@1: 0.6250 (0.6458), Acc@5: 0.8438 (0.8500)
2021-12-28 18:18:39,753 Val Step[0250/1563], Loss: 1.2779 (1.5828), Acc@1: 0.6562 (0.6642), Acc@5: 0.9375 (0.8622)
2021-12-28 18:18:41,237 Val Step[0300/1563], Loss: 2.0681 (1.6569), Acc@1: 0.5312 (0.6394), Acc@5: 0.8438 (0.8564)
2021-12-28 18:18:42,775 Val Step[0350/1563], Loss: 1.8827 (1.6663), Acc@1: 0.4688 (0.6296), Acc@5: 0.9062 (0.8610)
2021-12-28 18:18:44,175 Val Step[0400/1563], Loss: 1.5036 (1.6524), Acc@1: 0.6875 (0.6280), Acc@5: 0.9375 (0.8651)
2021-12-28 18:18:45,749 Val Step[0450/1563], Loss: 1.7161 (1.6502), Acc@1: 0.1875 (0.6256), Acc@5: 1.0000 (0.8669)
2021-12-28 18:18:47,289 Val Step[0500/1563], Loss: 0.6969 (1.6443), Acc@1: 0.9375 (0.6283), Acc@5: 0.9688 (0.8686)
2021-12-28 18:18:48,872 Val Step[0550/1563], Loss: 1.2226 (1.6132), Acc@1: 0.7188 (0.6365), Acc@5: 0.9688 (0.8725)
2021-12-28 18:18:50,423 Val Step[0600/1563], Loss: 1.2092 (1.6151), Acc@1: 0.7500 (0.6364), Acc@5: 0.8750 (0.8716)
2021-12-28 18:18:51,978 Val Step[0650/1563], Loss: 1.2616 (1.6385), Acc@1: 0.7500 (0.6328), Acc@5: 0.9688 (0.8674)
2021-12-28 18:18:53,453 Val Step[0700/1563], Loss: 1.9072 (1.6881), Acc@1: 0.6875 (0.6235), Acc@5: 0.7812 (0.8595)
2021-12-28 18:18:55,076 Val Step[0750/1563], Loss: 2.3763 (1.7353), Acc@1: 0.5938 (0.6148), Acc@5: 0.7500 (0.8515)
2021-12-28 18:18:56,650 Val Step[0800/1563], Loss: 2.4804 (1.7866), Acc@1: 0.5312 (0.6036), Acc@5: 0.7500 (0.8436)
2021-12-28 18:18:58,148 Val Step[0850/1563], Loss: 2.1081 (1.8177), Acc@1: 0.5000 (0.5979), Acc@5: 0.7812 (0.8386)
2021-12-28 18:18:59,697 Val Step[0900/1563], Loss: 0.6971 (1.8208), Acc@1: 0.8750 (0.5997), Acc@5: 0.9062 (0.8370)
2021-12-28 18:19:01,321 Val Step[0950/1563], Loss: 2.2767 (1.8511), Acc@1: 0.5938 (0.5940), Acc@5: 0.6562 (0.8321)
2021-12-28 18:19:02,881 Val Step[1000/1563], Loss: 1.0005 (1.8870), Acc@1: 0.8750 (0.5869), Acc@5: 0.9375 (0.8266)
2021-12-28 18:19:04,358 Val Step[1050/1563], Loss: 0.7511 (1.9041), Acc@1: 0.8750 (0.5844), Acc@5: 0.9688 (0.8238)
2021-12-28 18:19:05,957 Val Step[1100/1563], Loss: 1.8574 (1.9280), Acc@1: 0.5938 (0.5803), Acc@5: 0.7812 (0.8194)
2021-12-28 18:19:07,472 Val Step[1150/1563], Loss: 1.7685 (1.9520), Acc@1: 0.6875 (0.5757), Acc@5: 0.7812 (0.8156)
2021-12-28 18:19:08,965 Val Step[1200/1563], Loss: 2.2360 (1.9709), Acc@1: 0.5000 (0.5727), Acc@5: 0.7812 (0.8127)
2021-12-28 18:19:10,470 Val Step[1250/1563], Loss: 1.6560 (1.9895), Acc@1: 0.6250 (0.5701), Acc@5: 0.9375 (0.8097)
2021-12-28 18:19:12,044 Val Step[1300/1563], Loss: 2.0413 (2.0095), Acc@1: 0.5625 (0.5657), Acc@5: 0.7500 (0.8067)
2021-12-28 18:19:13,501 Val Step[1350/1563], Loss: 2.4406 (2.0323), Acc@1: 0.3438 (0.5613), Acc@5: 0.8125 (0.8026)
2021-12-28 18:19:15,002 Val Step[1400/1563], Loss: 1.5603 (2.0455), Acc@1: 0.6562 (0.5585), Acc@5: 0.8438 (0.8002)
2021-12-28 18:19:16,490 Val Step[1450/1563], Loss: 2.4784 (2.0511), Acc@1: 0.3125 (0.5571), Acc@5: 0.7812 (0.7995)
2021-12-28 18:19:17,989 Val Step[1500/1563], Loss: 1.9977 (2.0342), Acc@1: 0.5000 (0.5603), Acc@5: 0.9062 (0.8023)
2021-12-28 18:19:19,459 Val Step[1550/1563], Loss: 1.1719 (2.0271), Acc@1: 0.8750 (0.5615), Acc@5: 0.9062 (0.8032)
2021-12-28 18:19:20,358 ----- Epoch[036/310], Validation Loss: 2.0235, Validation Acc@1: 0.5623, Validation Acc@5: 0.8036, time: 112.97
2021-12-28 18:19:20,358 ----- Epoch[036/310], Train Loss: 4.5737, Train Acc: 0.2378, time: 1655.66, Best Val(epoch36) Acc@1: 0.5623
2021-12-28 18:19:20,574 Max accuracy so far: 0.5623 at epoch_36
2021-12-28 18:19:20,575 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 18:19:20,575 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 18:19:20,653 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 18:19:20,653 Now training epoch 37. LR=0.000963
2021-12-28 18:20:39,949 Epoch[037/310], Step[0000/1251], Loss: 5.1177(5.1177), Acc: 0.2109(0.2109)
2021-12-28 18:21:41,188 Epoch[037/310], Step[0050/1251], Loss: 4.6950(4.5645), Acc: 0.1357(0.2349)
2021-12-28 18:22:42,796 Epoch[037/310], Step[0100/1251], Loss: 4.6535(4.5528), Acc: 0.0918(0.2309)
2021-12-28 18:23:42,735 Epoch[037/310], Step[0150/1251], Loss: 4.8786(4.5439), Acc: 0.3057(0.2423)
2021-12-28 18:24:44,222 Epoch[037/310], Step[0200/1251], Loss: 4.6299(4.5363), Acc: 0.2285(0.2399)
2021-12-28 18:25:45,926 Epoch[037/310], Step[0250/1251], Loss: 4.5977(4.5424), Acc: 0.1279(0.2412)
2021-12-28 18:26:48,266 Epoch[037/310], Step[0300/1251], Loss: 4.9900(4.5561), Acc: 0.3291(0.2410)
2021-12-28 18:27:49,340 Epoch[037/310], Step[0350/1251], Loss: 4.4445(4.5529), Acc: 0.3564(0.2403)
2021-12-28 18:28:52,663 Epoch[037/310], Step[0400/1251], Loss: 5.1690(4.5626), Acc: 0.1992(0.2401)
2021-12-28 18:29:54,718 Epoch[037/310], Step[0450/1251], Loss: 4.7273(4.5655), Acc: 0.3018(0.2420)
2021-12-28 18:30:57,256 Epoch[037/310], Step[0500/1251], Loss: 4.7297(4.5569), Acc: 0.2480(0.2409)
2021-12-28 18:31:59,721 Epoch[037/310], Step[0550/1251], Loss: 5.0387(4.5596), Acc: 0.1211(0.2400)
2021-12-28 18:33:02,518 Epoch[037/310], Step[0600/1251], Loss: 5.0527(4.5575), Acc: 0.1904(0.2415)
2021-12-28 18:34:06,442 Epoch[037/310], Step[0650/1251], Loss: 4.9947(4.5583), Acc: 0.2441(0.2412)
2021-12-28 18:35:08,058 Epoch[037/310], Step[0700/1251], Loss: 4.4860(4.5584), Acc: 0.2139(0.2422)
2021-12-28 18:36:11,321 Epoch[037/310], Step[0750/1251], Loss: 4.5452(4.5531), Acc: 0.3096(0.2433)
2021-12-28 18:37:13,336 Epoch[037/310], Step[0800/1251], Loss: 3.9628(4.5514), Acc: 0.3350(0.2443)
2021-12-28 18:38:15,792 Epoch[037/310], Step[0850/1251], Loss: 4.5324(4.5492), Acc: 0.3545(0.2446)
2021-12-28 18:39:18,261 Epoch[037/310], Step[0900/1251], Loss: 4.0989(4.5518), Acc: 0.3447(0.2446)
2021-12-28 18:40:20,954 Epoch[037/310], Step[0950/1251], Loss: 4.3112(4.5537), Acc: 0.3770(0.2440)
2021-12-28 18:41:23,648 Epoch[037/310], Step[1000/1251], Loss: 4.7367(4.5558), Acc: 0.2451(0.2437)
2021-12-28 18:42:25,613 Epoch[037/310], Step[1050/1251], Loss: 4.6411(4.5558), Acc: 0.3369(0.2445)
2021-12-28 18:43:28,658 Epoch[037/310], Step[1100/1251], Loss: 4.1272(4.5560), Acc: 0.1445(0.2441)
2021-12-28 18:44:31,940 Epoch[037/310], Step[1150/1251], Loss: 4.0468(4.5568), Acc: 0.3506(0.2448)
2021-12-28 18:45:34,456 Epoch[037/310], Step[1200/1251], Loss: 5.0405(4.5573), Acc: 0.2246(0.2453)
2021-12-28 18:46:36,696 Epoch[037/310], Step[1250/1251], Loss: 4.5002(4.5572), Acc: 0.2031(0.2458)
2021-12-28 18:46:38,653 ----- Epoch[037/310], Train Loss: 4.5572, Train Acc: 0.2458, time: 1638.00, Best Val(epoch36) Acc@1: 0.5623
2021-12-28 18:46:38,653 Now training epoch 38. LR=0.000961
2021-12-28 18:48:06,539 Epoch[038/310], Step[0000/1251], Loss: 4.9016(4.9016), Acc: 0.2490(0.2490)
2021-12-28 18:49:09,523 Epoch[038/310], Step[0050/1251], Loss: 4.4358(4.5481), Acc: 0.2100(0.2371)
2021-12-28 18:50:11,846 Epoch[038/310], Step[0100/1251], Loss: 4.9953(4.5406), Acc: 0.2061(0.2465)
2021-12-28 18:51:14,046 Epoch[038/310], Step[0150/1251], Loss: 4.9092(4.5553), Acc: 0.1426(0.2405)
2021-12-28 18:52:16,595 Epoch[038/310], Step[0200/1251], Loss: 4.3275(4.5428), Acc: 0.2539(0.2427)
2021-12-28 18:53:19,574 Epoch[038/310], Step[0250/1251], Loss: 4.2587(4.5385), Acc: 0.2881(0.2446)
2021-12-28 18:54:23,106 Epoch[038/310], Step[0300/1251], Loss: 3.9338(4.5398), Acc: 0.2969(0.2453)
2021-12-28 18:55:25,676 Epoch[038/310], Step[0350/1251], Loss: 5.0197(4.5354), Acc: 0.2305(0.2461)
2021-12-28 18:56:28,745 Epoch[038/310], Step[0400/1251], Loss: 4.7437(4.5324), Acc: 0.2354(0.2468)
2021-12-28 18:57:31,358 Epoch[038/310], Step[0450/1251], Loss: 4.9282(4.5349), Acc: 0.2197(0.2453)
2021-12-28 18:58:33,269 Epoch[038/310], Step[0500/1251], Loss: 4.1817(4.5373), Acc: 0.1855(0.2445)
2021-12-28 18:59:34,868 Epoch[038/310], Step[0550/1251], Loss: 4.6566(4.5417), Acc: 0.2959(0.2450)
2021-12-28 19:00:39,271 Epoch[038/310], Step[0600/1251], Loss: 4.2696(4.5404), Acc: 0.1035(0.2452)
2021-12-28 19:01:43,043 Epoch[038/310], Step[0650/1251], Loss: 4.4556(4.5352), Acc: 0.2666(0.2453)
2021-12-28 19:02:45,210 Epoch[038/310], Step[0700/1251], Loss: 4.4244(4.5371), Acc: 0.1191(0.2441)
2021-12-28 19:03:47,693 Epoch[038/310], Step[0750/1251], Loss: 4.7487(4.5378), Acc: 0.2666(0.2445)
2021-12-28 19:04:50,674 Epoch[038/310], Step[0800/1251], Loss: 4.6427(4.5375), Acc: 0.1426(0.2452)
2021-12-28 19:05:53,597 Epoch[038/310], Step[0850/1251], Loss: 4.7096(4.5383), Acc: 0.2627(0.2452)
2021-12-28 19:06:57,068 Epoch[038/310], Step[0900/1251], Loss: 4.8422(4.5383), Acc: 0.2900(0.2455)
2021-12-28 19:08:00,190 Epoch[038/310], Step[0950/1251], Loss: 4.9692(4.5442), Acc: 0.1865(0.2443)
2021-12-28 19:09:04,540 Epoch[038/310], Step[1000/1251], Loss: 4.8798(4.5443), Acc: 0.2314(0.2444)
2021-12-28 19:10:09,039 Epoch[038/310], Step[1050/1251], Loss: 4.2922(4.5429), Acc: 0.3662(0.2438)
2021-12-28 19:11:13,422 Epoch[038/310], Step[1100/1251], Loss: 4.4956(4.5451), Acc: 0.3037(0.2434)
2021-12-28 19:12:17,598 Epoch[038/310], Step[1150/1251], Loss: 4.3344(4.5468), Acc: 0.2480(0.2434)
2021-12-28 19:13:20,886 Epoch[038/310], Step[1200/1251], Loss: 4.5739(4.5436), Acc: 0.3086(0.2435)
2021-12-28 19:14:24,288 Epoch[038/310], Step[1250/1251], Loss: 4.7012(4.5439), Acc: 0.0586(0.2431)
2021-12-28 19:14:26,378 ----- Validation after Epoch: 38
2021-12-28 19:15:30,551 Val Step[0000/1563], Loss: 0.9688 (0.9688), Acc@1: 0.8750 (0.8750), Acc@5: 0.9375 (0.9375)
2021-12-28 19:15:32,068 Val Step[0050/1563], Loss: 2.6718 (1.2975), Acc@1: 0.4375 (0.7359), Acc@5: 0.7500 (0.9050)
2021-12-28 19:15:33,510 Val Step[0100/1563], Loss: 2.1912 (1.6766), Acc@1: 0.4062 (0.6389), Acc@5: 0.8125 (0.8614)
2021-12-28 19:15:35,227 Val Step[0150/1563], Loss: 0.9393 (1.5762), Acc@1: 0.7500 (0.6664), Acc@5: 0.9688 (0.8723)
2021-12-28 19:15:36,815 Val Step[0200/1563], Loss: 1.7663 (1.6200), Acc@1: 0.6562 (0.6569), Acc@5: 0.8750 (0.8668)
2021-12-28 19:15:38,382 Val Step[0250/1563], Loss: 2.0361 (1.5462), Acc@1: 0.3750 (0.6738), Acc@5: 0.7812 (0.8772)
2021-12-28 19:15:39,918 Val Step[0300/1563], Loss: 2.2569 (1.6161), Acc@1: 0.4688 (0.6495), Acc@5: 0.7500 (0.8699)
2021-12-28 19:15:41,421 Val Step[0350/1563], Loss: 1.9432 (1.6286), Acc@1: 0.5000 (0.6433), Acc@5: 0.8125 (0.8713)
2021-12-28 19:15:42,907 Val Step[0400/1563], Loss: 1.7041 (1.6243), Acc@1: 0.6562 (0.6373), Acc@5: 0.8438 (0.8734)
2021-12-28 19:15:44,464 Val Step[0450/1563], Loss: 1.7277 (1.6297), Acc@1: 0.1875 (0.6328), Acc@5: 1.0000 (0.8744)
2021-12-28 19:15:46,011 Val Step[0500/1563], Loss: 0.8843 (1.6211), Acc@1: 0.7500 (0.6356), Acc@5: 0.9688 (0.8758)
2021-12-28 19:15:47,588 Val Step[0550/1563], Loss: 1.2774 (1.5960), Acc@1: 0.7188 (0.6436), Acc@5: 0.9375 (0.8793)
2021-12-28 19:15:49,205 Val Step[0600/1563], Loss: 1.3582 (1.5989), Acc@1: 0.7812 (0.6441), Acc@5: 0.9062 (0.8785)
2021-12-28 19:15:50,793 Val Step[0650/1563], Loss: 0.9352 (1.6233), Acc@1: 0.8125 (0.6405), Acc@5: 1.0000 (0.8745)
2021-12-28 19:15:52,296 Val Step[0700/1563], Loss: 2.4468 (1.6732), Acc@1: 0.4688 (0.6303), Acc@5: 0.7188 (0.8666)
2021-12-28 19:15:53,831 Val Step[0750/1563], Loss: 2.6364 (1.7153), Acc@1: 0.4375 (0.6224), Acc@5: 0.6875 (0.8599)
2021-12-28 19:15:55,314 Val Step[0800/1563], Loss: 2.2467 (1.7635), Acc@1: 0.5938 (0.6119), Acc@5: 0.7188 (0.8517)
2021-12-28 19:15:56,963 Val Step[0850/1563], Loss: 2.3040 (1.7971), Acc@1: 0.4688 (0.6050), Acc@5: 0.7188 (0.8466)
2021-12-28 19:15:58,620 Val Step[0900/1563], Loss: 0.7026 (1.7999), Acc@1: 0.8750 (0.6065), Acc@5: 0.9375 (0.8453)
2021-12-28 19:16:00,222 Val Step[0950/1563], Loss: 2.1297 (1.8266), Acc@1: 0.5938 (0.6020), Acc@5: 0.7500 (0.8409)
2021-12-28 19:16:01,861 Val Step[1000/1563], Loss: 1.0162 (1.8590), Acc@1: 0.9062 (0.5950), Acc@5: 1.0000 (0.8352)
2021-12-28 19:16:03,484 Val Step[1050/1563], Loss: 0.5833 (1.8770), Acc@1: 0.9375 (0.5919), Acc@5: 0.9688 (0.8323)
2021-12-28 19:16:05,076 Val Step[1100/1563], Loss: 2.0389 (1.8988), Acc@1: 0.6250 (0.5880), Acc@5: 0.6875 (0.8281)
2021-12-28 19:16:06,621 Val Step[1150/1563], Loss: 2.2559 (1.9199), Acc@1: 0.6562 (0.5846), Acc@5: 0.7500 (0.8247)
2021-12-28 19:16:08,154 Val Step[1200/1563], Loss: 2.1663 (1.9393), Acc@1: 0.6250 (0.5814), Acc@5: 0.7812 (0.8207)
2021-12-28 19:16:09,697 Val Step[1250/1563], Loss: 1.1145 (1.9590), Acc@1: 0.8750 (0.5784), Acc@5: 0.9375 (0.8175)
2021-12-28 19:16:11,275 Val Step[1300/1563], Loss: 1.6596 (1.9729), Acc@1: 0.7188 (0.5756), Acc@5: 0.7812 (0.8157)
2021-12-28 19:16:12,866 Val Step[1350/1563], Loss: 2.4822 (1.9948), Acc@1: 0.2500 (0.5715), Acc@5: 0.8125 (0.8120)
2021-12-28 19:16:14,389 Val Step[1400/1563], Loss: 2.0761 (2.0051), Acc@1: 0.5312 (0.5693), Acc@5: 0.8125 (0.8100)
2021-12-28 19:16:15,874 Val Step[1450/1563], Loss: 2.4351 (2.0116), Acc@1: 0.4062 (0.5681), Acc@5: 0.8125 (0.8090)
2021-12-28 19:16:17,439 Val Step[1500/1563], Loss: 2.4742 (1.9989), Acc@1: 0.4062 (0.5710), Acc@5: 0.7812 (0.8110)
2021-12-28 19:16:18,974 Val Step[1550/1563], Loss: 1.0189 (1.9914), Acc@1: 0.8750 (0.5725), Acc@5: 0.9062 (0.8120)
2021-12-28 19:16:20,758 ----- Epoch[038/310], Validation Loss: 1.9881, Validation Acc@1: 0.5733, Validation Acc@5: 0.8123, time: 114.38
2021-12-28 19:16:20,758 ----- Epoch[038/310], Train Loss: 4.5439, Train Acc: 0.2431, time: 1667.72, Best Val(epoch38) Acc@1: 0.5733
2021-12-28 19:16:20,961 Max accuracy so far: 0.5733 at epoch_38
2021-12-28 19:16:20,962 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 19:16:20,962 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 19:16:21,058 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 19:16:21,059 Now training epoch 39. LR=0.000959
2021-12-28 19:17:45,512 Epoch[039/310], Step[0000/1251], Loss: 4.7071(4.7071), Acc: 0.3096(0.3096)
2021-12-28 19:18:46,494 Epoch[039/310], Step[0050/1251], Loss: 5.0989(4.5571), Acc: 0.2012(0.2583)
2021-12-28 19:19:48,334 Epoch[039/310], Step[0100/1251], Loss: 4.2611(4.5498), Acc: 0.2881(0.2512)
2021-12-28 19:20:50,204 Epoch[039/310], Step[0150/1251], Loss: 4.7147(4.5527), Acc: 0.1191(0.2436)
2021-12-28 19:21:51,602 Epoch[039/310], Step[0200/1251], Loss: 4.9633(4.5543), Acc: 0.2861(0.2469)
2021-12-28 19:22:53,792 Epoch[039/310], Step[0250/1251], Loss: 4.8626(4.5628), Acc: 0.1836(0.2460)
2021-12-28 19:23:56,001 Epoch[039/310], Step[0300/1251], Loss: 3.7798(4.5541), Acc: 0.3428(0.2461)
2021-12-28 19:24:58,222 Epoch[039/310], Step[0350/1251], Loss: 3.6398(4.5488), Acc: 0.3457(0.2478)
2021-12-28 19:26:00,551 Epoch[039/310], Step[0400/1251], Loss: 3.8826(4.5456), Acc: 0.2490(0.2492)
2021-12-28 19:27:03,575 Epoch[039/310], Step[0450/1251], Loss: 4.5644(4.5447), Acc: 0.2500(0.2492)
2021-12-28 19:28:07,348 Epoch[039/310], Step[0500/1251], Loss: 4.5099(4.5405), Acc: 0.2764(0.2496)
2021-12-28 19:29:10,968 Epoch[039/310], Step[0550/1251], Loss: 4.7598(4.5407), Acc: 0.3008(0.2507)
2021-12-28 19:30:13,965 Epoch[039/310], Step[0600/1251], Loss: 5.0486(4.5353), Acc: 0.1172(0.2493)
2021-12-28 19:31:14,954 Epoch[039/310], Step[0650/1251], Loss: 4.6882(4.5401), Acc: 0.2744(0.2490)
2021-12-28 19:32:18,357 Epoch[039/310], Step[0700/1251], Loss: 4.6568(4.5417), Acc: 0.2666(0.2490)
2021-12-28 19:33:20,724 Epoch[039/310], Step[0750/1251], Loss: 4.9167(4.5418), Acc: 0.1348(0.2486)
2021-12-28 19:34:22,986 Epoch[039/310], Step[0800/1251], Loss: 4.5703(4.5382), Acc: 0.1641(0.2476)
2021-12-28 19:35:26,722 Epoch[039/310], Step[0850/1251], Loss: 5.0091(4.5378), Acc: 0.1533(0.2463)
2021-12-28 19:36:28,451 Epoch[039/310], Step[0900/1251], Loss: 4.6077(4.5359), Acc: 0.1807(0.2470)
2021-12-28 19:37:32,163 Epoch[039/310], Step[0950/1251], Loss: 4.6562(4.5385), Acc: 0.1387(0.2461)
2021-12-28 19:38:35,685 Epoch[039/310], Step[1000/1251], Loss: 4.8252(4.5392), Acc: 0.2393(0.2454)
2021-12-28 19:39:38,439 Epoch[039/310], Step[1050/1251], Loss: 4.8854(4.5378), Acc: 0.2275(0.2460)
2021-12-28 19:40:41,431 Epoch[039/310], Step[1100/1251], Loss: 4.7016(4.5397), Acc: 0.2256(0.2451)
2021-12-28 19:41:44,353 Epoch[039/310], Step[1150/1251], Loss: 4.5561(4.5381), Acc: 0.1494(0.2460)
2021-12-28 19:42:48,543 Epoch[039/310], Step[1200/1251], Loss: 4.1961(4.5331), Acc: 0.2764(0.2464)
2021-12-28 19:43:49,628 Epoch[039/310], Step[1250/1251], Loss: 4.5220(4.5335), Acc: 0.3350(0.2469)
2021-12-28 19:43:52,686 ----- Epoch[039/310], Train Loss: 4.5335, Train Acc: 0.2469, time: 1651.62, Best Val(epoch38) Acc@1: 0.5733
2021-12-28 19:43:52,686 Now training epoch 40. LR=0.000957
2021-12-28 19:45:18,576 Epoch[040/310], Step[0000/1251], Loss: 4.1738(4.1738), Acc: 0.3975(0.3975)
2021-12-28 19:46:21,724 Epoch[040/310], Step[0050/1251], Loss: 4.7620(4.4515), Acc: 0.2012(0.2730)
2021-12-28 19:47:24,344 Epoch[040/310], Step[0100/1251], Loss: 4.8773(4.4990), Acc: 0.2744(0.2596)
2021-12-28 19:48:26,194 Epoch[040/310], Step[0150/1251], Loss: 4.9063(4.5262), Acc: 0.1582(0.2537)
2021-12-28 19:49:29,094 Epoch[040/310], Step[0200/1251], Loss: 4.4677(4.5245), Acc: 0.1611(0.2533)
2021-12-28 19:50:31,800 Epoch[040/310], Step[0250/1251], Loss: 4.6907(4.5103), Acc: 0.2461(0.2556)
2021-12-28 19:51:35,411 Epoch[040/310], Step[0300/1251], Loss: 5.3098(4.5110), Acc: 0.1592(0.2525)
2021-12-28 19:52:37,441 Epoch[040/310], Step[0350/1251], Loss: 4.4448(4.5179), Acc: 0.3330(0.2501)
2021-12-28 19:53:39,843 Epoch[040/310], Step[0400/1251], Loss: 4.5959(4.5169), Acc: 0.3105(0.2509)
2021-12-28 19:54:42,439 Epoch[040/310], Step[0450/1251], Loss: 4.3909(4.5183), Acc: 0.2891(0.2504)
2021-12-28 19:55:45,393 Epoch[040/310], Step[0500/1251], Loss: 4.3310(4.5129), Acc: 0.3486(0.2485)
2021-12-28 19:56:46,878 Epoch[040/310], Step[0550/1251], Loss: 4.9222(4.5179), Acc: 0.2598(0.2494)
2021-12-28 19:57:49,534 Epoch[040/310], Step[0600/1251], Loss: 4.6271(4.5228), Acc: 0.2412(0.2485)
2021-12-28 19:58:53,093 Epoch[040/310], Step[0650/1251], Loss: 4.5137(4.5217), Acc: 0.3506(0.2492)
2021-12-28 19:59:56,383 Epoch[040/310], Step[0700/1251], Loss: 4.5047(4.5166), Acc: 0.2832(0.2496)
2021-12-28 20:00:58,569 Epoch[040/310], Step[0750/1251], Loss: 5.1840(4.5118), Acc: 0.1680(0.2495)
2021-12-28 20:01:59,950 Epoch[040/310], Step[0800/1251], Loss: 4.4455(4.5101), Acc: 0.1416(0.2491)
2021-12-28 20:03:01,866 Epoch[040/310], Step[0850/1251], Loss: 4.2575(4.5072), Acc: 0.1348(0.2481)
2021-12-28 20:04:05,916 Epoch[040/310], Step[0900/1251], Loss: 4.9107(4.5089), Acc: 0.1367(0.2480)
2021-12-28 20:05:10,233 Epoch[040/310], Step[0950/1251], Loss: 4.5863(4.5073), Acc: 0.3398(0.2471)
2021-12-28 20:06:14,273 Epoch[040/310], Step[1000/1251], Loss: 4.6837(4.5045), Acc: 0.0537(0.2460)
2021-12-28 20:07:17,880 Epoch[040/310], Step[1050/1251], Loss: 4.6704(4.5079), Acc: 0.2803(0.2456)
2021-12-28 20:08:21,082 Epoch[040/310], Step[1100/1251], Loss: 4.5883(4.5088), Acc: 0.1807(0.2449)
2021-12-28 20:09:25,008 Epoch[040/310], Step[1150/1251], Loss: 4.8968(4.5116), Acc: 0.2617(0.2450)
2021-12-28 20:10:27,610 Epoch[040/310], Step[1200/1251], Loss: 4.5352(4.5141), Acc: 0.2021(0.2448)
2021-12-28 20:11:30,644 Epoch[040/310], Step[1250/1251], Loss: 4.7961(4.5111), Acc: 0.1777(0.2452)
2021-12-28 20:11:32,771 ----- Validation after Epoch: 40
2021-12-28 20:12:40,777 Val Step[0000/1563], Loss: 1.1401 (1.1401), Acc@1: 0.8438 (0.8438), Acc@5: 0.8438 (0.8438)
2021-12-28 20:12:42,412 Val Step[0050/1563], Loss: 2.6088 (1.2425), Acc@1: 0.4375 (0.7384), Acc@5: 0.7500 (0.9001)
2021-12-28 20:12:43,958 Val Step[0100/1563], Loss: 2.5270 (1.6069), Acc@1: 0.3125 (0.6544), Acc@5: 0.7500 (0.8552)
2021-12-28 20:12:45,382 Val Step[0150/1563], Loss: 0.8934 (1.5081), Acc@1: 0.7812 (0.6730), Acc@5: 0.9688 (0.8704)
2021-12-28 20:12:46,923 Val Step[0200/1563], Loss: 1.3524 (1.5463), Acc@1: 0.7188 (0.6676), Acc@5: 0.9375 (0.8671)
2021-12-28 20:12:48,435 Val Step[0250/1563], Loss: 1.4840 (1.4805), Acc@1: 0.6250 (0.6807), Acc@5: 0.9375 (0.8756)
2021-12-28 20:12:50,012 Val Step[0300/1563], Loss: 1.5763 (1.5298), Acc@1: 0.6250 (0.6631), Acc@5: 0.8750 (0.8719)
2021-12-28 20:12:51,526 Val Step[0350/1563], Loss: 2.1128 (1.5495), Acc@1: 0.3750 (0.6535), Acc@5: 0.8438 (0.8742)
2021-12-28 20:12:52,972 Val Step[0400/1563], Loss: 1.5785 (1.5506), Acc@1: 0.6875 (0.6492), Acc@5: 0.9062 (0.8765)
2021-12-28 20:12:54,606 Val Step[0450/1563], Loss: 1.1640 (1.5561), Acc@1: 0.4688 (0.6437), Acc@5: 1.0000 (0.8771)
2021-12-28 20:12:56,239 Val Step[0500/1563], Loss: 0.5484 (1.5467), Acc@1: 0.9062 (0.6460), Acc@5: 0.9688 (0.8787)
2021-12-28 20:12:57,884 Val Step[0550/1563], Loss: 1.0252 (1.5245), Acc@1: 0.7188 (0.6529), Acc@5: 0.9375 (0.8812)
2021-12-28 20:12:59,457 Val Step[0600/1563], Loss: 0.9981 (1.5290), Acc@1: 0.7500 (0.6529), Acc@5: 0.9375 (0.8801)
2021-12-28 20:13:01,000 Val Step[0650/1563], Loss: 1.2194 (1.5552), Acc@1: 0.8125 (0.6492), Acc@5: 0.9375 (0.8754)
2021-12-28 20:13:02,601 Val Step[0700/1563], Loss: 2.3361 (1.6037), Acc@1: 0.5312 (0.6396), Acc@5: 0.7188 (0.8682)
2021-12-28 20:13:04,121 Val Step[0750/1563], Loss: 2.5383 (1.6466), Acc@1: 0.5312 (0.6318), Acc@5: 0.6562 (0.8616)
2021-12-28 20:13:05,636 Val Step[0800/1563], Loss: 2.0543 (1.6954), Acc@1: 0.5000 (0.6209), Acc@5: 0.8125 (0.8542)
2021-12-28 20:13:07,127 Val Step[0850/1563], Loss: 2.0688 (1.7302), Acc@1: 0.5625 (0.6143), Acc@5: 0.7812 (0.8493)
2021-12-28 20:13:08,645 Val Step[0900/1563], Loss: 0.6025 (1.7324), Acc@1: 0.8750 (0.6158), Acc@5: 0.9375 (0.8484)
2021-12-28 20:13:10,212 Val Step[0950/1563], Loss: 1.5758 (1.7604), Acc@1: 0.6875 (0.6111), Acc@5: 0.7812 (0.8438)
2021-12-28 20:13:11,722 Val Step[1000/1563], Loss: 0.9434 (1.7902), Acc@1: 0.8438 (0.6051), Acc@5: 0.9375 (0.8389)
2021-12-28 20:13:13,203 Val Step[1050/1563], Loss: 0.5300 (1.8088), Acc@1: 0.9062 (0.6016), Acc@5: 0.9688 (0.8364)
2021-12-28 20:13:14,770 Val Step[1100/1563], Loss: 2.0644 (1.8309), Acc@1: 0.6875 (0.5979), Acc@5: 0.7812 (0.8320)
2021-12-28 20:13:16,356 Val Step[1150/1563], Loss: 1.9034 (1.8533), Acc@1: 0.6875 (0.5935), Acc@5: 0.7812 (0.8288)
2021-12-28 20:13:18,001 Val Step[1200/1563], Loss: 1.8922 (1.8760), Acc@1: 0.6250 (0.5902), Acc@5: 0.8125 (0.8244)
2021-12-28 20:13:19,572 Val Step[1250/1563], Loss: 1.2891 (1.8949), Acc@1: 0.7500 (0.5880), Acc@5: 0.9375 (0.8211)
2021-12-28 20:13:21,162 Val Step[1300/1563], Loss: 1.5086 (1.9057), Acc@1: 0.7188 (0.5855), Acc@5: 0.8125 (0.8197)
2021-12-28 20:13:22,777 Val Step[1350/1563], Loss: 2.5763 (1.9287), Acc@1: 0.3438 (0.5810), Acc@5: 0.6875 (0.8156)
2021-12-28 20:13:24,409 Val Step[1400/1563], Loss: 1.9333 (1.9417), Acc@1: 0.6250 (0.5779), Acc@5: 0.8438 (0.8137)
2021-12-28 20:13:25,995 Val Step[1450/1563], Loss: 1.7379 (1.9482), Acc@1: 0.5938 (0.5764), Acc@5: 0.9062 (0.8130)
2021-12-28 20:13:27,492 Val Step[1500/1563], Loss: 2.4807 (1.9367), Acc@1: 0.3750 (0.5787), Acc@5: 0.8125 (0.8147)
2021-12-28 20:13:28,994 Val Step[1550/1563], Loss: 0.9874 (1.9321), Acc@1: 0.8750 (0.5794), Acc@5: 0.9062 (0.8155)
2021-12-28 20:13:29,997 ----- Epoch[040/310], Validation Loss: 1.9294, Validation Acc@1: 0.5800, Validation Acc@5: 0.8157, time: 117.22
2021-12-28 20:13:29,997 ----- Epoch[040/310], Train Loss: 4.5111, Train Acc: 0.2452, time: 1660.08, Best Val(epoch40) Acc@1: 0.5800
2021-12-28 20:13:30,203 Max accuracy so far: 0.5800 at epoch_40
2021-12-28 20:13:30,203 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 20:13:30,203 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 20:13:30,298 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 20:13:30,471 ----- Save model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-40-Loss-4.528062348743137.pdparams
2021-12-28 20:13:30,471 ----- Save optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-40-Loss-4.528062348743137.pdopt
2021-12-28 20:13:30,542 ----- Save ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-40-Loss-4.528062348743137-EMA.pdparams
2021-12-28 20:13:30,542 Now training epoch 41. LR=0.000955
2021-12-28 20:14:49,833 Epoch[041/310], Step[0000/1251], Loss: 4.1500(4.1500), Acc: 0.2168(0.2168)
2021-12-28 20:15:50,334 Epoch[041/310], Step[0050/1251], Loss: 3.9634(4.4768), Acc: 0.4277(0.2507)
2021-12-28 20:16:52,559 Epoch[041/310], Step[0100/1251], Loss: 4.7438(4.4473), Acc: 0.2168(0.2573)
2021-12-28 20:17:53,382 Epoch[041/310], Step[0150/1251], Loss: 4.9232(4.4703), Acc: 0.3018(0.2497)
2021-12-28 20:18:54,243 Epoch[041/310], Step[0200/1251], Loss: 4.6384(4.4596), Acc: 0.3662(0.2521)
2021-12-28 20:19:55,906 Epoch[041/310], Step[0250/1251], Loss: 4.6877(4.4754), Acc: 0.2773(0.2499)
2021-12-28 20:20:59,516 Epoch[041/310], Step[0300/1251], Loss: 5.0611(4.4865), Acc: 0.1152(0.2488)
2021-12-28 20:22:01,679 Epoch[041/310], Step[0350/1251], Loss: 4.9233(4.4898), Acc: 0.0820(0.2494)
2021-12-28 20:23:06,077 Epoch[041/310], Step[0400/1251], Loss: 4.9581(4.4917), Acc: 0.2109(0.2515)
2021-12-28 20:24:10,136 Epoch[041/310], Step[0450/1251], Loss: 4.4538(4.4998), Acc: 0.2500(0.2495)
2021-12-28 20:25:13,303 Epoch[041/310], Step[0500/1251], Loss: 4.6451(4.4963), Acc: 0.1934(0.2510)
2021-12-28 20:26:16,591 Epoch[041/310], Step[0550/1251], Loss: 4.2434(4.5004), Acc: 0.2764(0.2484)
2021-12-28 20:27:17,182 Epoch[041/310], Step[0600/1251], Loss: 4.0549(4.4906), Acc: 0.2939(0.2482)
2021-12-28 20:28:18,270 Epoch[041/310], Step[0650/1251], Loss: 4.7813(4.4946), Acc: 0.1875(0.2476)
2021-12-28 20:29:20,397 Epoch[041/310], Step[0700/1251], Loss: 4.3644(4.4962), Acc: 0.2725(0.2473)
2021-12-28 20:30:21,884 Epoch[041/310], Step[0750/1251], Loss: 4.5394(4.4944), Acc: 0.3027(0.2486)
2021-12-28 20:31:23,969 Epoch[041/310], Step[0800/1251], Loss: 4.5275(4.4970), Acc: 0.2539(0.2487)
2021-12-28 20:32:24,867 Epoch[041/310], Step[0850/1251], Loss: 4.7304(4.4989), Acc: 0.1475(0.2483)
2021-12-28 20:33:27,089 Epoch[041/310], Step[0900/1251], Loss: 4.6977(4.4984), Acc: 0.1201(0.2487)
2021-12-28 20:34:29,530 Epoch[041/310], Step[0950/1251], Loss: 4.1146(4.4950), Acc: 0.3799(0.2493)
2021-12-28 20:35:32,619 Epoch[041/310], Step[1000/1251], Loss: 3.9116(4.4969), Acc: 0.3242(0.2495)
2021-12-28 20:36:35,784 Epoch[041/310], Step[1050/1251], Loss: 4.4834(4.4972), Acc: 0.2832(0.2494)
2021-12-28 20:37:39,306 Epoch[041/310], Step[1100/1251], Loss: 4.4035(4.4955), Acc: 0.0586(0.2492)
2021-12-28 20:38:41,334 Epoch[041/310], Step[1150/1251], Loss: 4.5264(4.4992), Acc: 0.2871(0.2496)
2021-12-28 20:39:44,477 Epoch[041/310], Step[1200/1251], Loss: 4.5332(4.4951), Acc: 0.2178(0.2501)
2021-12-28 20:40:47,587 Epoch[041/310], Step[1250/1251], Loss: 4.6292(4.4922), Acc: 0.1396(0.2499)
2021-12-28 20:40:49,571 ----- Epoch[041/310], Train Loss: 4.4922, Train Acc: 0.2499, time: 1639.03, Best Val(epoch40) Acc@1: 0.5800
2021-12-28 20:40:49,571 Now training epoch 42. LR=0.000953
2021-12-28 20:42:15,640 Epoch[042/310], Step[0000/1251], Loss: 4.6414(4.6414), Acc: 0.2344(0.2344)
2021-12-28 20:43:18,083 Epoch[042/310], Step[0050/1251], Loss: 4.5984(4.5552), Acc: 0.2627(0.2367)
2021-12-28 20:44:19,879 Epoch[042/310], Step[0100/1251], Loss: 5.0735(4.5557), Acc: 0.2295(0.2448)
2021-12-28 20:45:22,218 Epoch[042/310], Step[0150/1251], Loss: 5.0649(4.5442), Acc: 0.2158(0.2434)
2021-12-28 20:46:24,957 Epoch[042/310], Step[0200/1251], Loss: 4.3156(4.5285), Acc: 0.2822(0.2459)
2021-12-28 20:47:28,593 Epoch[042/310], Step[0250/1251], Loss: 4.6219(4.5324), Acc: 0.2119(0.2483)
2021-12-28 20:48:31,700 Epoch[042/310], Step[0300/1251], Loss: 4.7832(4.5214), Acc: 0.2783(0.2525)
2021-12-28 20:49:34,112 Epoch[042/310], Step[0350/1251], Loss: 4.7473(4.5207), Acc: 0.0879(0.2521)
2021-12-28 20:50:34,818 Epoch[042/310], Step[0400/1251], Loss: 4.2901(4.5126), Acc: 0.2051(0.2527)
2021-12-28 20:51:36,664 Epoch[042/310], Step[0450/1251], Loss: 4.5036(4.5082), Acc: 0.2627(0.2512)
2021-12-28 20:52:39,319 Epoch[042/310], Step[0500/1251], Loss: 4.3512(4.5094), Acc: 0.3145(0.2497)
2021-12-28 20:53:42,848 Epoch[042/310], Step[0550/1251], Loss: 4.7865(4.5069), Acc: 0.1260(0.2488)
2021-12-28 20:54:46,573 Epoch[042/310], Step[0600/1251], Loss: 4.5356(4.5052), Acc: 0.2539(0.2495)
2021-12-28 20:55:48,229 Epoch[042/310], Step[0650/1251], Loss: 4.7875(4.5120), Acc: 0.3145(0.2495)
2021-12-28 20:56:50,527 Epoch[042/310], Step[0700/1251], Loss: 4.6196(4.5066), Acc: 0.3486(0.2496)
2021-12-28 20:57:51,939 Epoch[042/310], Step[0750/1251], Loss: 4.5606(4.5013), Acc: 0.2764(0.2514)
2021-12-28 20:58:53,788 Epoch[042/310], Step[0800/1251], Loss: 4.3473(4.4959), Acc: 0.3867(0.2524)
2021-12-28 20:59:55,362 Epoch[042/310], Step[0850/1251], Loss: 4.7983(4.4987), Acc: 0.2129(0.2521)
2021-12-28 21:00:57,075 Epoch[042/310], Step[0900/1251], Loss: 4.3579(4.5000), Acc: 0.2041(0.2515)
2021-12-28 21:01:59,279 Epoch[042/310], Step[0950/1251], Loss: 4.9607(4.5026), Acc: 0.2734(0.2512)
2021-12-28 21:03:02,890 Epoch[042/310], Step[1000/1251], Loss: 4.6275(4.5016), Acc: 0.2314(0.2508)
2021-12-28 21:04:06,465 Epoch[042/310], Step[1050/1251], Loss: 4.3604(4.5016), Acc: 0.2295(0.2509)
2021-12-28 21:05:09,751 Epoch[042/310], Step[1100/1251], Loss: 4.1629(4.5036), Acc: 0.3672(0.2511)
2021-12-28 21:06:11,681 Epoch[042/310], Step[1150/1251], Loss: 4.8240(4.5007), Acc: 0.2549(0.2511)
2021-12-28 21:07:14,455 Epoch[042/310], Step[1200/1251], Loss: 4.2305(4.4980), Acc: 0.3350(0.2510)
2021-12-28 21:08:16,216 Epoch[042/310], Step[1250/1251], Loss: 4.7280(4.4963), Acc: 0.1738(0.2514)
2021-12-28 21:08:18,150 ----- Validation after Epoch: 42
2021-12-28 21:09:22,173 Val Step[0000/1563], Loss: 1.0068 (1.0068), Acc@1: 0.8125 (0.8125), Acc@5: 0.9375 (0.9375)
2021-12-28 21:09:23,561 Val Step[0050/1563], Loss: 2.5757 (1.2354), Acc@1: 0.4375 (0.7371), Acc@5: 0.6875 (0.9038)
2021-12-28 21:09:25,031 Val Step[0100/1563], Loss: 2.0250 (1.5703), Acc@1: 0.3750 (0.6535), Acc@5: 0.9062 (0.8679)
2021-12-28 21:09:26,510 Val Step[0150/1563], Loss: 0.7022 (1.4879), Acc@1: 0.9062 (0.6714), Acc@5: 1.0000 (0.8781)
2021-12-28 21:09:27,934 Val Step[0200/1563], Loss: 1.7696 (1.5455), Acc@1: 0.5938 (0.6643), Acc@5: 0.8438 (0.8719)
2021-12-28 21:09:29,385 Val Step[0250/1563], Loss: 1.3005 (1.4751), Acc@1: 0.6875 (0.6789), Acc@5: 1.0000 (0.8818)
2021-12-28 21:09:30,856 Val Step[0300/1563], Loss: 1.9650 (1.5445), Acc@1: 0.5000 (0.6567), Acc@5: 0.9062 (0.8752)
2021-12-28 21:09:32,468 Val Step[0350/1563], Loss: 1.9670 (1.5580), Acc@1: 0.5000 (0.6516), Acc@5: 0.8438 (0.8768)
2021-12-28 21:09:34,084 Val Step[0400/1563], Loss: 1.7254 (1.5429), Acc@1: 0.6250 (0.6520), Acc@5: 0.8750 (0.8809)
2021-12-28 21:09:35,892 Val Step[0450/1563], Loss: 1.5642 (1.5507), Acc@1: 0.4062 (0.6480), Acc@5: 0.9688 (0.8817)
2021-12-28 21:09:37,673 Val Step[0500/1563], Loss: 0.5043 (1.5408), Acc@1: 0.9062 (0.6503), Acc@5: 1.0000 (0.8834)
2021-12-28 21:09:39,224 Val Step[0550/1563], Loss: 1.3736 (1.5193), Acc@1: 0.6562 (0.6557), Acc@5: 0.9375 (0.8864)
2021-12-28 21:09:40,774 Val Step[0600/1563], Loss: 1.2397 (1.5241), Acc@1: 0.7188 (0.6560), Acc@5: 0.9375 (0.8858)
2021-12-28 21:09:42,399 Val Step[0650/1563], Loss: 0.9559 (1.5483), Acc@1: 0.9062 (0.6518), Acc@5: 0.9688 (0.8817)
2021-12-28 21:09:43,942 Val Step[0700/1563], Loss: 2.0537 (1.5928), Acc@1: 0.5938 (0.6425), Acc@5: 0.8438 (0.8747)
2021-12-28 21:09:45,518 Val Step[0750/1563], Loss: 1.8497 (1.6389), Acc@1: 0.6875 (0.6332), Acc@5: 0.6875 (0.8673)
2021-12-28 21:09:46,996 Val Step[0800/1563], Loss: 2.6300 (1.6922), Acc@1: 0.4688 (0.6213), Acc@5: 0.6875 (0.8592)
2021-12-28 21:09:48,500 Val Step[0850/1563], Loss: 1.9108 (1.7280), Acc@1: 0.6250 (0.6147), Acc@5: 0.7812 (0.8535)
2021-12-28 21:09:50,083 Val Step[0900/1563], Loss: 0.8191 (1.7337), Acc@1: 0.8750 (0.6146), Acc@5: 0.9375 (0.8517)
2021-12-28 21:09:51,696 Val Step[0950/1563], Loss: 1.7561 (1.7663), Acc@1: 0.6562 (0.6083), Acc@5: 0.8125 (0.8457)
2021-12-28 21:09:53,240 Val Step[1000/1563], Loss: 0.9875 (1.7942), Acc@1: 0.8750 (0.6030), Acc@5: 0.9688 (0.8413)
2021-12-28 21:09:54,768 Val Step[1050/1563], Loss: 0.6581 (1.8128), Acc@1: 0.8750 (0.5998), Acc@5: 0.9688 (0.8388)
2021-12-28 21:09:56,310 Val Step[1100/1563], Loss: 1.7217 (1.8320), Acc@1: 0.6875 (0.5959), Acc@5: 0.7812 (0.8349)
2021-12-28 21:09:57,768 Val Step[1150/1563], Loss: 2.1381 (1.8506), Acc@1: 0.6875 (0.5928), Acc@5: 0.7500 (0.8320)
2021-12-28 21:09:59,340 Val Step[1200/1563], Loss: 2.2074 (1.8703), Acc@1: 0.5938 (0.5893), Acc@5: 0.7500 (0.8288)
2021-12-28 21:10:00,949 Val Step[1250/1563], Loss: 1.1503 (1.8880), Acc@1: 0.7812 (0.5870), Acc@5: 0.9375 (0.8256)
2021-12-28 21:10:02,504 Val Step[1300/1563], Loss: 1.4653 (1.9027), Acc@1: 0.7188 (0.5833), Acc@5: 0.8125 (0.8238)
2021-12-28 21:10:03,994 Val Step[1350/1563], Loss: 2.6970 (1.9237), Acc@1: 0.1562 (0.5796), Acc@5: 0.6875 (0.8200)
2021-12-28 21:10:05,495 Val Step[1400/1563], Loss: 2.0532 (1.9330), Acc@1: 0.5312 (0.5782), Acc@5: 0.7500 (0.8183)
2021-12-28 21:10:06,935 Val Step[1450/1563], Loss: 2.4521 (1.9395), Acc@1: 0.4375 (0.5770), Acc@5: 0.7500 (0.8172)
2021-12-28 21:10:08,455 Val Step[1500/1563], Loss: 2.4989 (1.9254), Acc@1: 0.4062 (0.5796), Acc@5: 0.7812 (0.8196)
2021-12-28 21:10:09,885 Val Step[1550/1563], Loss: 1.2779 (1.9205), Acc@1: 0.8125 (0.5807), Acc@5: 0.9062 (0.8205)
2021-12-28 21:10:10,792 ----- Epoch[042/310], Validation Loss: 1.9181, Validation Acc@1: 0.5813, Validation Acc@5: 0.8207, time: 112.64
2021-12-28 21:10:10,792 ----- Epoch[042/310], Train Loss: 4.4963, Train Acc: 0.2514, time: 1648.58, Best Val(epoch42) Acc@1: 0.5813
2021-12-28 21:10:11,026 Max accuracy so far: 0.5813 at epoch_42
2021-12-28 21:10:11,026 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 21:10:11,026 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 21:10:11,103 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 21:10:11,103 Now training epoch 43. LR=0.000950
2021-12-28 21:11:33,598 Epoch[043/310], Step[0000/1251], Loss: 4.7707(4.7707), Acc: 0.2979(0.2979)
2021-12-28 21:12:35,417 Epoch[043/310], Step[0050/1251], Loss: 4.5820(4.5263), Acc: 0.2393(0.2301)
2021-12-28 21:13:37,818 Epoch[043/310], Step[0100/1251], Loss: 5.1314(4.5086), Acc: 0.2275(0.2426)
2021-12-28 21:14:40,218 Epoch[043/310], Step[0150/1251], Loss: 4.6858(4.4611), Acc: 0.3281(0.2455)
2021-12-28 21:15:42,507 Epoch[043/310], Step[0200/1251], Loss: 3.7538(4.4481), Acc: 0.2100(0.2454)
2021-12-28 21:16:46,182 Epoch[043/310], Step[0250/1251], Loss: 4.6865(4.4463), Acc: 0.1631(0.2478)
2021-12-28 21:17:48,605 Epoch[043/310], Step[0300/1251], Loss: 4.4188(4.4512), Acc: 0.2559(0.2497)
2021-12-28 21:18:50,895 Epoch[043/310], Step[0350/1251], Loss: 4.3978(4.4614), Acc: 0.2793(0.2520)
2021-12-28 21:19:52,901 Epoch[043/310], Step[0400/1251], Loss: 3.9201(4.4582), Acc: 0.3447(0.2541)
2021-12-28 21:20:54,812 Epoch[043/310], Step[0450/1251], Loss: 4.1824(4.4542), Acc: 0.1445(0.2545)
2021-12-28 21:21:57,464 Epoch[043/310], Step[0500/1251], Loss: 5.0265(4.4583), Acc: 0.1797(0.2531)
2021-12-28 21:23:00,523 Epoch[043/310], Step[0550/1251], Loss: 3.8752(4.4535), Acc: 0.4141(0.2556)
2021-12-28 21:24:02,812 Epoch[043/310], Step[0600/1251], Loss: 3.8409(4.4525), Acc: 0.0957(0.2561)
2021-12-28 21:25:06,403 Epoch[043/310], Step[0650/1251], Loss: 5.1226(4.4546), Acc: 0.1699(0.2557)
2021-12-28 21:26:11,036 Epoch[043/310], Step[0700/1251], Loss: 4.2793(4.4582), Acc: 0.3984(0.2550)
2021-12-28 21:27:14,232 Epoch[043/310], Step[0750/1251], Loss: 4.4692(4.4590), Acc: 0.3516(0.2548)
2021-12-28 21:28:17,509 Epoch[043/310], Step[0800/1251], Loss: 4.2345(4.4625), Acc: 0.2725(0.2554)
2021-12-28 21:29:19,843 Epoch[043/310], Step[0850/1251], Loss: 4.6790(4.4696), Acc: 0.2070(0.2549)
2021-12-28 21:30:22,115 Epoch[043/310], Step[0900/1251], Loss: 4.4158(4.4729), Acc: 0.3262(0.2552)
2021-12-28 21:31:23,543 Epoch[043/310], Step[0950/1251], Loss: 5.0702(4.4680), Acc: 0.2529(0.2563)
2021-12-28 21:32:24,295 Epoch[043/310], Step[1000/1251], Loss: 3.9933(4.4721), Acc: 0.3340(0.2564)
2021-12-28 21:33:26,025 Epoch[043/310], Step[1050/1251], Loss: 4.6818(4.4744), Acc: 0.2305(0.2562)
2021-12-28 21:34:28,392 Epoch[043/310], Step[1100/1251], Loss: 4.5706(4.4752), Acc: 0.2197(0.2567)
2021-12-28 21:35:30,678 Epoch[043/310], Step[1150/1251], Loss: 4.5889(4.4749), Acc: 0.1553(0.2567)
2021-12-28 21:36:32,097 Epoch[043/310], Step[1200/1251], Loss: 4.7910(4.4747), Acc: 0.2500(0.2567)
2021-12-28 21:37:35,020 Epoch[043/310], Step[1250/1251], Loss: 4.2464(4.4734), Acc: 0.3799(0.2565)
2021-12-28 21:37:37,082 ----- Epoch[043/310], Train Loss: 4.4734, Train Acc: 0.2565, time: 1645.98, Best Val(epoch42) Acc@1: 0.5813
2021-12-28 21:37:37,082 Now training epoch 44. LR=0.000948
2021-12-28 21:38:59,437 Epoch[044/310], Step[0000/1251], Loss: 4.7976(4.7976), Acc: 0.2051(0.2051)
2021-12-28 21:40:01,389 Epoch[044/310], Step[0050/1251], Loss: 4.6945(4.4593), Acc: 0.3086(0.2499)
2021-12-28 21:41:03,397 Epoch[044/310], Step[0100/1251], Loss: 4.2106(4.4842), Acc: 0.1162(0.2502)
2021-12-28 21:42:05,824 Epoch[044/310], Step[0150/1251], Loss: 4.8394(4.4782), Acc: 0.0264(0.2513)
2021-12-28 21:43:07,945 Epoch[044/310], Step[0200/1251], Loss: 4.8196(4.4624), Acc: 0.2402(0.2523)
2021-12-28 21:44:10,248 Epoch[044/310], Step[0250/1251], Loss: 4.2489(4.4539), Acc: 0.3633(0.2547)
2021-12-28 21:45:13,431 Epoch[044/310], Step[0300/1251], Loss: 3.7657(4.4799), Acc: 0.3428(0.2530)
2021-12-28 21:46:18,008 Epoch[044/310], Step[0350/1251], Loss: 4.5831(4.4710), Acc: 0.3037(0.2523)
2021-12-28 21:47:21,989 Epoch[044/310], Step[0400/1251], Loss: 4.5486(4.4684), Acc: 0.2510(0.2546)
2021-12-28 21:48:24,902 Epoch[044/310], Step[0450/1251], Loss: 5.0626(4.4695), Acc: 0.2266(0.2550)
2021-12-28 21:49:27,573 Epoch[044/310], Step[0500/1251], Loss: 4.5420(4.4651), Acc: 0.3018(0.2542)
2021-12-28 21:50:29,888 Epoch[044/310], Step[0550/1251], Loss: 4.6917(4.4668), Acc: 0.2715(0.2563)
2021-12-28 21:51:33,663 Epoch[044/310], Step[0600/1251], Loss: 4.3517(4.4643), Acc: 0.3057(0.2569)
2021-12-28 21:52:37,641 Epoch[044/310], Step[0650/1251], Loss: 4.2968(4.4616), Acc: 0.3262(0.2586)
2021-12-28 21:53:41,545 Epoch[044/310], Step[0700/1251], Loss: 4.6936(4.4614), Acc: 0.1016(0.2579)
2021-12-28 21:54:45,618 Epoch[044/310], Step[0750/1251], Loss: 4.5504(4.4585), Acc: 0.3057(0.2576)
2021-12-28 21:55:49,338 Epoch[044/310], Step[0800/1251], Loss: 5.2062(4.4612), Acc: 0.0898(0.2566)
2021-12-28 21:56:53,637 Epoch[044/310], Step[0850/1251], Loss: 5.2809(4.4635), Acc: 0.1914(0.2553)
2021-12-28 21:57:56,592 Epoch[044/310], Step[0900/1251], Loss: 4.7939(4.4616), Acc: 0.3252(0.2562)
2021-12-28 21:58:59,980 Epoch[044/310], Step[0950/1251], Loss: 4.3413(4.4564), Acc: 0.3711(0.2566)
2021-12-28 22:00:03,320 Epoch[044/310], Step[1000/1251], Loss: 4.5564(4.4558), Acc: 0.2568(0.2566)
2021-12-28 22:01:07,135 Epoch[044/310], Step[1050/1251], Loss: 4.9716(4.4592), Acc: 0.2109(0.2560)
2021-12-28 22:02:11,193 Epoch[044/310], Step[1100/1251], Loss: 4.4253(4.4633), Acc: 0.3330(0.2553)
2021-12-28 22:03:14,365 Epoch[044/310], Step[1150/1251], Loss: 4.2563(4.4621), Acc: 0.1621(0.2565)
2021-12-28 22:04:17,933 Epoch[044/310], Step[1200/1251], Loss: 3.9334(4.4602), Acc: 0.3867(0.2565)
2021-12-28 22:05:21,103 Epoch[044/310], Step[1250/1251], Loss: 4.8222(4.4627), Acc: 0.2285(0.2561)
2021-12-28 22:05:23,125 ----- Validation after Epoch: 44
2021-12-28 22:06:29,040 Val Step[0000/1563], Loss: 0.9151 (0.9151), Acc@1: 0.9062 (0.9062), Acc@5: 0.9375 (0.9375)
2021-12-28 22:06:30,812 Val Step[0050/1563], Loss: 2.6126 (1.2753), Acc@1: 0.4062 (0.7445), Acc@5: 0.8125 (0.9099)
2021-12-28 22:06:32,313 Val Step[0100/1563], Loss: 2.6576 (1.6610), Acc@1: 0.1875 (0.6423), Acc@5: 0.7812 (0.8632)
2021-12-28 22:06:33,789 Val Step[0150/1563], Loss: 1.2710 (1.5603), Acc@1: 0.7812 (0.6639), Acc@5: 0.9375 (0.8748)
2021-12-28 22:06:35,426 Val Step[0200/1563], Loss: 1.5294 (1.6062), Acc@1: 0.7500 (0.6606), Acc@5: 0.9062 (0.8674)
2021-12-28 22:06:36,851 Val Step[0250/1563], Loss: 1.2700 (1.5237), Acc@1: 0.6875 (0.6768), Acc@5: 0.9688 (0.8789)
2021-12-28 22:06:38,304 Val Step[0300/1563], Loss: 1.8355 (1.5809), Acc@1: 0.5625 (0.6561), Acc@5: 0.9062 (0.8734)
2021-12-28 22:06:39,759 Val Step[0350/1563], Loss: 1.6197 (1.5860), Acc@1: 0.6562 (0.6495), Acc@5: 0.9062 (0.8762)
2021-12-28 22:06:41,254 Val Step[0400/1563], Loss: 1.7542 (1.5801), Acc@1: 0.6562 (0.6464), Acc@5: 0.8438 (0.8798)
2021-12-28 22:06:42,837 Val Step[0450/1563], Loss: 1.3809 (1.5857), Acc@1: 0.5625 (0.6428), Acc@5: 1.0000 (0.8800)
2021-12-28 22:06:44,423 Val Step[0500/1563], Loss: 0.7865 (1.5890), Acc@1: 0.8438 (0.6439), Acc@5: 0.9688 (0.8798)
2021-12-28 22:06:46,017 Val Step[0550/1563], Loss: 1.3234 (1.5589), Acc@1: 0.7500 (0.6516), Acc@5: 0.9375 (0.8834)
2021-12-28 22:06:47,651 Val Step[0600/1563], Loss: 1.0136 (1.5616), Acc@1: 0.8125 (0.6523), Acc@5: 0.9688 (0.8825)
2021-12-28 22:06:49,147 Val Step[0650/1563], Loss: 1.4012 (1.5871), Acc@1: 0.6875 (0.6483), Acc@5: 0.9375 (0.8790)
2021-12-28 22:06:50,596 Val Step[0700/1563], Loss: 1.7197 (1.6334), Acc@1: 0.6875 (0.6392), Acc@5: 0.8750 (0.8717)
2021-12-28 22:06:52,176 Val Step[0750/1563], Loss: 2.1200 (1.6795), Acc@1: 0.5938 (0.6303), Acc@5: 0.7188 (0.8645)
2021-12-28 22:06:53,737 Val Step[0800/1563], Loss: 1.8178 (1.7260), Acc@1: 0.5938 (0.6205), Acc@5: 0.8438 (0.8572)
2021-12-28 22:06:55,318 Val Step[0850/1563], Loss: 2.0676 (1.7559), Acc@1: 0.6250 (0.6150), Acc@5: 0.8125 (0.8529)
2021-12-28 22:06:56,929 Val Step[0900/1563], Loss: 0.6625 (1.7600), Acc@1: 0.9062 (0.6162), Acc@5: 0.9062 (0.8513)
2021-12-28 22:06:58,465 Val Step[0950/1563], Loss: 2.3231 (1.7893), Acc@1: 0.6250 (0.6107), Acc@5: 0.7500 (0.8469)
2021-12-28 22:07:00,043 Val Step[1000/1563], Loss: 0.6148 (1.8171), Acc@1: 0.9375 (0.6048), Acc@5: 1.0000 (0.8420)
2021-12-28 22:07:01,538 Val Step[1050/1563], Loss: 0.7151 (1.8354), Acc@1: 0.9375 (0.6011), Acc@5: 0.9688 (0.8396)
2021-12-28 22:07:03,095 Val Step[1100/1563], Loss: 1.8247 (1.8555), Acc@1: 0.6562 (0.5968), Acc@5: 0.7812 (0.8358)
2021-12-28 22:07:04,706 Val Step[1150/1563], Loss: 1.8556 (1.8756), Acc@1: 0.6875 (0.5932), Acc@5: 0.7812 (0.8325)
2021-12-28 22:07:06,221 Val Step[1200/1563], Loss: 1.6340 (1.8954), Acc@1: 0.7188 (0.5892), Acc@5: 0.8750 (0.8293)
2021-12-28 22:07:07,735 Val Step[1250/1563], Loss: 1.4334 (1.9159), Acc@1: 0.6875 (0.5859), Acc@5: 0.8750 (0.8254)
2021-12-28 22:07:09,299 Val Step[1300/1563], Loss: 1.4537 (1.9310), Acc@1: 0.6875 (0.5825), Acc@5: 0.8438 (0.8233)
2021-12-28 22:07:10,847 Val Step[1350/1563], Loss: 2.7311 (1.9542), Acc@1: 0.2500 (0.5779), Acc@5: 0.7188 (0.8195)
2021-12-28 22:07:12,455 Val Step[1400/1563], Loss: 1.9168 (1.9648), Acc@1: 0.5938 (0.5758), Acc@5: 0.8125 (0.8178)
2021-12-28 22:07:14,018 Val Step[1450/1563], Loss: 2.4983 (1.9705), Acc@1: 0.4375 (0.5751), Acc@5: 0.7812 (0.8167)
2021-12-28 22:07:15,516 Val Step[1500/1563], Loss: 2.4206 (1.9557), Acc@1: 0.4062 (0.5782), Acc@5: 0.8125 (0.8188)
2021-12-28 22:07:17,040 Val Step[1550/1563], Loss: 1.2853 (1.9532), Acc@1: 0.8438 (0.5785), Acc@5: 0.9062 (0.8190)
2021-12-28 22:07:17,941 ----- Epoch[044/310], Validation Loss: 1.9498, Validation Acc@1: 0.5792, Validation Acc@5: 0.8194, time: 114.81
2021-12-28 22:07:17,942 ----- Epoch[044/310], Train Loss: 4.4627, Train Acc: 0.2561, time: 1666.04, Best Val(epoch42) Acc@1: 0.5813
2021-12-28 22:07:17,942 Now training epoch 45. LR=0.000946
2021-12-28 22:08:42,551 Epoch[045/310], Step[0000/1251], Loss: 3.9298(3.9298), Acc: 0.4385(0.4385)
2021-12-28 22:09:43,382 Epoch[045/310], Step[0050/1251], Loss: 4.3735(4.4318), Acc: 0.2461(0.2724)
2021-12-28 22:10:45,121 Epoch[045/310], Step[0100/1251], Loss: 4.2864(4.4302), Acc: 0.3877(0.2667)
2021-12-28 22:11:45,201 Epoch[045/310], Step[0150/1251], Loss: 4.4192(4.4098), Acc: 0.3604(0.2694)
2021-12-28 22:12:46,913 Epoch[045/310], Step[0200/1251], Loss: 3.4894(4.4127), Acc: 0.2471(0.2684)
2021-12-28 22:13:49,028 Epoch[045/310], Step[0250/1251], Loss: 4.4738(4.4091), Acc: 0.0859(0.2673)
2021-12-28 22:14:51,298 Epoch[045/310], Step[0300/1251], Loss: 4.3576(4.4128), Acc: 0.1797(0.2667)
2021-12-28 22:15:54,450 Epoch[045/310], Step[0350/1251], Loss: 4.4346(4.4235), Acc: 0.1309(0.2639)
2021-12-28 22:16:56,771 Epoch[045/310], Step[0400/1251], Loss: 4.7368(4.4263), Acc: 0.3164(0.2640)
2021-12-28 22:17:59,502 Epoch[045/310], Step[0450/1251], Loss: 4.2426(4.4322), Acc: 0.2666(0.2623)
2021-12-28 22:19:02,349 Epoch[045/310], Step[0500/1251], Loss: 4.4321(4.4318), Acc: 0.3369(0.2603)
2021-12-28 22:20:04,723 Epoch[045/310], Step[0550/1251], Loss: 3.7636(4.4348), Acc: 0.1943(0.2604)
2021-12-28 22:21:06,070 Epoch[045/310], Step[0600/1251], Loss: 4.3211(4.4371), Acc: 0.2852(0.2583)
2021-12-28 22:22:09,550 Epoch[045/310], Step[0650/1251], Loss: 4.9394(4.4383), Acc: 0.1445(0.2569)
2021-12-28 22:23:12,490 Epoch[045/310], Step[0700/1251], Loss: 3.8976(4.4358), Acc: 0.3945(0.2569)
2021-12-28 22:24:13,675 Epoch[045/310], Step[0750/1251], Loss: 4.3558(4.4353), Acc: 0.2607(0.2568)
2021-12-28 22:25:15,231 Epoch[045/310], Step[0800/1251], Loss: 4.0880(4.4350), Acc: 0.4355(0.2577)
2021-12-28 22:26:17,867 Epoch[045/310], Step[0850/1251], Loss: 4.1109(4.4361), Acc: 0.3291(0.2575)
2021-12-28 22:27:19,051 Epoch[045/310], Step[0900/1251], Loss: 4.0592(4.4370), Acc: 0.3213(0.2576)
2021-12-28 22:28:21,976 Epoch[045/310], Step[0950/1251], Loss: 4.6769(4.4408), Acc: 0.2588(0.2571)
2021-12-28 22:29:25,325 Epoch[045/310], Step[1000/1251], Loss: 4.5912(4.4399), Acc: 0.2031(0.2571)
2021-12-28 22:30:27,730 Epoch[045/310], Step[1050/1251], Loss: 4.4728(4.4421), Acc: 0.3086(0.2577)
2021-12-28 22:31:31,486 Epoch[045/310], Step[1100/1251], Loss: 4.3309(4.4422), Acc: 0.2344(0.2569)
2021-12-28 22:32:33,413 Epoch[045/310], Step[1150/1251], Loss: 4.3787(4.4405), Acc: 0.2793(0.2571)
2021-12-28 22:33:36,081 Epoch[045/310], Step[1200/1251], Loss: 4.1373(4.4395), Acc: 0.2910(0.2569)
2021-12-28 22:34:37,585 Epoch[045/310], Step[1250/1251], Loss: 4.1009(4.4349), Acc: 0.4014(0.2583)
2021-12-28 22:34:39,499 ----- Epoch[045/310], Train Loss: 4.4349, Train Acc: 0.2583, time: 1641.55, Best Val(epoch42) Acc@1: 0.5813
2021-12-28 22:34:39,500 Now training epoch 46. LR=0.000943
2021-12-28 22:36:06,414 Epoch[046/310], Step[0000/1251], Loss: 4.8060(4.8060), Acc: 0.2324(0.2324)
2021-12-28 22:37:09,311 Epoch[046/310], Step[0050/1251], Loss: 4.6692(4.5189), Acc: 0.1279(0.2491)
2021-12-28 22:38:12,715 Epoch[046/310], Step[0100/1251], Loss: 4.8094(4.4688), Acc: 0.2568(0.2593)
2021-12-28 22:39:14,169 Epoch[046/310], Step[0150/1251], Loss: 4.4109(4.4642), Acc: 0.2266(0.2639)
2021-12-28 22:40:17,931 Epoch[046/310], Step[0200/1251], Loss: 4.4465(4.4511), Acc: 0.3418(0.2594)
2021-12-28 22:41:20,826 Epoch[046/310], Step[0250/1251], Loss: 5.0292(4.4429), Acc: 0.2236(0.2619)
2021-12-28 22:42:23,227 Epoch[046/310], Step[0300/1251], Loss: 4.6678(4.4416), Acc: 0.3047(0.2615)
2021-12-28 22:43:25,377 Epoch[046/310], Step[0350/1251], Loss: 4.2807(4.4325), Acc: 0.1719(0.2601)
2021-12-28 22:44:28,816 Epoch[046/310], Step[0400/1251], Loss: 4.0668(4.4298), Acc: 0.2988(0.2600)
2021-12-28 22:45:31,138 Epoch[046/310], Step[0450/1251], Loss: 4.4882(4.4381), Acc: 0.1514(0.2592)
2021-12-28 22:46:32,973 Epoch[046/310], Step[0500/1251], Loss: 4.3572(4.4419), Acc: 0.3701(0.2597)
2021-12-28 22:47:35,371 Epoch[046/310], Step[0550/1251], Loss: 4.2876(4.4408), Acc: 0.3564(0.2589)
2021-12-28 22:48:37,710 Epoch[046/310], Step[0600/1251], Loss: 4.5779(4.4374), Acc: 0.3359(0.2606)
2021-12-28 22:49:40,390 Epoch[046/310], Step[0650/1251], Loss: 4.4107(4.4412), Acc: 0.3857(0.2594)
2021-12-28 22:50:43,102 Epoch[046/310], Step[0700/1251], Loss: 4.6133(4.4442), Acc: 0.1855(0.2593)
2021-12-28 22:51:47,090 Epoch[046/310], Step[0750/1251], Loss: 4.8687(4.4449), Acc: 0.2861(0.2583)
2021-12-28 22:52:49,879 Epoch[046/310], Step[0800/1251], Loss: 4.2863(4.4441), Acc: 0.2461(0.2593)
2021-12-28 22:53:53,303 Epoch[046/310], Step[0850/1251], Loss: 4.9434(4.4496), Acc: 0.2744(0.2583)
2021-12-28 22:54:56,090 Epoch[046/310], Step[0900/1251], Loss: 4.4632(4.4467), Acc: 0.3496(0.2589)
2021-12-28 22:55:59,058 Epoch[046/310], Step[0950/1251], Loss: 4.3091(4.4455), Acc: 0.3232(0.2599)
2021-12-28 22:57:01,579 Epoch[046/310], Step[1000/1251], Loss: 4.5471(4.4412), Acc: 0.1260(0.2594)
2021-12-28 22:58:04,535 Epoch[046/310], Step[1050/1251], Loss: 4.8762(4.4407), Acc: 0.1475(0.2595)
2021-12-28 22:59:08,887 Epoch[046/310], Step[1100/1251], Loss: 4.5917(4.4416), Acc: 0.1064(0.2588)
2021-12-28 23:00:12,020 Epoch[046/310], Step[1150/1251], Loss: 4.6486(4.4392), Acc: 0.2432(0.2581)
2021-12-28 23:01:15,776 Epoch[046/310], Step[1200/1251], Loss: 4.5480(4.4351), Acc: 0.2656(0.2582)
2021-12-28 23:02:17,340 Epoch[046/310], Step[1250/1251], Loss: 4.8547(4.4345), Acc: 0.2910(0.2584)
2021-12-28 23:02:19,301 ----- Validation after Epoch: 46
2021-12-28 23:03:24,442 Val Step[0000/1563], Loss: 0.8234 (0.8234), Acc@1: 0.8750 (0.8750), Acc@5: 0.9375 (0.9375)
2021-12-28 23:03:26,027 Val Step[0050/1563], Loss: 3.1215 (1.2565), Acc@1: 0.3438 (0.7408), Acc@5: 0.6250 (0.9032)
2021-12-28 23:03:27,539 Val Step[0100/1563], Loss: 2.4630 (1.5904), Acc@1: 0.1250 (0.6522), Acc@5: 0.8438 (0.8639)
2021-12-28 23:03:29,088 Val Step[0150/1563], Loss: 0.6624 (1.4913), Acc@1: 0.8438 (0.6765), Acc@5: 1.0000 (0.8771)
2021-12-28 23:03:30,601 Val Step[0200/1563], Loss: 1.6574 (1.5245), Acc@1: 0.5312 (0.6741), Acc@5: 0.8750 (0.8727)
2021-12-28 23:03:32,099 Val Step[0250/1563], Loss: 1.3755 (1.4531), Acc@1: 0.6562 (0.6900), Acc@5: 1.0000 (0.8841)
2021-12-28 23:03:33,651 Val Step[0300/1563], Loss: 1.6388 (1.5410), Acc@1: 0.5625 (0.6613), Acc@5: 0.9062 (0.8750)
2021-12-28 23:03:35,296 Val Step[0350/1563], Loss: 1.6172 (1.5517), Acc@1: 0.5938 (0.6530), Acc@5: 0.9375 (0.8775)
2021-12-28 23:03:36,783 Val Step[0400/1563], Loss: 1.0407 (1.5391), Acc@1: 0.7812 (0.6506), Acc@5: 0.9688 (0.8814)
2021-12-28 23:03:38,395 Val Step[0450/1563], Loss: 1.2092 (1.5410), Acc@1: 0.6562 (0.6485), Acc@5: 0.9688 (0.8819)
2021-12-28 23:03:39,868 Val Step[0500/1563], Loss: 0.8269 (1.5315), Acc@1: 0.8750 (0.6511), Acc@5: 1.0000 (0.8833)
2021-12-28 23:03:41,433 Val Step[0550/1563], Loss: 1.0515 (1.5046), Acc@1: 0.7500 (0.6579), Acc@5: 0.9375 (0.8861)
2021-12-28 23:03:43,042 Val Step[0600/1563], Loss: 1.1347 (1.5066), Acc@1: 0.7500 (0.6581), Acc@5: 0.9062 (0.8862)
2021-12-28 23:03:44,611 Val Step[0650/1563], Loss: 1.2852 (1.5300), Acc@1: 0.6875 (0.6554), Acc@5: 0.9375 (0.8822)
2021-12-28 23:03:46,206 Val Step[0700/1563], Loss: 1.8580 (1.5761), Acc@1: 0.6250 (0.6460), Acc@5: 0.7812 (0.8754)
2021-12-28 23:03:47,730 Val Step[0750/1563], Loss: 2.3730 (1.6252), Acc@1: 0.4375 (0.6369), Acc@5: 0.7188 (0.8675)
2021-12-28 23:03:49,181 Val Step[0800/1563], Loss: 2.1151 (1.6767), Acc@1: 0.5938 (0.6257), Acc@5: 0.8125 (0.8599)
2021-12-28 23:03:50,622 Val Step[0850/1563], Loss: 1.8247 (1.7060), Acc@1: 0.5625 (0.6205), Acc@5: 0.8750 (0.8556)
2021-12-28 23:03:52,188 Val Step[0900/1563], Loss: 0.5950 (1.7075), Acc@1: 0.8750 (0.6222), Acc@5: 0.9062 (0.8540)
2021-12-28 23:03:53,727 Val Step[0950/1563], Loss: 1.7510 (1.7366), Acc@1: 0.7500 (0.6171), Acc@5: 0.7812 (0.8490)
2021-12-28 23:03:55,153 Val Step[1000/1563], Loss: 0.6500 (1.7672), Acc@1: 0.9062 (0.6110), Acc@5: 0.9688 (0.8443)
2021-12-28 23:03:56,667 Val Step[1050/1563], Loss: 0.6739 (1.7833), Acc@1: 0.8750 (0.6079), Acc@5: 0.9688 (0.8417)
2021-12-28 23:03:58,167 Val Step[1100/1563], Loss: 1.6511 (1.8048), Acc@1: 0.6875 (0.6038), Acc@5: 0.8125 (0.8378)
2021-12-28 23:03:59,684 Val Step[1150/1563], Loss: 1.9830 (1.8245), Acc@1: 0.7500 (0.6003), Acc@5: 0.7500 (0.8348)
2021-12-28 23:04:01,124 Val Step[1200/1563], Loss: 1.7171 (1.8454), Acc@1: 0.7188 (0.5970), Acc@5: 0.8438 (0.8312)
2021-12-28 23:04:02,639 Val Step[1250/1563], Loss: 1.4936 (1.8649), Acc@1: 0.7500 (0.5938), Acc@5: 0.9062 (0.8275)
2021-12-28 23:04:04,114 Val Step[1300/1563], Loss: 1.4152 (1.8768), Acc@1: 0.7500 (0.5915), Acc@5: 0.8438 (0.8260)
2021-12-28 23:04:05,592 Val Step[1350/1563], Loss: 2.8159 (1.8994), Acc@1: 0.2188 (0.5862), Acc@5: 0.6562 (0.8224)
2021-12-28 23:04:07,060 Val Step[1400/1563], Loss: 1.9697 (1.9098), Acc@1: 0.5938 (0.5841), Acc@5: 0.7812 (0.8207)
2021-12-28 23:04:08,546 Val Step[1450/1563], Loss: 2.1603 (1.9186), Acc@1: 0.3750 (0.5822), Acc@5: 0.8438 (0.8194)
2021-12-28 23:04:10,014 Val Step[1500/1563], Loss: 2.0918 (1.9032), Acc@1: 0.5000 (0.5851), Acc@5: 0.8750 (0.8217)
2021-12-28 23:04:11,471 Val Step[1550/1563], Loss: 1.1162 (1.8987), Acc@1: 0.8750 (0.5860), Acc@5: 0.9062 (0.8222)
2021-12-28 23:04:12,330 ----- Epoch[046/310], Validation Loss: 1.8961, Validation Acc@1: 0.5865, Validation Acc@5: 0.8226, time: 113.03
2021-12-28 23:04:12,330 ----- Epoch[046/310], Train Loss: 4.4345, Train Acc: 0.2584, time: 1659.80, Best Val(epoch46) Acc@1: 0.5865
2021-12-28 23:04:12,541 Max accuracy so far: 0.5865 at epoch_46
2021-12-28 23:04:12,541 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-28 23:04:12,541 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-28 23:04:12,624 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-28 23:04:12,625 Now training epoch 47. LR=0.000941
2021-12-28 23:05:41,315 Epoch[047/310], Step[0000/1251], Loss: 4.0672(4.0672), Acc: 0.3145(0.3145)
2021-12-28 23:06:44,860 Epoch[047/310], Step[0050/1251], Loss: 4.0895(4.4167), Acc: 0.1963(0.2559)
2021-12-28 23:07:47,157 Epoch[047/310], Step[0100/1251], Loss: 4.7190(4.4290), Acc: 0.1934(0.2502)
2021-12-28 23:08:50,012 Epoch[047/310], Step[0150/1251], Loss: 4.6558(4.4261), Acc: 0.2031(0.2541)
2021-12-28 23:09:51,429 Epoch[047/310], Step[0200/1251], Loss: 4.4627(4.4324), Acc: 0.3027(0.2570)
2021-12-28 23:10:54,854 Epoch[047/310], Step[0250/1251], Loss: 3.7337(4.4167), Acc: 0.2393(0.2568)
2021-12-28 23:11:57,268 Epoch[047/310], Step[0300/1251], Loss: 4.3648(4.4028), Acc: 0.2363(0.2610)
2021-12-28 23:12:59,265 Epoch[047/310], Step[0350/1251], Loss: 4.6071(4.3973), Acc: 0.2637(0.2612)
2021-12-28 23:14:02,302 Epoch[047/310], Step[0400/1251], Loss: 4.3277(4.4069), Acc: 0.1318(0.2584)
2021-12-28 23:15:04,573 Epoch[047/310], Step[0450/1251], Loss: 4.9430(4.4125), Acc: 0.1973(0.2584)
2021-12-28 23:16:05,025 Epoch[047/310], Step[0500/1251], Loss: 4.6244(4.4213), Acc: 0.2539(0.2572)
2021-12-28 23:17:04,768 Epoch[047/310], Step[0550/1251], Loss: 4.3911(4.4207), Acc: 0.2959(0.2586)
2021-12-28 23:18:05,864 Epoch[047/310], Step[0600/1251], Loss: 4.3550(4.4224), Acc: 0.2715(0.2581)
2021-12-28 23:19:08,295 Epoch[047/310], Step[0650/1251], Loss: 4.9448(4.4205), Acc: 0.1992(0.2587)
2021-12-28 23:20:11,106 Epoch[047/310], Step[0700/1251], Loss: 4.2957(4.4260), Acc: 0.2412(0.2577)
2021-12-28 23:21:14,117 Epoch[047/310], Step[0750/1251], Loss: 4.3243(4.4246), Acc: 0.3965(0.2581)
2021-12-28 23:22:15,950 Epoch[047/310], Step[0800/1251], Loss: 4.6688(4.4200), Acc: 0.2676(0.2594)
2021-12-28 23:23:18,125 Epoch[047/310], Step[0850/1251], Loss: 4.1921(4.4159), Acc: 0.1572(0.2598)
2021-12-28 23:24:21,241 Epoch[047/310], Step[0900/1251], Loss: 4.9015(4.4218), Acc: 0.2148(0.2597)
2021-12-28 23:25:23,917 Epoch[047/310], Step[0950/1251], Loss: 4.3984(4.4236), Acc: 0.3652(0.2588)
2021-12-28 23:26:26,145 Epoch[047/310], Step[1000/1251], Loss: 4.1630(4.4225), Acc: 0.4248(0.2592)
2021-12-28 23:27:27,409 Epoch[047/310], Step[1050/1251], Loss: 4.7373(4.4242), Acc: 0.1738(0.2598)
2021-12-28 23:28:29,070 Epoch[047/310], Step[1100/1251], Loss: 4.1754(4.4222), Acc: 0.3398(0.2598)
2021-12-28 23:29:31,354 Epoch[047/310], Step[1150/1251], Loss: 4.3652(4.4262), Acc: 0.1533(0.2594)
2021-12-28 23:30:33,885 Epoch[047/310], Step[1200/1251], Loss: 4.8512(4.4276), Acc: 0.2275(0.2587)
2021-12-28 23:31:36,396 Epoch[047/310], Step[1250/1251], Loss: 4.5773(4.4323), Acc: 0.3584(0.2583)
2021-12-28 23:31:38,618 ----- Epoch[047/310], Train Loss: 4.4323, Train Acc: 0.2583, time: 1645.99, Best Val(epoch46) Acc@1: 0.5865
2021-12-28 23:31:38,618 Now training epoch 48. LR=0.000938
2021-12-28 23:33:07,675 Epoch[048/310], Step[0000/1251], Loss: 4.6945(4.6945), Acc: 0.2656(0.2656)
2021-12-28 23:34:11,098 Epoch[048/310], Step[0050/1251], Loss: 4.2844(4.3792), Acc: 0.2588(0.2656)
2021-12-28 23:35:13,062 Epoch[048/310], Step[0100/1251], Loss: 4.4586(4.4229), Acc: 0.2988(0.2687)
2021-12-28 23:36:15,916 Epoch[048/310], Step[0150/1251], Loss: 4.4900(4.4136), Acc: 0.1953(0.2653)
2021-12-28 23:37:18,444 Epoch[048/310], Step[0200/1251], Loss: 4.2561(4.4274), Acc: 0.3604(0.2647)
2021-12-28 23:38:20,824 Epoch[048/310], Step[0250/1251], Loss: 4.2206(4.4186), Acc: 0.2676(0.2659)
2021-12-28 23:39:22,841 Epoch[048/310], Step[0300/1251], Loss: 4.4004(4.4258), Acc: 0.3428(0.2640)
2021-12-28 23:40:24,543 Epoch[048/310], Step[0350/1251], Loss: 4.0854(4.4178), Acc: 0.4492(0.2652)
2021-12-28 23:41:27,198 Epoch[048/310], Step[0400/1251], Loss: 4.3361(4.4166), Acc: 0.0420(0.2633)
2021-12-28 23:42:29,866 Epoch[048/310], Step[0450/1251], Loss: 4.2284(4.4209), Acc: 0.2236(0.2632)
2021-12-28 23:43:32,948 Epoch[048/310], Step[0500/1251], Loss: 4.1597(4.4238), Acc: 0.3438(0.2620)
2021-12-28 23:44:34,910 Epoch[048/310], Step[0550/1251], Loss: 4.6328(4.4241), Acc: 0.2754(0.2618)
2021-12-28 23:45:36,137 Epoch[048/310], Step[0600/1251], Loss: 4.2277(4.4220), Acc: 0.2031(0.2626)
2021-12-28 23:46:38,198 Epoch[048/310], Step[0650/1251], Loss: 4.6811(4.4269), Acc: 0.2119(0.2618)
2021-12-28 23:47:39,266 Epoch[048/310], Step[0700/1251], Loss: 4.5569(4.4214), Acc: 0.1680(0.2620)
2021-12-28 23:48:41,434 Epoch[048/310], Step[0750/1251], Loss: 4.3667(4.4206), Acc: 0.2871(0.2628)
2021-12-28 23:49:42,616 Epoch[048/310], Step[0800/1251], Loss: 4.5683(4.4222), Acc: 0.1465(0.2617)
2021-12-28 23:50:46,194 Epoch[048/310], Step[0850/1251], Loss: 4.3579(4.4214), Acc: 0.2549(0.2613)
2021-12-28 23:51:49,600 Epoch[048/310], Step[0900/1251], Loss: 3.8609(4.4173), Acc: 0.3643(0.2606)
2021-12-28 23:52:52,255 Epoch[048/310], Step[0950/1251], Loss: 4.1708(4.4163), Acc: 0.2324(0.2602)
2021-12-28 23:53:56,084 Epoch[048/310], Step[1000/1251], Loss: 4.5974(4.4124), Acc: 0.2842(0.2598)
2021-12-28 23:54:59,374 Epoch[048/310], Step[1050/1251], Loss: 4.2776(4.4132), Acc: 0.1758(0.2594)
2021-12-28 23:56:03,306 Epoch[048/310], Step[1100/1251], Loss: 4.7222(4.4171), Acc: 0.2266(0.2585)
2021-12-28 23:57:06,924 Epoch[048/310], Step[1150/1251], Loss: 4.4396(4.4147), Acc: 0.3867(0.2591)
2021-12-28 23:58:09,137 Epoch[048/310], Step[1200/1251], Loss: 4.4692(4.4165), Acc: 0.2559(0.2591)
2021-12-28 23:59:11,196 Epoch[048/310], Step[1250/1251], Loss: 3.8954(4.4180), Acc: 0.4492(0.2596)
2021-12-28 23:59:13,299 ----- Validation after Epoch: 48
2021-12-29 00:00:18,067 Val Step[0000/1563], Loss: 0.9239 (0.9239), Acc@1: 0.8438 (0.8438), Acc@5: 0.9375 (0.9375)
2021-12-29 00:00:19,716 Val Step[0050/1563], Loss: 2.3595 (1.2308), Acc@1: 0.5000 (0.7433), Acc@5: 0.7500 (0.9118)
2021-12-29 00:00:21,225 Val Step[0100/1563], Loss: 2.1342 (1.6014), Acc@1: 0.2812 (0.6485), Acc@5: 0.8750 (0.8639)
2021-12-29 00:00:22,726 Val Step[0150/1563], Loss: 0.9131 (1.4866), Acc@1: 0.7500 (0.6786), Acc@5: 0.9688 (0.8791)
2021-12-29 00:00:24,221 Val Step[0200/1563], Loss: 1.5948 (1.5317), Acc@1: 0.6562 (0.6735), Acc@5: 0.8750 (0.8719)
2021-12-29 00:00:25,673 Val Step[0250/1563], Loss: 1.0723 (1.4507), Acc@1: 0.7500 (0.6906), Acc@5: 0.9688 (0.8841)
2021-12-29 00:00:27,148 Val Step[0300/1563], Loss: 1.9549 (1.5136), Acc@1: 0.5938 (0.6690), Acc@5: 0.8750 (0.8800)
2021-12-29 00:00:28,645 Val Step[0350/1563], Loss: 1.5642 (1.5260), Acc@1: 0.6875 (0.6614), Acc@5: 0.9062 (0.8816)
2021-12-29 00:00:30,180 Val Step[0400/1563], Loss: 1.9253 (1.5215), Acc@1: 0.6562 (0.6581), Acc@5: 0.9062 (0.8843)
2021-12-29 00:00:31,694 Val Step[0450/1563], Loss: 1.6095 (1.5207), Acc@1: 0.2812 (0.6540), Acc@5: 0.9688 (0.8855)
2021-12-29 00:00:33,244 Val Step[0500/1563], Loss: 0.5397 (1.5101), Acc@1: 0.9375 (0.6576), Acc@5: 1.0000 (0.8875)
2021-12-29 00:00:35,081 Val Step[0550/1563], Loss: 1.0376 (1.4853), Acc@1: 0.7500 (0.6645), Acc@5: 0.9688 (0.8902)
2021-12-29 00:00:36,603 Val Step[0600/1563], Loss: 1.1811 (1.4909), Acc@1: 0.7188 (0.6639), Acc@5: 0.9062 (0.8898)
2021-12-29 00:00:38,045 Val Step[0650/1563], Loss: 0.9781 (1.5153), Acc@1: 0.8438 (0.6607), Acc@5: 0.9688 (0.8855)
2021-12-29 00:00:39,595 Val Step[0700/1563], Loss: 1.8500 (1.5586), Acc@1: 0.6250 (0.6521), Acc@5: 0.7500 (0.8786)
2021-12-29 00:00:41,140 Val Step[0750/1563], Loss: 2.1034 (1.6058), Acc@1: 0.5000 (0.6432), Acc@5: 0.8125 (0.8713)
2021-12-29 00:00:42,554 Val Step[0800/1563], Loss: 1.6599 (1.6559), Acc@1: 0.7188 (0.6323), Acc@5: 0.8438 (0.8634)
2021-12-29 00:00:44,088 Val Step[0850/1563], Loss: 1.7299 (1.6880), Acc@1: 0.6562 (0.6264), Acc@5: 0.8438 (0.8582)
2021-12-29 00:00:45,661 Val Step[0900/1563], Loss: 0.8064 (1.6915), Acc@1: 0.8438 (0.6277), Acc@5: 0.9062 (0.8567)
2021-12-29 00:00:47,278 Val Step[0950/1563], Loss: 2.0109 (1.7204), Acc@1: 0.5938 (0.6222), Acc@5: 0.7812 (0.8518)
2021-12-29 00:00:48,736 Val Step[1000/1563], Loss: 0.6750 (1.7492), Acc@1: 0.9375 (0.6156), Acc@5: 0.9688 (0.8476)
2021-12-29 00:00:50,235 Val Step[1050/1563], Loss: 0.5421 (1.7663), Acc@1: 0.9375 (0.6125), Acc@5: 0.9688 (0.8449)
2021-12-29 00:00:51,789 Val Step[1100/1563], Loss: 1.5679 (1.7866), Acc@1: 0.7500 (0.6090), Acc@5: 0.8125 (0.8414)
2021-12-29 00:00:53,314 Val Step[1150/1563], Loss: 1.6049 (1.8030), Acc@1: 0.7188 (0.6063), Acc@5: 0.7812 (0.8389)
2021-12-29 00:00:54,812 Val Step[1200/1563], Loss: 1.6711 (1.8176), Acc@1: 0.7500 (0.6037), Acc@5: 0.8438 (0.8364)
2021-12-29 00:00:56,273 Val Step[1250/1563], Loss: 1.3292 (1.8329), Acc@1: 0.7812 (0.6020), Acc@5: 0.9062 (0.8335)
2021-12-29 00:00:57,762 Val Step[1300/1563], Loss: 1.3452 (1.8466), Acc@1: 0.7812 (0.5988), Acc@5: 0.8438 (0.8317)
2021-12-29 00:00:59,223 Val Step[1350/1563], Loss: 2.7055 (1.8694), Acc@1: 0.2500 (0.5944), Acc@5: 0.7188 (0.8279)
2021-12-29 00:01:00,681 Val Step[1400/1563], Loss: 1.8197 (1.8788), Acc@1: 0.5625 (0.5923), Acc@5: 0.8125 (0.8264)
2021-12-29 00:01:02,098 Val Step[1450/1563], Loss: 2.0039 (1.8866), Acc@1: 0.5938 (0.5908), Acc@5: 0.8750 (0.8252)
2021-12-29 00:01:03,627 Val Step[1500/1563], Loss: 2.1821 (1.8752), Acc@1: 0.5000 (0.5933), Acc@5: 0.9062 (0.8270)
2021-12-29 00:01:05,078 Val Step[1550/1563], Loss: 1.0279 (1.8741), Acc@1: 0.8750 (0.5934), Acc@5: 0.9062 (0.8271)
2021-12-29 00:01:05,961 ----- Epoch[048/310], Validation Loss: 1.8718, Validation Acc@1: 0.5939, Validation Acc@5: 0.8274, time: 112.66
2021-12-29 00:01:05,962 ----- Epoch[048/310], Train Loss: 4.4180, Train Acc: 0.2596, time: 1654.68, Best Val(epoch48) Acc@1: 0.5939
2021-12-29 00:01:06,192 Max accuracy so far: 0.5939 at epoch_48
2021-12-29 00:01:06,192 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 00:01:06,192 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 00:01:06,272 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 00:01:06,272 Now training epoch 49. LR=0.000936
2021-12-29 00:02:43,165 Epoch[049/310], Step[0000/1251], Loss: 4.1713(4.1713), Acc: 0.2852(0.2852)
2021-12-29 00:03:45,698 Epoch[049/310], Step[0050/1251], Loss: 4.5375(4.3929), Acc: 0.2412(0.2565)
2021-12-29 00:04:47,892 Epoch[049/310], Step[0100/1251], Loss: 4.6737(4.4220), Acc: 0.3193(0.2622)
2021-12-29 00:05:49,624 Epoch[049/310], Step[0150/1251], Loss: 4.5084(4.4154), Acc: 0.2822(0.2628)
2021-12-29 00:06:51,763 Epoch[049/310], Step[0200/1251], Loss: 4.4352(4.4088), Acc: 0.3330(0.2663)
2021-12-29 00:07:53,214 Epoch[049/310], Step[0250/1251], Loss: 3.8750(4.4188), Acc: 0.2158(0.2639)
2021-12-29 00:08:55,988 Epoch[049/310], Step[0300/1251], Loss: 4.5509(4.4079), Acc: 0.3145(0.2666)
2021-12-29 00:09:59,980 Epoch[049/310], Step[0350/1251], Loss: 3.9751(4.4094), Acc: 0.2588(0.2664)
2021-12-29 00:11:04,259 Epoch[049/310], Step[0400/1251], Loss: 4.3103(4.4094), Acc: 0.3037(0.2629)
2021-12-29 00:12:08,771 Epoch[049/310], Step[0450/1251], Loss: 4.5272(4.4049), Acc: 0.1768(0.2637)
2021-12-29 00:13:12,028 Epoch[049/310], Step[0500/1251], Loss: 4.6440(4.4071), Acc: 0.1963(0.2640)
2021-12-29 00:14:14,292 Epoch[049/310], Step[0550/1251], Loss: 3.8157(4.4079), Acc: 0.3496(0.2634)
2021-12-29 00:15:16,959 Epoch[049/310], Step[0600/1251], Loss: 4.3634(4.4147), Acc: 0.3037(0.2622)
2021-12-29 00:16:19,929 Epoch[049/310], Step[0650/1251], Loss: 4.1453(4.4174), Acc: 0.1836(0.2617)
2021-12-29 00:17:22,883 Epoch[049/310], Step[0700/1251], Loss: 4.0306(4.4135), Acc: 0.1318(0.2614)
2021-12-29 00:18:25,501 Epoch[049/310], Step[0750/1251], Loss: 3.7963(4.4149), Acc: 0.3926(0.2616)
2021-12-29 00:19:27,517 Epoch[049/310], Step[0800/1251], Loss: 4.2887(4.4191), Acc: 0.3506(0.2609)
2021-12-29 00:20:30,533 Epoch[049/310], Step[0850/1251], Loss: 4.4572(4.4175), Acc: 0.3262(0.2615)
2021-12-29 00:21:32,029 Epoch[049/310], Step[0900/1251], Loss: 4.6504(4.4155), Acc: 0.1172(0.2610)
2021-12-29 00:22:33,836 Epoch[049/310], Step[0950/1251], Loss: 4.2457(4.4189), Acc: 0.2842(0.2614)
2021-12-29 00:23:35,922 Epoch[049/310], Step[1000/1251], Loss: 4.2066(4.4217), Acc: 0.3057(0.2614)
2021-12-29 00:24:38,213 Epoch[049/310], Step[1050/1251], Loss: 3.8985(4.4201), Acc: 0.3076(0.2603)
2021-12-29 00:25:39,786 Epoch[049/310], Step[1100/1251], Loss: 4.1792(4.4209), Acc: 0.3936(0.2611)
2021-12-29 00:26:44,187 Epoch[049/310], Step[1150/1251], Loss: 4.1523(4.4183), Acc: 0.2373(0.2610)
2021-12-29 00:27:48,366 Epoch[049/310], Step[1200/1251], Loss: 4.4814(4.4195), Acc: 0.2041(0.2605)
2021-12-29 00:28:51,211 Epoch[049/310], Step[1250/1251], Loss: 4.4107(4.4207), Acc: 0.2480(0.2602)
2021-12-29 00:28:53,083 ----- Epoch[049/310], Train Loss: 4.4207, Train Acc: 0.2602, time: 1666.81, Best Val(epoch48) Acc@1: 0.5939
2021-12-29 00:28:53,083 Now training epoch 50. LR=0.000933
2021-12-29 00:30:15,887 Epoch[050/310], Step[0000/1251], Loss: 4.1471(4.1471), Acc: 0.3916(0.3916)
2021-12-29 00:31:18,392 Epoch[050/310], Step[0050/1251], Loss: 3.9486(4.4454), Acc: 0.4277(0.2406)
2021-12-29 00:32:19,435 Epoch[050/310], Step[0100/1251], Loss: 3.6955(4.3515), Acc: 0.3359(0.2567)
2021-12-29 00:33:20,458 Epoch[050/310], Step[0150/1251], Loss: 3.7111(4.3375), Acc: 0.3291(0.2663)
2021-12-29 00:34:23,825 Epoch[050/310], Step[0200/1251], Loss: 4.3372(4.3362), Acc: 0.2871(0.2635)
2021-12-29 00:35:26,392 Epoch[050/310], Step[0250/1251], Loss: 3.9242(4.3347), Acc: 0.3311(0.2668)
2021-12-29 00:36:29,327 Epoch[050/310], Step[0300/1251], Loss: 4.0321(4.3525), Acc: 0.4170(0.2648)
2021-12-29 00:37:32,727 Epoch[050/310], Step[0350/1251], Loss: 4.1091(4.3507), Acc: 0.2197(0.2639)
2021-12-29 00:38:35,409 Epoch[050/310], Step[0400/1251], Loss: 3.9800(4.3655), Acc: 0.2998(0.2657)
2021-12-29 00:39:37,417 Epoch[050/310], Step[0450/1251], Loss: 4.5453(4.3719), Acc: 0.1523(0.2666)
2021-12-29 00:40:40,351 Epoch[050/310], Step[0500/1251], Loss: 4.6394(4.3734), Acc: 0.1797(0.2649)
2021-12-29 00:41:43,198 Epoch[050/310], Step[0550/1251], Loss: 4.6855(4.3761), Acc: 0.2979(0.2637)
2021-12-29 00:42:46,774 Epoch[050/310], Step[0600/1251], Loss: 4.4426(4.3792), Acc: 0.3184(0.2643)
2021-12-29 00:43:49,517 Epoch[050/310], Step[0650/1251], Loss: 4.4075(4.3845), Acc: 0.3330(0.2638)
2021-12-29 00:44:50,099 Epoch[050/310], Step[0700/1251], Loss: 4.1458(4.3835), Acc: 0.2324(0.2645)
2021-12-29 00:45:54,083 Epoch[050/310], Step[0750/1251], Loss: 4.1715(4.3851), Acc: 0.3564(0.2639)
2021-12-29 00:46:58,135 Epoch[050/310], Step[0800/1251], Loss: 4.7920(4.3820), Acc: 0.2686(0.2640)
2021-12-29 00:48:00,789 Epoch[050/310], Step[0850/1251], Loss: 4.2850(4.3850), Acc: 0.3486(0.2642)
2021-12-29 00:49:04,667 Epoch[050/310], Step[0900/1251], Loss: 3.5240(4.3868), Acc: 0.3770(0.2632)
2021-12-29 00:50:08,156 Epoch[050/310], Step[0950/1251], Loss: 4.5232(4.3872), Acc: 0.3740(0.2627)
2021-12-29 00:51:11,480 Epoch[050/310], Step[1000/1251], Loss: 5.1372(4.3880), Acc: 0.2334(0.2633)
2021-12-29 00:52:15,275 Epoch[050/310], Step[1050/1251], Loss: 4.1596(4.3886), Acc: 0.1348(0.2636)
2021-12-29 00:53:18,893 Epoch[050/310], Step[1100/1251], Loss: 4.0048(4.3898), Acc: 0.2949(0.2630)
2021-12-29 00:54:20,861 Epoch[050/310], Step[1150/1251], Loss: 4.9839(4.3904), Acc: 0.2188(0.2636)
2021-12-29 00:55:22,778 Epoch[050/310], Step[1200/1251], Loss: 5.0889(4.3912), Acc: 0.2295(0.2633)
2021-12-29 00:56:26,189 Epoch[050/310], Step[1250/1251], Loss: 4.7495(4.3928), Acc: 0.3193(0.2634)
2021-12-29 00:56:28,135 ----- Validation after Epoch: 50
2021-12-29 00:57:27,589 Val Step[0000/1563], Loss: 0.8481 (0.8481), Acc@1: 0.8750 (0.8750), Acc@5: 0.9375 (0.9375)
2021-12-29 00:57:29,087 Val Step[0050/1563], Loss: 2.5960 (1.2174), Acc@1: 0.3438 (0.7463), Acc@5: 0.6875 (0.9081)
2021-12-29 00:57:30,618 Val Step[0100/1563], Loss: 2.0899 (1.5445), Acc@1: 0.4688 (0.6556), Acc@5: 0.8438 (0.8688)
2021-12-29 00:57:32,112 Val Step[0150/1563], Loss: 0.8494 (1.4335), Acc@1: 0.8125 (0.6792), Acc@5: 0.9062 (0.8802)
2021-12-29 00:57:33,682 Val Step[0200/1563], Loss: 1.4511 (1.4898), Acc@1: 0.7188 (0.6713), Acc@5: 0.9375 (0.8725)
2021-12-29 00:57:35,254 Val Step[0250/1563], Loss: 0.8886 (1.4167), Acc@1: 0.8438 (0.6890), Acc@5: 0.9688 (0.8840)
2021-12-29 00:57:36,738 Val Step[0300/1563], Loss: 1.8246 (1.4780), Acc@1: 0.5938 (0.6707), Acc@5: 0.8125 (0.8790)
2021-12-29 00:57:38,233 Val Step[0350/1563], Loss: 1.8839 (1.4965), Acc@1: 0.5312 (0.6627), Acc@5: 0.9062 (0.8790)
2021-12-29 00:57:39,720 Val Step[0400/1563], Loss: 2.1201 (1.4922), Acc@1: 0.5000 (0.6588), Acc@5: 0.8438 (0.8823)
2021-12-29 00:57:41,466 Val Step[0450/1563], Loss: 1.0817 (1.4863), Acc@1: 0.5625 (0.6569), Acc@5: 1.0000 (0.8855)
2021-12-29 00:57:43,005 Val Step[0500/1563], Loss: 0.4215 (1.4788), Acc@1: 0.9688 (0.6601), Acc@5: 1.0000 (0.8870)
2021-12-29 00:57:44,576 Val Step[0550/1563], Loss: 1.4747 (1.4524), Acc@1: 0.6875 (0.6686), Acc@5: 0.8750 (0.8898)
2021-12-29 00:57:46,157 Val Step[0600/1563], Loss: 1.0403 (1.4559), Acc@1: 0.7500 (0.6698), Acc@5: 0.9375 (0.8893)
2021-12-29 00:57:47,785 Val Step[0650/1563], Loss: 1.1159 (1.4812), Acc@1: 0.8125 (0.6661), Acc@5: 0.9688 (0.8847)
2021-12-29 00:57:49,244 Val Step[0700/1563], Loss: 1.8914 (1.5179), Acc@1: 0.6562 (0.6581), Acc@5: 0.8125 (0.8798)
2021-12-29 00:57:50,806 Val Step[0750/1563], Loss: 1.9880 (1.5657), Acc@1: 0.5938 (0.6488), Acc@5: 0.7500 (0.8725)
2021-12-29 00:57:52,358 Val Step[0800/1563], Loss: 1.4195 (1.6085), Acc@1: 0.7812 (0.6393), Acc@5: 0.9688 (0.8666)
2021-12-29 00:57:53,891 Val Step[0850/1563], Loss: 2.1323 (1.6402), Acc@1: 0.4688 (0.6330), Acc@5: 0.7812 (0.8616)
2021-12-29 00:57:55,518 Val Step[0900/1563], Loss: 0.5826 (1.6435), Acc@1: 0.9062 (0.6339), Acc@5: 0.9062 (0.8602)
2021-12-29 00:57:57,120 Val Step[0950/1563], Loss: 1.9171 (1.6726), Acc@1: 0.6250 (0.6281), Acc@5: 0.7812 (0.8556)
2021-12-29 00:57:58,594 Val Step[1000/1563], Loss: 0.7068 (1.7026), Acc@1: 0.8750 (0.6214), Acc@5: 0.9688 (0.8509)
2021-12-29 00:58:00,082 Val Step[1050/1563], Loss: 0.8119 (1.7200), Acc@1: 0.8750 (0.6174), Acc@5: 0.9688 (0.8489)
2021-12-29 00:58:01,625 Val Step[1100/1563], Loss: 1.7185 (1.7406), Acc@1: 0.6562 (0.6137), Acc@5: 0.8438 (0.8456)
2021-12-29 00:58:03,149 Val Step[1150/1563], Loss: 1.6012 (1.7586), Acc@1: 0.7188 (0.6110), Acc@5: 0.8125 (0.8427)
2021-12-29 00:58:04,595 Val Step[1200/1563], Loss: 1.5580 (1.7770), Acc@1: 0.7500 (0.6077), Acc@5: 0.8438 (0.8395)
2021-12-29 00:58:06,097 Val Step[1250/1563], Loss: 1.0490 (1.7945), Acc@1: 0.7812 (0.6048), Acc@5: 0.9375 (0.8364)
2021-12-29 00:58:07,583 Val Step[1300/1563], Loss: 1.3532 (1.8093), Acc@1: 0.7500 (0.6013), Acc@5: 0.8125 (0.8344)
2021-12-29 00:58:09,075 Val Step[1350/1563], Loss: 2.9368 (1.8332), Acc@1: 0.1562 (0.5968), Acc@5: 0.6562 (0.8304)
2021-12-29 00:58:10,591 Val Step[1400/1563], Loss: 1.3496 (1.8434), Acc@1: 0.7500 (0.5950), Acc@5: 0.8750 (0.8285)
2021-12-29 00:58:12,056 Val Step[1450/1563], Loss: 2.0757 (1.8491), Acc@1: 0.5312 (0.5939), Acc@5: 0.8438 (0.8280)
2021-12-29 00:58:13,535 Val Step[1500/1563], Loss: 2.0425 (1.8371), Acc@1: 0.5625 (0.5962), Acc@5: 0.8750 (0.8299)
2021-12-29 00:58:15,114 Val Step[1550/1563], Loss: 0.9245 (1.8335), Acc@1: 0.8750 (0.5968), Acc@5: 0.9062 (0.8305)
2021-12-29 00:58:15,957 ----- Epoch[050/310], Validation Loss: 1.8307, Validation Acc@1: 0.5974, Validation Acc@5: 0.8308, time: 107.82
2021-12-29 00:58:15,957 ----- Epoch[050/310], Train Loss: 4.3928, Train Acc: 0.2634, time: 1655.05, Best Val(epoch50) Acc@1: 0.5974
2021-12-29 00:58:16,167 Max accuracy so far: 0.5974 at epoch_50
2021-12-29 00:58:16,168 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 00:58:16,168 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 00:58:16,253 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 00:58:16,422 ----- Save model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-50-Loss-4.38356495665894.pdparams
2021-12-29 00:58:16,422 ----- Save optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-50-Loss-4.38356495665894.pdopt
2021-12-29 00:58:16,495 ----- Save ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-50-Loss-4.38356495665894-EMA.pdparams
2021-12-29 00:58:16,495 Now training epoch 51. LR=0.000931
2021-12-29 00:59:35,642 Epoch[051/310], Step[0000/1251], Loss: 4.5998(4.5998), Acc: 0.2725(0.2725)
2021-12-29 01:00:38,776 Epoch[051/310], Step[0050/1251], Loss: 4.3059(4.3862), Acc: 0.2080(0.2406)
2021-12-29 01:01:42,056 Epoch[051/310], Step[0100/1251], Loss: 4.6199(4.4145), Acc: 0.3350(0.2488)
2021-12-29 01:02:44,066 Epoch[051/310], Step[0150/1251], Loss: 4.6568(4.4235), Acc: 0.1475(0.2561)
2021-12-29 01:03:44,481 Epoch[051/310], Step[0200/1251], Loss: 4.5353(4.4057), Acc: 0.3115(0.2570)
2021-12-29 01:04:46,869 Epoch[051/310], Step[0250/1251], Loss: 4.1983(4.3953), Acc: 0.3379(0.2609)
2021-12-29 01:05:48,184 Epoch[051/310], Step[0300/1251], Loss: 3.5441(4.3881), Acc: 0.2324(0.2675)
2021-12-29 01:06:50,319 Epoch[051/310], Step[0350/1251], Loss: 4.2207(4.4000), Acc: 0.2705(0.2621)
2021-12-29 01:07:50,332 Epoch[051/310], Step[0400/1251], Loss: 4.5312(4.3914), Acc: 0.1982(0.2639)
2021-12-29 01:08:51,688 Epoch[051/310], Step[0450/1251], Loss: 4.0659(4.3996), Acc: 0.1611(0.2634)
2021-12-29 01:09:53,904 Epoch[051/310], Step[0500/1251], Loss: 4.2139(4.4049), Acc: 0.3369(0.2617)
2021-12-29 01:10:56,566 Epoch[051/310], Step[0550/1251], Loss: 4.1645(4.4040), Acc: 0.1328(0.2623)
2021-12-29 01:12:00,121 Epoch[051/310], Step[0600/1251], Loss: 4.0084(4.4037), Acc: 0.3105(0.2609)
2021-12-29 01:13:03,381 Epoch[051/310], Step[0650/1251], Loss: 4.1347(4.4006), Acc: 0.3584(0.2611)
2021-12-29 01:14:07,248 Epoch[051/310], Step[0700/1251], Loss: 4.5229(4.3993), Acc: 0.2793(0.2607)
2021-12-29 01:15:10,765 Epoch[051/310], Step[0750/1251], Loss: 4.2905(4.3977), Acc: 0.3340(0.2599)
2021-12-29 01:16:15,263 Epoch[051/310], Step[0800/1251], Loss: 3.7239(4.4019), Acc: 0.4355(0.2591)
2021-12-29 01:17:18,238 Epoch[051/310], Step[0850/1251], Loss: 4.5057(4.4018), Acc: 0.3643(0.2610)
2021-12-29 01:18:21,736 Epoch[051/310], Step[0900/1251], Loss: 4.7067(4.4023), Acc: 0.1582(0.2616)
2021-12-29 01:19:24,205 Epoch[051/310], Step[0950/1251], Loss: 4.6932(4.4027), Acc: 0.2119(0.2609)
2021-12-29 01:20:26,378 Epoch[051/310], Step[1000/1251], Loss: 4.3108(4.4028), Acc: 0.1826(0.2609)
2021-12-29 01:21:28,509 Epoch[051/310], Step[1050/1251], Loss: 4.2808(4.3991), Acc: 0.3115(0.2612)
2021-12-29 01:22:30,068 Epoch[051/310], Step[1100/1251], Loss: 4.3655(4.4003), Acc: 0.3125(0.2614)
2021-12-29 01:23:31,598 Epoch[051/310], Step[1150/1251], Loss: 4.5299(4.4012), Acc: 0.1699(0.2607)
2021-12-29 01:24:33,186 Epoch[051/310], Step[1200/1251], Loss: 4.1997(4.4010), Acc: 0.0723(0.2612)
2021-12-29 01:25:36,263 Epoch[051/310], Step[1250/1251], Loss: 4.5322(4.4035), Acc: 0.3369(0.2616)
2021-12-29 01:25:38,398 ----- Epoch[051/310], Train Loss: 4.4035, Train Acc: 0.2616, time: 1641.90, Best Val(epoch50) Acc@1: 0.5974
2021-12-29 01:25:38,398 Now training epoch 52. LR=0.000928
2021-12-29 01:27:01,414 Epoch[052/310], Step[0000/1251], Loss: 3.9598(3.9598), Acc: 0.3848(0.3848)
2021-12-29 01:28:05,188 Epoch[052/310], Step[0050/1251], Loss: 4.5532(4.2832), Acc: 0.3350(0.2880)
2021-12-29 01:29:07,594 Epoch[052/310], Step[0100/1251], Loss: 4.2929(4.3116), Acc: 0.2744(0.2753)
2021-12-29 01:30:08,595 Epoch[052/310], Step[0150/1251], Loss: 5.0953(4.3228), Acc: 0.2305(0.2787)
2021-12-29 01:31:12,403 Epoch[052/310], Step[0200/1251], Loss: 4.1978(4.3409), Acc: 0.2637(0.2703)
2021-12-29 01:32:16,385 Epoch[052/310], Step[0250/1251], Loss: 3.5413(4.3387), Acc: 0.1934(0.2650)
2021-12-29 01:33:19,885 Epoch[052/310], Step[0300/1251], Loss: 4.6999(4.3485), Acc: 0.2432(0.2671)
2021-12-29 01:34:21,671 Epoch[052/310], Step[0350/1251], Loss: 4.0417(4.3565), Acc: 0.4473(0.2690)
2021-12-29 01:35:24,303 Epoch[052/310], Step[0400/1251], Loss: 4.4360(4.3488), Acc: 0.2920(0.2729)
2021-12-29 01:36:28,293 Epoch[052/310], Step[0450/1251], Loss: 4.1499(4.3554), Acc: 0.4189(0.2746)
2021-12-29 01:37:31,341 Epoch[052/310], Step[0500/1251], Loss: 4.7134(4.3500), Acc: 0.2168(0.2750)
2021-12-29 01:38:35,148 Epoch[052/310], Step[0550/1251], Loss: 4.1889(4.3552), Acc: 0.1885(0.2744)
2021-12-29 01:39:38,301 Epoch[052/310], Step[0600/1251], Loss: 4.5374(4.3579), Acc: 0.3369(0.2721)
2021-12-29 01:40:41,595 Epoch[052/310], Step[0650/1251], Loss: 4.9543(4.3599), Acc: 0.2461(0.2717)
2021-12-29 01:41:45,205 Epoch[052/310], Step[0700/1251], Loss: 4.1219(4.3598), Acc: 0.1396(0.2718)
2021-12-29 01:42:47,795 Epoch[052/310], Step[0750/1251], Loss: 4.4685(4.3529), Acc: 0.0098(0.2728)
2021-12-29 01:43:50,766 Epoch[052/310], Step[0800/1251], Loss: 4.3330(4.3533), Acc: 0.3506(0.2724)
2021-12-29 01:44:54,403 Epoch[052/310], Step[0850/1251], Loss: 4.6254(4.3535), Acc: 0.3145(0.2718)
2021-12-29 01:45:57,296 Epoch[052/310], Step[0900/1251], Loss: 4.6367(4.3589), Acc: 0.3125(0.2700)
2021-12-29 01:47:00,941 Epoch[052/310], Step[0950/1251], Loss: 3.7761(4.3585), Acc: 0.2764(0.2693)
2021-12-29 01:48:04,155 Epoch[052/310], Step[1000/1251], Loss: 3.8722(4.3578), Acc: 0.3350(0.2692)
2021-12-29 01:49:07,914 Epoch[052/310], Step[1050/1251], Loss: 4.7666(4.3555), Acc: 0.3057(0.2700)
2021-12-29 01:50:10,364 Epoch[052/310], Step[1100/1251], Loss: 4.5705(4.3611), Acc: 0.3115(0.2697)
2021-12-29 01:51:13,522 Epoch[052/310], Step[1150/1251], Loss: 4.5097(4.3643), Acc: 0.2559(0.2688)
2021-12-29 01:52:16,402 Epoch[052/310], Step[1200/1251], Loss: 4.3908(4.3640), Acc: 0.2480(0.2696)
2021-12-29 01:53:19,436 Epoch[052/310], Step[1250/1251], Loss: 4.3757(4.3652), Acc: 0.2617(0.2685)
2021-12-29 01:53:21,383 ----- Validation after Epoch: 52
2021-12-29 01:54:21,706 Val Step[0000/1563], Loss: 1.0686 (1.0686), Acc@1: 0.8438 (0.8438), Acc@5: 0.9375 (0.9375)
2021-12-29 01:54:23,424 Val Step[0050/1563], Loss: 2.8216 (1.1979), Acc@1: 0.3750 (0.7598), Acc@5: 0.6875 (0.9118)
2021-12-29 01:54:24,981 Val Step[0100/1563], Loss: 2.2640 (1.5636), Acc@1: 0.4375 (0.6556), Acc@5: 0.8125 (0.8735)
2021-12-29 01:54:26,478 Val Step[0150/1563], Loss: 0.9071 (1.4787), Acc@1: 0.8750 (0.6786), Acc@5: 0.9688 (0.8820)
2021-12-29 01:54:28,036 Val Step[0200/1563], Loss: 1.7352 (1.4947), Acc@1: 0.5625 (0.6777), Acc@5: 0.9062 (0.8806)
2021-12-29 01:54:29,494 Val Step[0250/1563], Loss: 1.2537 (1.4204), Acc@1: 0.6562 (0.6942), Acc@5: 0.9688 (0.8901)
2021-12-29 01:54:31,009 Val Step[0300/1563], Loss: 2.1663 (1.4849), Acc@1: 0.3750 (0.6707), Acc@5: 0.7188 (0.8838)
2021-12-29 01:54:32,435 Val Step[0350/1563], Loss: 1.7499 (1.4955), Acc@1: 0.6562 (0.6656), Acc@5: 0.8438 (0.8850)
2021-12-29 01:54:33,959 Val Step[0400/1563], Loss: 1.4353 (1.4890), Acc@1: 0.7500 (0.6626), Acc@5: 0.9375 (0.8884)
2021-12-29 01:54:35,581 Val Step[0450/1563], Loss: 1.0427 (1.4872), Acc@1: 0.6875 (0.6615), Acc@5: 1.0000 (0.8900)
2021-12-29 01:54:37,215 Val Step[0500/1563], Loss: 0.5933 (1.4780), Acc@1: 0.9062 (0.6634), Acc@5: 1.0000 (0.8919)
2021-12-29 01:54:38,762 Val Step[0550/1563], Loss: 1.1363 (1.4541), Acc@1: 0.7500 (0.6708), Acc@5: 0.9375 (0.8947)
2021-12-29 01:54:40,374 Val Step[0600/1563], Loss: 0.9982 (1.4578), Acc@1: 0.8125 (0.6711), Acc@5: 0.9062 (0.8940)
2021-12-29 01:54:41,891 Val Step[0650/1563], Loss: 0.8773 (1.4873), Acc@1: 0.8750 (0.6657), Acc@5: 0.9688 (0.8891)
2021-12-29 01:54:43,337 Val Step[0700/1563], Loss: 2.1754 (1.5323), Acc@1: 0.5625 (0.6568), Acc@5: 0.7812 (0.8830)
2021-12-29 01:54:44,832 Val Step[0750/1563], Loss: 1.8439 (1.5766), Acc@1: 0.5938 (0.6488), Acc@5: 0.8438 (0.8761)
2021-12-29 01:54:46,294 Val Step[0800/1563], Loss: 1.9460 (1.6246), Acc@1: 0.5938 (0.6384), Acc@5: 0.9062 (0.8688)
2021-12-29 01:54:47,814 Val Step[0850/1563], Loss: 1.9152 (1.6565), Acc@1: 0.5625 (0.6320), Acc@5: 0.8750 (0.8636)
2021-12-29 01:54:49,348 Val Step[0900/1563], Loss: 1.0388 (1.6606), Acc@1: 0.8125 (0.6332), Acc@5: 0.9375 (0.8622)
2021-12-29 01:54:51,008 Val Step[0950/1563], Loss: 1.6760 (1.6882), Acc@1: 0.6562 (0.6287), Acc@5: 0.7812 (0.8571)
2021-12-29 01:54:52,361 Val Step[1000/1563], Loss: 0.6997 (1.7180), Acc@1: 0.9062 (0.6234), Acc@5: 0.9688 (0.8522)
2021-12-29 01:54:53,713 Val Step[1050/1563], Loss: 0.8925 (1.7381), Acc@1: 0.8438 (0.6192), Acc@5: 0.9688 (0.8496)
2021-12-29 01:54:55,015 Val Step[1100/1563], Loss: 2.1210 (1.7593), Acc@1: 0.5938 (0.6152), Acc@5: 0.7812 (0.8457)
2021-12-29 01:54:56,320 Val Step[1150/1563], Loss: 1.8433 (1.7785), Acc@1: 0.7188 (0.6122), Acc@5: 0.7812 (0.8426)
2021-12-29 01:54:57,606 Val Step[1200/1563], Loss: 1.5945 (1.7982), Acc@1: 0.7188 (0.6089), Acc@5: 0.8438 (0.8394)
2021-12-29 01:54:59,213 Val Step[1250/1563], Loss: 0.7632 (1.8144), Acc@1: 0.9062 (0.6062), Acc@5: 0.9375 (0.8367)
2021-12-29 01:55:00,657 Val Step[1300/1563], Loss: 1.7425 (1.8272), Acc@1: 0.7188 (0.6043), Acc@5: 0.8125 (0.8349)
2021-12-29 01:55:02,160 Val Step[1350/1563], Loss: 2.5101 (1.8540), Acc@1: 0.3125 (0.5986), Acc@5: 0.6875 (0.8304)
2021-12-29 01:55:03,636 Val Step[1400/1563], Loss: 1.5849 (1.8620), Acc@1: 0.6562 (0.5969), Acc@5: 0.8438 (0.8296)
2021-12-29 01:55:05,066 Val Step[1450/1563], Loss: 2.3206 (1.8679), Acc@1: 0.3750 (0.5953), Acc@5: 0.8438 (0.8290)
2021-12-29 01:55:06,581 Val Step[1500/1563], Loss: 2.6486 (1.8558), Acc@1: 0.3750 (0.5978), Acc@5: 0.7188 (0.8309)
2021-12-29 01:55:08,014 Val Step[1550/1563], Loss: 0.9795 (1.8537), Acc@1: 0.8750 (0.5986), Acc@5: 0.9062 (0.8311)
2021-12-29 01:55:08,850 ----- Epoch[052/310], Validation Loss: 1.8512, Validation Acc@1: 0.5992, Validation Acc@5: 0.8314, time: 107.46
2021-12-29 01:55:08,850 ----- Epoch[052/310], Train Loss: 4.3652, Train Acc: 0.2685, time: 1662.98, Best Val(epoch52) Acc@1: 0.5992
2021-12-29 01:55:09,057 Max accuracy so far: 0.5992 at epoch_52
2021-12-29 01:55:09,057 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 01:55:09,058 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 01:55:09,151 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 01:55:09,151 Now training epoch 53. LR=0.000925
2021-12-29 01:56:26,440 Epoch[053/310], Step[0000/1251], Loss: 4.3175(4.3175), Acc: 0.1602(0.1602)
2021-12-29 01:57:28,278 Epoch[053/310], Step[0050/1251], Loss: 4.4117(4.4451), Acc: 0.2168(0.2559)
2021-12-29 01:58:29,888 Epoch[053/310], Step[0100/1251], Loss: 4.6672(4.3845), Acc: 0.3408(0.2771)
2021-12-29 01:59:30,690 Epoch[053/310], Step[0150/1251], Loss: 5.1381(4.3909), Acc: 0.2236(0.2666)
2021-12-29 02:00:32,866 Epoch[053/310], Step[0200/1251], Loss: 4.4793(4.3879), Acc: 0.3691(0.2648)
2021-12-29 02:01:35,574 Epoch[053/310], Step[0250/1251], Loss: 4.9041(4.3900), Acc: 0.2500(0.2680)
2021-12-29 02:02:36,755 Epoch[053/310], Step[0300/1251], Loss: 4.6023(4.3889), Acc: 0.2559(0.2690)
2021-12-29 02:03:39,978 Epoch[053/310], Step[0350/1251], Loss: 4.8824(4.3954), Acc: 0.1494(0.2659)
2021-12-29 02:04:41,572 Epoch[053/310], Step[0400/1251], Loss: 4.2434(4.3994), Acc: 0.3428(0.2666)
2021-12-29 02:05:42,514 Epoch[053/310], Step[0450/1251], Loss: 5.1432(4.4023), Acc: 0.2061(0.2678)
2021-12-29 02:06:45,186 Epoch[053/310], Step[0500/1251], Loss: 4.2235(4.4037), Acc: 0.3877(0.2666)
2021-12-29 02:07:45,676 Epoch[053/310], Step[0550/1251], Loss: 4.6658(4.3946), Acc: 0.2881(0.2682)
2021-12-29 02:08:48,792 Epoch[053/310], Step[0600/1251], Loss: 4.0195(4.3895), Acc: 0.1582(0.2690)
2021-12-29 02:09:50,057 Epoch[053/310], Step[0650/1251], Loss: 4.5256(4.3873), Acc: 0.0908(0.2695)
2021-12-29 02:10:51,351 Epoch[053/310], Step[0700/1251], Loss: 4.0819(4.3814), Acc: 0.4111(0.2707)
2021-12-29 02:11:54,637 Epoch[053/310], Step[0750/1251], Loss: 4.2637(4.3775), Acc: 0.3252(0.2701)
2021-12-29 02:12:56,586 Epoch[053/310], Step[0800/1251], Loss: 4.8271(4.3721), Acc: 0.1768(0.2706)
2021-12-29 02:14:00,226 Epoch[053/310], Step[0850/1251], Loss: 5.1966(4.3737), Acc: 0.1494(0.2697)
2021-12-29 02:15:03,582 Epoch[053/310], Step[0900/1251], Loss: 4.6633(4.3749), Acc: 0.2881(0.2692)
2021-12-29 02:16:06,813 Epoch[053/310], Step[0950/1251], Loss: 4.2085(4.3730), Acc: 0.4082(0.2691)
2021-12-29 02:17:09,198 Epoch[053/310], Step[1000/1251], Loss: 4.7806(4.3777), Acc: 0.2646(0.2687)
2021-12-29 02:18:11,437 Epoch[053/310], Step[1050/1251], Loss: 4.0869(4.3765), Acc: 0.1807(0.2676)
2021-12-29 02:19:15,022 Epoch[053/310], Step[1100/1251], Loss: 4.7335(4.3808), Acc: 0.3379(0.2672)
2021-12-29 02:20:18,101 Epoch[053/310], Step[1150/1251], Loss: 4.6490(4.3781), Acc: 0.2373(0.2667)
2021-12-29 02:21:21,487 Epoch[053/310], Step[1200/1251], Loss: 4.3154(4.3774), Acc: 0.3984(0.2661)
2021-12-29 02:22:23,994 Epoch[053/310], Step[1250/1251], Loss: 4.9688(4.3754), Acc: 0.2686(0.2658)
2021-12-29 02:22:25,951 ----- Epoch[053/310], Train Loss: 4.3754, Train Acc: 0.2658, time: 1636.80, Best Val(epoch52) Acc@1: 0.5992
2021-12-29 02:22:25,952 Now training epoch 54. LR=0.000923
2021-12-29 02:23:45,895 Epoch[054/310], Step[0000/1251], Loss: 4.0356(4.0356), Acc: 0.3955(0.3955)
2021-12-29 02:24:47,106 Epoch[054/310], Step[0050/1251], Loss: 4.3197(4.2830), Acc: 0.3936(0.2680)
2021-12-29 02:25:49,693 Epoch[054/310], Step[0100/1251], Loss: 4.6695(4.3024), Acc: 0.2559(0.2797)
2021-12-29 02:26:51,752 Epoch[054/310], Step[0150/1251], Loss: 4.9327(4.3000), Acc: 0.2461(0.2723)
2021-12-29 02:27:54,394 Epoch[054/310], Step[0200/1251], Loss: 4.2161(4.3035), Acc: 0.1562(0.2652)
2021-12-29 02:28:57,651 Epoch[054/310], Step[0250/1251], Loss: 4.3929(4.3186), Acc: 0.2471(0.2623)
2021-12-29 02:30:00,302 Epoch[054/310], Step[0300/1251], Loss: 3.9900(4.3306), Acc: 0.4033(0.2649)
2021-12-29 02:31:03,498 Epoch[054/310], Step[0350/1251], Loss: 4.2552(4.3372), Acc: 0.2441(0.2676)
2021-12-29 02:32:04,573 Epoch[054/310], Step[0400/1251], Loss: 4.2553(4.3444), Acc: 0.1523(0.2663)
2021-12-29 02:33:06,771 Epoch[054/310], Step[0450/1251], Loss: 4.3794(4.3474), Acc: 0.2461(0.2630)
2021-12-29 02:34:10,305 Epoch[054/310], Step[0500/1251], Loss: 4.6801(4.3512), Acc: 0.1904(0.2620)
2021-12-29 02:35:13,246 Epoch[054/310], Step[0550/1251], Loss: 4.8600(4.3609), Acc: 0.2637(0.2601)
2021-12-29 02:36:16,440 Epoch[054/310], Step[0600/1251], Loss: 4.3379(4.3594), Acc: 0.3428(0.2606)
2021-12-29 02:37:19,553 Epoch[054/310], Step[0650/1251], Loss: 4.4073(4.3582), Acc: 0.2461(0.2601)
2021-12-29 02:38:22,749 Epoch[054/310], Step[0700/1251], Loss: 4.3460(4.3577), Acc: 0.1611(0.2611)
2021-12-29 02:39:26,460 Epoch[054/310], Step[0750/1251], Loss: 3.9644(4.3613), Acc: 0.1250(0.2622)
2021-12-29 02:40:28,396 Epoch[054/310], Step[0800/1251], Loss: 4.1250(4.3574), Acc: 0.2529(0.2631)
2021-12-29 02:41:31,101 Epoch[054/310], Step[0850/1251], Loss: 3.9569(4.3566), Acc: 0.3857(0.2645)
2021-12-29 02:42:34,746 Epoch[054/310], Step[0900/1251], Loss: 4.6048(4.3583), Acc: 0.3330(0.2645)
2021-12-29 02:43:38,079 Epoch[054/310], Step[0950/1251], Loss: 4.3289(4.3555), Acc: 0.1787(0.2656)
2021-12-29 02:44:39,900 Epoch[054/310], Step[1000/1251], Loss: 4.5558(4.3568), Acc: 0.3203(0.2656)
2021-12-29 02:45:42,666 Epoch[054/310], Step[1050/1251], Loss: 4.5335(4.3572), Acc: 0.2920(0.2655)
2021-12-29 02:46:45,030 Epoch[054/310], Step[1100/1251], Loss: 3.6925(4.3568), Acc: 0.4590(0.2657)
2021-12-29 02:47:47,635 Epoch[054/310], Step[1150/1251], Loss: 4.2586(4.3562), Acc: 0.3447(0.2654)
2021-12-29 02:48:51,570 Epoch[054/310], Step[1200/1251], Loss: 4.4712(4.3557), Acc: 0.2969(0.2644)
2021-12-29 02:49:53,482 Epoch[054/310], Step[1250/1251], Loss: 4.3243(4.3529), Acc: 0.3408(0.2646)
2021-12-29 02:49:55,361 ----- Validation after Epoch: 54
2021-12-29 02:50:53,485 Val Step[0000/1563], Loss: 0.7993 (0.7993), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2021-12-29 02:50:55,149 Val Step[0050/1563], Loss: 3.0254 (1.1224), Acc@1: 0.3438 (0.7475), Acc@5: 0.6875 (0.9136)
2021-12-29 02:50:56,669 Val Step[0100/1563], Loss: 2.3543 (1.5321), Acc@1: 0.3750 (0.6501), Acc@5: 0.8125 (0.8645)
2021-12-29 02:50:58,278 Val Step[0150/1563], Loss: 0.6077 (1.4222), Acc@1: 0.9062 (0.6753), Acc@5: 0.9688 (0.8779)
2021-12-29 02:50:59,887 Val Step[0200/1563], Loss: 1.3107 (1.4423), Acc@1: 0.6875 (0.6777), Acc@5: 0.9375 (0.8753)
2021-12-29 02:51:01,467 Val Step[0250/1563], Loss: 1.0263 (1.3659), Acc@1: 0.7500 (0.6938), Acc@5: 0.9688 (0.8860)
2021-12-29 02:51:03,026 Val Step[0300/1563], Loss: 1.8289 (1.4308), Acc@1: 0.5000 (0.6716), Acc@5: 0.8125 (0.8811)
2021-12-29 02:51:04,558 Val Step[0350/1563], Loss: 1.6570 (1.4337), Acc@1: 0.5000 (0.6676), Acc@5: 0.9062 (0.8847)
2021-12-29 02:51:06,105 Val Step[0400/1563], Loss: 1.4288 (1.4211), Acc@1: 0.7188 (0.6655), Acc@5: 0.9375 (0.8884)
2021-12-29 02:51:07,682 Val Step[0450/1563], Loss: 1.4051 (1.4252), Acc@1: 0.5000 (0.6623), Acc@5: 0.9688 (0.8896)
2021-12-29 02:51:09,243 Val Step[0500/1563], Loss: 0.4535 (1.4214), Acc@1: 0.9375 (0.6629), Acc@5: 1.0000 (0.8908)
2021-12-29 02:51:10,795 Val Step[0550/1563], Loss: 1.1470 (1.3964), Acc@1: 0.7812 (0.6705), Acc@5: 0.9062 (0.8936)
2021-12-29 02:51:12,345 Val Step[0600/1563], Loss: 1.0228 (1.4019), Acc@1: 0.7812 (0.6705), Acc@5: 0.9375 (0.8934)
2021-12-29 02:51:13,861 Val Step[0650/1563], Loss: 0.9470 (1.4255), Acc@1: 0.8125 (0.6667), Acc@5: 0.9688 (0.8896)
2021-12-29 02:51:15,388 Val Step[0700/1563], Loss: 2.0293 (1.4750), Acc@1: 0.6562 (0.6571), Acc@5: 0.7812 (0.8825)
2021-12-29 02:51:16,974 Val Step[0750/1563], Loss: 1.7378 (1.5210), Acc@1: 0.6562 (0.6491), Acc@5: 0.7812 (0.8753)
2021-12-29 02:51:18,434 Val Step[0800/1563], Loss: 2.0178 (1.5705), Acc@1: 0.5625 (0.6378), Acc@5: 0.8125 (0.8686)
2021-12-29 02:51:19,921 Val Step[0850/1563], Loss: 2.0719 (1.6053), Acc@1: 0.5000 (0.6314), Acc@5: 0.7812 (0.8633)
2021-12-29 02:51:21,454 Val Step[0900/1563], Loss: 0.7150 (1.6117), Acc@1: 0.8438 (0.6316), Acc@5: 0.9375 (0.8617)
2021-12-29 02:51:23,045 Val Step[0950/1563], Loss: 1.9583 (1.6442), Acc@1: 0.6875 (0.6259), Acc@5: 0.7500 (0.8566)
2021-12-29 02:51:24,580 Val Step[1000/1563], Loss: 1.0312 (1.6746), Acc@1: 0.8125 (0.6209), Acc@5: 0.9375 (0.8521)
2021-12-29 02:51:26,227 Val Step[1050/1563], Loss: 0.5014 (1.6931), Acc@1: 0.9062 (0.6177), Acc@5: 0.9688 (0.8495)
2021-12-29 02:51:27,751 Val Step[1100/1563], Loss: 1.6600 (1.7113), Acc@1: 0.6875 (0.6153), Acc@5: 0.8438 (0.8465)
2021-12-29 02:51:29,218 Val Step[1150/1563], Loss: 2.0388 (1.7346), Acc@1: 0.6250 (0.6113), Acc@5: 0.7500 (0.8433)
2021-12-29 02:51:30,722 Val Step[1200/1563], Loss: 1.4376 (1.7550), Acc@1: 0.7812 (0.6081), Acc@5: 0.8125 (0.8396)
2021-12-29 02:51:32,188 Val Step[1250/1563], Loss: 0.8579 (1.7715), Acc@1: 0.9375 (0.6061), Acc@5: 0.9375 (0.8369)
2021-12-29 02:51:33,690 Val Step[1300/1563], Loss: 1.4696 (1.7818), Acc@1: 0.6875 (0.6041), Acc@5: 0.8438 (0.8356)
2021-12-29 02:51:35,233 Val Step[1350/1563], Loss: 2.3353 (1.8052), Acc@1: 0.3438 (0.5992), Acc@5: 0.7500 (0.8319)
2021-12-29 02:51:36,725 Val Step[1400/1563], Loss: 1.6476 (1.8182), Acc@1: 0.7188 (0.5968), Acc@5: 0.8438 (0.8298)
2021-12-29 02:51:38,190 Val Step[1450/1563], Loss: 2.7824 (1.8258), Acc@1: 0.2812 (0.5952), Acc@5: 0.7188 (0.8286)
2021-12-29 02:51:39,629 Val Step[1500/1563], Loss: 2.3107 (1.8115), Acc@1: 0.4688 (0.5982), Acc@5: 0.8438 (0.8307)
2021-12-29 02:51:41,131 Val Step[1550/1563], Loss: 0.9526 (1.8057), Acc@1: 0.8750 (0.5995), Acc@5: 0.9375 (0.8315)
2021-12-29 02:51:42,003 ----- Epoch[054/310], Validation Loss: 1.8030, Validation Acc@1: 0.5999, Validation Acc@5: 0.8318, time: 106.64
2021-12-29 02:51:42,004 ----- Epoch[054/310], Train Loss: 4.3529, Train Acc: 0.2646, time: 1649.41, Best Val(epoch54) Acc@1: 0.5999
2021-12-29 02:51:42,212 Max accuracy so far: 0.5999 at epoch_54
2021-12-29 02:51:42,213 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 02:51:42,213 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 02:51:42,309 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 02:51:42,309 Now training epoch 55. LR=0.000920
2021-12-29 02:52:58,792 Epoch[055/310], Step[0000/1251], Loss: 4.1896(4.1896), Acc: 0.1924(0.1924)
2021-12-29 02:54:01,389 Epoch[055/310], Step[0050/1251], Loss: 3.9184(4.3658), Acc: 0.4082(0.2809)
2021-12-29 02:55:04,599 Epoch[055/310], Step[0100/1251], Loss: 4.8031(4.4025), Acc: 0.1074(0.2705)
2021-12-29 02:56:05,812 Epoch[055/310], Step[0150/1251], Loss: 4.5424(4.4185), Acc: 0.2480(0.2681)
2021-12-29 02:57:09,617 Epoch[055/310], Step[0200/1251], Loss: 4.1580(4.4037), Acc: 0.3408(0.2648)
2021-12-29 02:58:12,340 Epoch[055/310], Step[0250/1251], Loss: 4.8463(4.4062), Acc: 0.2324(0.2674)
2021-12-29 02:59:15,449 Epoch[055/310], Step[0300/1251], Loss: 4.1823(4.4015), Acc: 0.2285(0.2689)
2021-12-29 03:00:17,084 Epoch[055/310], Step[0350/1251], Loss: 4.4905(4.3980), Acc: 0.3564(0.2669)
2021-12-29 03:01:20,600 Epoch[055/310], Step[0400/1251], Loss: 4.9541(4.3828), Acc: 0.1738(0.2672)
2021-12-29 03:02:24,047 Epoch[055/310], Step[0450/1251], Loss: 4.5911(4.3698), Acc: 0.3477(0.2695)
2021-12-29 03:03:26,332 Epoch[055/310], Step[0500/1251], Loss: 3.9075(4.3720), Acc: 0.2588(0.2699)
2021-12-29 03:04:29,326 Epoch[055/310], Step[0550/1251], Loss: 4.7671(4.3653), Acc: 0.0703(0.2699)
2021-12-29 03:05:31,318 Epoch[055/310], Step[0600/1251], Loss: 4.3702(4.3684), Acc: 0.0371(0.2692)
2021-12-29 03:06:34,223 Epoch[055/310], Step[0650/1251], Loss: 4.0378(4.3716), Acc: 0.3271(0.2674)
2021-12-29 03:07:35,790 Epoch[055/310], Step[0700/1251], Loss: 4.4932(4.3761), Acc: 0.3311(0.2674)
2021-12-29 03:08:37,574 Epoch[055/310], Step[0750/1251], Loss: 3.9680(4.3749), Acc: 0.4229(0.2679)
2021-12-29 03:09:37,189 Epoch[055/310], Step[0800/1251], Loss: 4.5818(4.3785), Acc: 0.1875(0.2675)
2021-12-29 03:10:38,354 Epoch[055/310], Step[0850/1251], Loss: 4.0961(4.3790), Acc: 0.4277(0.2682)
2021-12-29 03:11:40,459 Epoch[055/310], Step[0900/1251], Loss: 4.2494(4.3781), Acc: 0.4033(0.2690)
2021-12-29 03:12:41,123 Epoch[055/310], Step[0950/1251], Loss: 4.7928(4.3757), Acc: 0.2842(0.2701)
2021-12-29 03:13:44,428 Epoch[055/310], Step[1000/1251], Loss: 4.2475(4.3730), Acc: 0.2646(0.2699)
2021-12-29 03:14:48,796 Epoch[055/310], Step[1050/1251], Loss: 4.1522(4.3746), Acc: 0.4033(0.2697)
2021-12-29 03:15:52,941 Epoch[055/310], Step[1100/1251], Loss: 4.2751(4.3757), Acc: 0.3828(0.2690)
2021-12-29 03:16:56,515 Epoch[055/310], Step[1150/1251], Loss: 4.6005(4.3760), Acc: 0.1855(0.2686)
2021-12-29 03:18:00,086 Epoch[055/310], Step[1200/1251], Loss: 4.4301(4.3789), Acc: 0.3232(0.2688)
2021-12-29 03:19:02,770 Epoch[055/310], Step[1250/1251], Loss: 4.1091(4.3778), Acc: 0.2041(0.2699)
2021-12-29 03:19:04,722 ----- Epoch[055/310], Train Loss: 4.3778, Train Acc: 0.2699, time: 1642.41, Best Val(epoch54) Acc@1: 0.5999
2021-12-29 03:19:04,722 Now training epoch 56. LR=0.000917
2021-12-29 03:20:23,977 Epoch[056/310], Step[0000/1251], Loss: 4.1414(4.1414), Acc: 0.0303(0.0303)
2021-12-29 03:21:26,394 Epoch[056/310], Step[0050/1251], Loss: 4.3542(4.3024), Acc: 0.3389(0.2748)
2021-12-29 03:22:28,853 Epoch[056/310], Step[0100/1251], Loss: 4.7995(4.2943), Acc: 0.0986(0.2741)
2021-12-29 03:23:31,014 Epoch[056/310], Step[0150/1251], Loss: 4.3382(4.3358), Acc: 0.4170(0.2761)
2021-12-29 03:24:35,368 Epoch[056/310], Step[0200/1251], Loss: 4.4021(4.3460), Acc: 0.3184(0.2687)
2021-12-29 03:25:37,863 Epoch[056/310], Step[0250/1251], Loss: 4.8609(4.3505), Acc: 0.1621(0.2663)
2021-12-29 03:26:40,647 Epoch[056/310], Step[0300/1251], Loss: 4.6615(4.3374), Acc: 0.1641(0.2703)
2021-12-29 03:27:43,836 Epoch[056/310], Step[0350/1251], Loss: 3.9602(4.3468), Acc: 0.3057(0.2684)
2021-12-29 03:28:46,983 Epoch[056/310], Step[0400/1251], Loss: 4.1626(4.3525), Acc: 0.2646(0.2651)
2021-12-29 03:29:49,915 Epoch[056/310], Step[0450/1251], Loss: 4.2746(4.3488), Acc: 0.2031(0.2669)
2021-12-29 03:30:52,285 Epoch[056/310], Step[0500/1251], Loss: 4.3845(4.3512), Acc: 0.3721(0.2670)
2021-12-29 03:31:54,217 Epoch[056/310], Step[0550/1251], Loss: 4.9229(4.3556), Acc: 0.2510(0.2682)
2021-12-29 03:32:56,635 Epoch[056/310], Step[0600/1251], Loss: 3.8011(4.3473), Acc: 0.3584(0.2665)
2021-12-29 03:33:59,473 Epoch[056/310], Step[0650/1251], Loss: 4.1548(4.3554), Acc: 0.1768(0.2668)
2021-12-29 03:35:03,621 Epoch[056/310], Step[0700/1251], Loss: 4.6443(4.3591), Acc: 0.0820(0.2658)
2021-12-29 03:36:07,418 Epoch[056/310], Step[0750/1251], Loss: 3.4843(4.3611), Acc: 0.4531(0.2664)
2021-12-29 03:37:10,783 Epoch[056/310], Step[0800/1251], Loss: 4.1875(4.3603), Acc: 0.3633(0.2670)
2021-12-29 03:38:14,241 Epoch[056/310], Step[0850/1251], Loss: 3.9193(4.3594), Acc: 0.3213(0.2676)
2021-12-29 03:39:15,993 Epoch[056/310], Step[0900/1251], Loss: 4.4601(4.3629), Acc: 0.1289(0.2675)
2021-12-29 03:40:17,966 Epoch[056/310], Step[0950/1251], Loss: 4.8241(4.3648), Acc: 0.2031(0.2685)
2021-12-29 03:41:19,840 Epoch[056/310], Step[1000/1251], Loss: 4.2243(4.3628), Acc: 0.3750(0.2688)
2021-12-29 03:42:21,859 Epoch[056/310], Step[1050/1251], Loss: 3.5307(4.3627), Acc: 0.3496(0.2697)
2021-12-29 03:43:23,244 Epoch[056/310], Step[1100/1251], Loss: 4.3187(4.3632), Acc: 0.3477(0.2693)
2021-12-29 03:44:24,723 Epoch[056/310], Step[1150/1251], Loss: 4.5758(4.3633), Acc: 0.2764(0.2687)
2021-12-29 03:45:27,987 Epoch[056/310], Step[1200/1251], Loss: 4.3672(4.3639), Acc: 0.3184(0.2678)
2021-12-29 03:46:31,394 Epoch[056/310], Step[1250/1251], Loss: 4.6640(4.3611), Acc: 0.0898(0.2687)
2021-12-29 03:46:33,466 ----- Validation after Epoch: 56
2021-12-29 03:47:33,156 Val Step[0000/1563], Loss: 0.8740 (0.8740), Acc@1: 0.9062 (0.9062), Acc@5: 0.9375 (0.9375)
2021-12-29 03:47:34,784 Val Step[0050/1563], Loss: 2.4830 (1.0946), Acc@1: 0.4375 (0.7684), Acc@5: 0.8125 (0.9265)
2021-12-29 03:47:36,308 Val Step[0100/1563], Loss: 2.5508 (1.5318), Acc@1: 0.3438 (0.6640), Acc@5: 0.7812 (0.8744)
2021-12-29 03:47:37,831 Val Step[0150/1563], Loss: 0.7941 (1.4286), Acc@1: 0.9062 (0.6858), Acc@5: 0.9688 (0.8866)
2021-12-29 03:47:39,336 Val Step[0200/1563], Loss: 1.5505 (1.4461), Acc@1: 0.5938 (0.6853), Acc@5: 0.9062 (0.8846)
2021-12-29 03:47:40,819 Val Step[0250/1563], Loss: 1.6504 (1.3794), Acc@1: 0.5938 (0.6987), Acc@5: 0.8750 (0.8940)
2021-12-29 03:47:42,347 Val Step[0300/1563], Loss: 1.3456 (1.4421), Acc@1: 0.6562 (0.6768), Acc@5: 0.9375 (0.8886)
2021-12-29 03:47:43,858 Val Step[0350/1563], Loss: 1.4721 (1.4617), Acc@1: 0.6875 (0.6669), Acc@5: 0.8750 (0.8889)
2021-12-29 03:47:45,365 Val Step[0400/1563], Loss: 1.1889 (1.4520), Acc@1: 0.7188 (0.6633), Acc@5: 0.9375 (0.8918)
2021-12-29 03:47:46,920 Val Step[0450/1563], Loss: 1.1576 (1.4481), Acc@1: 0.7188 (0.6615), Acc@5: 0.9688 (0.8939)
2021-12-29 03:47:48,540 Val Step[0500/1563], Loss: 0.4764 (1.4412), Acc@1: 0.9688 (0.6644), Acc@5: 1.0000 (0.8949)
2021-12-29 03:47:50,062 Val Step[0550/1563], Loss: 0.9836 (1.4140), Acc@1: 0.8438 (0.6724), Acc@5: 0.9375 (0.8977)
2021-12-29 03:47:51,636 Val Step[0600/1563], Loss: 0.7905 (1.4164), Acc@1: 0.7812 (0.6729), Acc@5: 1.0000 (0.8970)
2021-12-29 03:47:53,219 Val Step[0650/1563], Loss: 1.0534 (1.4405), Acc@1: 0.7188 (0.6677), Acc@5: 0.9688 (0.8932)
2021-12-29 03:47:54,848 Val Step[0700/1563], Loss: 1.4885 (1.4844), Acc@1: 0.7812 (0.6590), Acc@5: 0.8750 (0.8867)
2021-12-29 03:47:56,433 Val Step[0750/1563], Loss: 2.2084 (1.5319), Acc@1: 0.5625 (0.6509), Acc@5: 0.7812 (0.8794)
2021-12-29 03:47:57,993 Val Step[0800/1563], Loss: 2.0475 (1.5771), Acc@1: 0.6250 (0.6415), Acc@5: 0.7812 (0.8725)
2021-12-29 03:47:59,557 Val Step[0850/1563], Loss: 2.1008 (1.6114), Acc@1: 0.5625 (0.6361), Acc@5: 0.7500 (0.8671)
2021-12-29 03:48:01,117 Val Step[0900/1563], Loss: 0.7927 (1.6149), Acc@1: 0.8750 (0.6375), Acc@5: 0.9375 (0.8658)
2021-12-29 03:48:02,813 Val Step[0950/1563], Loss: 1.5460 (1.6449), Acc@1: 0.7812 (0.6316), Acc@5: 0.8125 (0.8607)
2021-12-29 03:48:04,367 Val Step[1000/1563], Loss: 1.0234 (1.6747), Acc@1: 0.9062 (0.6259), Acc@5: 0.9688 (0.8565)
2021-12-29 03:48:05,918 Val Step[1050/1563], Loss: 0.3549 (1.6915), Acc@1: 0.9688 (0.6234), Acc@5: 0.9688 (0.8543)
2021-12-29 03:48:07,496 Val Step[1100/1563], Loss: 1.4250 (1.7125), Acc@1: 0.7188 (0.6198), Acc@5: 0.8750 (0.8509)
2021-12-29 03:48:09,074 Val Step[1150/1563], Loss: 1.5258 (1.7327), Acc@1: 0.7500 (0.6162), Acc@5: 0.7812 (0.8476)
2021-12-29 03:48:10,633 Val Step[1200/1563], Loss: 1.7157 (1.7538), Acc@1: 0.7500 (0.6122), Acc@5: 0.8438 (0.8437)
2021-12-29 03:48:12,185 Val Step[1250/1563], Loss: 1.1822 (1.7696), Acc@1: 0.8125 (0.6100), Acc@5: 0.9062 (0.8409)
2021-12-29 03:48:13,760 Val Step[1300/1563], Loss: 1.6291 (1.7816), Acc@1: 0.7188 (0.6076), Acc@5: 0.8438 (0.8394)
2021-12-29 03:48:15,267 Val Step[1350/1563], Loss: 2.6230 (1.8010), Acc@1: 0.2500 (0.6036), Acc@5: 0.7500 (0.8362)
2021-12-29 03:48:16,800 Val Step[1400/1563], Loss: 1.8456 (1.8118), Acc@1: 0.5312 (0.6009), Acc@5: 0.8438 (0.8344)
2021-12-29 03:48:18,382 Val Step[1450/1563], Loss: 1.8108 (1.8175), Acc@1: 0.5625 (0.5998), Acc@5: 0.8125 (0.8333)
2021-12-29 03:48:19,932 Val Step[1500/1563], Loss: 2.1336 (1.8047), Acc@1: 0.4375 (0.6023), Acc@5: 0.8750 (0.8353)
2021-12-29 03:48:21,551 Val Step[1550/1563], Loss: 1.1971 (1.7991), Acc@1: 0.8750 (0.6033), Acc@5: 0.9062 (0.8360)
2021-12-29 03:48:22,643 ----- Epoch[056/310], Validation Loss: 1.7968, Validation Acc@1: 0.6038, Validation Acc@5: 0.8362, time: 109.17
2021-12-29 03:48:22,644 ----- Epoch[056/310], Train Loss: 4.3611, Train Acc: 0.2687, time: 1648.74, Best Val(epoch56) Acc@1: 0.6038
2021-12-29 03:48:22,846 Max accuracy so far: 0.6038 at epoch_56
2021-12-29 03:48:22,846 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 03:48:22,847 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 03:48:22,935 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 03:48:22,936 Now training epoch 57. LR=0.000914
2021-12-29 03:49:38,866 Epoch[057/310], Step[0000/1251], Loss: 4.3661(4.3661), Acc: 0.1768(0.1768)
2021-12-29 03:50:41,420 Epoch[057/310], Step[0050/1251], Loss: 4.1153(4.3057), Acc: 0.3555(0.2605)
2021-12-29 03:51:42,897 Epoch[057/310], Step[0100/1251], Loss: 4.0873(4.3093), Acc: 0.2012(0.2694)
2021-12-29 03:52:45,310 Epoch[057/310], Step[0150/1251], Loss: 4.8787(4.3372), Acc: 0.1826(0.2704)
2021-12-29 03:53:48,513 Epoch[057/310], Step[0200/1251], Loss: 4.1321(4.3312), Acc: 0.3018(0.2718)
2021-12-29 03:54:50,270 Epoch[057/310], Step[0250/1251], Loss: 4.2241(4.3359), Acc: 0.2656(0.2718)
2021-12-29 03:55:52,569 Epoch[057/310], Step[0300/1251], Loss: 4.6967(4.3436), Acc: 0.2910(0.2719)
2021-12-29 03:56:55,120 Epoch[057/310], Step[0350/1251], Loss: 4.1639(4.3555), Acc: 0.2314(0.2726)
2021-12-29 03:57:58,575 Epoch[057/310], Step[0400/1251], Loss: 4.2093(4.3536), Acc: 0.2959(0.2716)
2021-12-29 03:59:02,287 Epoch[057/310], Step[0450/1251], Loss: 4.3194(4.3506), Acc: 0.2578(0.2703)
2021-12-29 04:00:03,830 Epoch[057/310], Step[0500/1251], Loss: 4.3414(4.3563), Acc: 0.3398(0.2711)
2021-12-29 04:01:05,754 Epoch[057/310], Step[0550/1251], Loss: 3.4818(4.3545), Acc: 0.1250(0.2709)
2021-12-29 04:02:07,759 Epoch[057/310], Step[0600/1251], Loss: 4.3905(4.3534), Acc: 0.2998(0.2719)
2021-12-29 04:03:11,658 Epoch[057/310], Step[0650/1251], Loss: 4.5022(4.3604), Acc: 0.3818(0.2711)
2021-12-29 04:04:15,663 Epoch[057/310], Step[0700/1251], Loss: 3.7679(4.3581), Acc: 0.2129(0.2700)
2021-12-29 04:05:18,105 Epoch[057/310], Step[0750/1251], Loss: 5.0758(4.3557), Acc: 0.2246(0.2700)
2021-12-29 04:06:20,813 Epoch[057/310], Step[0800/1251], Loss: 4.1337(4.3564), Acc: 0.1631(0.2705)
2021-12-29 04:07:22,580 Epoch[057/310], Step[0850/1251], Loss: 4.7344(4.3587), Acc: 0.2344(0.2715)
2021-12-29 04:08:25,455 Epoch[057/310], Step[0900/1251], Loss: 5.0191(4.3625), Acc: 0.2910(0.2711)
2021-12-29 04:09:27,680 Epoch[057/310], Step[0950/1251], Loss: 4.3508(4.3611), Acc: 0.3604(0.2716)
2021-12-29 04:10:29,564 Epoch[057/310], Step[1000/1251], Loss: 4.8375(4.3626), Acc: 0.2754(0.2707)
2021-12-29 04:11:31,884 Epoch[057/310], Step[1050/1251], Loss: 3.4837(4.3582), Acc: 0.3672(0.2720)
2021-12-29 04:12:35,171 Epoch[057/310], Step[1100/1251], Loss: 3.5410(4.3545), Acc: 0.3955(0.2727)
2021-12-29 04:13:38,740 Epoch[057/310], Step[1150/1251], Loss: 4.0883(4.3540), Acc: 0.1504(0.2729)
2021-12-29 04:14:40,022 Epoch[057/310], Step[1200/1251], Loss: 4.6834(4.3572), Acc: 0.3174(0.2732)
2021-12-29 04:15:43,407 Epoch[057/310], Step[1250/1251], Loss: 4.1379(4.3580), Acc: 0.4170(0.2727)
2021-12-29 04:15:45,454 ----- Epoch[057/310], Train Loss: 4.3580, Train Acc: 0.2727, time: 1642.51, Best Val(epoch56) Acc@1: 0.6038
2021-12-29 04:15:45,454 Now training epoch 58. LR=0.000911
2021-12-29 04:17:04,911 Epoch[058/310], Step[0000/1251], Loss: 3.8540(3.8540), Acc: 0.3066(0.3066)
2021-12-29 04:18:07,310 Epoch[058/310], Step[0050/1251], Loss: 4.0062(4.2080), Acc: 0.1963(0.2903)
2021-12-29 04:19:09,825 Epoch[058/310], Step[0100/1251], Loss: 4.1628(4.2427), Acc: 0.1689(0.2804)
2021-12-29 04:20:12,470 Epoch[058/310], Step[0150/1251], Loss: 4.1102(4.2523), Acc: 0.1035(0.2816)
2021-12-29 04:21:13,490 Epoch[058/310], Step[0200/1251], Loss: 3.8574(4.2702), Acc: 0.1982(0.2802)
2021-12-29 04:22:15,987 Epoch[058/310], Step[0250/1251], Loss: 4.5243(4.2750), Acc: 0.3379(0.2779)
2021-12-29 04:23:19,313 Epoch[058/310], Step[0300/1251], Loss: 4.0078(4.2859), Acc: 0.3105(0.2773)
2021-12-29 04:24:22,895 Epoch[058/310], Step[0350/1251], Loss: 4.6206(4.2897), Acc: 0.2822(0.2780)
2021-12-29 04:25:24,780 Epoch[058/310], Step[0400/1251], Loss: 4.1637(4.2979), Acc: 0.3223(0.2764)
2021-12-29 04:26:28,670 Epoch[058/310], Step[0450/1251], Loss: 4.4874(4.3094), Acc: 0.1650(0.2756)
2021-12-29 04:27:32,279 Epoch[058/310], Step[0500/1251], Loss: 4.8482(4.3104), Acc: 0.1680(0.2782)
2021-12-29 04:28:35,712 Epoch[058/310], Step[0550/1251], Loss: 4.9973(4.3188), Acc: 0.1816(0.2756)
2021-12-29 04:29:37,888 Epoch[058/310], Step[0600/1251], Loss: 4.1532(4.3164), Acc: 0.2773(0.2763)
2021-12-29 04:30:40,022 Epoch[058/310], Step[0650/1251], Loss: 4.1476(4.3177), Acc: 0.3994(0.2793)
2021-12-29 04:31:42,605 Epoch[058/310], Step[0700/1251], Loss: 4.7180(4.3204), Acc: 0.2861(0.2787)
2021-12-29 04:32:45,893 Epoch[058/310], Step[0750/1251], Loss: 4.2994(4.3233), Acc: 0.1953(0.2779)
2021-12-29 04:33:48,796 Epoch[058/310], Step[0800/1251], Loss: 4.8367(4.3259), Acc: 0.2510(0.2774)
2021-12-29 04:34:51,123 Epoch[058/310], Step[0850/1251], Loss: 4.2412(4.3267), Acc: 0.2002(0.2768)
2021-12-29 04:35:54,277 Epoch[058/310], Step[0900/1251], Loss: 4.3981(4.3233), Acc: 0.3398(0.2765)
2021-12-29 04:36:57,560 Epoch[058/310], Step[0950/1251], Loss: 4.8530(4.3276), Acc: 0.2100(0.2762)
2021-12-29 04:38:01,520 Epoch[058/310], Step[1000/1251], Loss: 4.0723(4.3314), Acc: 0.1846(0.2759)
2021-12-29 04:39:04,491 Epoch[058/310], Step[1050/1251], Loss: 3.9257(4.3285), Acc: 0.3867(0.2768)
2021-12-29 04:40:07,585 Epoch[058/310], Step[1100/1251], Loss: 4.5016(4.3280), Acc: 0.3096(0.2764)
2021-12-29 04:41:11,716 Epoch[058/310], Step[1150/1251], Loss: 4.8886(4.3252), Acc: 0.1934(0.2760)
2021-12-29 04:42:13,658 Epoch[058/310], Step[1200/1251], Loss: 4.0190(4.3242), Acc: 0.2578(0.2751)
2021-12-29 04:43:14,782 Epoch[058/310], Step[1250/1251], Loss: 4.3482(4.3231), Acc: 0.3262(0.2746)
2021-12-29 04:43:17,031 ----- Validation after Epoch: 58
2021-12-29 04:44:17,700 Val Step[0000/1563], Loss: 1.0786 (1.0786), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2021-12-29 04:44:19,306 Val Step[0050/1563], Loss: 2.5409 (1.1545), Acc@1: 0.3750 (0.7714), Acc@5: 0.8438 (0.9234)
2021-12-29 04:44:20,846 Val Step[0100/1563], Loss: 1.9633 (1.5597), Acc@1: 0.4688 (0.6649), Acc@5: 0.8438 (0.8787)
2021-12-29 04:44:22,456 Val Step[0150/1563], Loss: 0.6285 (1.4467), Acc@1: 0.8750 (0.6910), Acc@5: 1.0000 (0.8887)
2021-12-29 04:44:23,982 Val Step[0200/1563], Loss: 1.5401 (1.4953), Acc@1: 0.6250 (0.6824), Acc@5: 0.9062 (0.8834)
2021-12-29 04:44:25,449 Val Step[0250/1563], Loss: 0.9485 (1.4211), Acc@1: 0.8125 (0.6996), Acc@5: 1.0000 (0.8929)
2021-12-29 04:44:26,964 Val Step[0300/1563], Loss: 1.7077 (1.4838), Acc@1: 0.6250 (0.6805), Acc@5: 0.8750 (0.8875)
2021-12-29 04:44:28,494 Val Step[0350/1563], Loss: 1.6583 (1.4916), Acc@1: 0.6562 (0.6761), Acc@5: 0.8750 (0.8908)
2021-12-29 04:44:30,095 Val Step[0400/1563], Loss: 1.6346 (1.4926), Acc@1: 0.6875 (0.6700), Acc@5: 0.9062 (0.8931)
2021-12-29 04:44:31,608 Val Step[0450/1563], Loss: 1.0349 (1.4898), Acc@1: 0.6250 (0.6683), Acc@5: 1.0000 (0.8951)
2021-12-29 04:44:33,093 Val Step[0500/1563], Loss: 0.5843 (1.4821), Acc@1: 0.9375 (0.6700), Acc@5: 1.0000 (0.8965)
2021-12-29 04:44:34,805 Val Step[0550/1563], Loss: 1.4032 (1.4544), Acc@1: 0.5938 (0.6779), Acc@5: 0.9375 (0.8992)
2021-12-29 04:44:36,370 Val Step[0600/1563], Loss: 1.1039 (1.4554), Acc@1: 0.8125 (0.6781), Acc@5: 0.9375 (0.8983)
2021-12-29 04:44:37,834 Val Step[0650/1563], Loss: 1.1200 (1.4834), Acc@1: 0.7500 (0.6727), Acc@5: 0.9688 (0.8941)
2021-12-29 04:44:39,327 Val Step[0700/1563], Loss: 2.2917 (1.5262), Acc@1: 0.5312 (0.6639), Acc@5: 0.7812 (0.8879)
2021-12-29 04:44:40,888 Val Step[0750/1563], Loss: 1.9270 (1.5701), Acc@1: 0.5938 (0.6558), Acc@5: 0.7500 (0.8814)
2021-12-29 04:44:42,346 Val Step[0800/1563], Loss: 1.7984 (1.6158), Acc@1: 0.5625 (0.6460), Acc@5: 0.8750 (0.8747)
2021-12-29 04:44:43,910 Val Step[0850/1563], Loss: 2.0212 (1.6485), Acc@1: 0.4688 (0.6390), Acc@5: 0.8438 (0.8698)
2021-12-29 04:44:45,487 Val Step[0900/1563], Loss: 0.7276 (1.6530), Acc@1: 0.8750 (0.6394), Acc@5: 0.9062 (0.8682)
2021-12-29 04:44:47,062 Val Step[0950/1563], Loss: 1.8083 (1.6803), Acc@1: 0.6250 (0.6340), Acc@5: 0.8125 (0.8640)
2021-12-29 04:44:48,527 Val Step[1000/1563], Loss: 0.9120 (1.7074), Acc@1: 0.9375 (0.6285), Acc@5: 1.0000 (0.8591)
2021-12-29 04:44:50,023 Val Step[1050/1563], Loss: 0.5170 (1.7266), Acc@1: 0.9688 (0.6247), Acc@5: 0.9688 (0.8561)
2021-12-29 04:44:51,532 Val Step[1100/1563], Loss: 1.5904 (1.7463), Acc@1: 0.6875 (0.6215), Acc@5: 0.8125 (0.8520)
2021-12-29 04:44:52,924 Val Step[1150/1563], Loss: 1.6076 (1.7656), Acc@1: 0.7812 (0.6185), Acc@5: 0.7812 (0.8488)
2021-12-29 04:44:54,393 Val Step[1200/1563], Loss: 1.2524 (1.7829), Acc@1: 0.7812 (0.6151), Acc@5: 0.8438 (0.8457)
2021-12-29 04:44:55,979 Val Step[1250/1563], Loss: 0.9301 (1.7975), Acc@1: 0.8438 (0.6133), Acc@5: 0.9062 (0.8431)
2021-12-29 04:44:57,493 Val Step[1300/1563], Loss: 1.5150 (1.8093), Acc@1: 0.7812 (0.6102), Acc@5: 0.8438 (0.8418)
2021-12-29 04:44:58,966 Val Step[1350/1563], Loss: 2.4235 (1.8313), Acc@1: 0.3125 (0.6056), Acc@5: 0.7812 (0.8378)
2021-12-29 04:45:00,443 Val Step[1400/1563], Loss: 1.6736 (1.8409), Acc@1: 0.7188 (0.6040), Acc@5: 0.8125 (0.8363)
2021-12-29 04:45:01,933 Val Step[1450/1563], Loss: 2.4349 (1.8443), Acc@1: 0.2812 (0.6033), Acc@5: 0.7812 (0.8359)
2021-12-29 04:45:03,480 Val Step[1500/1563], Loss: 1.8863 (1.8317), Acc@1: 0.5938 (0.6060), Acc@5: 0.8438 (0.8380)
2021-12-29 04:45:05,070 Val Step[1550/1563], Loss: 1.2176 (1.8271), Acc@1: 0.8750 (0.6065), Acc@5: 0.9062 (0.8387)
2021-12-29 04:45:05,924 ----- Epoch[058/310], Validation Loss: 1.8250, Validation Acc@1: 0.6071, Validation Acc@5: 0.8388, time: 108.89
2021-12-29 04:45:05,924 ----- Epoch[058/310], Train Loss: 4.3231, Train Acc: 0.2746, time: 1651.57, Best Val(epoch58) Acc@1: 0.6071
2021-12-29 04:45:06,142 Max accuracy so far: 0.6071 at epoch_58
2021-12-29 04:45:06,142 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 04:45:06,142 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 04:45:06,235 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 04:45:06,235 Now training epoch 59. LR=0.000908
2021-12-29 04:46:26,655 Epoch[059/310], Step[0000/1251], Loss: 4.5078(4.5078), Acc: 0.2119(0.2119)
2021-12-29 04:47:29,190 Epoch[059/310], Step[0050/1251], Loss: 4.2363(4.2668), Acc: 0.3936(0.2861)
2021-12-29 04:48:32,124 Epoch[059/310], Step[0100/1251], Loss: 4.8923(4.3091), Acc: 0.2012(0.2785)
2021-12-29 04:49:34,491 Epoch[059/310], Step[0150/1251], Loss: 4.3842(4.3503), Acc: 0.3838(0.2727)
2021-12-29 04:50:37,282 Epoch[059/310], Step[0200/1251], Loss: 4.1577(4.3560), Acc: 0.2852(0.2720)
2021-12-29 04:51:39,397 Epoch[059/310], Step[0250/1251], Loss: 4.5548(4.3409), Acc: 0.2705(0.2748)
2021-12-29 04:52:41,876 Epoch[059/310], Step[0300/1251], Loss: 4.5348(4.3438), Acc: 0.3154(0.2756)
2021-12-29 04:53:43,374 Epoch[059/310], Step[0350/1251], Loss: 4.2230(4.3474), Acc: 0.1924(0.2735)
2021-12-29 04:54:44,721 Epoch[059/310], Step[0400/1251], Loss: 4.4070(4.3395), Acc: 0.3750(0.2738)
2021-12-29 04:55:48,248 Epoch[059/310], Step[0450/1251], Loss: 3.9621(4.3408), Acc: 0.2959(0.2736)
2021-12-29 04:56:50,557 Epoch[059/310], Step[0500/1251], Loss: 4.1993(4.3388), Acc: 0.3076(0.2728)
2021-12-29 04:57:53,813 Epoch[059/310], Step[0550/1251], Loss: 4.3858(4.3410), Acc: 0.3125(0.2726)
2021-12-29 04:58:55,087 Epoch[059/310], Step[0600/1251], Loss: 4.5920(4.3412), Acc: 0.2441(0.2725)
2021-12-29 04:59:58,314 Epoch[059/310], Step[0650/1251], Loss: 4.1085(4.3404), Acc: 0.3555(0.2721)
2021-12-29 05:01:00,062 Epoch[059/310], Step[0700/1251], Loss: 3.8756(4.3375), Acc: 0.3594(0.2727)
2021-12-29 05:02:03,810 Epoch[059/310], Step[0750/1251], Loss: 4.1701(4.3402), Acc: 0.4189(0.2734)
2021-12-29 05:03:08,008 Epoch[059/310], Step[0800/1251], Loss: 4.1265(4.3421), Acc: 0.3418(0.2732)
2021-12-29 05:04:10,662 Epoch[059/310], Step[0850/1251], Loss: 4.9229(4.3408), Acc: 0.1475(0.2734)
2021-12-29 05:05:12,890 Epoch[059/310], Step[0900/1251], Loss: 4.1403(4.3467), Acc: 0.2666(0.2731)
2021-12-29 05:06:16,354 Epoch[059/310], Step[0950/1251], Loss: 4.2820(4.3461), Acc: 0.2285(0.2720)
2021-12-29 05:07:17,634 Epoch[059/310], Step[1000/1251], Loss: 4.7370(4.3472), Acc: 0.2998(0.2716)
2021-12-29 05:08:19,827 Epoch[059/310], Step[1050/1251], Loss: 4.8786(4.3507), Acc: 0.2070(0.2717)
2021-12-29 05:09:21,349 Epoch[059/310], Step[1100/1251], Loss: 3.8376(4.3476), Acc: 0.3096(0.2716)
2021-12-29 05:10:23,410 Epoch[059/310], Step[1150/1251], Loss: 4.2392(4.3440), Acc: 0.2441(0.2731)
2021-12-29 05:11:26,179 Epoch[059/310], Step[1200/1251], Loss: 4.0929(4.3409), Acc: 0.1143(0.2732)
2021-12-29 05:12:28,154 Epoch[059/310], Step[1250/1251], Loss: 3.9973(4.3406), Acc: 0.2783(0.2727)
2021-12-29 05:12:30,130 ----- Epoch[059/310], Train Loss: 4.3406, Train Acc: 0.2727, time: 1643.89, Best Val(epoch58) Acc@1: 0.6071
2021-12-29 05:12:30,130 Now training epoch 60. LR=0.000905
2021-12-29 05:13:51,149 Epoch[060/310], Step[0000/1251], Loss: 4.4950(4.4950), Acc: 0.2568(0.2568)
2021-12-29 05:14:53,573 Epoch[060/310], Step[0050/1251], Loss: 4.6602(4.3563), Acc: 0.3281(0.2693)
2021-12-29 05:15:56,313 Epoch[060/310], Step[0100/1251], Loss: 4.3900(4.3598), Acc: 0.3223(0.2764)
2021-12-29 05:16:58,145 Epoch[060/310], Step[0150/1251], Loss: 4.1253(4.3450), Acc: 0.1475(0.2734)
2021-12-29 05:17:59,186 Epoch[060/310], Step[0200/1251], Loss: 4.4794(4.3435), Acc: 0.2705(0.2673)
2021-12-29 05:19:01,508 Epoch[060/310], Step[0250/1251], Loss: 4.0574(4.3410), Acc: 0.1865(0.2659)
2021-12-29 05:20:04,727 Epoch[060/310], Step[0300/1251], Loss: 4.4983(4.3411), Acc: 0.2188(0.2672)
2021-12-29 05:21:06,671 Epoch[060/310], Step[0350/1251], Loss: 4.1644(4.3399), Acc: 0.3936(0.2700)
2021-12-29 05:22:09,793 Epoch[060/310], Step[0400/1251], Loss: 4.3116(4.3309), Acc: 0.2842(0.2712)
2021-12-29 05:23:13,042 Epoch[060/310], Step[0450/1251], Loss: 4.4404(4.3305), Acc: 0.3145(0.2701)
2021-12-29 05:24:15,236 Epoch[060/310], Step[0500/1251], Loss: 3.6845(4.3328), Acc: 0.3643(0.2717)
2021-12-29 05:25:17,809 Epoch[060/310], Step[0550/1251], Loss: 4.6039(4.3337), Acc: 0.2236(0.2715)
2021-12-29 05:26:20,530 Epoch[060/310], Step[0600/1251], Loss: 4.4757(4.3323), Acc: 0.2344(0.2724)
2021-12-29 05:27:23,160 Epoch[060/310], Step[0650/1251], Loss: 4.2169(4.3296), Acc: 0.3760(0.2722)
2021-12-29 05:28:25,603 Epoch[060/310], Step[0700/1251], Loss: 4.5232(4.3281), Acc: 0.1533(0.2728)
2021-12-29 05:29:29,389 Epoch[060/310], Step[0750/1251], Loss: 4.4181(4.3306), Acc: 0.1621(0.2700)
2021-12-29 05:30:31,738 Epoch[060/310], Step[0800/1251], Loss: 4.2419(4.3319), Acc: 0.3574(0.2687)
2021-12-29 05:31:34,666 Epoch[060/310], Step[0850/1251], Loss: 4.0647(4.3290), Acc: 0.4463(0.2696)
2021-12-29 05:32:38,091 Epoch[060/310], Step[0900/1251], Loss: 4.2808(4.3268), Acc: 0.2891(0.2702)
2021-12-29 05:33:40,985 Epoch[060/310], Step[0950/1251], Loss: 4.4589(4.3307), Acc: 0.3008(0.2707)
2021-12-29 05:34:42,661 Epoch[060/310], Step[1000/1251], Loss: 4.2175(4.3296), Acc: 0.3623(0.2711)
2021-12-29 05:35:44,624 Epoch[060/310], Step[1050/1251], Loss: 4.5224(4.3306), Acc: 0.2842(0.2714)
2021-12-29 05:36:47,464 Epoch[060/310], Step[1100/1251], Loss: 4.2308(4.3334), Acc: 0.2432(0.2707)
2021-12-29 05:37:49,542 Epoch[060/310], Step[1150/1251], Loss: 4.5579(4.3350), Acc: 0.3359(0.2708)
2021-12-29 05:38:53,030 Epoch[060/310], Step[1200/1251], Loss: 4.7451(4.3340), Acc: 0.3594(0.2713)
2021-12-29 05:39:54,901 Epoch[060/310], Step[1250/1251], Loss: 4.6023(4.3327), Acc: 0.2031(0.2727)
2021-12-29 05:39:56,951 ----- Validation after Epoch: 60
2021-12-29 05:40:58,164 Val Step[0000/1563], Loss: 0.7076 (0.7076), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-29 05:40:59,740 Val Step[0050/1563], Loss: 2.6526 (1.1591), Acc@1: 0.4062 (0.7702), Acc@5: 0.6875 (0.9252)
2021-12-29 05:41:01,213 Val Step[0100/1563], Loss: 1.9982 (1.5275), Acc@1: 0.4375 (0.6708), Acc@5: 0.8438 (0.8793)
2021-12-29 05:41:02,709 Val Step[0150/1563], Loss: 1.1298 (1.4408), Acc@1: 0.8125 (0.6933), Acc@5: 0.9062 (0.8891)
2021-12-29 05:41:04,205 Val Step[0200/1563], Loss: 1.8959 (1.4658), Acc@1: 0.5000 (0.6892), Acc@5: 0.8750 (0.8871)
2021-12-29 05:41:05,696 Val Step[0250/1563], Loss: 1.3305 (1.4017), Acc@1: 0.5000 (0.7028), Acc@5: 1.0000 (0.8965)
2021-12-29 05:41:07,187 Val Step[0300/1563], Loss: 2.0343 (1.4814), Acc@1: 0.5625 (0.6770), Acc@5: 0.8438 (0.8882)
2021-12-29 05:41:08,647 Val Step[0350/1563], Loss: 1.8446 (1.4749), Acc@1: 0.5000 (0.6757), Acc@5: 0.8750 (0.8910)
2021-12-29 05:41:10,259 Val Step[0400/1563], Loss: 1.4567 (1.4684), Acc@1: 0.8125 (0.6728), Acc@5: 0.9375 (0.8934)
2021-12-29 05:41:11,800 Val Step[0450/1563], Loss: 1.2902 (1.4606), Acc@1: 0.6250 (0.6706), Acc@5: 0.9688 (0.8959)
2021-12-29 05:41:13,316 Val Step[0500/1563], Loss: 0.5023 (1.4531), Acc@1: 0.9375 (0.6717), Acc@5: 1.0000 (0.8974)
2021-12-29 05:41:14,922 Val Step[0550/1563], Loss: 0.8958 (1.4211), Acc@1: 0.8125 (0.6808), Acc@5: 1.0000 (0.9010)
2021-12-29 05:41:16,627 Val Step[0600/1563], Loss: 1.0885 (1.4221), Acc@1: 0.7500 (0.6811), Acc@5: 0.9688 (0.8999)
2021-12-29 05:41:18,215 Val Step[0650/1563], Loss: 1.2437 (1.4453), Acc@1: 0.7500 (0.6777), Acc@5: 0.9375 (0.8961)
2021-12-29 05:41:19,847 Val Step[0700/1563], Loss: 1.6027 (1.4912), Acc@1: 0.6875 (0.6683), Acc@5: 0.8750 (0.8891)
2021-12-29 05:41:21,385 Val Step[0750/1563], Loss: 1.8988 (1.5332), Acc@1: 0.5938 (0.6598), Acc@5: 0.7812 (0.8827)
2021-12-29 05:41:22,913 Val Step[0800/1563], Loss: 1.4981 (1.5773), Acc@1: 0.6250 (0.6500), Acc@5: 0.9375 (0.8761)
2021-12-29 05:41:24,453 Val Step[0850/1563], Loss: 2.2817 (1.6092), Acc@1: 0.4375 (0.6438), Acc@5: 0.7500 (0.8715)
2021-12-29 05:41:26,018 Val Step[0900/1563], Loss: 0.7931 (1.6126), Acc@1: 0.9062 (0.6449), Acc@5: 0.9375 (0.8698)
2021-12-29 05:41:27,576 Val Step[0950/1563], Loss: 1.7954 (1.6433), Acc@1: 0.6875 (0.6388), Acc@5: 0.7812 (0.8643)
2021-12-29 05:41:29,138 Val Step[1000/1563], Loss: 0.8515 (1.6728), Acc@1: 0.9375 (0.6329), Acc@5: 0.9688 (0.8597)
2021-12-29 05:41:30,656 Val Step[1050/1563], Loss: 0.4334 (1.6917), Acc@1: 0.9688 (0.6296), Acc@5: 1.0000 (0.8573)
2021-12-29 05:41:32,243 Val Step[1100/1563], Loss: 1.7692 (1.7112), Acc@1: 0.7500 (0.6263), Acc@5: 0.7812 (0.8537)
2021-12-29 05:41:33,774 Val Step[1150/1563], Loss: 1.9576 (1.7361), Acc@1: 0.6562 (0.6214), Acc@5: 0.7812 (0.8501)
2021-12-29 05:41:35,426 Val Step[1200/1563], Loss: 1.5683 (1.7579), Acc@1: 0.8125 (0.6179), Acc@5: 0.8438 (0.8466)
2021-12-29 05:41:36,964 Val Step[1250/1563], Loss: 0.8770 (1.7732), Acc@1: 0.8750 (0.6160), Acc@5: 0.9375 (0.8440)
2021-12-29 05:41:38,434 Val Step[1300/1563], Loss: 1.2437 (1.7862), Acc@1: 0.7188 (0.6134), Acc@5: 0.8750 (0.8424)
2021-12-29 05:41:39,894 Val Step[1350/1563], Loss: 2.4799 (1.8109), Acc@1: 0.3125 (0.6089), Acc@5: 0.7812 (0.8383)
2021-12-29 05:41:41,408 Val Step[1400/1563], Loss: 1.6536 (1.8193), Acc@1: 0.6562 (0.6068), Acc@5: 0.8750 (0.8371)
2021-12-29 05:41:42,879 Val Step[1450/1563], Loss: 1.8101 (1.8273), Acc@1: 0.5625 (0.6049), Acc@5: 0.8438 (0.8360)
2021-12-29 05:41:44,426 Val Step[1500/1563], Loss: 1.6688 (1.8169), Acc@1: 0.5625 (0.6072), Acc@5: 0.9062 (0.8375)
2021-12-29 05:41:45,815 Val Step[1550/1563], Loss: 1.0783 (1.8132), Acc@1: 0.8750 (0.6082), Acc@5: 0.9062 (0.8381)
2021-12-29 05:41:46,690 ----- Epoch[060/310], Validation Loss: 1.8101, Validation Acc@1: 0.6089, Validation Acc@5: 0.8385, time: 109.74
2021-12-29 05:41:46,690 ----- Epoch[060/310], Train Loss: 4.3327, Train Acc: 0.2727, time: 1646.82, Best Val(epoch60) Acc@1: 0.6089
2021-12-29 05:41:46,916 Max accuracy so far: 0.6089 at epoch_60
2021-12-29 05:41:46,917 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 05:41:46,917 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 05:41:46,993 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 05:41:47,159 ----- Save model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-60-Loss-4.335726212349822.pdparams
2021-12-29 05:41:47,159 ----- Save optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-60-Loss-4.335726212349822.pdopt
2021-12-29 05:41:47,226 ----- Save ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-60-Loss-4.335726212349822-EMA.pdparams
2021-12-29 05:41:47,227 Now training epoch 61. LR=0.000902
2021-12-29 05:43:06,202 Epoch[061/310], Step[0000/1251], Loss: 3.8192(3.8192), Acc: 0.4434(0.4434)
2021-12-29 05:44:08,672 Epoch[061/310], Step[0050/1251], Loss: 4.7126(4.2636), Acc: 0.2979(0.2822)
2021-12-29 05:45:10,750 Epoch[061/310], Step[0100/1251], Loss: 4.5444(4.2862), Acc: 0.3262(0.2626)
2021-12-29 05:46:13,820 Epoch[061/310], Step[0150/1251], Loss: 4.1111(4.2994), Acc: 0.3555(0.2606)
2021-12-29 05:47:16,696 Epoch[061/310], Step[0200/1251], Loss: 4.7291(4.3265), Acc: 0.1982(0.2578)
2021-12-29 05:48:19,042 Epoch[061/310], Step[0250/1251], Loss: 4.1660(4.3197), Acc: 0.2754(0.2621)
2021-12-29 05:49:22,085 Epoch[061/310], Step[0300/1251], Loss: 5.1095(4.3144), Acc: 0.2588(0.2639)
2021-12-29 05:50:25,635 Epoch[061/310], Step[0350/1251], Loss: 4.5565(4.3168), Acc: 0.1914(0.2648)
2021-12-29 05:51:29,550 Epoch[061/310], Step[0400/1251], Loss: 4.0705(4.3253), Acc: 0.3311(0.2642)
2021-12-29 05:52:32,216 Epoch[061/310], Step[0450/1251], Loss: 4.1204(4.3295), Acc: 0.3330(0.2663)
2021-12-29 05:53:33,774 Epoch[061/310], Step[0500/1251], Loss: 4.6822(4.3240), Acc: 0.2129(0.2669)
2021-12-29 05:54:37,187 Epoch[061/310], Step[0550/1251], Loss: 3.8782(4.3258), Acc: 0.3291(0.2676)
2021-12-29 05:55:40,720 Epoch[061/310], Step[0600/1251], Loss: 4.8601(4.3275), Acc: 0.2812(0.2670)
2021-12-29 05:56:43,783 Epoch[061/310], Step[0650/1251], Loss: 3.9886(4.3268), Acc: 0.2500(0.2670)
2021-12-29 05:57:45,533 Epoch[061/310], Step[0700/1251], Loss: 4.1893(4.3254), Acc: 0.3203(0.2677)
2021-12-29 05:58:48,104 Epoch[061/310], Step[0750/1251], Loss: 4.7007(4.3243), Acc: 0.0811(0.2688)
2021-12-29 05:59:51,430 Epoch[061/310], Step[0800/1251], Loss: 4.5410(4.3256), Acc: 0.2041(0.2699)
2021-12-29 06:00:53,839 Epoch[061/310], Step[0850/1251], Loss: 4.1765(4.3260), Acc: 0.2549(0.2707)
2021-12-29 06:01:55,189 Epoch[061/310], Step[0900/1251], Loss: 4.2959(4.3214), Acc: 0.3174(0.2720)
2021-12-29 06:02:58,557 Epoch[061/310], Step[0950/1251], Loss: 3.9562(4.3165), Acc: 0.2646(0.2723)
2021-12-29 06:04:00,890 Epoch[061/310], Step[1000/1251], Loss: 4.1791(4.3173), Acc: 0.2695(0.2723)
2021-12-29 06:05:02,790 Epoch[061/310], Step[1050/1251], Loss: 4.8711(4.3190), Acc: 0.2041(0.2724)
2021-12-29 06:06:04,610 Epoch[061/310], Step[1100/1251], Loss: 4.0418(4.3186), Acc: 0.2354(0.2716)
2021-12-29 06:07:05,493 Epoch[061/310], Step[1150/1251], Loss: 4.4597(4.3186), Acc: 0.2754(0.2725)
2021-12-29 06:08:08,324 Epoch[061/310], Step[1200/1251], Loss: 4.3716(4.3187), Acc: 0.2217(0.2728)
2021-12-29 06:09:09,869 Epoch[061/310], Step[1250/1251], Loss: 4.5890(4.3187), Acc: 0.3213(0.2729)
2021-12-29 06:09:11,864 ----- Epoch[061/310], Train Loss: 4.3187, Train Acc: 0.2729, time: 1644.63, Best Val(epoch60) Acc@1: 0.6089
2021-12-29 06:09:11,865 Now training epoch 62. LR=0.000899
2021-12-29 06:10:32,996 Epoch[062/310], Step[0000/1251], Loss: 4.2932(4.2932), Acc: 0.2100(0.2100)
2021-12-29 06:11:35,404 Epoch[062/310], Step[0050/1251], Loss: 3.7343(4.3016), Acc: 0.3652(0.2775)
2021-12-29 06:12:37,739 Epoch[062/310], Step[0100/1251], Loss: 4.5586(4.2832), Acc: 0.2676(0.2843)
2021-12-29 06:13:39,052 Epoch[062/310], Step[0150/1251], Loss: 4.3140(4.2783), Acc: 0.3135(0.2797)
2021-12-29 06:14:41,817 Epoch[062/310], Step[0200/1251], Loss: 4.0506(4.2814), Acc: 0.2334(0.2785)
2021-12-29 06:15:44,218 Epoch[062/310], Step[0250/1251], Loss: 4.1382(4.2878), Acc: 0.2930(0.2768)
2021-12-29 06:16:46,312 Epoch[062/310], Step[0300/1251], Loss: 5.0101(4.2949), Acc: 0.1553(0.2809)
2021-12-29 06:17:50,240 Epoch[062/310], Step[0350/1251], Loss: 3.9616(4.2983), Acc: 0.2324(0.2796)
2021-12-29 06:18:52,313 Epoch[062/310], Step[0400/1251], Loss: 5.1779(4.3025), Acc: 0.2178(0.2803)
2021-12-29 06:19:55,389 Epoch[062/310], Step[0450/1251], Loss: 4.5901(4.3068), Acc: 0.2002(0.2791)
2021-12-29 06:20:59,082 Epoch[062/310], Step[0500/1251], Loss: 4.0404(4.3097), Acc: 0.0674(0.2780)
2021-12-29 06:22:01,375 Epoch[062/310], Step[0550/1251], Loss: 4.8756(4.3150), Acc: 0.2920(0.2768)
2021-12-29 06:23:02,499 Epoch[062/310], Step[0600/1251], Loss: 4.2587(4.3152), Acc: 0.3506(0.2791)
2021-12-29 06:24:05,486 Epoch[062/310], Step[0650/1251], Loss: 4.6323(4.3204), Acc: 0.2646(0.2787)
2021-12-29 06:25:05,362 Epoch[062/310], Step[0700/1251], Loss: 4.2484(4.3168), Acc: 0.2021(0.2806)
2021-12-29 06:26:05,632 Epoch[062/310], Step[0750/1251], Loss: 4.3073(4.3178), Acc: 0.2988(0.2817)
2021-12-29 06:27:06,609 Epoch[062/310], Step[0800/1251], Loss: 4.2616(4.3191), Acc: 0.1045(0.2811)
2021-12-29 06:28:08,596 Epoch[062/310], Step[0850/1251], Loss: 4.2352(4.3171), Acc: 0.3242(0.2805)
2021-12-29 06:29:10,825 Epoch[062/310], Step[0900/1251], Loss: 4.2886(4.3182), Acc: 0.0430(0.2798)
2021-12-29 06:30:14,314 Epoch[062/310], Step[0950/1251], Loss: 3.8490(4.3200), Acc: 0.3320(0.2785)
2021-12-29 06:31:17,297 Epoch[062/310], Step[1000/1251], Loss: 4.4218(4.3207), Acc: 0.2227(0.2801)
2021-12-29 06:32:20,377 Epoch[062/310], Step[1050/1251], Loss: 4.4474(4.3206), Acc: 0.2607(0.2792)
2021-12-29 06:33:24,211 Epoch[062/310], Step[1100/1251], Loss: 4.2852(4.3190), Acc: 0.3604(0.2807)
2021-12-29 06:34:26,837 Epoch[062/310], Step[1150/1251], Loss: 4.0124(4.3184), Acc: 0.3633(0.2801)
2021-12-29 06:35:30,611 Epoch[062/310], Step[1200/1251], Loss: 3.9312(4.3175), Acc: 0.0068(0.2806)
2021-12-29 06:36:33,320 Epoch[062/310], Step[1250/1251], Loss: 4.2863(4.3162), Acc: 0.3359(0.2806)
2021-12-29 06:36:35,356 ----- Validation after Epoch: 62
2021-12-29 06:37:35,878 Val Step[0000/1563], Loss: 0.8225 (0.8225), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-29 06:37:37,591 Val Step[0050/1563], Loss: 3.1900 (1.1237), Acc@1: 0.3438 (0.7770), Acc@5: 0.6250 (0.9228)
2021-12-29 06:37:39,121 Val Step[0100/1563], Loss: 2.0498 (1.4810), Acc@1: 0.4688 (0.6726), Acc@5: 0.8125 (0.8840)
2021-12-29 06:37:40,745 Val Step[0150/1563], Loss: 0.7019 (1.3819), Acc@1: 0.8750 (0.6993), Acc@5: 0.9688 (0.8922)
2021-12-29 06:37:42,175 Val Step[0200/1563], Loss: 1.7580 (1.4314), Acc@1: 0.5312 (0.6915), Acc@5: 0.9062 (0.8871)
2021-12-29 06:37:43,655 Val Step[0250/1563], Loss: 0.9292 (1.3640), Acc@1: 0.8438 (0.7058), Acc@5: 1.0000 (0.8957)
2021-12-29 06:37:45,194 Val Step[0300/1563], Loss: 1.9042 (1.4389), Acc@1: 0.5000 (0.6822), Acc@5: 0.8125 (0.8871)
2021-12-29 06:37:46,742 Val Step[0350/1563], Loss: 1.4920 (1.4365), Acc@1: 0.5938 (0.6795), Acc@5: 0.9062 (0.8918)
2021-12-29 06:37:48,222 Val Step[0400/1563], Loss: 1.2154 (1.4292), Acc@1: 0.8125 (0.6763), Acc@5: 0.9688 (0.8943)
2021-12-29 06:37:49,763 Val Step[0450/1563], Loss: 1.2502 (1.4382), Acc@1: 0.5625 (0.6720), Acc@5: 0.9688 (0.8948)
2021-12-29 06:37:51,232 Val Step[0500/1563], Loss: 0.4972 (1.4222), Acc@1: 0.9375 (0.6753), Acc@5: 1.0000 (0.8966)
2021-12-29 06:37:52,801 Val Step[0550/1563], Loss: 1.4854 (1.3967), Acc@1: 0.6875 (0.6826), Acc@5: 0.8750 (0.8997)
2021-12-29 06:37:54,357 Val Step[0600/1563], Loss: 1.1337 (1.3983), Acc@1: 0.7188 (0.6834), Acc@5: 0.9062 (0.8991)
2021-12-29 06:37:55,854 Val Step[0650/1563], Loss: 0.9648 (1.4242), Acc@1: 0.8750 (0.6793), Acc@5: 0.9688 (0.8952)
2021-12-29 06:37:57,352 Val Step[0700/1563], Loss: 1.6168 (1.4711), Acc@1: 0.6875 (0.6700), Acc@5: 0.7812 (0.8877)
2021-12-29 06:37:58,886 Val Step[0750/1563], Loss: 1.9426 (1.5101), Acc@1: 0.5625 (0.6628), Acc@5: 0.8125 (0.8819)
2021-12-29 06:38:00,304 Val Step[0800/1563], Loss: 1.8074 (1.5532), Acc@1: 0.6250 (0.6534), Acc@5: 0.8125 (0.8756)
2021-12-29 06:38:01,804 Val Step[0850/1563], Loss: 2.2072 (1.5884), Acc@1: 0.4688 (0.6460), Acc@5: 0.7500 (0.8706)
2021-12-29 06:38:03,345 Val Step[0900/1563], Loss: 0.6246 (1.5883), Acc@1: 0.8750 (0.6478), Acc@5: 0.9375 (0.8696)
2021-12-29 06:38:04,898 Val Step[0950/1563], Loss: 1.6024 (1.6179), Acc@1: 0.6875 (0.6421), Acc@5: 0.8438 (0.8650)
2021-12-29 06:38:06,386 Val Step[1000/1563], Loss: 0.9704 (1.6466), Acc@1: 0.8750 (0.6369), Acc@5: 1.0000 (0.8607)
2021-12-29 06:38:07,807 Val Step[1050/1563], Loss: 0.5758 (1.6676), Acc@1: 0.9375 (0.6323), Acc@5: 0.9688 (0.8578)
2021-12-29 06:38:09,275 Val Step[1100/1563], Loss: 1.3747 (1.6872), Acc@1: 0.7188 (0.6282), Acc@5: 0.9062 (0.8545)
2021-12-29 06:38:10,788 Val Step[1150/1563], Loss: 1.6678 (1.7081), Acc@1: 0.7500 (0.6242), Acc@5: 0.7812 (0.8513)
2021-12-29 06:38:12,254 Val Step[1200/1563], Loss: 1.6543 (1.7304), Acc@1: 0.8125 (0.6207), Acc@5: 0.8438 (0.8474)
2021-12-29 06:38:13,719 Val Step[1250/1563], Loss: 0.8357 (1.7478), Acc@1: 0.8750 (0.6183), Acc@5: 0.9062 (0.8447)
2021-12-29 06:38:15,233 Val Step[1300/1563], Loss: 1.4953 (1.7592), Acc@1: 0.7500 (0.6156), Acc@5: 0.8125 (0.8434)
2021-12-29 06:38:16,780 Val Step[1350/1563], Loss: 2.3965 (1.7819), Acc@1: 0.2500 (0.6109), Acc@5: 0.7500 (0.8396)
2021-12-29 06:38:18,297 Val Step[1400/1563], Loss: 1.8126 (1.7913), Acc@1: 0.6250 (0.6087), Acc@5: 0.8125 (0.8380)
2021-12-29 06:38:19,745 Val Step[1450/1563], Loss: 1.6811 (1.7985), Acc@1: 0.6250 (0.6071), Acc@5: 0.9062 (0.8373)
2021-12-29 06:38:21,171 Val Step[1500/1563], Loss: 1.6358 (1.7856), Acc@1: 0.5938 (0.6100), Acc@5: 0.9375 (0.8395)
2021-12-29 06:38:22,653 Val Step[1550/1563], Loss: 1.2014 (1.7804), Acc@1: 0.8750 (0.6109), Acc@5: 0.9062 (0.8401)
2021-12-29 06:38:23,524 ----- Epoch[062/310], Validation Loss: 1.7779, Validation Acc@1: 0.6114, Validation Acc@5: 0.8404, time: 108.17
2021-12-29 06:38:23,525 ----- Epoch[062/310], Train Loss: 4.3162, Train Acc: 0.2806, time: 1643.49, Best Val(epoch62) Acc@1: 0.6114
2021-12-29 06:38:23,725 Max accuracy so far: 0.6114 at epoch_62
2021-12-29 06:38:23,725 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 06:38:23,725 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 06:38:23,818 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 06:38:23,818 Now training epoch 63. LR=0.000896
2021-12-29 06:39:40,919 Epoch[063/310], Step[0000/1251], Loss: 3.9334(3.9334), Acc: 0.3486(0.3486)
2021-12-29 06:40:42,461 Epoch[063/310], Step[0050/1251], Loss: 4.5029(4.3583), Acc: 0.1494(0.2680)
2021-12-29 06:41:44,078 Epoch[063/310], Step[0100/1251], Loss: 4.0468(4.3243), Acc: 0.2754(0.2816)
2021-12-29 06:42:45,998 Epoch[063/310], Step[0150/1251], Loss: 4.6032(4.3197), Acc: 0.2695(0.2838)
2021-12-29 06:43:48,968 Epoch[063/310], Step[0200/1251], Loss: 4.5815(4.3270), Acc: 0.1650(0.2786)
2021-12-29 06:44:50,518 Epoch[063/310], Step[0250/1251], Loss: 4.5987(4.3258), Acc: 0.3525(0.2811)
2021-12-29 06:45:53,184 Epoch[063/310], Step[0300/1251], Loss: 3.8088(4.3290), Acc: 0.2383(0.2792)
2021-12-29 06:46:54,355 Epoch[063/310], Step[0350/1251], Loss: 5.2340(4.3245), Acc: 0.1377(0.2802)
2021-12-29 06:47:57,035 Epoch[063/310], Step[0400/1251], Loss: 4.8031(4.3201), Acc: 0.2090(0.2798)
2021-12-29 06:49:00,043 Epoch[063/310], Step[0450/1251], Loss: 4.5438(4.3114), Acc: 0.2363(0.2786)
2021-12-29 06:50:03,864 Epoch[063/310], Step[0500/1251], Loss: 4.5177(4.3108), Acc: 0.2559(0.2779)
2021-12-29 06:51:05,681 Epoch[063/310], Step[0550/1251], Loss: 4.4380(4.3119), Acc: 0.2393(0.2769)
2021-12-29 06:52:07,574 Epoch[063/310], Step[0600/1251], Loss: 4.1412(4.3093), Acc: 0.1855(0.2773)
2021-12-29 06:53:10,808 Epoch[063/310], Step[0650/1251], Loss: 4.6350(4.3069), Acc: 0.2520(0.2775)
2021-12-29 06:54:14,483 Epoch[063/310], Step[0700/1251], Loss: 4.3098(4.3032), Acc: 0.1719(0.2767)
2021-12-29 06:55:17,493 Epoch[063/310], Step[0750/1251], Loss: 4.1562(4.3019), Acc: 0.2510(0.2775)
2021-12-29 06:56:19,825 Epoch[063/310], Step[0800/1251], Loss: 4.4668(4.3048), Acc: 0.3379(0.2766)
2021-12-29 06:57:21,412 Epoch[063/310], Step[0850/1251], Loss: 4.4682(4.3038), Acc: 0.2891(0.2771)
2021-12-29 06:58:25,345 Epoch[063/310], Step[0900/1251], Loss: 4.3806(4.3023), Acc: 0.3057(0.2766)
2021-12-29 06:59:28,834 Epoch[063/310], Step[0950/1251], Loss: 4.4286(4.3011), Acc: 0.3994(0.2771)
2021-12-29 07:00:32,246 Epoch[063/310], Step[1000/1251], Loss: 4.5588(4.3023), Acc: 0.3311(0.2774)
2021-12-29 07:01:36,092 Epoch[063/310], Step[1050/1251], Loss: 3.6777(4.3063), Acc: 0.4385(0.2775)
2021-12-29 07:02:39,656 Epoch[063/310], Step[1100/1251], Loss: 4.5905(4.3053), Acc: 0.2295(0.2777)
2021-12-29 07:03:43,805 Epoch[063/310], Step[1150/1251], Loss: 4.8545(4.3056), Acc: 0.2920(0.2765)
2021-12-29 07:04:46,174 Epoch[063/310], Step[1200/1251], Loss: 4.5363(4.3073), Acc: 0.3428(0.2760)
2021-12-29 07:05:49,421 Epoch[063/310], Step[1250/1251], Loss: 4.1856(4.3074), Acc: 0.3965(0.2761)
2021-12-29 07:05:51,406 ----- Epoch[063/310], Train Loss: 4.3074, Train Acc: 0.2761, time: 1647.58, Best Val(epoch62) Acc@1: 0.6114
2021-12-29 07:05:51,406 Now training epoch 64. LR=0.000892
2021-12-29 07:07:19,819 Epoch[064/310], Step[0000/1251], Loss: 4.9947(4.9947), Acc: 0.2578(0.2578)
2021-12-29 07:08:23,043 Epoch[064/310], Step[0050/1251], Loss: 4.0790(4.3471), Acc: 0.3359(0.2732)
2021-12-29 07:09:25,872 Epoch[064/310], Step[0100/1251], Loss: 4.8596(4.2891), Acc: 0.2422(0.2859)
2021-12-29 07:10:28,668 Epoch[064/310], Step[0150/1251], Loss: 4.5077(4.2854), Acc: 0.0830(0.2824)
2021-12-29 07:11:31,418 Epoch[064/310], Step[0200/1251], Loss: 3.8478(4.2835), Acc: 0.3721(0.2853)
2021-12-29 07:12:33,706 Epoch[064/310], Step[0250/1251], Loss: 4.5686(4.2843), Acc: 0.2861(0.2845)
2021-12-29 07:13:37,763 Epoch[064/310], Step[0300/1251], Loss: 4.0450(4.2836), Acc: 0.4385(0.2838)
2021-12-29 07:14:40,321 Epoch[064/310], Step[0350/1251], Loss: 4.7165(4.2829), Acc: 0.2578(0.2840)
2021-12-29 07:15:43,460 Epoch[064/310], Step[0400/1251], Loss: 4.6399(4.2900), Acc: 0.3408(0.2831)
2021-12-29 07:16:47,121 Epoch[064/310], Step[0450/1251], Loss: 3.9856(4.2919), Acc: 0.1787(0.2831)
2021-12-29 07:17:48,920 Epoch[064/310], Step[0500/1251], Loss: 4.2047(4.2942), Acc: 0.3438(0.2842)
2021-12-29 07:18:51,113 Epoch[064/310], Step[0550/1251], Loss: 4.0843(4.3008), Acc: 0.4014(0.2820)
2021-12-29 07:19:52,335 Epoch[064/310], Step[0600/1251], Loss: 4.0331(4.2952), Acc: 0.3496(0.2812)
2021-12-29 07:20:53,475 Epoch[064/310], Step[0650/1251], Loss: 4.0971(4.3033), Acc: 0.2529(0.2804)
2021-12-29 07:21:57,357 Epoch[064/310], Step[0700/1251], Loss: 4.9501(4.3038), Acc: 0.2324(0.2795)
2021-12-29 07:23:00,872 Epoch[064/310], Step[0750/1251], Loss: 4.1631(4.3009), Acc: 0.4102(0.2782)
2021-12-29 07:24:02,843 Epoch[064/310], Step[0800/1251], Loss: 4.5357(4.2979), Acc: 0.2520(0.2784)
2021-12-29 07:25:07,019 Epoch[064/310], Step[0850/1251], Loss: 4.2218(4.2955), Acc: 0.3037(0.2785)
2021-12-29 07:26:10,926 Epoch[064/310], Step[0900/1251], Loss: 3.4945(4.2948), Acc: 0.2275(0.2777)
2021-12-29 07:27:13,950 Epoch[064/310], Step[0950/1251], Loss: 4.5338(4.2956), Acc: 0.3076(0.2780)
2021-12-29 07:28:18,027 Epoch[064/310], Step[1000/1251], Loss: 4.4802(4.2940), Acc: 0.1650(0.2775)
2021-12-29 07:29:20,924 Epoch[064/310], Step[1050/1251], Loss: 3.9654(4.2936), Acc: 0.4688(0.2768)
2021-12-29 07:30:22,469 Epoch[064/310], Step[1100/1251], Loss: 4.2333(4.2944), Acc: 0.2539(0.2765)
2021-12-29 07:31:23,737 Epoch[064/310], Step[1150/1251], Loss: 4.5831(4.2977), Acc: 0.1758(0.2759)
2021-12-29 07:32:25,378 Epoch[064/310], Step[1200/1251], Loss: 3.9944(4.2975), Acc: 0.3916(0.2753)
2021-12-29 07:33:27,834 Epoch[064/310], Step[1250/1251], Loss: 4.1276(4.2969), Acc: 0.2432(0.2754)
2021-12-29 07:33:29,802 ----- Validation after Epoch: 64
2021-12-29 07:34:29,958 Val Step[0000/1563], Loss: 0.8148 (0.8148), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-29 07:34:31,700 Val Step[0050/1563], Loss: 2.6662 (1.0351), Acc@1: 0.2812 (0.7727), Acc@5: 0.7188 (0.9246)
2021-12-29 07:34:33,327 Val Step[0100/1563], Loss: 1.7488 (1.3822), Acc@1: 0.4688 (0.6816), Acc@5: 0.8750 (0.8849)
2021-12-29 07:34:34,935 Val Step[0150/1563], Loss: 0.7913 (1.3193), Acc@1: 0.8438 (0.6997), Acc@5: 0.9688 (0.8932)
2021-12-29 07:34:36,509 Val Step[0200/1563], Loss: 1.4425 (1.3404), Acc@1: 0.6875 (0.7018), Acc@5: 0.9375 (0.8904)
2021-12-29 07:34:38,030 Val Step[0250/1563], Loss: 0.7028 (1.2817), Acc@1: 0.8438 (0.7161), Acc@5: 0.9688 (0.8972)
2021-12-29 07:34:39,536 Val Step[0300/1563], Loss: 1.7079 (1.3364), Acc@1: 0.5000 (0.6965), Acc@5: 0.8438 (0.8931)
2021-12-29 07:34:41,009 Val Step[0350/1563], Loss: 1.7544 (1.3492), Acc@1: 0.5000 (0.6898), Acc@5: 0.8125 (0.8961)
2021-12-29 07:34:42,479 Val Step[0400/1563], Loss: 1.1603 (1.3581), Acc@1: 0.8438 (0.6838), Acc@5: 0.9375 (0.8978)
2021-12-29 07:34:44,087 Val Step[0450/1563], Loss: 1.1904 (1.3592), Acc@1: 0.6250 (0.6824), Acc@5: 0.9688 (0.8993)
2021-12-29 07:34:45,692 Val Step[0500/1563], Loss: 0.7588 (1.3519), Acc@1: 0.8438 (0.6844), Acc@5: 1.0000 (0.9003)
2021-12-29 07:34:47,319 Val Step[0550/1563], Loss: 1.1960 (1.3322), Acc@1: 0.7500 (0.6910), Acc@5: 0.9375 (0.9025)
2021-12-29 07:34:48,896 Val Step[0600/1563], Loss: 0.9914 (1.3391), Acc@1: 0.8125 (0.6903), Acc@5: 0.9375 (0.9013)
2021-12-29 07:34:50,380 Val Step[0650/1563], Loss: 1.3650 (1.3604), Acc@1: 0.6562 (0.6862), Acc@5: 0.9688 (0.8978)
2021-12-29 07:34:51,850 Val Step[0700/1563], Loss: 1.8378 (1.4045), Acc@1: 0.6250 (0.6768), Acc@5: 0.8438 (0.8914)
2021-12-29 07:34:53,459 Val Step[0750/1563], Loss: 1.9882 (1.4476), Acc@1: 0.6250 (0.6688), Acc@5: 0.7188 (0.8856)
2021-12-29 07:34:55,041 Val Step[0800/1563], Loss: 1.7482 (1.4932), Acc@1: 0.6562 (0.6592), Acc@5: 0.8125 (0.8791)
2021-12-29 07:34:56,639 Val Step[0850/1563], Loss: 1.7707 (1.5256), Acc@1: 0.6562 (0.6530), Acc@5: 0.9062 (0.8747)
2021-12-29 07:34:58,116 Val Step[0900/1563], Loss: 0.6825 (1.5306), Acc@1: 0.8750 (0.6539), Acc@5: 0.9375 (0.8736)
2021-12-29 07:34:59,667 Val Step[0950/1563], Loss: 1.8690 (1.5587), Acc@1: 0.6250 (0.6489), Acc@5: 0.7812 (0.8687)
2021-12-29 07:35:01,225 Val Step[1000/1563], Loss: 0.6726 (1.5870), Acc@1: 0.9375 (0.6433), Acc@5: 1.0000 (0.8646)
2021-12-29 07:35:02,711 Val Step[1050/1563], Loss: 0.6422 (1.6044), Acc@1: 0.9062 (0.6396), Acc@5: 0.9688 (0.8622)
2021-12-29 07:35:04,263 Val Step[1100/1563], Loss: 1.5606 (1.6266), Acc@1: 0.6875 (0.6351), Acc@5: 0.7500 (0.8582)
2021-12-29 07:35:05,861 Val Step[1150/1563], Loss: 1.8631 (1.6475), Acc@1: 0.6562 (0.6315), Acc@5: 0.7500 (0.8552)
2021-12-29 07:35:07,430 Val Step[1200/1563], Loss: 1.7597 (1.6677), Acc@1: 0.6562 (0.6284), Acc@5: 0.8438 (0.8521)
2021-12-29 07:35:09,016 Val Step[1250/1563], Loss: 0.9933 (1.6861), Acc@1: 0.8125 (0.6256), Acc@5: 0.9375 (0.8491)
2021-12-29 07:35:10,524 Val Step[1300/1563], Loss: 1.4034 (1.7009), Acc@1: 0.7188 (0.6223), Acc@5: 0.7812 (0.8470)
2021-12-29 07:35:12,075 Val Step[1350/1563], Loss: 2.4840 (1.7230), Acc@1: 0.2812 (0.6177), Acc@5: 0.7188 (0.8432)
2021-12-29 07:35:13,626 Val Step[1400/1563], Loss: 1.4096 (1.7309), Acc@1: 0.7188 (0.6159), Acc@5: 0.8750 (0.8422)
2021-12-29 07:35:15,147 Val Step[1450/1563], Loss: 2.3967 (1.7365), Acc@1: 0.3125 (0.6146), Acc@5: 0.8125 (0.8413)
2021-12-29 07:35:16,689 Val Step[1500/1563], Loss: 2.6258 (1.7226), Acc@1: 0.3125 (0.6173), Acc@5: 0.8125 (0.8436)
2021-12-29 07:35:18,220 Val Step[1550/1563], Loss: 1.0416 (1.7191), Acc@1: 0.8750 (0.6179), Acc@5: 0.9062 (0.8441)
2021-12-29 07:35:19,115 ----- Epoch[064/310], Validation Loss: 1.7164, Validation Acc@1: 0.6184, Validation Acc@5: 0.8444, time: 109.31
2021-12-29 07:35:19,116 ----- Epoch[064/310], Train Loss: 4.2969, Train Acc: 0.2754, time: 1658.39, Best Val(epoch64) Acc@1: 0.6184
2021-12-29 07:35:19,324 Max accuracy so far: 0.6184 at epoch_64
2021-12-29 07:35:19,324 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 07:35:19,324 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 07:35:19,402 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 07:35:19,403 Now training epoch 65. LR=0.000889
2021-12-29 07:36:43,788 Epoch[065/310], Step[0000/1251], Loss: 4.3300(4.3300), Acc: 0.2676(0.2676)
2021-12-29 07:37:45,931 Epoch[065/310], Step[0050/1251], Loss: 4.2241(4.2032), Acc: 0.2529(0.2817)
2021-12-29 07:38:47,325 Epoch[065/310], Step[0100/1251], Loss: 4.4875(4.2293), Acc: 0.2578(0.2931)
2021-12-29 07:39:48,551 Epoch[065/310], Step[0150/1251], Loss: 4.5941(4.2347), Acc: 0.3623(0.2849)
2021-12-29 07:40:49,441 Epoch[065/310], Step[0200/1251], Loss: 4.4237(4.2601), Acc: 0.3379(0.2809)
2021-12-29 07:41:51,532 Epoch[065/310], Step[0250/1251], Loss: 4.3214(4.2750), Acc: 0.3301(0.2822)
2021-12-29 07:42:53,538 Epoch[065/310], Step[0300/1251], Loss: 4.0833(4.2807), Acc: 0.3740(0.2841)
2021-12-29 07:43:55,418 Epoch[065/310], Step[0350/1251], Loss: 4.2031(4.2920), Acc: 0.2910(0.2821)
2021-12-29 07:44:57,225 Epoch[065/310], Step[0400/1251], Loss: 4.0016(4.2878), Acc: 0.2949(0.2834)
2021-12-29 07:45:59,960 Epoch[065/310], Step[0450/1251], Loss: 4.4362(4.2868), Acc: 0.2676(0.2809)
2021-12-29 07:47:01,747 Epoch[065/310], Step[0500/1251], Loss: 4.4349(4.2919), Acc: 0.3359(0.2786)
2021-12-29 07:48:04,150 Epoch[065/310], Step[0550/1251], Loss: 4.9189(4.2965), Acc: 0.2891(0.2792)
2021-12-29 07:49:06,091 Epoch[065/310], Step[0600/1251], Loss: 5.1643(4.2987), Acc: 0.1455(0.2781)
2021-12-29 07:50:08,075 Epoch[065/310], Step[0650/1251], Loss: 4.0553(4.2969), Acc: 0.4199(0.2778)
2021-12-29 07:51:09,805 Epoch[065/310], Step[0700/1251], Loss: 4.3506(4.2965), Acc: 0.3574(0.2798)
2021-12-29 07:52:12,616 Epoch[065/310], Step[0750/1251], Loss: 4.4890(4.2995), Acc: 0.3203(0.2790)
2021-12-29 07:53:15,450 Epoch[065/310], Step[0800/1251], Loss: 4.7485(4.3021), Acc: 0.2295(0.2778)
2021-12-29 07:54:17,960 Epoch[065/310], Step[0850/1251], Loss: 4.2216(4.3065), Acc: 0.1553(0.2768)
2021-12-29 07:55:21,933 Epoch[065/310], Step[0900/1251], Loss: 4.1797(4.3081), Acc: 0.1729(0.2763)
2021-12-29 07:56:24,668 Epoch[065/310], Step[0950/1251], Loss: 4.4607(4.3123), Acc: 0.2900(0.2755)
2021-12-29 07:57:26,305 Epoch[065/310], Step[1000/1251], Loss: 4.1215(4.3130), Acc: 0.2139(0.2746)
2021-12-29 07:58:29,011 Epoch[065/310], Step[1050/1251], Loss: 3.4990(4.3117), Acc: 0.3711(0.2744)
2021-12-29 07:59:32,523 Epoch[065/310], Step[1100/1251], Loss: 3.9353(4.3122), Acc: 0.3291(0.2748)
2021-12-29 08:00:35,795 Epoch[065/310], Step[1150/1251], Loss: 4.0407(4.3125), Acc: 0.3291(0.2753)
2021-12-29 08:01:40,055 Epoch[065/310], Step[1200/1251], Loss: 4.5487(4.3089), Acc: 0.3203(0.2755)
2021-12-29 08:02:44,290 Epoch[065/310], Step[1250/1251], Loss: 4.4243(4.3075), Acc: 0.3477(0.2751)
2021-12-29 08:02:46,253 ----- Epoch[065/310], Train Loss: 4.3075, Train Acc: 0.2751, time: 1646.85, Best Val(epoch64) Acc@1: 0.6184
2021-12-29 08:02:46,253 Now training epoch 66. LR=0.000886
2021-12-29 08:04:04,036 Epoch[066/310], Step[0000/1251], Loss: 4.3951(4.3951), Acc: 0.2793(0.2793)
2021-12-29 08:05:06,571 Epoch[066/310], Step[0050/1251], Loss: 4.2078(4.3558), Acc: 0.3926(0.2781)
2021-12-29 08:06:07,848 Epoch[066/310], Step[0100/1251], Loss: 4.8036(4.3171), Acc: 0.3340(0.2789)
2021-12-29 08:07:10,066 Epoch[066/310], Step[0150/1251], Loss: 4.5556(4.2951), Acc: 0.2051(0.2835)
2021-12-29 08:08:12,734 Epoch[066/310], Step[0200/1251], Loss: 4.3285(4.2830), Acc: 0.2070(0.2854)
2021-12-29 08:09:14,752 Epoch[066/310], Step[0250/1251], Loss: 4.1749(4.2897), Acc: 0.2188(0.2882)
2021-12-29 08:10:17,736 Epoch[066/310], Step[0300/1251], Loss: 4.5827(4.3002), Acc: 0.2549(0.2873)
2021-12-29 08:11:20,935 Epoch[066/310], Step[0350/1251], Loss: 4.1221(4.3026), Acc: 0.3662(0.2867)
2021-12-29 08:12:24,699 Epoch[066/310], Step[0400/1251], Loss: 4.1466(4.3021), Acc: 0.4121(0.2852)
2021-12-29 08:13:27,910 Epoch[066/310], Step[0450/1251], Loss: 4.6606(4.3018), Acc: 0.2910(0.2860)
2021-12-29 08:14:29,808 Epoch[066/310], Step[0500/1251], Loss: 3.7108(4.3065), Acc: 0.3447(0.2864)
2021-12-29 08:15:32,191 Epoch[066/310], Step[0550/1251], Loss: 4.3268(4.3007), Acc: 0.3018(0.2867)
2021-12-29 08:16:35,148 Epoch[066/310], Step[0600/1251], Loss: 4.7447(4.3041), Acc: 0.1973(0.2859)
2021-12-29 08:17:38,459 Epoch[066/310], Step[0650/1251], Loss: 4.5747(4.3004), Acc: 0.2744(0.2865)
2021-12-29 08:18:41,647 Epoch[066/310], Step[0700/1251], Loss: 4.3800(4.3016), Acc: 0.2021(0.2863)
2021-12-29 08:19:45,243 Epoch[066/310], Step[0750/1251], Loss: 4.0253(4.3022), Acc: 0.1465(0.2864)
2021-12-29 08:20:49,114 Epoch[066/310], Step[0800/1251], Loss: 4.2584(4.3046), Acc: 0.2559(0.2860)
2021-12-29 08:21:51,788 Epoch[066/310], Step[0850/1251], Loss: 4.1881(4.3053), Acc: 0.2842(0.2859)
2021-12-29 08:22:55,480 Epoch[066/310], Step[0900/1251], Loss: 4.3593(4.3069), Acc: 0.3574(0.2852)
2021-12-29 08:23:58,850 Epoch[066/310], Step[0950/1251], Loss: 4.5411(4.3057), Acc: 0.2793(0.2849)
2021-12-29 08:25:02,159 Epoch[066/310], Step[1000/1251], Loss: 3.8967(4.3060), Acc: 0.2441(0.2831)
2021-12-29 08:26:05,030 Epoch[066/310], Step[1050/1251], Loss: 4.3053(4.3078), Acc: 0.2490(0.2832)
2021-12-29 08:27:06,561 Epoch[066/310], Step[1100/1251], Loss: 4.2134(4.3084), Acc: 0.3906(0.2831)
2021-12-29 08:28:08,211 Epoch[066/310], Step[1150/1251], Loss: 4.4175(4.3048), Acc: 0.3633(0.2844)
2021-12-29 08:29:09,816 Epoch[066/310], Step[1200/1251], Loss: 4.0065(4.3049), Acc: 0.4375(0.2849)
2021-12-29 08:30:10,119 Epoch[066/310], Step[1250/1251], Loss: 4.6648(4.3056), Acc: 0.0967(0.2851)
2021-12-29 08:30:12,301 ----- Validation after Epoch: 66
2021-12-29 08:31:12,181 Val Step[0000/1563], Loss: 0.7935 (0.7935), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-29 08:31:13,645 Val Step[0050/1563], Loss: 2.6829 (1.1005), Acc@1: 0.3438 (0.7702), Acc@5: 0.6875 (0.9301)
2021-12-29 08:31:15,095 Val Step[0100/1563], Loss: 1.9203 (1.4408), Acc@1: 0.5938 (0.6748), Acc@5: 0.8750 (0.8868)
2021-12-29 08:31:16,495 Val Step[0150/1563], Loss: 0.8362 (1.3532), Acc@1: 0.8438 (0.6981), Acc@5: 0.9375 (0.8942)
2021-12-29 08:31:17,872 Val Step[0200/1563], Loss: 1.3035 (1.3670), Acc@1: 0.7500 (0.7020), Acc@5: 0.9062 (0.8912)
2021-12-29 08:31:19,238 Val Step[0250/1563], Loss: 1.3070 (1.3009), Acc@1: 0.6562 (0.7153), Acc@5: 0.9375 (0.8998)
2021-12-29 08:31:20,589 Val Step[0300/1563], Loss: 1.8055 (1.3673), Acc@1: 0.4688 (0.6941), Acc@5: 0.8750 (0.8939)
2021-12-29 08:31:21,918 Val Step[0350/1563], Loss: 1.5357 (1.3718), Acc@1: 0.5312 (0.6885), Acc@5: 0.9062 (0.8977)
2021-12-29 08:31:23,352 Val Step[0400/1563], Loss: 1.3100 (1.3813), Acc@1: 0.7500 (0.6817), Acc@5: 0.9688 (0.8988)
2021-12-29 08:31:24,722 Val Step[0450/1563], Loss: 0.9703 (1.3812), Acc@1: 0.7188 (0.6802), Acc@5: 1.0000 (0.8998)
2021-12-29 08:31:26,243 Val Step[0500/1563], Loss: 0.7560 (1.3742), Acc@1: 0.8750 (0.6823), Acc@5: 1.0000 (0.9003)
2021-12-29 08:31:27,688 Val Step[0550/1563], Loss: 1.2493 (1.3512), Acc@1: 0.6875 (0.6890), Acc@5: 0.9062 (0.9031)
2021-12-29 08:31:29,163 Val Step[0600/1563], Loss: 0.9357 (1.3513), Acc@1: 0.8750 (0.6894), Acc@5: 0.9375 (0.9033)
2021-12-29 08:31:30,455 Val Step[0650/1563], Loss: 1.0881 (1.3726), Acc@1: 0.7500 (0.6858), Acc@5: 0.9688 (0.8998)
2021-12-29 08:31:31,836 Val Step[0700/1563], Loss: 1.8422 (1.4132), Acc@1: 0.5625 (0.6756), Acc@5: 0.8750 (0.8943)
2021-12-29 08:31:33,341 Val Step[0750/1563], Loss: 1.6189 (1.4558), Acc@1: 0.6875 (0.6677), Acc@5: 0.8438 (0.8876)
2021-12-29 08:31:34,837 Val Step[0800/1563], Loss: 1.6462 (1.5046), Acc@1: 0.6562 (0.6569), Acc@5: 0.8438 (0.8805)
2021-12-29 08:31:36,328 Val Step[0850/1563], Loss: 1.9668 (1.5351), Acc@1: 0.5000 (0.6521), Acc@5: 0.8750 (0.8758)
2021-12-29 08:31:37,831 Val Step[0900/1563], Loss: 0.5888 (1.5391), Acc@1: 0.8750 (0.6526), Acc@5: 0.9688 (0.8742)
2021-12-29 08:31:39,347 Val Step[0950/1563], Loss: 1.5414 (1.5659), Acc@1: 0.6875 (0.6475), Acc@5: 0.7812 (0.8697)
2021-12-29 08:31:40,765 Val Step[1000/1563], Loss: 0.6630 (1.5932), Acc@1: 0.9375 (0.6417), Acc@5: 0.9688 (0.8653)
2021-12-29 08:31:42,155 Val Step[1050/1563], Loss: 0.6525 (1.6104), Acc@1: 0.9375 (0.6382), Acc@5: 0.9688 (0.8630)
2021-12-29 08:31:43,640 Val Step[1100/1563], Loss: 1.5161 (1.6320), Acc@1: 0.6875 (0.6339), Acc@5: 0.7812 (0.8594)
2021-12-29 08:31:45,151 Val Step[1150/1563], Loss: 1.8522 (1.6499), Acc@1: 0.6562 (0.6309), Acc@5: 0.7812 (0.8568)
2021-12-29 08:31:46,834 Val Step[1200/1563], Loss: 1.4344 (1.6648), Acc@1: 0.7812 (0.6285), Acc@5: 0.9062 (0.8540)
2021-12-29 08:31:48,540 Val Step[1250/1563], Loss: 1.1317 (1.6832), Acc@1: 0.7500 (0.6255), Acc@5: 0.9375 (0.8513)
2021-12-29 08:31:50,244 Val Step[1300/1563], Loss: 1.2986 (1.6961), Acc@1: 0.7500 (0.6223), Acc@5: 0.8125 (0.8497)
2021-12-29 08:31:51,741 Val Step[1350/1563], Loss: 2.6289 (1.7171), Acc@1: 0.2812 (0.6182), Acc@5: 0.6875 (0.8459)
2021-12-29 08:31:53,239 Val Step[1400/1563], Loss: 1.3507 (1.7285), Acc@1: 0.6875 (0.6155), Acc@5: 0.9062 (0.8441)
2021-12-29 08:31:54,800 Val Step[1450/1563], Loss: 2.1857 (1.7341), Acc@1: 0.3750 (0.6144), Acc@5: 0.8750 (0.8432)
2021-12-29 08:31:56,371 Val Step[1500/1563], Loss: 1.9573 (1.7239), Acc@1: 0.5625 (0.6164), Acc@5: 0.8438 (0.8449)
2021-12-29 08:31:57,930 Val Step[1550/1563], Loss: 1.1937 (1.7226), Acc@1: 0.8750 (0.6164), Acc@5: 0.9062 (0.8454)
2021-12-29 08:31:58,812 ----- Epoch[066/310], Validation Loss: 1.7202, Validation Acc@1: 0.6170, Validation Acc@5: 0.8456, time: 106.51
2021-12-29 08:31:58,813 ----- Epoch[066/310], Train Loss: 4.3056, Train Acc: 0.2851, time: 1646.04, Best Val(epoch64) Acc@1: 0.6184
2021-12-29 08:31:58,813 Now training epoch 67. LR=0.000882
2021-12-29 08:33:17,724 Epoch[067/310], Step[0000/1251], Loss: 3.8471(3.8471), Acc: 0.3350(0.3350)
2021-12-29 08:34:19,414 Epoch[067/310], Step[0050/1251], Loss: 4.7221(4.3523), Acc: 0.3047(0.2761)
2021-12-29 08:35:21,001 Epoch[067/310], Step[0100/1251], Loss: 4.5849(4.3334), Acc: 0.3086(0.2759)
2021-12-29 08:36:23,441 Epoch[067/310], Step[0150/1251], Loss: 3.9905(4.3158), Acc: 0.4229(0.2790)
2021-12-29 08:37:26,540 Epoch[067/310], Step[0200/1251], Loss: 4.3844(4.3338), Acc: 0.2314(0.2752)
2021-12-29 08:38:26,301 Epoch[067/310], Step[0250/1251], Loss: 4.0424(4.3267), Acc: 0.4502(0.2782)
2021-12-29 08:39:27,972 Epoch[067/310], Step[0300/1251], Loss: 4.2686(4.3311), Acc: 0.4053(0.2790)
2021-12-29 08:40:30,452 Epoch[067/310], Step[0350/1251], Loss: 3.9037(4.3197), Acc: 0.4346(0.2787)
2021-12-29 08:41:32,167 Epoch[067/310], Step[0400/1251], Loss: 4.3776(4.3150), Acc: 0.2119(0.2787)
2021-12-29 08:42:33,600 Epoch[067/310], Step[0450/1251], Loss: 4.0598(4.3077), Acc: 0.2500(0.2788)
2021-12-29 08:43:35,538 Epoch[067/310], Step[0500/1251], Loss: 4.6448(4.2999), Acc: 0.2490(0.2784)
2021-12-29 08:44:38,050 Epoch[067/310], Step[0550/1251], Loss: 4.2254(4.3002), Acc: 0.2246(0.2789)
2021-12-29 08:45:41,410 Epoch[067/310], Step[0600/1251], Loss: 4.5525(4.3069), Acc: 0.3408(0.2782)
2021-12-29 08:46:44,825 Epoch[067/310], Step[0650/1251], Loss: 4.3458(4.3073), Acc: 0.3066(0.2771)
2021-12-29 08:47:45,019 Epoch[067/310], Step[0700/1251], Loss: 4.0463(4.3085), Acc: 0.3174(0.2771)
2021-12-29 08:48:47,826 Epoch[067/310], Step[0750/1251], Loss: 4.0265(4.3096), Acc: 0.4414(0.2780)
2021-12-29 08:49:48,964 Epoch[067/310], Step[0800/1251], Loss: 4.6432(4.3068), Acc: 0.3418(0.2799)
2021-12-29 08:50:52,051 Epoch[067/310], Step[0850/1251], Loss: 4.7480(4.3019), Acc: 0.2129(0.2783)
2021-12-29 08:51:54,877 Epoch[067/310], Step[0900/1251], Loss: 3.9498(4.3030), Acc: 0.2773(0.2777)
2021-12-29 08:52:58,753 Epoch[067/310], Step[0950/1251], Loss: 4.2369(4.3005), Acc: 0.3281(0.2783)
2021-12-29 08:54:01,264 Epoch[067/310], Step[1000/1251], Loss: 3.8282(4.3026), Acc: 0.4111(0.2787)
2021-12-29 08:55:05,243 Epoch[067/310], Step[1050/1251], Loss: 4.5121(4.3036), Acc: 0.2920(0.2783)
2021-12-29 08:56:06,552 Epoch[067/310], Step[1100/1251], Loss: 4.3339(4.3032), Acc: 0.3594(0.2786)
2021-12-29 08:57:09,388 Epoch[067/310], Step[1150/1251], Loss: 4.3304(4.3045), Acc: 0.3340(0.2785)
2021-12-29 08:58:12,229 Epoch[067/310], Step[1200/1251], Loss: 4.5302(4.3022), Acc: 0.2832(0.2782)
2021-12-29 08:59:15,628 Epoch[067/310], Step[1250/1251], Loss: 4.4747(4.3030), Acc: 0.2158(0.2780)
2021-12-29 08:59:17,820 ----- Epoch[067/310], Train Loss: 4.3030, Train Acc: 0.2780, time: 1639.00, Best Val(epoch64) Acc@1: 0.6184
2021-12-29 08:59:17,821 Now training epoch 68. LR=0.000879
2021-12-29 09:00:41,904 Epoch[068/310], Step[0000/1251], Loss: 4.4050(4.4050), Acc: 0.2627(0.2627)
2021-12-29 09:01:43,267 Epoch[068/310], Step[0050/1251], Loss: 4.6158(4.3021), Acc: 0.1992(0.2976)
2021-12-29 09:02:44,387 Epoch[068/310], Step[0100/1251], Loss: 4.0373(4.2801), Acc: 0.2695(0.2866)
2021-12-29 09:03:46,430 Epoch[068/310], Step[0150/1251], Loss: 4.4320(4.2813), Acc: 0.3232(0.2874)
2021-12-29 09:04:49,130 Epoch[068/310], Step[0200/1251], Loss: 4.5777(4.2897), Acc: 0.2070(0.2849)
2021-12-29 09:05:51,504 Epoch[068/310], Step[0250/1251], Loss: 4.3086(4.2751), Acc: 0.3340(0.2864)
2021-12-29 09:06:53,487 Epoch[068/310], Step[0300/1251], Loss: 4.1756(4.2858), Acc: 0.1807(0.2860)
2021-12-29 09:07:57,656 Epoch[068/310], Step[0350/1251], Loss: 4.7860(4.2932), Acc: 0.2275(0.2815)
2021-12-29 09:09:00,454 Epoch[068/310], Step[0400/1251], Loss: 3.7694(4.2873), Acc: 0.4775(0.2810)
2021-12-29 09:10:04,764 Epoch[068/310], Step[0450/1251], Loss: 4.3017(4.2899), Acc: 0.2578(0.2795)
2021-12-29 09:11:08,765 Epoch[068/310], Step[0500/1251], Loss: 4.1951(4.2895), Acc: 0.2412(0.2764)
2021-12-29 09:12:10,855 Epoch[068/310], Step[0550/1251], Loss: 4.4473(4.2953), Acc: 0.2744(0.2761)
2021-12-29 09:13:14,025 Epoch[068/310], Step[0600/1251], Loss: 4.6774(4.2893), Acc: 0.2939(0.2763)
2021-12-29 09:14:15,869 Epoch[068/310], Step[0650/1251], Loss: 3.7719(4.2900), Acc: 0.3125(0.2766)
2021-12-29 09:15:19,332 Epoch[068/310], Step[0700/1251], Loss: 4.6423(4.2890), Acc: 0.2402(0.2770)
2021-12-29 09:16:22,474 Epoch[068/310], Step[0750/1251], Loss: 4.4917(4.2880), Acc: 0.2422(0.2770)
2021-12-29 09:17:26,080 Epoch[068/310], Step[0800/1251], Loss: 4.3684(4.2926), Acc: 0.2217(0.2751)
2021-12-29 09:18:29,701 Epoch[068/310], Step[0850/1251], Loss: 3.8830(4.2951), Acc: 0.3594(0.2747)
2021-12-29 09:19:33,697 Epoch[068/310], Step[0900/1251], Loss: 3.7558(4.2999), Acc: 0.3555(0.2737)
2021-12-29 09:20:37,156 Epoch[068/310], Step[0950/1251], Loss: 4.1084(4.2991), Acc: 0.4248(0.2740)
2021-12-29 09:21:39,749 Epoch[068/310], Step[1000/1251], Loss: 4.1217(4.2986), Acc: 0.0605(0.2744)
2021-12-29 09:22:42,844 Epoch[068/310], Step[1050/1251], Loss: 3.8086(4.2994), Acc: 0.1777(0.2746)
2021-12-29 09:23:46,973 Epoch[068/310], Step[1100/1251], Loss: 4.1787(4.2986), Acc: 0.3867(0.2751)
2021-12-29 09:24:50,825 Epoch[068/310], Step[1150/1251], Loss: 4.4333(4.3028), Acc: 0.3203(0.2748)
2021-12-29 09:25:55,365 Epoch[068/310], Step[1200/1251], Loss: 4.5127(4.3012), Acc: 0.3018(0.2749)
2021-12-29 09:26:57,598 Epoch[068/310], Step[1250/1251], Loss: 4.0627(4.2997), Acc: 0.1904(0.2749)
2021-12-29 09:26:59,564 ----- Validation after Epoch: 68
2021-12-29 09:28:01,789 Val Step[0000/1563], Loss: 1.0318 (1.0318), Acc@1: 0.8750 (0.8750), Acc@5: 0.9375 (0.9375)
2021-12-29 09:28:03,368 Val Step[0050/1563], Loss: 2.8251 (1.1641), Acc@1: 0.3750 (0.7604), Acc@5: 0.6875 (0.9185)
2021-12-29 09:28:05,051 Val Step[0100/1563], Loss: 1.9951 (1.5232), Acc@1: 0.5312 (0.6649), Acc@5: 0.8125 (0.8725)
2021-12-29 09:28:06,529 Val Step[0150/1563], Loss: 0.7443 (1.4007), Acc@1: 0.8438 (0.6958), Acc@5: 0.9375 (0.8866)
2021-12-29 09:28:08,074 Val Step[0200/1563], Loss: 1.4340 (1.4322), Acc@1: 0.6562 (0.6919), Acc@5: 0.9375 (0.8837)
2021-12-29 09:28:09,575 Val Step[0250/1563], Loss: 0.9299 (1.3640), Acc@1: 0.8438 (0.7057), Acc@5: 1.0000 (0.8932)
2021-12-29 09:28:11,046 Val Step[0300/1563], Loss: 1.7390 (1.4288), Acc@1: 0.5938 (0.6841), Acc@5: 0.9375 (0.8869)
2021-12-29 09:28:12,490 Val Step[0350/1563], Loss: 1.6298 (1.4342), Acc@1: 0.5312 (0.6783), Acc@5: 0.8750 (0.8897)
2021-12-29 09:28:13,930 Val Step[0400/1563], Loss: 1.6924 (1.4232), Acc@1: 0.6562 (0.6754), Acc@5: 0.9062 (0.8929)
2021-12-29 09:28:15,442 Val Step[0450/1563], Loss: 1.0172 (1.4264), Acc@1: 0.7500 (0.6722), Acc@5: 0.9688 (0.8943)
2021-12-29 09:28:16,969 Val Step[0500/1563], Loss: 0.5113 (1.4171), Acc@1: 0.9375 (0.6745), Acc@5: 1.0000 (0.8953)
2021-12-29 09:28:18,622 Val Step[0550/1563], Loss: 1.4108 (1.3934), Acc@1: 0.6875 (0.6818), Acc@5: 0.9375 (0.8980)
2021-12-29 09:28:20,268 Val Step[0600/1563], Loss: 1.0712 (1.3964), Acc@1: 0.7500 (0.6825), Acc@5: 0.9062 (0.8974)
2021-12-29 09:28:21,829 Val Step[0650/1563], Loss: 0.7977 (1.4189), Acc@1: 0.9062 (0.6792), Acc@5: 0.9688 (0.8935)
2021-12-29 09:28:23,312 Val Step[0700/1563], Loss: 1.9024 (1.4599), Acc@1: 0.6562 (0.6709), Acc@5: 0.8750 (0.8875)
2021-12-29 09:28:24,799 Val Step[0750/1563], Loss: 1.9947 (1.4967), Acc@1: 0.5625 (0.6645), Acc@5: 0.7812 (0.8820)
2021-12-29 09:28:26,270 Val Step[0800/1563], Loss: 1.2867 (1.5397), Acc@1: 0.7188 (0.6543), Acc@5: 0.9375 (0.8757)
2021-12-29 09:28:27,721 Val Step[0850/1563], Loss: 2.0367 (1.5697), Acc@1: 0.5312 (0.6486), Acc@5: 0.8125 (0.8719)
2021-12-29 09:28:29,230 Val Step[0900/1563], Loss: 0.5849 (1.5723), Acc@1: 0.8750 (0.6494), Acc@5: 0.9688 (0.8707)
2021-12-29 09:28:30,756 Val Step[0950/1563], Loss: 2.1984 (1.5977), Acc@1: 0.5312 (0.6442), Acc@5: 0.7812 (0.8672)
2021-12-29 09:28:32,236 Val Step[1000/1563], Loss: 0.7247 (1.6253), Acc@1: 0.9688 (0.6385), Acc@5: 1.0000 (0.8634)
2021-12-29 09:28:33,771 Val Step[1050/1563], Loss: 0.4081 (1.6409), Acc@1: 0.9688 (0.6354), Acc@5: 0.9688 (0.8609)
2021-12-29 09:28:35,424 Val Step[1100/1563], Loss: 1.4961 (1.6616), Acc@1: 0.6875 (0.6315), Acc@5: 0.8438 (0.8572)
2021-12-29 09:28:37,051 Val Step[1150/1563], Loss: 1.5077 (1.6789), Acc@1: 0.7500 (0.6288), Acc@5: 0.8125 (0.8544)
2021-12-29 09:28:38,560 Val Step[1200/1563], Loss: 1.9765 (1.6994), Acc@1: 0.6562 (0.6251), Acc@5: 0.8438 (0.8513)
2021-12-29 09:28:40,077 Val Step[1250/1563], Loss: 1.3926 (1.7164), Acc@1: 0.7500 (0.6228), Acc@5: 0.8750 (0.8486)
2021-12-29 09:28:41,656 Val Step[1300/1563], Loss: 1.4398 (1.7290), Acc@1: 0.7188 (0.6201), Acc@5: 0.8125 (0.8468)
2021-12-29 09:28:43,186 Val Step[1350/1563], Loss: 2.9088 (1.7503), Acc@1: 0.2500 (0.6161), Acc@5: 0.6562 (0.8432)
2021-12-29 09:28:44,779 Val Step[1400/1563], Loss: 1.6261 (1.7593), Acc@1: 0.6250 (0.6143), Acc@5: 0.9062 (0.8417)
2021-12-29 09:28:46,324 Val Step[1450/1563], Loss: 1.6095 (1.7646), Acc@1: 0.6875 (0.6133), Acc@5: 0.9375 (0.8413)
2021-12-29 09:28:47,848 Val Step[1500/1563], Loss: 1.7460 (1.7509), Acc@1: 0.5000 (0.6164), Acc@5: 0.8750 (0.8433)
2021-12-29 09:28:49,366 Val Step[1550/1563], Loss: 1.1661 (1.7491), Acc@1: 0.8750 (0.6165), Acc@5: 0.9062 (0.8436)
2021-12-29 09:28:50,347 ----- Epoch[068/310], Validation Loss: 1.7470, Validation Acc@1: 0.6169, Validation Acc@5: 0.8439, time: 110.78
2021-12-29 09:28:50,347 ----- Epoch[068/310], Train Loss: 4.2997, Train Acc: 0.2749, time: 1661.74, Best Val(epoch64) Acc@1: 0.6184
2021-12-29 09:28:50,347 Now training epoch 69. LR=0.000876
2021-12-29 09:30:25,666 Epoch[069/310], Step[0000/1251], Loss: 4.1662(4.1662), Acc: 0.3125(0.3125)
2021-12-29 09:31:27,892 Epoch[069/310], Step[0050/1251], Loss: 4.8220(4.3210), Acc: 0.1992(0.2872)
2021-12-29 09:32:30,412 Epoch[069/310], Step[0100/1251], Loss: 3.5627(4.2844), Acc: 0.3516(0.2813)
2021-12-29 09:33:32,052 Epoch[069/310], Step[0150/1251], Loss: 4.3146(4.2658), Acc: 0.1621(0.2822)
2021-12-29 09:34:34,658 Epoch[069/310], Step[0200/1251], Loss: 4.3823(4.2814), Acc: 0.3525(0.2772)
2021-12-29 09:35:36,755 Epoch[069/310], Step[0250/1251], Loss: 3.7759(4.2691), Acc: 0.3477(0.2786)
2021-12-29 09:36:38,796 Epoch[069/310], Step[0300/1251], Loss: 4.4447(4.2716), Acc: 0.1865(0.2785)
2021-12-29 09:37:41,511 Epoch[069/310], Step[0350/1251], Loss: 4.8195(4.2674), Acc: 0.1865(0.2779)
2021-12-29 09:38:44,210 Epoch[069/310], Step[0400/1251], Loss: 4.8092(4.2710), Acc: 0.2480(0.2790)
2021-12-29 09:39:46,138 Epoch[069/310], Step[0450/1251], Loss: 3.9091(4.2599), Acc: 0.1982(0.2818)
2021-12-29 09:40:49,952 Epoch[069/310], Step[0500/1251], Loss: 4.4527(4.2687), Acc: 0.2744(0.2814)
2021-12-29 09:41:52,986 Epoch[069/310], Step[0550/1251], Loss: 4.1137(4.2697), Acc: 0.1855(0.2803)
2021-12-29 09:42:55,684 Epoch[069/310], Step[0600/1251], Loss: 4.0254(4.2723), Acc: 0.3750(0.2809)
2021-12-29 09:43:59,119 Epoch[069/310], Step[0650/1251], Loss: 4.5650(4.2783), Acc: 0.2607(0.2804)
2021-12-29 09:45:01,356 Epoch[069/310], Step[0700/1251], Loss: 4.5270(4.2803), Acc: 0.1621(0.2799)
2021-12-29 09:46:05,015 Epoch[069/310], Step[0750/1251], Loss: 4.4184(4.2781), Acc: 0.2490(0.2795)
2021-12-29 09:47:08,523 Epoch[069/310], Step[0800/1251], Loss: 4.1052(4.2773), Acc: 0.3828(0.2790)
2021-12-29 09:48:11,279 Epoch[069/310], Step[0850/1251], Loss: 4.3210(4.2718), Acc: 0.3594(0.2796)
2021-12-29 09:49:14,145 Epoch[069/310], Step[0900/1251], Loss: 4.1411(4.2706), Acc: 0.3604(0.2794)
2021-12-29 09:50:16,038 Epoch[069/310], Step[0950/1251], Loss: 4.4273(4.2753), Acc: 0.3340(0.2789)
2021-12-29 09:51:18,959 Epoch[069/310], Step[1000/1251], Loss: 4.8282(4.2745), Acc: 0.2949(0.2788)
2021-12-29 09:52:21,893 Epoch[069/310], Step[1050/1251], Loss: 4.2748(4.2749), Acc: 0.2021(0.2790)
2021-12-29 09:53:24,566 Epoch[069/310], Step[1100/1251], Loss: 3.9134(4.2740), Acc: 0.4209(0.2792)
2021-12-29 09:54:26,505 Epoch[069/310], Step[1150/1251], Loss: 4.7888(4.2781), Acc: 0.1924(0.2785)
2021-12-29 09:55:28,671 Epoch[069/310], Step[1200/1251], Loss: 4.1828(4.2765), Acc: 0.2695(0.2789)
2021-12-29 09:56:30,490 Epoch[069/310], Step[1250/1251], Loss: 4.0665(4.2769), Acc: 0.3057(0.2792)
2021-12-29 09:56:32,515 ----- Epoch[069/310], Train Loss: 4.2769, Train Acc: 0.2792, time: 1662.16, Best Val(epoch64) Acc@1: 0.6184
2021-12-29 09:56:32,516 Now training epoch 70. LR=0.000872
2021-12-29 09:57:58,270 Epoch[070/310], Step[0000/1251], Loss: 4.5685(4.5685), Acc: 0.3066(0.3066)
2021-12-29 09:59:01,221 Epoch[070/310], Step[0050/1251], Loss: 4.7479(4.2629), Acc: 0.2803(0.2769)
2021-12-29 10:00:03,337 Epoch[070/310], Step[0100/1251], Loss: 4.2291(4.2537), Acc: 0.2578(0.2868)
2021-12-29 10:01:06,422 Epoch[070/310], Step[0150/1251], Loss: 4.5444(4.2716), Acc: 0.1104(0.2820)
2021-12-29 10:02:08,783 Epoch[070/310], Step[0200/1251], Loss: 4.5193(4.2626), Acc: 0.2314(0.2810)
2021-12-29 10:03:11,150 Epoch[070/310], Step[0250/1251], Loss: 4.4640(4.2665), Acc: 0.2559(0.2812)
2021-12-29 10:04:13,363 Epoch[070/310], Step[0300/1251], Loss: 4.7102(4.2784), Acc: 0.1826(0.2801)
2021-12-29 10:05:16,573 Epoch[070/310], Step[0350/1251], Loss: 4.4170(4.2753), Acc: 0.3389(0.2769)
2021-12-29 10:06:19,112 Epoch[070/310], Step[0400/1251], Loss: 4.5249(4.2671), Acc: 0.3379(0.2760)
2021-12-29 10:07:20,411 Epoch[070/310], Step[0450/1251], Loss: 4.1121(4.2671), Acc: 0.3760(0.2756)
2021-12-29 10:08:20,732 Epoch[070/310], Step[0500/1251], Loss: 4.4011(4.2632), Acc: 0.1191(0.2776)
2021-12-29 10:09:22,980 Epoch[070/310], Step[0550/1251], Loss: 3.9145(4.2686), Acc: 0.3896(0.2771)
2021-12-29 10:10:25,681 Epoch[070/310], Step[0600/1251], Loss: 4.5976(4.2733), Acc: 0.2373(0.2794)
2021-12-29 10:11:28,205 Epoch[070/310], Step[0650/1251], Loss: 4.0017(4.2747), Acc: 0.3271(0.2796)
2021-12-29 10:12:30,782 Epoch[070/310], Step[0700/1251], Loss: 4.6997(4.2805), Acc: 0.2305(0.2781)
2021-12-29 10:13:33,495 Epoch[070/310], Step[0750/1251], Loss: 4.0180(4.2806), Acc: 0.3965(0.2782)
2021-12-29 10:14:36,819 Epoch[070/310], Step[0800/1251], Loss: 4.3174(4.2810), Acc: 0.3867(0.2778)
2021-12-29 10:15:40,180 Epoch[070/310], Step[0850/1251], Loss: 4.3654(4.2820), Acc: 0.2285(0.2760)
2021-12-29 10:16:44,433 Epoch[070/310], Step[0900/1251], Loss: 4.3532(4.2836), Acc: 0.3350(0.2752)
2021-12-29 10:17:47,434 Epoch[070/310], Step[0950/1251], Loss: 4.0727(4.2830), Acc: 0.4004(0.2762)
2021-12-29 10:18:48,880 Epoch[070/310], Step[1000/1251], Loss: 3.9878(4.2840), Acc: 0.1875(0.2776)
2021-12-29 10:19:51,718 Epoch[070/310], Step[1050/1251], Loss: 4.9482(4.2872), Acc: 0.2715(0.2778)
2021-12-29 10:20:54,615 Epoch[070/310], Step[1100/1251], Loss: 4.3153(4.2881), Acc: 0.2197(0.2784)
2021-12-29 10:21:57,726 Epoch[070/310], Step[1150/1251], Loss: 4.1758(4.2861), Acc: 0.2256(0.2794)
2021-12-29 10:23:00,813 Epoch[070/310], Step[1200/1251], Loss: 3.7524(4.2825), Acc: 0.4756(0.2796)
2021-12-29 10:24:04,363 Epoch[070/310], Step[1250/1251], Loss: 4.3697(4.2845), Acc: 0.2988(0.2785)
2021-12-29 10:24:06,594 ----- Validation after Epoch: 70
2021-12-29 10:25:10,914 Val Step[0000/1563], Loss: 0.7366 (0.7366), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2021-12-29 10:25:12,450 Val Step[0050/1563], Loss: 2.5237 (1.0759), Acc@1: 0.4688 (0.7806), Acc@5: 0.7500 (0.9308)
2021-12-29 10:25:14,068 Val Step[0100/1563], Loss: 1.8267 (1.4164), Acc@1: 0.6250 (0.6894), Acc@5: 0.8438 (0.8895)
2021-12-29 10:25:15,605 Val Step[0150/1563], Loss: 0.7682 (1.3242), Acc@1: 0.8438 (0.7094), Acc@5: 1.0000 (0.8990)
2021-12-29 10:25:17,154 Val Step[0200/1563], Loss: 1.5149 (1.3511), Acc@1: 0.6875 (0.7080), Acc@5: 0.9375 (0.8946)
2021-12-29 10:25:18,609 Val Step[0250/1563], Loss: 1.0461 (1.2968), Acc@1: 0.7500 (0.7206), Acc@5: 1.0000 (0.9020)
2021-12-29 10:25:20,109 Val Step[0300/1563], Loss: 2.0640 (1.3661), Acc@1: 0.4375 (0.6973), Acc@5: 0.8438 (0.8962)
2021-12-29 10:25:21,619 Val Step[0350/1563], Loss: 1.8549 (1.3755), Acc@1: 0.5312 (0.6932), Acc@5: 0.8750 (0.8996)
2021-12-29 10:25:23,130 Val Step[0400/1563], Loss: 1.3278 (1.3760), Acc@1: 0.7500 (0.6885), Acc@5: 0.9688 (0.9013)
2021-12-29 10:25:24,781 Val Step[0450/1563], Loss: 1.1640 (1.3798), Acc@1: 0.4062 (0.6856), Acc@5: 1.0000 (0.9028)
2021-12-29 10:25:26,377 Val Step[0500/1563], Loss: 0.6997 (1.3685), Acc@1: 0.9062 (0.6877), Acc@5: 1.0000 (0.9048)
2021-12-29 10:25:27,858 Val Step[0550/1563], Loss: 1.0075 (1.3398), Acc@1: 0.7812 (0.6954), Acc@5: 0.9688 (0.9080)
2021-12-29 10:25:29,482 Val Step[0600/1563], Loss: 1.0182 (1.3443), Acc@1: 0.8438 (0.6963), Acc@5: 0.9375 (0.9068)
2021-12-29 10:25:30,957 Val Step[0650/1563], Loss: 1.3036 (1.3670), Acc@1: 0.6562 (0.6928), Acc@5: 0.9375 (0.9031)
2021-12-29 10:25:32,469 Val Step[0700/1563], Loss: 1.9333 (1.4133), Acc@1: 0.6562 (0.6830), Acc@5: 0.7500 (0.8965)
2021-12-29 10:25:33,993 Val Step[0750/1563], Loss: 2.0849 (1.4597), Acc@1: 0.5625 (0.6742), Acc@5: 0.7188 (0.8894)
2021-12-29 10:25:35,646 Val Step[0800/1563], Loss: 1.3829 (1.5065), Acc@1: 0.6875 (0.6636), Acc@5: 0.9375 (0.8827)
2021-12-29 10:25:37,187 Val Step[0850/1563], Loss: 1.8612 (1.5422), Acc@1: 0.5312 (0.6569), Acc@5: 0.8438 (0.8773)
2021-12-29 10:25:38,721 Val Step[0900/1563], Loss: 0.5566 (1.5440), Acc@1: 0.9062 (0.6578), Acc@5: 0.9688 (0.8763)
2021-12-29 10:25:40,322 Val Step[0950/1563], Loss: 1.9203 (1.5731), Acc@1: 0.6250 (0.6520), Acc@5: 0.7812 (0.8712)
2021-12-29 10:25:41,786 Val Step[1000/1563], Loss: 0.8104 (1.6015), Acc@1: 0.9062 (0.6462), Acc@5: 0.9688 (0.8669)
2021-12-29 10:25:43,304 Val Step[1050/1563], Loss: 0.5685 (1.6193), Acc@1: 0.9375 (0.6430), Acc@5: 0.9688 (0.8643)
2021-12-29 10:25:44,814 Val Step[1100/1563], Loss: 1.4613 (1.6396), Acc@1: 0.6875 (0.6390), Acc@5: 0.9375 (0.8611)
2021-12-29 10:25:46,246 Val Step[1150/1563], Loss: 1.6264 (1.6607), Acc@1: 0.7188 (0.6357), Acc@5: 0.7500 (0.8578)
2021-12-29 10:25:47,781 Val Step[1200/1563], Loss: 1.8644 (1.6802), Acc@1: 0.6562 (0.6318), Acc@5: 0.8438 (0.8551)
2021-12-29 10:25:49,274 Val Step[1250/1563], Loss: 1.1464 (1.6994), Acc@1: 0.8125 (0.6287), Acc@5: 0.9062 (0.8514)
2021-12-29 10:25:50,760 Val Step[1300/1563], Loss: 1.2856 (1.7104), Acc@1: 0.7812 (0.6268), Acc@5: 0.8125 (0.8503)
2021-12-29 10:25:52,200 Val Step[1350/1563], Loss: 2.8249 (1.7327), Acc@1: 0.1875 (0.6224), Acc@5: 0.6562 (0.8466)
2021-12-29 10:25:53,670 Val Step[1400/1563], Loss: 1.7720 (1.7413), Acc@1: 0.6562 (0.6206), Acc@5: 0.7812 (0.8455)
2021-12-29 10:25:55,073 Val Step[1450/1563], Loss: 2.3653 (1.7489), Acc@1: 0.4062 (0.6190), Acc@5: 0.8125 (0.8446)
2021-12-29 10:25:56,506 Val Step[1500/1563], Loss: 2.3473 (1.7361), Acc@1: 0.3438 (0.6214), Acc@5: 0.8125 (0.8465)
2021-12-29 10:25:57,910 Val Step[1550/1563], Loss: 1.0851 (1.7310), Acc@1: 0.8750 (0.6225), Acc@5: 0.9062 (0.8471)
2021-12-29 10:25:58,799 ----- Epoch[070/310], Validation Loss: 1.7286, Validation Acc@1: 0.6229, Validation Acc@5: 0.8473, time: 112.20
2021-12-29 10:25:58,799 ----- Epoch[070/310], Train Loss: 4.2845, Train Acc: 0.2785, time: 1654.07, Best Val(epoch70) Acc@1: 0.6229
2021-12-29 10:25:59,008 Max accuracy so far: 0.6229 at epoch_70
2021-12-29 10:25:59,008 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 10:25:59,008 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 10:25:59,103 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 10:25:59,267 ----- Save model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-70-Loss-4.2601642305616565.pdparams
2021-12-29 10:25:59,268 ----- Save optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-70-Loss-4.2601642305616565.pdopt
2021-12-29 10:25:59,336 ----- Save ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/PiT-Epoch-70-Loss-4.2601642305616565-EMA.pdparams
2021-12-29 10:25:59,336 Now training epoch 71. LR=0.000869
2021-12-29 10:27:21,679 Epoch[071/310], Step[0000/1251], Loss: 4.7991(4.7991), Acc: 0.2881(0.2881)
2021-12-29 10:28:25,108 Epoch[071/310], Step[0050/1251], Loss: 4.0272(4.3141), Acc: 0.4316(0.2849)
2021-12-29 10:29:26,236 Epoch[071/310], Step[0100/1251], Loss: 4.0249(4.2823), Acc: 0.3574(0.2919)
2021-12-29 10:30:29,625 Epoch[071/310], Step[0150/1251], Loss: 4.6490(4.2843), Acc: 0.1738(0.2940)
2021-12-29 10:31:32,795 Epoch[071/310], Step[0200/1251], Loss: 3.8732(4.2819), Acc: 0.4678(0.2907)
2021-12-29 10:32:35,710 Epoch[071/310], Step[0250/1251], Loss: 4.2278(4.2865), Acc: 0.2705(0.2867)
2021-12-29 10:33:38,849 Epoch[071/310], Step[0300/1251], Loss: 4.3230(4.2762), Acc: 0.3457(0.2875)
2021-12-29 10:34:41,282 Epoch[071/310], Step[0350/1251], Loss: 4.1339(4.2779), Acc: 0.2598(0.2877)
2021-12-29 10:35:44,080 Epoch[071/310], Step[0400/1251], Loss: 3.5407(4.2723), Acc: 0.1973(0.2867)
2021-12-29 10:36:46,250 Epoch[071/310], Step[0450/1251], Loss: 3.9806(4.2666), Acc: 0.2969(0.2841)
2021-12-29 10:37:49,004 Epoch[071/310], Step[0500/1251], Loss: 4.3229(4.2679), Acc: 0.2334(0.2834)
2021-12-29 10:38:50,971 Epoch[071/310], Step[0550/1251], Loss: 3.9024(4.2639), Acc: 0.4287(0.2830)
2021-12-29 10:39:54,735 Epoch[071/310], Step[0600/1251], Loss: 4.3451(4.2610), Acc: 0.2461(0.2817)
2021-12-29 10:40:59,045 Epoch[071/310], Step[0650/1251], Loss: 4.3478(4.2596), Acc: 0.2920(0.2820)
2021-12-29 10:42:03,168 Epoch[071/310], Step[0700/1251], Loss: 4.0433(4.2608), Acc: 0.4102(0.2829)
2021-12-29 10:43:07,206 Epoch[071/310], Step[0750/1251], Loss: 4.7545(4.2580), Acc: 0.2695(0.2836)
2021-12-29 10:44:07,947 Epoch[071/310], Step[0800/1251], Loss: 4.8014(4.2632), Acc: 0.2012(0.2833)
2021-12-29 10:45:09,500 Epoch[071/310], Step[0850/1251], Loss: 4.0563(4.2634), Acc: 0.3340(0.2832)
2021-12-29 10:46:09,989 Epoch[071/310], Step[0900/1251], Loss: 4.2314(4.2652), Acc: 0.1162(0.2831)
2021-12-29 10:47:11,387 Epoch[071/310], Step[0950/1251], Loss: 4.1143(4.2628), Acc: 0.2070(0.2837)
2021-12-29 10:48:13,292 Epoch[071/310], Step[1000/1251], Loss: 4.4410(4.2656), Acc: 0.3154(0.2841)
2021-12-29 10:49:16,979 Epoch[071/310], Step[1050/1251], Loss: 4.1251(4.2671), Acc: 0.3760(0.2831)
2021-12-29 10:50:19,315 Epoch[071/310], Step[1100/1251], Loss: 5.2431(4.2716), Acc: 0.1475(0.2822)
2021-12-29 10:51:23,028 Epoch[071/310], Step[1150/1251], Loss: 4.0634(4.2749), Acc: 0.3193(0.2816)
2021-12-29 10:52:27,039 Epoch[071/310], Step[1200/1251], Loss: 4.3258(4.2737), Acc: 0.2617(0.2828)
2021-12-29 10:53:29,677 Epoch[071/310], Step[1250/1251], Loss: 4.3254(4.2708), Acc: 0.2949(0.2834)
2021-12-29 10:53:31,631 ----- Epoch[071/310], Train Loss: 4.2708, Train Acc: 0.2834, time: 1652.29, Best Val(epoch70) Acc@1: 0.6229
2021-12-29 10:53:31,631 Now training epoch 72. LR=0.000865
2021-12-29 10:54:58,097 Epoch[072/310], Step[0000/1251], Loss: 3.4385(3.4385), Acc: 0.5215(0.5215)
2021-12-29 10:55:59,951 Epoch[072/310], Step[0050/1251], Loss: 4.5263(4.2608), Acc: 0.2861(0.2792)
2021-12-29 10:57:01,698 Epoch[072/310], Step[0100/1251], Loss: 4.5115(4.2562), Acc: 0.1387(0.2748)
2021-12-29 10:58:02,869 Epoch[072/310], Step[0150/1251], Loss: 4.6593(4.2770), Acc: 0.2236(0.2746)
2021-12-29 10:59:05,630 Epoch[072/310], Step[0200/1251], Loss: 4.2547(4.2592), Acc: 0.3750(0.2788)
2021-12-29 11:00:07,705 Epoch[072/310], Step[0250/1251], Loss: 4.6133(4.2712), Acc: 0.2129(0.2735)
2021-12-29 11:01:10,913 Epoch[072/310], Step[0300/1251], Loss: 4.0163(4.2660), Acc: 0.4180(0.2765)
2021-12-29 11:02:14,312 Epoch[072/310], Step[0350/1251], Loss: 4.1892(4.2626), Acc: 0.2959(0.2740)
2021-12-29 11:03:17,251 Epoch[072/310], Step[0400/1251], Loss: 4.2696(4.2547), Acc: 0.3242(0.2728)
2021-12-29 11:04:20,864 Epoch[072/310], Step[0450/1251], Loss: 4.2430(4.2484), Acc: 0.2676(0.2723)
2021-12-29 11:05:23,728 Epoch[072/310], Step[0500/1251], Loss: 3.8996(4.2524), Acc: 0.3848(0.2739)
2021-12-29 11:06:25,475 Epoch[072/310], Step[0550/1251], Loss: 4.1112(4.2518), Acc: 0.1748(0.2733)
2021-12-29 11:07:27,240 Epoch[072/310], Step[0600/1251], Loss: 4.2279(4.2517), Acc: 0.3721(0.2740)
2021-12-29 11:08:28,561 Epoch[072/310], Step[0650/1251], Loss: 4.3445(4.2555), Acc: 0.2529(0.2754)
2021-12-29 11:09:31,415 Epoch[072/310], Step[0700/1251], Loss: 3.6040(4.2568), Acc: 0.4746(0.2771)
2021-12-29 11:10:34,692 Epoch[072/310], Step[0750/1251], Loss: 5.0016(4.2563), Acc: 0.1553(0.2775)
2021-12-29 11:11:36,397 Epoch[072/310], Step[0800/1251], Loss: 3.9115(4.2602), Acc: 0.4033(0.2779)
2021-12-29 11:12:38,356 Epoch[072/310], Step[0850/1251], Loss: 4.7468(4.2588), Acc: 0.2979(0.2779)
2021-12-29 11:13:39,874 Epoch[072/310], Step[0900/1251], Loss: 3.9470(4.2554), Acc: 0.1260(0.2779)
2021-12-29 11:14:41,332 Epoch[072/310], Step[0950/1251], Loss: 3.9412(4.2564), Acc: 0.2754(0.2788)
2021-12-29 11:15:43,660 Epoch[072/310], Step[1000/1251], Loss: 4.1215(4.2631), Acc: 0.3691(0.2783)
2021-12-29 11:16:45,644 Epoch[072/310], Step[1050/1251], Loss: 4.2962(4.2618), Acc: 0.3262(0.2789)
2021-12-29 11:17:48,187 Epoch[072/310], Step[1100/1251], Loss: 4.4811(4.2634), Acc: 0.2861(0.2798)
2021-12-29 11:18:51,441 Epoch[072/310], Step[1150/1251], Loss: 4.2662(4.2651), Acc: 0.3691(0.2802)
2021-12-29 11:19:53,865 Epoch[072/310], Step[1200/1251], Loss: 4.5099(4.2650), Acc: 0.2607(0.2805)
2021-12-29 11:20:55,317 Epoch[072/310], Step[1250/1251], Loss: 4.0520(4.2640), Acc: 0.3320(0.2809)
2021-12-29 11:20:57,281 ----- Validation after Epoch: 72
2021-12-29 11:21:59,658 Val Step[0000/1563], Loss: 0.8839 (0.8839), Acc@1: 0.8750 (0.8750), Acc@5: 0.9062 (0.9062)
2021-12-29 11:22:01,200 Val Step[0050/1563], Loss: 2.3980 (1.0890), Acc@1: 0.4062 (0.7745), Acc@5: 0.7188 (0.9283)
2021-12-29 11:22:02,676 Val Step[0100/1563], Loss: 1.9454 (1.4385), Acc@1: 0.5000 (0.6810), Acc@5: 0.8438 (0.8874)
2021-12-29 11:22:04,201 Val Step[0150/1563], Loss: 0.7200 (1.3479), Acc@1: 0.8750 (0.7065), Acc@5: 0.9375 (0.8957)
2021-12-29 11:22:05,767 Val Step[0200/1563], Loss: 1.3246 (1.3655), Acc@1: 0.7188 (0.7051), Acc@5: 0.9375 (0.8938)
2021-12-29 11:22:07,217 Val Step[0250/1563], Loss: 1.1934 (1.3078), Acc@1: 0.7500 (0.7186), Acc@5: 0.9688 (0.9020)
2021-12-29 11:22:08,833 Val Step[0300/1563], Loss: 1.6628 (1.3759), Acc@1: 0.5625 (0.6956), Acc@5: 0.8438 (0.8958)
2021-12-29 11:22:10,340 Val Step[0350/1563], Loss: 1.3995 (1.3905), Acc@1: 0.6250 (0.6879), Acc@5: 0.9375 (0.8980)
2021-12-29 11:22:11,813 Val Step[0400/1563], Loss: 1.1403 (1.3884), Acc@1: 0.7188 (0.6823), Acc@5: 0.9688 (0.9001)
2021-12-29 11:22:13,345 Val Step[0450/1563], Loss: 0.8568 (1.3856), Acc@1: 0.7500 (0.6810), Acc@5: 0.9688 (0.9012)
2021-12-29 11:22:14,966 Val Step[0500/1563], Loss: 0.4042 (1.3748), Acc@1: 0.9062 (0.6830), Acc@5: 1.0000 (0.9031)
2021-12-29 11:22:16,565 Val Step[0550/1563], Loss: 1.0310 (1.3438), Acc@1: 0.7188 (0.6916), Acc@5: 0.9688 (0.9059)
2021-12-29 11:22:18,164 Val Step[0600/1563], Loss: 1.0481 (1.3478), Acc@1: 0.7500 (0.6916), Acc@5: 0.9062 (0.9055)
2021-12-29 11:22:19,643 Val Step[0650/1563], Loss: 0.8574 (1.3687), Acc@1: 0.9375 (0.6885), Acc@5: 0.9688 (0.9023)
2021-12-29 11:22:21,066 Val Step[0700/1563], Loss: 2.1534 (1.4101), Acc@1: 0.6250 (0.6797), Acc@5: 0.8125 (0.8957)
2021-12-29 11:22:22,716 Val Step[0750/1563], Loss: 1.7086 (1.4518), Acc@1: 0.6250 (0.6724), Acc@5: 0.8438 (0.8891)
2021-12-29 11:22:24,214 Val Step[0800/1563], Loss: 1.3460 (1.4955), Acc@1: 0.7188 (0.6627), Acc@5: 0.9375 (0.8831)
2021-12-29 11:22:25,681 Val Step[0850/1563], Loss: 1.6727 (1.5269), Acc@1: 0.6250 (0.6567), Acc@5: 0.9062 (0.8785)
2021-12-29 11:22:27,277 Val Step[0900/1563], Loss: 0.7118 (1.5311), Acc@1: 0.8438 (0.6579), Acc@5: 0.9062 (0.8771)
2021-12-29 11:22:28,826 Val Step[0950/1563], Loss: 1.6982 (1.5585), Acc@1: 0.6562 (0.6521), Acc@5: 0.8438 (0.8725)
2021-12-29 11:22:30,279 Val Step[1000/1563], Loss: 0.8605 (1.5843), Acc@1: 0.9062 (0.6473), Acc@5: 0.9375 (0.8679)
2021-12-29 11:22:31,707 Val Step[1050/1563], Loss: 0.5634 (1.6016), Acc@1: 0.9375 (0.6439), Acc@5: 0.9688 (0.8654)
2021-12-29 11:22:33,165 Val Step[1100/1563], Loss: 1.5915 (1.6228), Acc@1: 0.6562 (0.6395), Acc@5: 0.9062 (0.8617)
2021-12-29 11:22:34,793 Val Step[1150/1563], Loss: 1.6073 (1.6421), Acc@1: 0.7188 (0.6359), Acc@5: 0.8125 (0.8585)
2021-12-29 11:22:36,369 Val Step[1200/1563], Loss: 1.6824 (1.6603), Acc@1: 0.7188 (0.6323), Acc@5: 0.8438 (0.8557)
2021-12-29 11:22:37,897 Val Step[1250/1563], Loss: 0.9662 (1.6772), Acc@1: 0.7812 (0.6294), Acc@5: 0.9375 (0.8528)
2021-12-29 11:22:39,385 Val Step[1300/1563], Loss: 1.3240 (1.6914), Acc@1: 0.7812 (0.6262), Acc@5: 0.8750 (0.8511)
2021-12-29 11:22:40,843 Val Step[1350/1563], Loss: 3.0250 (1.7105), Acc@1: 0.1875 (0.6222), Acc@5: 0.6250 (0.8479)
2021-12-29 11:22:42,411 Val Step[1400/1563], Loss: 1.3478 (1.7208), Acc@1: 0.7500 (0.6202), Acc@5: 0.8438 (0.8464)
2021-12-29 11:22:43,944 Val Step[1450/1563], Loss: 1.9881 (1.7292), Acc@1: 0.5625 (0.6185), Acc@5: 0.8438 (0.8450)
2021-12-29 11:22:45,408 Val Step[1500/1563], Loss: 1.9960 (1.7151), Acc@1: 0.4375 (0.6217), Acc@5: 0.8438 (0.8470)
2021-12-29 11:22:46,965 Val Step[1550/1563], Loss: 1.0409 (1.7120), Acc@1: 0.8750 (0.6227), Acc@5: 0.9062 (0.8475)
2021-12-29 11:22:48,283 ----- Epoch[072/310], Validation Loss: 1.7092, Validation Acc@1: 0.6233, Validation Acc@5: 0.8479, time: 111.00
2021-12-29 11:22:48,283 ----- Epoch[072/310], Train Loss: 4.2640, Train Acc: 0.2809, time: 1645.65, Best Val(epoch72) Acc@1: 0.6233
2021-12-29 11:22:48,493 Max accuracy so far: 0.6233 at epoch_72
2021-12-29 11:22:48,494 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 11:22:48,494 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 11:22:48,572 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 11:22:48,572 Now training epoch 73. LR=0.000862
2021-12-29 11:24:17,412 Epoch[073/310], Step[0000/1251], Loss: 4.1652(4.1652), Acc: 0.1865(0.1865)
2021-12-29 11:25:19,143 Epoch[073/310], Step[0050/1251], Loss: 4.2144(4.2338), Acc: 0.2939(0.2996)
2021-12-29 11:26:21,573 Epoch[073/310], Step[0100/1251], Loss: 3.7845(4.2497), Acc: 0.2783(0.2945)
2021-12-29 11:27:23,418 Epoch[073/310], Step[0150/1251], Loss: 3.6124(4.2557), Acc: 0.2705(0.2972)
2021-12-29 11:28:25,410 Epoch[073/310], Step[0200/1251], Loss: 4.7546(4.2478), Acc: 0.2305(0.3044)
2021-12-29 11:29:27,490 Epoch[073/310], Step[0250/1251], Loss: 4.0226(4.2365), Acc: 0.2656(0.3018)
2021-12-29 11:30:27,926 Epoch[073/310], Step[0300/1251], Loss: 4.6659(4.2324), Acc: 0.1719(0.2987)
2021-12-29 11:31:30,747 Epoch[073/310], Step[0350/1251], Loss: 4.5907(4.2448), Acc: 0.2676(0.2979)
2021-12-29 11:32:32,885 Epoch[073/310], Step[0400/1251], Loss: 4.0850(4.2426), Acc: 0.2910(0.2972)
2021-12-29 11:33:36,059 Epoch[073/310], Step[0450/1251], Loss: 4.2766(4.2422), Acc: 0.0557(0.2972)
2021-12-29 11:34:39,452 Epoch[073/310], Step[0500/1251], Loss: 3.8239(4.2446), Acc: 0.4268(0.2965)
2021-12-29 11:35:42,497 Epoch[073/310], Step[0550/1251], Loss: 4.4021(4.2453), Acc: 0.2461(0.2956)
2021-12-29 11:36:43,843 Epoch[073/310], Step[0600/1251], Loss: 4.7014(4.2445), Acc: 0.1348(0.2951)
2021-12-29 11:37:48,085 Epoch[073/310], Step[0650/1251], Loss: 4.4395(4.2499), Acc: 0.3760(0.2952)
2021-12-29 11:38:49,168 Epoch[073/310], Step[0700/1251], Loss: 4.7266(4.2511), Acc: 0.2402(0.2949)
2021-12-29 11:39:49,939 Epoch[073/310], Step[0750/1251], Loss: 4.6335(4.2542), Acc: 0.1758(0.2932)
2021-12-29 11:40:51,707 Epoch[073/310], Step[0800/1251], Loss: 4.4418(4.2551), Acc: 0.2793(0.2916)
2021-12-29 11:41:55,475 Epoch[073/310], Step[0850/1251], Loss: 3.8560(4.2574), Acc: 0.3564(0.2903)
2021-12-29 11:42:58,528 Epoch[073/310], Step[0900/1251], Loss: 3.8197(4.2598), Acc: 0.3398(0.2898)
2021-12-29 11:44:00,208 Epoch[073/310], Step[0950/1251], Loss: 4.3511(4.2565), Acc: 0.2441(0.2903)
2021-12-29 11:45:04,675 Epoch[073/310], Step[1000/1251], Loss: 3.7078(4.2567), Acc: 0.3242(0.2896)
2021-12-29 11:46:06,582 Epoch[073/310], Step[1050/1251], Loss: 3.7089(4.2561), Acc: 0.2783(0.2899)
2021-12-29 11:47:10,169 Epoch[073/310], Step[1100/1251], Loss: 4.6122(4.2584), Acc: 0.1289(0.2886)
2021-12-29 11:48:13,800 Epoch[073/310], Step[1150/1251], Loss: 4.2245(4.2585), Acc: 0.2617(0.2877)
2021-12-29 11:49:17,499 Epoch[073/310], Step[1200/1251], Loss: 4.4285(4.2619), Acc: 0.2188(0.2876)
2021-12-29 11:50:19,099 Epoch[073/310], Step[1250/1251], Loss: 4.6047(4.2648), Acc: 0.2695(0.2875)
2021-12-29 11:50:21,938 ----- Epoch[073/310], Train Loss: 4.2648, Train Acc: 0.2875, time: 1653.36, Best Val(epoch72) Acc@1: 0.6233
2021-12-29 11:50:21,938 Now training epoch 74. LR=0.000858
2021-12-29 11:51:47,292 Epoch[074/310], Step[0000/1251], Loss: 4.3119(4.3119), Acc: 0.1602(0.1602)
2021-12-29 11:52:50,717 Epoch[074/310], Step[0050/1251], Loss: 4.3077(4.1841), Acc: 0.0762(0.2966)
2021-12-29 11:53:53,517 Epoch[074/310], Step[0100/1251], Loss: 4.3653(4.2064), Acc: 0.3135(0.2952)
2021-12-29 11:54:56,125 Epoch[074/310], Step[0150/1251], Loss: 4.2290(4.2156), Acc: 0.2529(0.2860)
2021-12-29 11:55:58,560 Epoch[074/310], Step[0200/1251], Loss: 3.6109(4.2167), Acc: 0.2666(0.2853)
2021-12-29 11:57:01,112 Epoch[074/310], Step[0250/1251], Loss: 4.1986(4.2254), Acc: 0.1934(0.2835)
2021-12-29 11:58:04,605 Epoch[074/310], Step[0300/1251], Loss: 4.7316(4.2178), Acc: 0.2812(0.2840)
2021-12-29 11:59:08,690 Epoch[074/310], Step[0350/1251], Loss: 4.6816(4.2252), Acc: 0.2236(0.2841)
2021-12-29 12:00:11,749 Epoch[074/310], Step[0400/1251], Loss: 4.0539(4.2220), Acc: 0.4502(0.2836)
2021-12-29 12:01:15,108 Epoch[074/310], Step[0450/1251], Loss: 4.1278(4.2150), Acc: 0.2656(0.2842)
2021-12-29 12:02:16,426 Epoch[074/310], Step[0500/1251], Loss: 4.7408(4.2177), Acc: 0.1621(0.2832)
2021-12-29 12:03:19,027 Epoch[074/310], Step[0550/1251], Loss: 4.3252(4.2179), Acc: 0.2793(0.2831)
2021-12-29 12:04:23,112 Epoch[074/310], Step[0600/1251], Loss: 4.5137(4.2234), Acc: 0.3105(0.2834)
2021-12-29 12:05:25,225 Epoch[074/310], Step[0650/1251], Loss: 4.2727(4.2255), Acc: 0.0508(0.2829)
2021-12-29 12:06:27,835 Epoch[074/310], Step[0700/1251], Loss: 4.4464(4.2271), Acc: 0.3213(0.2826)
2021-12-29 12:07:31,777 Epoch[074/310], Step[0750/1251], Loss: 3.5756(4.2250), Acc: 0.2646(0.2826)
2021-12-29 12:08:34,373 Epoch[074/310], Step[0800/1251], Loss: 3.8850(4.2273), Acc: 0.4326(0.2830)
2021-12-29 12:09:37,413 Epoch[074/310], Step[0850/1251], Loss: 4.4850(4.2309), Acc: 0.2549(0.2836)
2021-12-29 12:10:39,659 Epoch[074/310], Step[0900/1251], Loss: 4.2239(4.2333), Acc: 0.3936(0.2825)
2021-12-29 12:11:41,835 Epoch[074/310], Step[0950/1251], Loss: 4.2509(4.2334), Acc: 0.3545(0.2828)
2021-12-29 12:12:42,554 Epoch[074/310], Step[1000/1251], Loss: 4.6604(4.2325), Acc: 0.1484(0.2835)
2021-12-29 12:13:45,745 Epoch[074/310], Step[1050/1251], Loss: 4.8177(4.2319), Acc: 0.1055(0.2846)
2021-12-29 12:14:47,447 Epoch[074/310], Step[1100/1251], Loss: 4.0702(4.2333), Acc: 0.1133(0.2854)
2021-12-29 12:15:49,186 Epoch[074/310], Step[1150/1251], Loss: 4.3155(4.2345), Acc: 0.2314(0.2852)
2021-12-29 12:16:51,286 Epoch[074/310], Step[1200/1251], Loss: 4.1575(4.2350), Acc: 0.2100(0.2850)
2021-12-29 12:17:54,147 Epoch[074/310], Step[1250/1251], Loss: 4.2266(4.2370), Acc: 0.3428(0.2841)
2021-12-29 12:17:56,186 ----- Validation after Epoch: 74
2021-12-29 12:18:55,554 Val Step[0000/1563], Loss: 0.6087 (0.6087), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-29 12:18:57,251 Val Step[0050/1563], Loss: 2.3475 (1.0520), Acc@1: 0.4688 (0.7831), Acc@5: 0.8750 (0.9375)
2021-12-29 12:18:58,799 Val Step[0100/1563], Loss: 2.2860 (1.3752), Acc@1: 0.2500 (0.6918), Acc@5: 0.8125 (0.8963)
2021-12-29 12:19:00,311 Val Step[0150/1563], Loss: 0.6232 (1.2971), Acc@1: 0.8750 (0.7094), Acc@5: 0.9375 (0.9042)
2021-12-29 12:19:01,786 Val Step[0200/1563], Loss: 1.1694 (1.3358), Acc@1: 0.6562 (0.7065), Acc@5: 0.9062 (0.8977)
2021-12-29 12:19:03,242 Val Step[0250/1563], Loss: 1.2215 (1.2806), Acc@1: 0.6250 (0.7173), Acc@5: 0.9688 (0.9045)
2021-12-29 12:19:04,745 Val Step[0300/1563], Loss: 1.5804 (1.3472), Acc@1: 0.5625 (0.6941), Acc@5: 0.9062 (0.8991)
2021-12-29 12:19:06,172 Val Step[0350/1563], Loss: 1.3342 (1.3530), Acc@1: 0.7188 (0.6913), Acc@5: 0.8750 (0.9009)
2021-12-29 12:19:07,626 Val Step[0400/1563], Loss: 1.1595 (1.3427), Acc@1: 0.7812 (0.6894), Acc@5: 0.9688 (0.9032)
2021-12-29 12:19:09,151 Val Step[0450/1563], Loss: 1.3149 (1.3449), Acc@1: 0.4688 (0.6863), Acc@5: 1.0000 (0.9042)
2021-12-29 12:19:10,774 Val Step[0500/1563], Loss: 0.7630 (1.3361), Acc@1: 0.8750 (0.6894), Acc@5: 1.0000 (0.9051)
2021-12-29 12:19:12,300 Val Step[0550/1563], Loss: 0.9947 (1.3116), Acc@1: 0.7500 (0.6961), Acc@5: 0.9062 (0.9077)
2021-12-29 12:19:13,923 Val Step[0600/1563], Loss: 1.0730 (1.3143), Acc@1: 0.8125 (0.6963), Acc@5: 0.9375 (0.9078)
2021-12-29 12:19:15,600 Val Step[0650/1563], Loss: 1.1226 (1.3370), Acc@1: 0.8125 (0.6916), Acc@5: 0.9688 (0.9045)
2021-12-29 12:19:17,111 Val Step[0700/1563], Loss: 1.8598 (1.3793), Acc@1: 0.6250 (0.6821), Acc@5: 0.9062 (0.8990)
2021-12-29 12:19:18,730 Val Step[0750/1563], Loss: 1.9159 (1.4206), Acc@1: 0.5938 (0.6747), Acc@5: 0.8125 (0.8933)
2021-12-29 12:19:20,240 Val Step[0800/1563], Loss: 1.2654 (1.4695), Acc@1: 0.6875 (0.6638), Acc@5: 0.9062 (0.8862)
2021-12-29 12:19:21,726 Val Step[0850/1563], Loss: 1.9469 (1.5002), Acc@1: 0.5312 (0.6578), Acc@5: 0.8438 (0.8821)
2021-12-29 12:19:23,250 Val Step[0900/1563], Loss: 0.7511 (1.5016), Acc@1: 0.8750 (0.6601), Acc@5: 0.9062 (0.8812)
2021-12-29 12:19:24,816 Val Step[0950/1563], Loss: 1.5332 (1.5282), Acc@1: 0.7812 (0.6553), Acc@5: 0.8125 (0.8767)
2021-12-29 12:19:26,306 Val Step[1000/1563], Loss: 0.7825 (1.5567), Acc@1: 0.9375 (0.6496), Acc@5: 0.9688 (0.8720)
2021-12-29 12:19:27,756 Val Step[1050/1563], Loss: 0.5426 (1.5774), Acc@1: 0.9688 (0.6460), Acc@5: 0.9688 (0.8692)
2021-12-29 12:19:29,296 Val Step[1100/1563], Loss: 1.4558 (1.5980), Acc@1: 0.6250 (0.6421), Acc@5: 0.9062 (0.8662)
2021-12-29 12:19:30,741 Val Step[1150/1563], Loss: 1.7233 (1.6197), Acc@1: 0.7188 (0.6388), Acc@5: 0.7812 (0.8628)
2021-12-29 12:19:32,170 Val Step[1200/1563], Loss: 1.8160 (1.6380), Acc@1: 0.7188 (0.6354), Acc@5: 0.8438 (0.8599)
2021-12-29 12:19:33,666 Val Step[1250/1563], Loss: 1.2384 (1.6538), Acc@1: 0.7500 (0.6331), Acc@5: 0.8750 (0.8570)
2021-12-29 12:19:35,293 Val Step[1300/1563], Loss: 1.5341 (1.6672), Acc@1: 0.7188 (0.6302), Acc@5: 0.8438 (0.8558)
2021-12-29 12:19:36,810 Val Step[1350/1563], Loss: 2.1584 (1.6875), Acc@1: 0.4062 (0.6261), Acc@5: 0.8125 (0.8520)
2021-12-29 12:19:38,340 Val Step[1400/1563], Loss: 1.4861 (1.6952), Acc@1: 0.7188 (0.6247), Acc@5: 0.8438 (0.8506)
2021-12-29 12:19:39,799 Val Step[1450/1563], Loss: 1.8240 (1.7036), Acc@1: 0.4688 (0.6231), Acc@5: 0.9062 (0.8493)
2021-12-29 12:19:41,241 Val Step[1500/1563], Loss: 1.6370 (1.6903), Acc@1: 0.5000 (0.6258), Acc@5: 0.9062 (0.8513)
2021-12-29 12:19:42,631 Val Step[1550/1563], Loss: 0.9237 (1.6892), Acc@1: 0.8750 (0.6261), Acc@5: 0.9062 (0.8514)
2021-12-29 12:19:43,488 ----- Epoch[074/310], Validation Loss: 1.6855, Validation Acc@1: 0.6270, Validation Acc@5: 0.8519, time: 107.30
2021-12-29 12:19:43,488 ----- Epoch[074/310], Train Loss: 4.2370, Train Acc: 0.2841, time: 1654.24, Best Val(epoch74) Acc@1: 0.6270
2021-12-29 12:19:43,699 Max accuracy so far: 0.6270 at epoch_74
2021-12-29 12:19:43,699 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdparams
2021-12-29 12:19:43,699 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT.pdopt
2021-12-29 12:19:43,794 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211228-01-15-41/Best_PiT-EMA.pdparams
2021-12-29 12:19:43,795 Now training epoch 75. LR=0.000854
2021-12-29 12:21:17,774 Epoch[075/310], Step[0000/1251], Loss: 4.0728(4.0728), Acc: 0.3428(0.3428)
2021-12-29 12:22:18,609 Epoch[075/310], Step[0050/1251], Loss: 4.2389(4.2068), Acc: 0.3623(0.2904)
2021-12-29 12:23:19,056 Epoch[075/310], Step[0100/1251], Loss: 4.6375(4.2711), Acc: 0.1240(0.2725)
2021-12-29 12:24:19,094 Epoch[075/310], Step[0150/1251], Loss: 4.4898(4.2645), Acc: 0.1924(0.2796)
2021-12-29 12:25:19,336 Epoch[075/310], Step[0200/1251], Loss: 3.8208(4.2624), Acc: 0.3076(0.2816)
2021-12-29 12:26:18,707 Epoch[075/310], Step[0250/1251], Loss: 4.0691(4.2659), Acc: 0.3330(0.2752)
2021-12-29 12:27:16,894 Epoch[075/310], Step[0300/1251], Loss: 4.4763(4.2626), Acc: 0.3779(0.2764)
2021-12-29 12:28:15,531 Epoch[075/310], Step[0350/1251], Loss: 4.2333(4.2628), Acc: 0.2510(0.2778)
2021-12-29 12:29:13,705 Epoch[075/310], Step[0400/1251], Loss: 4.6816(4.2693), Acc: 0.3350(0.2783)
2021-12-29 12:30:13,954 Epoch[075/310], Step[0450/1251], Loss: 4.1575(4.2656), Acc: 0.4131(0.2788)
2021-12-29 12:31:12,894 Epoch[075/310], Step[0500/1251], Loss: 4.5683(4.2581), Acc: 0.2188(0.2812)
2021-12-29 12:32:12,054 Epoch[075/310], Step[0550/1251], Loss: 4.3019(4.2647), Acc: 0.3789(0.2807)
2021-12-29 12:33:10,684 Epoch[075/310], Step[0600/1251], Loss: 4.3125(4.2601), Acc: 0.1953(0.2801)
2021-12-29 12:34:10,575 Epoch[075/310], Step[0650/1251], Loss: 4.3941(4.2518), Acc: 0.2646(0.2823)
2021-12-29 12:35:10,557 Epoch[075/310], Step[0700/1251], Loss: 4.8664(4.2567), Acc: 0.2334(0.2821)
2021-12-29 12:36:11,408 Epoch[075/310], Step[0750/1251], Loss: 4.1835(4.2541), Acc: 0.4043(0.2820)
2021-12-29 12:37:11,230 Epoch[075/310], Step[0800/1251], Loss: 4.6170(4.2542), Acc: 0.2051(0.2819)
2021-12-29 12:38:10,530 Epoch[075/310], Step[0850/1251], Loss: 4.3442(4.2573), Acc: 0.2959(0.2817)
2021-12-29 12:39:10,196 Epoch[075/310], Step[0900/1251], Loss: 4.6041(4.2601), Acc: 0.2705(0.2820)
2021-12-29 12:40:10,255 Epoch[075/310], Step[0950/1251], Loss: 4.5222(4.2632), Acc: 0.2861(0.2814)
2021-12-29 12:41:10,301 Epoch[075/310], Step[1000/1251], Loss: 4.5668(4.2623), Acc: 0.0488(0.2822)
2021-12-29 12:42:11,257 Epoch[075/310], Step[1050/1251], Loss: 4.3275(4.2586), Acc: 0.1328(0.2826)
2021-12-29 12:43:12,973 Epoch[075/310], Step[1100/1251], Loss: 4.0493(4.2565), Acc: 0.4346(0.2832)
2021-12-29 12:44:12,566 Epoch[075/310], Step[1150/1251], Loss: 4.4383(4.2558), Acc: 0.2920(0.2832)
2021-12-29 12:45:12,706 Epoch[075/310], Step[1200/1251], Loss: 3.8931(4.2553), Acc: 0.3916(0.2834)
2021-12-29 12:46:13,973 Epoch[075/310], Step[1250/1251], Loss: 4.4553(4.2527), Acc: 0.2266(0.2837)
2021-12-29 12:46:16,361 ----- Epoch[075/310], Train Loss: 4.2527, Train Acc: 0.2837, time: 1592.56, Best Val(epoch74) Acc@1: 0.6270
2021-12-29 12:46:16,361 Now training epoch 76. LR=0.000851
2021-12-29 12:47:33,436 Epoch[076/310], Step[0000/1251], Loss: 4.0000(4.0000), Acc: 0.3701(0.3701)
2021-12-29 12:48:34,075 Epoch[076/310], Step[0050/1251], Loss: 4.0039(4.2382), Acc: 0.2979(0.2744)
2021-12-29 12:49:35,014 Epoch[076/310], Step[0100/1251], Loss: 4.4556(4.2455), Acc: 0.4043(0.2824)
2021-12-29 12:50:34,935 Epoch[076/310], Step[0150/1251], Loss: 3.8092(4.2496), Acc: 0.3760(0.2782)
2021-12-29 12:51:35,181 Epoch[076/310], Step[0200/1251], Loss: 4.7352(4.2390), Acc: 0.2900(0.2867)
2021-12-29 12:52:35,130 Epoch[076/310], Step[0250/1251], Loss: 3.6843(4.2386), Acc: 0.3477(0.2864)
2021-12-29 12:53:33,449 Epoch[076/310], Step[0300/1251], Loss: 3.8470(4.2387), Acc: 0.2617(0.2876)
2021-12-29 12:54:33,334 Epoch[076/310], Step[0350/1251], Loss: 3.6649(4.2400), Acc: 0.4951(0.2860)
2021-12-29 12:55:33,181 Epoch[076/310], Step[0400/1251], Loss: 4.2352(4.2550), Acc: 0.1719(0.2846)
2021-12-29 12:56:33,812 Epoch[076/310], Step[0450/1251], Loss: 3.7158(4.2524), Acc: 0.3701(0.2837)
2021-12-29 12:57:34,102 Epoch[076/310], Step[0500/1251], Loss: 4.0324(4.2536), Acc: 0.4434(0.2847)
2021-12-29 12:58:35,862 Epoch[076/310], Step[0550/1251], Loss: 3.7348(4.2533), Acc: 0.3828(0.2840)
2021-12-29 12:59:36,740 Epoch[076/310], Step[0600/1251], Loss: 4.0098(4.2506), Acc: 0.3096(0.2859)
2021-12-29 13:00:35,112 Epoch[076/310], Step[0650/1251], Loss: 3.9757(4.2556), Acc: 0.3467(0.2859)
2021-12-29 13:01:34,494 Epoch[076/310], Step[0700/1251], Loss: 3.9809(4.2548), Acc: 0.4697(0.2840)
2021-12-29 13:02:34,498 Epoch[076/310], Step[0750/1251], Loss: 4.1064(4.2518), Acc: 0.0996(0.2851)
2021-12-29 13:03:35,754 Epoch[076/310], Step[0800/1251], Loss: 3.4772(4.2524), Acc: 0.3779(0.2850)
2021-12-29 13:04:36,550 Epoch[076/310], Step[0850/1251], Loss: 4.3177(4.2544), Acc: 0.2344(0.2855)
2021-12-29 13:05:38,059 Epoch[076/310], Step[0900/1251], Loss: 4.2922(4.2519), Acc: 0.1221(0.2855)
2021-12-29 13:06:37,645 Epoch[076/310], Step[0950/1251], Loss: 3.7976(4.2494), Acc: 0.2646(0.2868)
2021-12-29 13:07:37,941 Epoch[076/310], Step[1000/1251], Loss: 3.7601(4.2523), Acc: 0.2734(0.2856)
2021-12-29 13:08:38,938 Epoch[076/310], Step[1050/1251], Loss: 4.8230(4.2573), Acc: 0.2617(0.2854)
2021-12-29 13:09:39,647 Epoch[076/310], Step[1100/1251], Loss: 4.2483(4.2547), Acc: 0.3652(0.2855)
2021-12-29 13:10:40,767 Epoch[076/310], Step[1150/1251], Loss: 4.1202(4.2531), Acc: 0.1230(0.2850)
2021-12-29 13:11:41,800 Epoch[076/310], Step[1200/1251], Loss: 3.9596(4.2511), Acc: 0.3486(0.2846)
2021-12-29 13:12:43,338 Epoch[076/310], Step[1250/1251], Loss: 4.7045(4.2489), Acc: 0.2910(0.2849)
2021-12-29 13:12:45,007 ----- Validation after Epoch: 76
