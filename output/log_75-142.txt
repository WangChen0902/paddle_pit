2021-12-29 17:43:15,642 
AMP: False
AUG:
  AUTO_AUGMENT: None
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: None
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RE_COUNT: 1
  RE_MODE: pixel
  RE_PROB: 0.25
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.9
  DATASET: imagenet2012
  DATA_PATH: ./Light_ILSVRC2012
  IMAGENET_MEAN: [0.485, 0.456, 0.406]
  IMAGENET_STD: [0.229, 0.224, 0.225]
  IMAGE_SIZE: 224
  NUM_WORKERS: 8
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.0
  DISTILL: False
  DROPOUT: 0.0
  DROP_PATH: 0.1
  NAME: pit_ti
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: ./resume/Best_PiT
  TRANS:
    BASE_DIMS: [32, 32, 32]
    DEPTH: [2, 6, 4]
    HEADS: [2, 4, 8]
    PATCH_SIZE: 16
    STRIDE: 8
  TYPE: PiT
NGPUS: 4
REPORT_FREQ: 50
SAVE: /root/paddlejob/workspace/output/train-20211229-17-42-20
SAVE_FREQ: 10
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 1
  AUTO_AUGMENT: True
  BASE_LR: 0.0005
  COLOR_JITTER: 0.4
  COOLDOWN_EPOCHS: 10
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  DISTILLATION_ALPHA: 0.5
  DISTILLATION_TAU: 1.0
  DISTILLATION_TYPE: none
  END_LR: 5e-06
  GRAD_CLIP: 5.0
  LAST_EPOCH: 74
  LINEAR_SCALED_LR: None
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  MODEL_EMA: True
  MODEL_EMA_DECAY: 0.99996
  NUM_EPOCHS: 300
  OPTIMIZER:
    BETAS: (0.9, 0.999)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RANDOM_ERASE_COUNT: 1
  RANDOM_ERASE_MODE: pixel
  RANDOM_ERASE_PROB: 0.25
  RANDOM_ERASE_SPLIT: False
  SMOOTHING: 0.1
  TEACHER_MODEL: ./regnety_160
  WARMUP_EPOCHS: 20
  WARMUP_START_LR: 5e-07
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 2
2021-12-29 17:43:15,643 ----- world_size = 4, local_rank = 0
2021-12-29 17:43:15,780 ----- Total # of train batch (single gpu): 1251
2021-12-29 17:43:15,780 ----- Total # of val batch (single gpu): 1563
2021-12-29 17:43:16,898 ----- Resume Training: Load model and optmizer from ./resume/Best_PiT
2021-12-29 17:43:17,232 ----- Load model ema from ./resume/Best_PiT-EMA.pdparams
2021-12-29 17:43:17,232 Start training from epoch 75.
2021-12-29 17:43:17,232 Now training epoch 75. LR=0.000854
2021-12-29 17:44:28,289 Epoch[075/310], Step[0000/1251], Loss: 4.3319(4.3319), Acc: 0.1729(0.1729)
2021-12-29 17:45:29,003 Epoch[075/310], Step[0050/1251], Loss: 4.2744(4.2899), Acc: 0.2666(0.2657)
2021-12-29 17:46:28,951 Epoch[075/310], Step[0100/1251], Loss: 4.3630(4.2937), Acc: 0.2334(0.2736)
2021-12-29 17:47:28,475 Epoch[075/310], Step[0150/1251], Loss: 4.4538(4.2766), Acc: 0.2803(0.2733)
2021-12-29 17:48:28,127 Epoch[075/310], Step[0200/1251], Loss: 3.8393(4.2516), Acc: 0.3223(0.2746)
2021-12-29 17:49:29,332 Epoch[075/310], Step[0250/1251], Loss: 3.9418(4.2343), Acc: 0.4541(0.2779)
2021-12-29 17:50:28,873 Epoch[075/310], Step[0300/1251], Loss: 4.1352(4.2174), Acc: 0.3281(0.2830)
2021-12-29 17:51:30,623 Epoch[075/310], Step[0350/1251], Loss: 4.5983(4.2402), Acc: 0.2871(0.2835)
2021-12-29 17:52:31,779 Epoch[075/310], Step[0400/1251], Loss: 4.4727(4.2372), Acc: 0.2939(0.2826)
2021-12-29 17:53:32,158 Epoch[075/310], Step[0450/1251], Loss: 4.4259(4.2448), Acc: 0.1211(0.2809)
2021-12-29 17:54:32,831 Epoch[075/310], Step[0500/1251], Loss: 3.8099(4.2431), Acc: 0.4043(0.2818)
2021-12-29 17:55:31,689 Epoch[075/310], Step[0550/1251], Loss: 4.1865(4.2352), Acc: 0.3564(0.2831)
2021-12-29 17:56:32,340 Epoch[075/310], Step[0600/1251], Loss: 4.0569(4.2375), Acc: 0.3984(0.2839)
2021-12-29 17:57:33,148 Epoch[075/310], Step[0650/1251], Loss: 5.1181(4.2401), Acc: 0.1445(0.2824)
2021-12-29 17:58:33,504 Epoch[075/310], Step[0700/1251], Loss: 4.7825(4.2399), Acc: 0.2930(0.2833)
2021-12-29 17:59:34,389 Epoch[075/310], Step[0750/1251], Loss: 4.1985(4.2409), Acc: 0.4014(0.2845)
2021-12-29 18:00:33,566 Epoch[075/310], Step[0800/1251], Loss: 4.4838(4.2414), Acc: 0.2852(0.2843)
2021-12-29 18:01:34,664 Epoch[075/310], Step[0850/1251], Loss: 4.5720(4.2363), Acc: 0.1445(0.2838)
2021-12-29 18:02:35,978 Epoch[075/310], Step[0900/1251], Loss: 4.7362(4.2364), Acc: 0.2148(0.2843)
2021-12-29 18:03:36,812 Epoch[075/310], Step[0950/1251], Loss: 4.1372(4.2346), Acc: 0.2695(0.2849)
2021-12-29 18:04:37,261 Epoch[075/310], Step[1000/1251], Loss: 4.1887(4.2357), Acc: 0.1250(0.2841)
2021-12-29 18:05:37,457 Epoch[075/310], Step[1050/1251], Loss: 3.9343(4.2363), Acc: 0.2734(0.2824)
2021-12-29 18:06:37,600 Epoch[075/310], Step[1100/1251], Loss: 4.4262(4.2360), Acc: 0.2412(0.2819)
2021-12-29 18:07:37,998 Epoch[075/310], Step[1150/1251], Loss: 4.6439(4.2371), Acc: 0.1592(0.2815)
2021-12-29 18:08:39,902 Epoch[075/310], Step[1200/1251], Loss: 4.6861(4.2381), Acc: 0.3115(0.2813)
2021-12-29 18:09:41,040 Epoch[075/310], Step[1250/1251], Loss: 4.1725(4.2387), Acc: 0.2627(0.2810)
2021-12-29 18:09:42,631 ----- Epoch[075/310], Train Loss: 4.2387, Train Acc: 0.2810, time: 1585.40
2021-12-29 18:09:42,799 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 18:09:42,799 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 18:09:42,843 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 18:09:42,843 Now training epoch 76. LR=0.000851
2021-12-29 18:10:56,624 Epoch[076/310], Step[0000/1251], Loss: 4.0901(4.0901), Acc: 0.1094(0.1094)
2021-12-29 18:11:55,929 Epoch[076/310], Step[0050/1251], Loss: 3.6150(4.3015), Acc: 0.2188(0.2777)
2021-12-29 18:12:54,835 Epoch[076/310], Step[0100/1251], Loss: 4.5009(4.2517), Acc: 0.0508(0.2755)
2021-12-29 18:13:53,445 Epoch[076/310], Step[0150/1251], Loss: 4.0065(4.2658), Acc: 0.3047(0.2781)
2021-12-29 18:14:53,511 Epoch[076/310], Step[0200/1251], Loss: 4.4032(4.2581), Acc: 0.2725(0.2811)
2021-12-29 18:15:53,892 Epoch[076/310], Step[0250/1251], Loss: 4.4840(4.2494), Acc: 0.2354(0.2840)
2021-12-29 18:16:52,267 Epoch[076/310], Step[0300/1251], Loss: 3.9549(4.2526), Acc: 0.1670(0.2861)
2021-12-29 18:17:52,814 Epoch[076/310], Step[0350/1251], Loss: 4.6955(4.2557), Acc: 0.2490(0.2848)
2021-12-29 18:18:53,053 Epoch[076/310], Step[0400/1251], Loss: 3.8629(4.2521), Acc: 0.3320(0.2843)
2021-12-29 18:19:52,902 Epoch[076/310], Step[0450/1251], Loss: 3.7462(4.2514), Acc: 0.4189(0.2833)
2021-12-29 18:20:52,607 Epoch[076/310], Step[0500/1251], Loss: 4.3228(4.2498), Acc: 0.1211(0.2819)
2021-12-29 18:21:51,670 Epoch[076/310], Step[0550/1251], Loss: 3.9281(4.2492), Acc: 0.2402(0.2823)
2021-12-29 18:22:50,545 Epoch[076/310], Step[0600/1251], Loss: 4.4149(4.2443), Acc: 0.1348(0.2845)
2021-12-29 18:23:51,475 Epoch[076/310], Step[0650/1251], Loss: 4.3097(4.2373), Acc: 0.2490(0.2832)
2021-12-29 18:24:49,794 Epoch[076/310], Step[0700/1251], Loss: 3.9972(4.2277), Acc: 0.4590(0.2845)
2021-12-29 18:25:49,669 Epoch[076/310], Step[0750/1251], Loss: 4.4247(4.2261), Acc: 0.2207(0.2854)
2021-12-29 18:26:48,196 Epoch[076/310], Step[0800/1251], Loss: 4.3499(4.2295), Acc: 0.3252(0.2838)
2021-12-29 18:27:48,521 Epoch[076/310], Step[0850/1251], Loss: 4.4229(4.2292), Acc: 0.3105(0.2827)
2021-12-29 18:28:48,202 Epoch[076/310], Step[0900/1251], Loss: 4.0005(4.2327), Acc: 0.2783(0.2826)
2021-12-29 18:29:47,211 Epoch[076/310], Step[0950/1251], Loss: 4.1231(4.2364), Acc: 0.3730(0.2823)
2021-12-29 18:30:47,166 Epoch[076/310], Step[1000/1251], Loss: 4.7418(4.2374), Acc: 0.1719(0.2819)
2021-12-29 18:31:48,448 Epoch[076/310], Step[1050/1251], Loss: 4.2391(4.2356), Acc: 0.1650(0.2808)
2021-12-29 18:32:49,208 Epoch[076/310], Step[1100/1251], Loss: 4.3524(4.2393), Acc: 0.1455(0.2807)
2021-12-29 18:33:49,586 Epoch[076/310], Step[1150/1251], Loss: 4.3181(4.2398), Acc: 0.3008(0.2803)
2021-12-29 18:34:50,568 Epoch[076/310], Step[1200/1251], Loss: 4.3908(4.2402), Acc: 0.3301(0.2813)
2021-12-29 18:35:49,366 Epoch[076/310], Step[1250/1251], Loss: 3.6563(4.2407), Acc: 0.4551(0.2819)
2021-12-29 18:35:51,118 ----- Validation after Epoch: 76
2021-12-29 18:36:42,861 Val Step[0000/1563], Loss: 0.7398 (0.7398), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2021-12-29 18:36:44,291 Val Step[0050/1563], Loss: 2.9741 (1.1000), Acc@1: 0.3438 (0.7812), Acc@5: 0.7500 (0.9350)
2021-12-29 18:36:45,643 Val Step[0100/1563], Loss: 2.3021 (1.4545), Acc@1: 0.4375 (0.6841), Acc@5: 0.8125 (0.8892)
2021-12-29 18:36:47,002 Val Step[0150/1563], Loss: 0.8492 (1.3743), Acc@1: 0.8438 (0.7028), Acc@5: 0.9688 (0.8963)
2021-12-29 18:36:48,350 Val Step[0200/1563], Loss: 1.4576 (1.3991), Acc@1: 0.6562 (0.7009), Acc@5: 0.9062 (0.8904)
2021-12-29 18:36:49,657 Val Step[0250/1563], Loss: 1.0189 (1.3241), Acc@1: 0.8125 (0.7195), Acc@5: 1.0000 (0.9011)
2021-12-29 18:36:51,079 Val Step[0300/1563], Loss: 1.2708 (1.3842), Acc@1: 0.7188 (0.6993), Acc@5: 0.9375 (0.8971)
2021-12-29 18:36:52,433 Val Step[0350/1563], Loss: 1.4348 (1.3804), Acc@1: 0.6562 (0.6951), Acc@5: 0.9062 (0.9004)
2021-12-29 18:36:53,814 Val Step[0400/1563], Loss: 1.5026 (1.3737), Acc@1: 0.6875 (0.6913), Acc@5: 0.9062 (0.9028)
2021-12-29 18:36:55,303 Val Step[0450/1563], Loss: 1.3651 (1.3778), Acc@1: 0.5000 (0.6862), Acc@5: 0.9688 (0.9036)
2021-12-29 18:36:56,646 Val Step[0500/1563], Loss: 0.5257 (1.3729), Acc@1: 0.9375 (0.6882), Acc@5: 1.0000 (0.9046)
2021-12-29 18:36:58,076 Val Step[0550/1563], Loss: 0.9695 (1.3455), Acc@1: 0.7812 (0.6964), Acc@5: 0.9688 (0.9076)
2021-12-29 18:36:59,453 Val Step[0600/1563], Loss: 1.1437 (1.3507), Acc@1: 0.7812 (0.6965), Acc@5: 0.9062 (0.9068)
2021-12-29 18:37:00,844 Val Step[0650/1563], Loss: 1.2056 (1.3709), Acc@1: 0.7812 (0.6939), Acc@5: 0.9688 (0.9035)
2021-12-29 18:37:02,179 Val Step[0700/1563], Loss: 1.8361 (1.4134), Acc@1: 0.6250 (0.6847), Acc@5: 0.8125 (0.8967)
2021-12-29 18:37:03,554 Val Step[0750/1563], Loss: 1.7901 (1.4558), Acc@1: 0.5938 (0.6756), Acc@5: 0.8125 (0.8904)
2021-12-29 18:37:04,916 Val Step[0800/1563], Loss: 1.4120 (1.4981), Acc@1: 0.6250 (0.6667), Acc@5: 0.9688 (0.8839)
2021-12-29 18:37:06,361 Val Step[0850/1563], Loss: 1.6992 (1.5274), Acc@1: 0.6250 (0.6614), Acc@5: 0.8750 (0.8796)
2021-12-29 18:37:07,948 Val Step[0900/1563], Loss: 0.6987 (1.5300), Acc@1: 0.8750 (0.6620), Acc@5: 0.9375 (0.8785)
2021-12-29 18:37:09,456 Val Step[0950/1563], Loss: 1.5377 (1.5580), Acc@1: 0.7188 (0.6565), Acc@5: 0.9062 (0.8741)
2021-12-29 18:37:10,744 Val Step[1000/1563], Loss: 0.7675 (1.5860), Acc@1: 0.9062 (0.6513), Acc@5: 0.9688 (0.8698)
2021-12-29 18:37:12,068 Val Step[1050/1563], Loss: 0.5602 (1.6037), Acc@1: 0.9062 (0.6480), Acc@5: 0.9688 (0.8671)
2021-12-29 18:37:13,404 Val Step[1100/1563], Loss: 1.3771 (1.6223), Acc@1: 0.6875 (0.6443), Acc@5: 0.8125 (0.8634)
2021-12-29 18:37:14,754 Val Step[1150/1563], Loss: 1.7750 (1.6422), Acc@1: 0.6875 (0.6409), Acc@5: 0.8125 (0.8602)
2021-12-29 18:37:16,051 Val Step[1200/1563], Loss: 1.2813 (1.6596), Acc@1: 0.7188 (0.6378), Acc@5: 0.8438 (0.8570)
2021-12-29 18:37:17,358 Val Step[1250/1563], Loss: 0.9102 (1.6744), Acc@1: 0.8750 (0.6358), Acc@5: 0.9375 (0.8540)
2021-12-29 18:37:18,703 Val Step[1300/1563], Loss: 1.4301 (1.6868), Acc@1: 0.7812 (0.6328), Acc@5: 0.8750 (0.8526)
2021-12-29 18:37:20,029 Val Step[1350/1563], Loss: 2.6514 (1.7116), Acc@1: 0.2500 (0.6276), Acc@5: 0.6562 (0.8484)
2021-12-29 18:37:21,328 Val Step[1400/1563], Loss: 1.6676 (1.7225), Acc@1: 0.7188 (0.6249), Acc@5: 0.7812 (0.8466)
2021-12-29 18:37:22,683 Val Step[1450/1563], Loss: 1.7698 (1.7294), Acc@1: 0.5938 (0.6236), Acc@5: 0.9062 (0.8461)
2021-12-29 18:37:24,068 Val Step[1500/1563], Loss: 2.0771 (1.7151), Acc@1: 0.5000 (0.6264), Acc@5: 0.8750 (0.8483)
2021-12-29 18:37:25,460 Val Step[1550/1563], Loss: 1.0431 (1.7140), Acc@1: 0.8750 (0.6267), Acc@5: 0.9062 (0.8485)
2021-12-29 18:37:26,293 ----- Epoch[076/310], Validation Loss: 1.7120, Validation Acc@1: 0.6269, Validation Acc@5: 0.8487, time: 95.17
2021-12-29 18:37:26,293 ----- Epoch[076/310], Train Loss: 4.2407, Train Acc: 0.2819, time: 1568.27, Best Val(epoch76) Acc@1: 0.6269
2021-12-29 18:37:26,464 Max accuracy so far: 0.6269 at epoch_76
2021-12-29 18:37:26,464 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-29 18:37:26,464 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-29 18:37:26,507 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-29 18:37:26,660 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 18:37:26,660 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 18:37:26,793 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 18:37:26,793 Now training epoch 77. LR=0.000847
2021-12-29 18:38:34,702 Epoch[077/310], Step[0000/1251], Loss: 3.9685(3.9685), Acc: 0.3389(0.3389)
2021-12-29 18:39:32,294 Epoch[077/310], Step[0050/1251], Loss: 3.9447(4.1712), Acc: 0.3213(0.3045)
2021-12-29 18:40:30,762 Epoch[077/310], Step[0100/1251], Loss: 3.9079(4.1877), Acc: 0.3604(0.3044)
2021-12-29 18:41:28,538 Epoch[077/310], Step[0150/1251], Loss: 4.3342(4.1991), Acc: 0.3809(0.2931)
2021-12-29 18:42:26,521 Epoch[077/310], Step[0200/1251], Loss: 3.9654(4.2136), Acc: 0.3848(0.2945)
2021-12-29 18:43:27,018 Epoch[077/310], Step[0250/1251], Loss: 3.8234(4.2122), Acc: 0.4707(0.2955)
2021-12-29 18:44:27,858 Epoch[077/310], Step[0300/1251], Loss: 4.1872(4.2088), Acc: 0.3672(0.2942)
2021-12-29 18:45:28,119 Epoch[077/310], Step[0350/1251], Loss: 4.7341(4.2120), Acc: 0.3135(0.2953)
2021-12-29 18:46:27,128 Epoch[077/310], Step[0400/1251], Loss: 4.2897(4.2136), Acc: 0.4092(0.2949)
2021-12-29 18:47:28,216 Epoch[077/310], Step[0450/1251], Loss: 4.3006(4.2118), Acc: 0.3359(0.2925)
2021-12-29 18:48:28,704 Epoch[077/310], Step[0500/1251], Loss: 4.4332(4.2112), Acc: 0.1357(0.2907)
2021-12-29 18:49:29,494 Epoch[077/310], Step[0550/1251], Loss: 3.7198(4.2176), Acc: 0.3438(0.2896)
2021-12-29 18:50:30,455 Epoch[077/310], Step[0600/1251], Loss: 4.1989(4.2187), Acc: 0.4180(0.2910)
2021-12-29 18:51:31,126 Epoch[077/310], Step[0650/1251], Loss: 4.8091(4.2193), Acc: 0.2959(0.2902)
2021-12-29 18:52:31,734 Epoch[077/310], Step[0700/1251], Loss: 4.3164(4.2233), Acc: 0.1045(0.2887)
2021-12-29 18:53:33,733 Epoch[077/310], Step[0750/1251], Loss: 4.1458(4.2280), Acc: 0.1504(0.2869)
2021-12-29 18:54:35,532 Epoch[077/310], Step[0800/1251], Loss: 4.4932(4.2313), Acc: 0.2734(0.2875)
2021-12-29 18:55:36,712 Epoch[077/310], Step[0850/1251], Loss: 4.3406(4.2322), Acc: 0.2324(0.2868)
2021-12-29 18:56:38,456 Epoch[077/310], Step[0900/1251], Loss: 4.2667(4.2330), Acc: 0.2471(0.2845)
2021-12-29 18:57:38,959 Epoch[077/310], Step[0950/1251], Loss: 4.1210(4.2299), Acc: 0.1084(0.2849)
2021-12-29 18:58:40,111 Epoch[077/310], Step[1000/1251], Loss: 4.5180(4.2306), Acc: 0.3008(0.2852)
2021-12-29 18:59:41,719 Epoch[077/310], Step[1050/1251], Loss: 4.2462(4.2291), Acc: 0.1982(0.2847)
2021-12-29 19:00:43,507 Epoch[077/310], Step[1100/1251], Loss: 4.2776(4.2300), Acc: 0.1768(0.2843)
2021-12-29 19:01:43,979 Epoch[077/310], Step[1150/1251], Loss: 4.5172(4.2351), Acc: 0.3057(0.2841)
2021-12-29 19:02:44,343 Epoch[077/310], Step[1200/1251], Loss: 4.0434(4.2318), Acc: 0.3652(0.2841)
2021-12-29 19:03:44,734 Epoch[077/310], Step[1250/1251], Loss: 4.2859(4.2356), Acc: 0.3203(0.2831)
2021-12-29 19:03:46,349 ----- Epoch[077/310], Train Loss: 4.2356, Train Acc: 0.2831, time: 1579.55, Best Val(epoch76) Acc@1: 0.6269
2021-12-29 19:03:46,530 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 19:03:46,531 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 19:03:46,639 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 19:03:46,640 Now training epoch 78. LR=0.000843
2021-12-29 19:05:02,093 Epoch[078/310], Step[0000/1251], Loss: 4.2700(4.2700), Acc: 0.0371(0.0371)
2021-12-29 19:06:00,355 Epoch[078/310], Step[0050/1251], Loss: 4.1965(4.2691), Acc: 0.3691(0.2853)
2021-12-29 19:06:58,793 Epoch[078/310], Step[0100/1251], Loss: 4.0805(4.2674), Acc: 0.2754(0.2876)
2021-12-29 19:07:56,746 Epoch[078/310], Step[0150/1251], Loss: 4.1956(4.2452), Acc: 0.3818(0.2948)
2021-12-29 19:08:55,022 Epoch[078/310], Step[0200/1251], Loss: 4.6705(4.2293), Acc: 0.2119(0.2919)
2021-12-29 19:09:53,312 Epoch[078/310], Step[0250/1251], Loss: 4.2041(4.2316), Acc: 0.2148(0.2852)
2021-12-29 19:10:51,965 Epoch[078/310], Step[0300/1251], Loss: 4.1261(4.2316), Acc: 0.1426(0.2883)
2021-12-29 19:11:50,717 Epoch[078/310], Step[0350/1251], Loss: 4.0715(4.2214), Acc: 0.2236(0.2904)
2021-12-29 19:12:49,914 Epoch[078/310], Step[0400/1251], Loss: 4.2455(4.2187), Acc: 0.1670(0.2910)
2021-12-29 19:13:49,310 Epoch[078/310], Step[0450/1251], Loss: 3.6976(4.2188), Acc: 0.5000(0.2899)
2021-12-29 19:14:48,615 Epoch[078/310], Step[0500/1251], Loss: 4.4405(4.2269), Acc: 0.2529(0.2890)
2021-12-29 19:15:48,583 Epoch[078/310], Step[0550/1251], Loss: 4.3374(4.2252), Acc: 0.2227(0.2895)
2021-12-29 19:16:50,080 Epoch[078/310], Step[0600/1251], Loss: 4.1246(4.2249), Acc: 0.2842(0.2894)
2021-12-29 19:17:50,417 Epoch[078/310], Step[0650/1251], Loss: 4.0582(4.2280), Acc: 0.2764(0.2882)
2021-12-29 19:18:52,980 Epoch[078/310], Step[0700/1251], Loss: 4.2487(4.2322), Acc: 0.3545(0.2869)
2021-12-29 19:19:54,750 Epoch[078/310], Step[0750/1251], Loss: 3.8949(4.2353), Acc: 0.3281(0.2859)
2021-12-29 19:20:56,222 Epoch[078/310], Step[0800/1251], Loss: 4.0183(4.2407), Acc: 0.3330(0.2841)
2021-12-29 19:21:57,060 Epoch[078/310], Step[0850/1251], Loss: 4.3810(4.2465), Acc: 0.3555(0.2825)
2021-12-29 19:22:57,881 Epoch[078/310], Step[0900/1251], Loss: 4.3424(4.2458), Acc: 0.1729(0.2832)
2021-12-29 19:23:58,573 Epoch[078/310], Step[0950/1251], Loss: 4.2343(4.2472), Acc: 0.1738(0.2826)
2021-12-29 19:24:57,570 Epoch[078/310], Step[1000/1251], Loss: 4.1420(4.2447), Acc: 0.3691(0.2825)
2021-12-29 19:25:56,173 Epoch[078/310], Step[1050/1251], Loss: 4.6442(4.2452), Acc: 0.2188(0.2833)
2021-12-29 19:26:55,857 Epoch[078/310], Step[1100/1251], Loss: 4.9188(4.2443), Acc: 0.1787(0.2836)
2021-12-29 19:27:53,961 Epoch[078/310], Step[1150/1251], Loss: 3.3024(4.2399), Acc: 0.5303(0.2845)
2021-12-29 19:28:53,908 Epoch[078/310], Step[1200/1251], Loss: 4.3429(4.2416), Acc: 0.3223(0.2847)
2021-12-29 19:29:55,587 Epoch[078/310], Step[1250/1251], Loss: 4.7249(4.2426), Acc: 0.3242(0.2842)
2021-12-29 19:29:57,460 ----- Validation after Epoch: 78
2021-12-29 19:30:53,007 Val Step[0000/1563], Loss: 0.9865 (0.9865), Acc@1: 0.8125 (0.8125), Acc@5: 0.9375 (0.9375)
2021-12-29 19:30:54,468 Val Step[0050/1563], Loss: 2.4552 (1.0704), Acc@1: 0.4375 (0.7721), Acc@5: 0.8438 (0.9344)
2021-12-29 19:30:55,981 Val Step[0100/1563], Loss: 2.0676 (1.4372), Acc@1: 0.5312 (0.6835), Acc@5: 0.8125 (0.8874)
2021-12-29 19:30:57,397 Val Step[0150/1563], Loss: 0.7697 (1.3474), Acc@1: 0.8750 (0.7055), Acc@5: 0.9062 (0.8967)
2021-12-29 19:30:58,813 Val Step[0200/1563], Loss: 1.2110 (1.3697), Acc@1: 0.7188 (0.7060), Acc@5: 0.9062 (0.8941)
2021-12-29 19:31:00,279 Val Step[0250/1563], Loss: 0.9800 (1.2951), Acc@1: 0.8438 (0.7246), Acc@5: 0.9688 (0.9040)
2021-12-29 19:31:01,707 Val Step[0300/1563], Loss: 1.5585 (1.3648), Acc@1: 0.5625 (0.6992), Acc@5: 0.9062 (0.9005)
2021-12-29 19:31:03,143 Val Step[0350/1563], Loss: 1.5566 (1.3701), Acc@1: 0.6875 (0.6956), Acc@5: 0.8750 (0.9027)
2021-12-29 19:31:04,630 Val Step[0400/1563], Loss: 1.5404 (1.3705), Acc@1: 0.6562 (0.6898), Acc@5: 0.9062 (0.9036)
2021-12-29 19:31:06,088 Val Step[0450/1563], Loss: 0.8625 (1.3740), Acc@1: 0.8438 (0.6863), Acc@5: 1.0000 (0.9053)
2021-12-29 19:31:07,570 Val Step[0500/1563], Loss: 0.5665 (1.3648), Acc@1: 0.9375 (0.6890), Acc@5: 1.0000 (0.9066)
2021-12-29 19:31:09,014 Val Step[0550/1563], Loss: 0.9260 (1.3391), Acc@1: 0.7188 (0.6957), Acc@5: 0.9688 (0.9088)
2021-12-29 19:31:10,331 Val Step[0600/1563], Loss: 0.8707 (1.3452), Acc@1: 0.7812 (0.6957), Acc@5: 0.9688 (0.9080)
2021-12-29 19:31:11,663 Val Step[0650/1563], Loss: 0.9665 (1.3667), Acc@1: 0.8750 (0.6920), Acc@5: 0.9375 (0.9045)
2021-12-29 19:31:13,024 Val Step[0700/1563], Loss: 1.4343 (1.4072), Acc@1: 0.7188 (0.6825), Acc@5: 0.8750 (0.8989)
2021-12-29 19:31:14,507 Val Step[0750/1563], Loss: 1.9859 (1.4480), Acc@1: 0.5938 (0.6749), Acc@5: 0.7812 (0.8923)
2021-12-29 19:31:15,804 Val Step[0800/1563], Loss: 1.6419 (1.4921), Acc@1: 0.7188 (0.6658), Acc@5: 0.8750 (0.8860)
2021-12-29 19:31:17,099 Val Step[0850/1563], Loss: 2.2666 (1.5269), Acc@1: 0.4688 (0.6591), Acc@5: 0.7812 (0.8803)
2021-12-29 19:31:18,456 Val Step[0900/1563], Loss: 0.6019 (1.5286), Acc@1: 0.8750 (0.6612), Acc@5: 0.9062 (0.8792)
2021-12-29 19:31:19,879 Val Step[0950/1563], Loss: 1.8907 (1.5552), Acc@1: 0.6250 (0.6562), Acc@5: 0.8125 (0.8753)
2021-12-29 19:31:21,239 Val Step[1000/1563], Loss: 0.7093 (1.5813), Acc@1: 0.9375 (0.6510), Acc@5: 0.9688 (0.8718)
2021-12-29 19:31:22,678 Val Step[1050/1563], Loss: 0.5706 (1.5985), Acc@1: 0.9375 (0.6477), Acc@5: 0.9688 (0.8694)
2021-12-29 19:31:24,054 Val Step[1100/1563], Loss: 1.7144 (1.6179), Acc@1: 0.6875 (0.6440), Acc@5: 0.7500 (0.8664)
2021-12-29 19:31:25,394 Val Step[1150/1563], Loss: 1.9081 (1.6380), Acc@1: 0.7188 (0.6403), Acc@5: 0.7500 (0.8635)
2021-12-29 19:31:26,771 Val Step[1200/1563], Loss: 1.7304 (1.6590), Acc@1: 0.6875 (0.6365), Acc@5: 0.8438 (0.8598)
2021-12-29 19:31:28,170 Val Step[1250/1563], Loss: 1.0742 (1.6776), Acc@1: 0.8438 (0.6335), Acc@5: 0.9062 (0.8564)
2021-12-29 19:31:29,532 Val Step[1300/1563], Loss: 1.3762 (1.6918), Acc@1: 0.7188 (0.6307), Acc@5: 0.8750 (0.8544)
2021-12-29 19:31:30,912 Val Step[1350/1563], Loss: 2.8223 (1.7132), Acc@1: 0.2812 (0.6257), Acc@5: 0.6250 (0.8509)
2021-12-29 19:31:32,341 Val Step[1400/1563], Loss: 1.4182 (1.7233), Acc@1: 0.7500 (0.6237), Acc@5: 0.8750 (0.8491)
2021-12-29 19:31:33,718 Val Step[1450/1563], Loss: 2.1770 (1.7287), Acc@1: 0.4062 (0.6228), Acc@5: 0.8125 (0.8484)
2021-12-29 19:31:35,065 Val Step[1500/1563], Loss: 2.1883 (1.7138), Acc@1: 0.3438 (0.6257), Acc@5: 0.9062 (0.8507)
2021-12-29 19:31:36,408 Val Step[1550/1563], Loss: 1.0157 (1.7096), Acc@1: 0.8750 (0.6266), Acc@5: 0.9062 (0.8510)
2021-12-29 19:31:37,218 ----- Epoch[078/310], Validation Loss: 1.7074, Validation Acc@1: 0.6270, Validation Acc@5: 0.8512, time: 99.76
2021-12-29 19:31:37,218 ----- Epoch[078/310], Train Loss: 4.2426, Train Acc: 0.2842, time: 1570.82, Best Val(epoch78) Acc@1: 0.6270
2021-12-29 19:31:37,484 Max accuracy so far: 0.6270 at epoch_78
2021-12-29 19:31:37,484 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-29 19:31:37,485 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-29 19:31:37,570 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-29 19:31:38,200 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 19:31:38,200 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 19:31:38,252 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 19:31:38,252 Now training epoch 79. LR=0.000839
2021-12-29 19:32:48,793 Epoch[079/310], Step[0000/1251], Loss: 4.0471(4.0471), Acc: 0.3193(0.3193)
2021-12-29 19:33:46,467 Epoch[079/310], Step[0050/1251], Loss: 4.5375(4.2256), Acc: 0.2188(0.2716)
2021-12-29 19:34:42,175 Epoch[079/310], Step[0100/1251], Loss: 4.5878(4.2236), Acc: 0.2754(0.2967)
2021-12-29 19:35:41,163 Epoch[079/310], Step[0150/1251], Loss: 4.0179(4.1930), Acc: 0.4434(0.3043)
2021-12-29 19:36:40,048 Epoch[079/310], Step[0200/1251], Loss: 4.3243(4.2030), Acc: 0.2871(0.2946)
2021-12-29 19:37:39,053 Epoch[079/310], Step[0250/1251], Loss: 4.6131(4.2181), Acc: 0.3428(0.2938)
2021-12-29 19:38:39,574 Epoch[079/310], Step[0300/1251], Loss: 4.3614(4.2165), Acc: 0.2441(0.2923)
2021-12-29 19:39:40,291 Epoch[079/310], Step[0350/1251], Loss: 4.3135(4.2165), Acc: 0.2939(0.2913)
2021-12-29 19:40:40,875 Epoch[079/310], Step[0400/1251], Loss: 4.4666(4.2214), Acc: 0.2793(0.2917)
2021-12-29 19:41:40,336 Epoch[079/310], Step[0450/1251], Loss: 3.7525(4.2271), Acc: 0.4551(0.2907)
2021-12-29 19:42:38,850 Epoch[079/310], Step[0500/1251], Loss: 4.4406(4.2219), Acc: 0.3799(0.2905)
2021-12-29 19:43:37,808 Epoch[079/310], Step[0550/1251], Loss: 4.0629(4.2186), Acc: 0.1543(0.2909)
2021-12-29 19:44:39,619 Epoch[079/310], Step[0600/1251], Loss: 3.7333(4.2219), Acc: 0.1982(0.2891)
2021-12-29 19:45:38,475 Epoch[079/310], Step[0650/1251], Loss: 4.4818(4.2238), Acc: 0.3750(0.2869)
2021-12-29 19:46:38,810 Epoch[079/310], Step[0700/1251], Loss: 3.8171(4.2267), Acc: 0.4521(0.2860)
2021-12-29 19:47:38,910 Epoch[079/310], Step[0750/1251], Loss: 4.4893(4.2227), Acc: 0.2529(0.2867)
2021-12-29 19:48:38,323 Epoch[079/310], Step[0800/1251], Loss: 4.1265(4.2189), Acc: 0.4043(0.2869)
2021-12-29 19:49:38,900 Epoch[079/310], Step[0850/1251], Loss: 4.6338(4.2214), Acc: 0.2402(0.2863)
2021-12-29 19:50:37,844 Epoch[079/310], Step[0900/1251], Loss: 3.9423(4.2244), Acc: 0.1914(0.2860)
2021-12-29 19:51:37,737 Epoch[079/310], Step[0950/1251], Loss: 3.3781(4.2258), Acc: 0.3926(0.2869)
2021-12-29 19:52:36,783 Epoch[079/310], Step[1000/1251], Loss: 4.7182(4.2281), Acc: 0.2236(0.2863)
2021-12-29 19:53:36,404 Epoch[079/310], Step[1050/1251], Loss: 4.3527(4.2253), Acc: 0.2432(0.2866)
2021-12-29 19:54:37,051 Epoch[079/310], Step[1100/1251], Loss: 3.9609(4.2270), Acc: 0.1641(0.2867)
2021-12-29 19:55:37,037 Epoch[079/310], Step[1150/1251], Loss: 4.5273(4.2264), Acc: 0.3105(0.2868)
2021-12-29 19:56:37,343 Epoch[079/310], Step[1200/1251], Loss: 4.0095(4.2265), Acc: 0.4355(0.2867)
2021-12-29 19:57:37,973 Epoch[079/310], Step[1250/1251], Loss: 4.6365(4.2281), Acc: 0.3008(0.2856)
2021-12-29 19:57:39,602 ----- Epoch[079/310], Train Loss: 4.2281, Train Acc: 0.2856, time: 1561.35, Best Val(epoch78) Acc@1: 0.6270
2021-12-29 19:57:39,783 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 19:57:39,784 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 19:57:39,871 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 19:57:39,872 Now training epoch 80. LR=0.000835
2021-12-29 19:58:49,831 Epoch[080/310], Step[0000/1251], Loss: 4.3832(4.3832), Acc: 0.1934(0.1934)
2021-12-29 19:59:48,836 Epoch[080/310], Step[0050/1251], Loss: 4.6382(4.3069), Acc: 0.2578(0.2826)
2021-12-29 20:00:47,792 Epoch[080/310], Step[0100/1251], Loss: 3.4259(4.2724), Acc: 0.3633(0.2880)
2021-12-29 20:01:47,335 Epoch[080/310], Step[0150/1251], Loss: 4.6465(4.2978), Acc: 0.2109(0.2862)
2021-12-29 20:02:46,698 Epoch[080/310], Step[0200/1251], Loss: 4.0506(4.2808), Acc: 0.3975(0.2816)
2021-12-29 20:03:47,037 Epoch[080/310], Step[0250/1251], Loss: 4.4123(4.2651), Acc: 0.2109(0.2807)
2021-12-29 20:04:47,513 Epoch[080/310], Step[0300/1251], Loss: 4.1585(4.2693), Acc: 0.2852(0.2804)
2021-12-29 20:05:49,379 Epoch[080/310], Step[0350/1251], Loss: 3.9202(4.2617), Acc: 0.1172(0.2817)
2021-12-29 20:06:48,386 Epoch[080/310], Step[0400/1251], Loss: 4.6303(4.2594), Acc: 0.3047(0.2838)
2021-12-29 20:07:48,466 Epoch[080/310], Step[0450/1251], Loss: 3.7712(4.2451), Acc: 0.1904(0.2868)
2021-12-29 20:08:49,825 Epoch[080/310], Step[0500/1251], Loss: 4.6969(4.2441), Acc: 0.1582(0.2860)
2021-12-29 20:09:50,596 Epoch[080/310], Step[0550/1251], Loss: 4.3374(4.2464), Acc: 0.3203(0.2849)
2021-12-29 20:10:50,536 Epoch[080/310], Step[0600/1251], Loss: 4.2316(4.2366), Acc: 0.2705(0.2845)
2021-12-29 20:11:50,193 Epoch[080/310], Step[0650/1251], Loss: 3.7246(4.2371), Acc: 0.2871(0.2840)
2021-12-29 20:12:50,584 Epoch[080/310], Step[0700/1251], Loss: 3.9808(4.2370), Acc: 0.2432(0.2839)
2021-12-29 20:13:49,962 Epoch[080/310], Step[0750/1251], Loss: 4.2670(4.2327), Acc: 0.3887(0.2848)
2021-12-29 20:14:46,215 Epoch[080/310], Step[0800/1251], Loss: 4.6738(4.2329), Acc: 0.2822(0.2863)
2021-12-29 20:15:43,649 Epoch[080/310], Step[0850/1251], Loss: 4.3338(4.2349), Acc: 0.2129(0.2864)
2021-12-29 20:16:43,854 Epoch[080/310], Step[0900/1251], Loss: 3.9994(4.2346), Acc: 0.2549(0.2862)
2021-12-29 20:17:44,831 Epoch[080/310], Step[0950/1251], Loss: 4.0036(4.2330), Acc: 0.3623(0.2856)
2021-12-29 20:18:45,467 Epoch[080/310], Step[1000/1251], Loss: 4.2415(4.2323), Acc: 0.3730(0.2852)
2021-12-29 20:19:46,415 Epoch[080/310], Step[1050/1251], Loss: 4.0932(4.2322), Acc: 0.4678(0.2838)
2021-12-29 20:20:47,418 Epoch[080/310], Step[1100/1251], Loss: 4.3527(4.2310), Acc: 0.2910(0.2839)
2021-12-29 20:21:47,484 Epoch[080/310], Step[1150/1251], Loss: 4.2352(4.2282), Acc: 0.4502(0.2845)
2021-12-29 20:22:48,563 Epoch[080/310], Step[1200/1251], Loss: 4.9400(4.2291), Acc: 0.2041(0.2842)
2021-12-29 20:23:49,632 Epoch[080/310], Step[1250/1251], Loss: 4.2390(4.2313), Acc: 0.1963(0.2852)
2021-12-29 20:23:51,326 ----- Validation after Epoch: 80
2021-12-29 20:24:44,016 Val Step[0000/1563], Loss: 0.8251 (0.8251), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2021-12-29 20:24:45,408 Val Step[0050/1563], Loss: 2.9397 (1.0613), Acc@1: 0.4062 (0.7745), Acc@5: 0.6562 (0.9289)
2021-12-29 20:24:46,873 Val Step[0100/1563], Loss: 1.8676 (1.4163), Acc@1: 0.4688 (0.6816), Acc@5: 0.8750 (0.8830)
2021-12-29 20:24:48,370 Val Step[0150/1563], Loss: 0.9250 (1.3201), Acc@1: 0.8750 (0.7057), Acc@5: 0.9062 (0.8938)
2021-12-29 20:24:49,809 Val Step[0200/1563], Loss: 1.4621 (1.3373), Acc@1: 0.6875 (0.7076), Acc@5: 0.9062 (0.8902)
2021-12-29 20:24:51,220 Val Step[0250/1563], Loss: 1.1314 (1.2724), Acc@1: 0.7500 (0.7234), Acc@5: 1.0000 (0.8999)
2021-12-29 20:24:52,659 Val Step[0300/1563], Loss: 1.4175 (1.3369), Acc@1: 0.6250 (0.7046), Acc@5: 0.9375 (0.8942)
2021-12-29 20:24:53,986 Val Step[0350/1563], Loss: 1.4952 (1.3532), Acc@1: 0.7500 (0.6977), Acc@5: 0.9062 (0.8979)
2021-12-29 20:24:55,325 Val Step[0400/1563], Loss: 2.0393 (1.3553), Acc@1: 0.5625 (0.6915), Acc@5: 0.8750 (0.9004)
2021-12-29 20:24:56,686 Val Step[0450/1563], Loss: 1.0985 (1.3535), Acc@1: 0.6875 (0.6890), Acc@5: 0.9688 (0.9024)
2021-12-29 20:24:58,082 Val Step[0500/1563], Loss: 0.4781 (1.3414), Acc@1: 0.9375 (0.6921), Acc@5: 1.0000 (0.9041)
2021-12-29 20:24:59,592 Val Step[0550/1563], Loss: 0.9532 (1.3182), Acc@1: 0.8125 (0.6996), Acc@5: 0.9375 (0.9063)
2021-12-29 20:25:00,967 Val Step[0600/1563], Loss: 0.9128 (1.3220), Acc@1: 0.8125 (0.6999), Acc@5: 0.9375 (0.9057)
2021-12-29 20:25:02,295 Val Step[0650/1563], Loss: 0.8989 (1.3464), Acc@1: 0.9062 (0.6947), Acc@5: 0.9688 (0.9021)
2021-12-29 20:25:03,737 Val Step[0700/1563], Loss: 1.9836 (1.3867), Acc@1: 0.5938 (0.6860), Acc@5: 0.8750 (0.8966)
2021-12-29 20:25:05,178 Val Step[0750/1563], Loss: 1.8271 (1.4276), Acc@1: 0.5938 (0.6786), Acc@5: 0.8125 (0.8900)
2021-12-29 20:25:06,464 Val Step[0800/1563], Loss: 1.8881 (1.4756), Acc@1: 0.6250 (0.6688), Acc@5: 0.7500 (0.8835)
2021-12-29 20:25:07,912 Val Step[0850/1563], Loss: 1.8457 (1.5031), Acc@1: 0.6250 (0.6633), Acc@5: 0.8438 (0.8801)
2021-12-29 20:25:09,262 Val Step[0900/1563], Loss: 0.5117 (1.5028), Acc@1: 0.9062 (0.6652), Acc@5: 1.0000 (0.8793)
2021-12-29 20:25:10,691 Val Step[0950/1563], Loss: 1.7613 (1.5288), Acc@1: 0.5625 (0.6600), Acc@5: 0.7500 (0.8751)
2021-12-29 20:25:12,009 Val Step[1000/1563], Loss: 1.1574 (1.5593), Acc@1: 0.7500 (0.6530), Acc@5: 0.9688 (0.8708)
2021-12-29 20:25:13,376 Val Step[1050/1563], Loss: 0.4131 (1.5762), Acc@1: 0.9688 (0.6498), Acc@5: 1.0000 (0.8687)
2021-12-29 20:25:14,710 Val Step[1100/1563], Loss: 1.3981 (1.5944), Acc@1: 0.7188 (0.6459), Acc@5: 0.8750 (0.8654)
2021-12-29 20:25:16,047 Val Step[1150/1563], Loss: 1.4923 (1.6140), Acc@1: 0.7188 (0.6426), Acc@5: 0.8125 (0.8630)
2021-12-29 20:25:17,453 Val Step[1200/1563], Loss: 1.8468 (1.6360), Acc@1: 0.6562 (0.6385), Acc@5: 0.8438 (0.8593)
2021-12-29 20:25:18,773 Val Step[1250/1563], Loss: 0.7457 (1.6526), Acc@1: 0.8750 (0.6359), Acc@5: 0.9062 (0.8564)
2021-12-29 20:25:20,241 Val Step[1300/1563], Loss: 1.5786 (1.6679), Acc@1: 0.7500 (0.6329), Acc@5: 0.8438 (0.8545)
2021-12-29 20:25:21,569 Val Step[1350/1563], Loss: 2.5648 (1.6895), Acc@1: 0.3750 (0.6280), Acc@5: 0.7188 (0.8512)
2021-12-29 20:25:23,019 Val Step[1400/1563], Loss: 1.3833 (1.6976), Acc@1: 0.7188 (0.6262), Acc@5: 0.8750 (0.8500)
2021-12-29 20:25:24,453 Val Step[1450/1563], Loss: 1.9431 (1.7058), Acc@1: 0.5625 (0.6243), Acc@5: 0.9062 (0.8493)
2021-12-29 20:25:25,871 Val Step[1500/1563], Loss: 2.0270 (1.6936), Acc@1: 0.5000 (0.6268), Acc@5: 0.9062 (0.8509)
2021-12-29 20:25:27,308 Val Step[1550/1563], Loss: 1.1708 (1.6932), Acc@1: 0.8750 (0.6266), Acc@5: 0.9062 (0.8511)
2021-12-29 20:25:28,246 ----- Epoch[080/310], Validation Loss: 1.6904, Validation Acc@1: 0.6271, Validation Acc@5: 0.8513, time: 96.92
2021-12-29 20:25:28,247 ----- Epoch[080/310], Train Loss: 4.2313, Train Acc: 0.2852, time: 1571.45, Best Val(epoch80) Acc@1: 0.6271
2021-12-29 20:25:28,434 Max accuracy so far: 0.6271 at epoch_80
2021-12-29 20:25:28,434 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-29 20:25:28,434 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-29 20:25:28,543 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-29 20:25:28,676 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-80-Loss-4.2179115380791075.pdparams
2021-12-29 20:25:28,677 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-80-Loss-4.2179115380791075.pdopt
2021-12-29 20:25:28,717 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-80-Loss-4.2179115380791075-EMA.pdparams
2021-12-29 20:25:28,717 Now training epoch 81. LR=0.000832
2021-12-29 20:26:44,223 Epoch[081/310], Step[0000/1251], Loss: 3.8186(3.8186), Acc: 0.3145(0.3145)
2021-12-29 20:27:41,960 Epoch[081/310], Step[0050/1251], Loss: 4.2906(4.2220), Acc: 0.2227(0.2969)
2021-12-29 20:28:39,098 Epoch[081/310], Step[0100/1251], Loss: 4.2654(4.2056), Acc: 0.3604(0.2890)
2021-12-29 20:29:36,896 Epoch[081/310], Step[0150/1251], Loss: 4.1157(4.2075), Acc: 0.4160(0.2827)
2021-12-29 20:30:36,017 Epoch[081/310], Step[0200/1251], Loss: 4.1217(4.2135), Acc: 0.2930(0.2872)
2021-12-29 20:31:34,740 Epoch[081/310], Step[0250/1251], Loss: 4.3861(4.2069), Acc: 0.2500(0.2896)
2021-12-29 20:32:35,072 Epoch[081/310], Step[0300/1251], Loss: 4.1666(4.2036), Acc: 0.2959(0.2872)
2021-12-29 20:33:34,722 Epoch[081/310], Step[0350/1251], Loss: 4.4903(4.2026), Acc: 0.2080(0.2892)
2021-12-29 20:34:34,017 Epoch[081/310], Step[0400/1251], Loss: 4.0335(4.2170), Acc: 0.2012(0.2872)
2021-12-29 20:35:35,472 Epoch[081/310], Step[0450/1251], Loss: 4.0496(4.2154), Acc: 0.2100(0.2873)
2021-12-29 20:36:35,626 Epoch[081/310], Step[0500/1251], Loss: 4.4360(4.2145), Acc: 0.2979(0.2882)
2021-12-29 20:37:35,367 Epoch[081/310], Step[0550/1251], Loss: 3.9742(4.2112), Acc: 0.3018(0.2878)
2021-12-29 20:38:34,094 Epoch[081/310], Step[0600/1251], Loss: 4.7800(4.2138), Acc: 0.1875(0.2884)
2021-12-29 20:39:34,438 Epoch[081/310], Step[0650/1251], Loss: 4.6795(4.2110), Acc: 0.2422(0.2895)
2021-12-29 20:40:34,042 Epoch[081/310], Step[0700/1251], Loss: 4.6226(4.2103), Acc: 0.1328(0.2896)
2021-12-29 20:41:33,881 Epoch[081/310], Step[0750/1251], Loss: 4.9003(4.2115), Acc: 0.2695(0.2896)
2021-12-29 20:42:31,904 Epoch[081/310], Step[0800/1251], Loss: 3.9215(4.2135), Acc: 0.2031(0.2894)
2021-12-29 20:43:32,691 Epoch[081/310], Step[0850/1251], Loss: 4.6151(4.2151), Acc: 0.2939(0.2896)
2021-12-29 20:44:32,560 Epoch[081/310], Step[0900/1251], Loss: 4.1042(4.2150), Acc: 0.2305(0.2892)
2021-12-29 20:45:32,852 Epoch[081/310], Step[0950/1251], Loss: 4.2172(4.2141), Acc: 0.4033(0.2887)
2021-12-29 20:46:31,649 Epoch[081/310], Step[1000/1251], Loss: 4.0193(4.2152), Acc: 0.2520(0.2894)
2021-12-29 20:47:32,640 Epoch[081/310], Step[1050/1251], Loss: 4.1828(4.2164), Acc: 0.3076(0.2892)
2021-12-29 20:48:31,751 Epoch[081/310], Step[1100/1251], Loss: 4.7217(4.2181), Acc: 0.1348(0.2891)
2021-12-29 20:49:31,916 Epoch[081/310], Step[1150/1251], Loss: 4.3462(4.2220), Acc: 0.3467(0.2885)
2021-12-29 20:50:31,308 Epoch[081/310], Step[1200/1251], Loss: 4.2397(4.2202), Acc: 0.2324(0.2886)
2021-12-29 20:51:31,571 Epoch[081/310], Step[1250/1251], Loss: 4.5356(4.2210), Acc: 0.2783(0.2890)
2021-12-29 20:51:33,955 ----- Epoch[081/310], Train Loss: 4.2210, Train Acc: 0.2890, time: 1565.23, Best Val(epoch80) Acc@1: 0.6271
2021-12-29 20:51:34,249 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 20:51:34,250 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 20:51:34,344 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 20:51:34,344 Now training epoch 82. LR=0.000828
2021-12-29 20:52:42,766 Epoch[082/310], Step[0000/1251], Loss: 4.2315(4.2315), Acc: 0.2900(0.2900)
2021-12-29 20:53:40,499 Epoch[082/310], Step[0050/1251], Loss: 4.0195(4.1666), Acc: 0.4561(0.3279)
2021-12-29 20:54:40,346 Epoch[082/310], Step[0100/1251], Loss: 3.6153(4.2109), Acc: 0.2705(0.2999)
2021-12-29 20:55:39,756 Epoch[082/310], Step[0150/1251], Loss: 4.6173(4.2002), Acc: 0.3467(0.2936)
2021-12-29 20:56:38,928 Epoch[082/310], Step[0200/1251], Loss: 4.5897(4.2045), Acc: 0.1895(0.2908)
2021-12-29 20:57:39,467 Epoch[082/310], Step[0250/1251], Loss: 3.6858(4.1935), Acc: 0.2236(0.2897)
2021-12-29 20:58:38,942 Epoch[082/310], Step[0300/1251], Loss: 4.3063(4.1817), Acc: 0.4014(0.2932)
2021-12-29 20:59:38,457 Epoch[082/310], Step[0350/1251], Loss: 4.2487(4.1888), Acc: 0.3818(0.2934)
2021-12-29 21:00:37,980 Epoch[082/310], Step[0400/1251], Loss: 4.6146(4.1921), Acc: 0.2441(0.2939)
2021-12-29 21:01:37,875 Epoch[082/310], Step[0450/1251], Loss: 4.2759(4.1921), Acc: 0.4004(0.2970)
2021-12-29 21:02:37,897 Epoch[082/310], Step[0500/1251], Loss: 4.1798(4.1893), Acc: 0.4072(0.2962)
2021-12-29 21:03:38,058 Epoch[082/310], Step[0550/1251], Loss: 4.2414(4.1944), Acc: 0.2178(0.2924)
2021-12-29 21:04:36,935 Epoch[082/310], Step[0600/1251], Loss: 4.4164(4.1945), Acc: 0.3857(0.2939)
2021-12-29 21:05:37,264 Epoch[082/310], Step[0650/1251], Loss: 3.9316(4.1997), Acc: 0.2695(0.2951)
2021-12-29 21:06:38,042 Epoch[082/310], Step[0700/1251], Loss: 4.5618(4.2049), Acc: 0.3486(0.2947)
2021-12-29 21:07:38,252 Epoch[082/310], Step[0750/1251], Loss: 3.7773(4.2049), Acc: 0.2861(0.2939)
2021-12-29 21:08:38,578 Epoch[082/310], Step[0800/1251], Loss: 4.4612(4.2020), Acc: 0.2686(0.2949)
2021-12-29 21:09:38,751 Epoch[082/310], Step[0850/1251], Loss: 4.1503(4.1989), Acc: 0.3291(0.2951)
2021-12-29 21:10:39,735 Epoch[082/310], Step[0900/1251], Loss: 4.5984(4.2011), Acc: 0.2041(0.2943)
2021-12-29 21:11:40,856 Epoch[082/310], Step[0950/1251], Loss: 4.5618(4.2040), Acc: 0.3369(0.2937)
2021-12-29 21:12:40,417 Epoch[082/310], Step[1000/1251], Loss: 4.5745(4.2065), Acc: 0.2305(0.2925)
2021-12-29 21:13:41,102 Epoch[082/310], Step[1050/1251], Loss: 3.4632(4.2102), Acc: 0.3906(0.2914)
2021-12-29 21:14:41,396 Epoch[082/310], Step[1100/1251], Loss: 4.3535(4.2080), Acc: 0.3262(0.2923)
2021-12-29 21:15:42,103 Epoch[082/310], Step[1150/1251], Loss: 4.0469(4.2074), Acc: 0.3926(0.2921)
2021-12-29 21:16:42,224 Epoch[082/310], Step[1200/1251], Loss: 4.2356(4.2076), Acc: 0.1377(0.2912)
2021-12-29 21:17:42,573 Epoch[082/310], Step[1250/1251], Loss: 4.3887(4.2031), Acc: 0.3779(0.2920)
2021-12-29 21:17:44,810 ----- Validation after Epoch: 82
2021-12-29 21:18:35,228 Val Step[0000/1563], Loss: 0.7266 (0.7266), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-29 21:18:36,573 Val Step[0050/1563], Loss: 2.2990 (1.0183), Acc@1: 0.4688 (0.8002), Acc@5: 0.8750 (0.9424)
2021-12-29 21:18:38,012 Val Step[0100/1563], Loss: 2.1932 (1.3401), Acc@1: 0.3750 (0.7039), Acc@5: 0.8125 (0.9044)
2021-12-29 21:18:39,447 Val Step[0150/1563], Loss: 0.7255 (1.2740), Acc@1: 0.8750 (0.7237), Acc@5: 1.0000 (0.9087)
2021-12-29 21:18:40,872 Val Step[0200/1563], Loss: 1.6650 (1.3270), Acc@1: 0.5938 (0.7156), Acc@5: 0.9062 (0.9002)
2021-12-29 21:18:42,259 Val Step[0250/1563], Loss: 1.0533 (1.2696), Acc@1: 0.8438 (0.7286), Acc@5: 1.0000 (0.9075)
2021-12-29 21:18:43,655 Val Step[0300/1563], Loss: 1.4137 (1.3334), Acc@1: 0.6875 (0.7081), Acc@5: 0.9062 (0.9021)
2021-12-29 21:18:45,087 Val Step[0350/1563], Loss: 1.3805 (1.3331), Acc@1: 0.6562 (0.7045), Acc@5: 0.8750 (0.9057)
2021-12-29 21:18:46,509 Val Step[0400/1563], Loss: 1.6021 (1.3350), Acc@1: 0.6250 (0.6990), Acc@5: 0.9688 (0.9074)
2021-12-29 21:18:47,946 Val Step[0450/1563], Loss: 1.8391 (1.3470), Acc@1: 0.1250 (0.6935), Acc@5: 1.0000 (0.9076)
2021-12-29 21:18:49,357 Val Step[0500/1563], Loss: 0.5648 (1.3368), Acc@1: 0.8750 (0.6954), Acc@5: 1.0000 (0.9084)
2021-12-29 21:18:50,761 Val Step[0550/1563], Loss: 1.1778 (1.3188), Acc@1: 0.7188 (0.7012), Acc@5: 0.9062 (0.9102)
2021-12-29 21:18:52,201 Val Step[0600/1563], Loss: 0.8425 (1.3264), Acc@1: 0.8438 (0.7004), Acc@5: 0.9375 (0.9091)
2021-12-29 21:18:53,666 Val Step[0650/1563], Loss: 0.8815 (1.3456), Acc@1: 0.8125 (0.6972), Acc@5: 0.9688 (0.9057)
2021-12-29 21:18:54,968 Val Step[0700/1563], Loss: 2.1297 (1.3857), Acc@1: 0.5625 (0.6887), Acc@5: 0.7812 (0.9001)
2021-12-29 21:18:56,374 Val Step[0750/1563], Loss: 2.0921 (1.4261), Acc@1: 0.5312 (0.6803), Acc@5: 0.7500 (0.8941)
2021-12-29 21:18:57,725 Val Step[0800/1563], Loss: 1.6828 (1.4700), Acc@1: 0.5938 (0.6710), Acc@5: 0.8750 (0.8875)
2021-12-29 21:18:59,069 Val Step[0850/1563], Loss: 2.0929 (1.5038), Acc@1: 0.5000 (0.6634), Acc@5: 0.7188 (0.8829)
2021-12-29 21:19:00,423 Val Step[0900/1563], Loss: 0.5393 (1.5043), Acc@1: 0.8750 (0.6653), Acc@5: 0.9688 (0.8818)
2021-12-29 21:19:01,913 Val Step[0950/1563], Loss: 1.6392 (1.5292), Acc@1: 0.6875 (0.6601), Acc@5: 0.7812 (0.8779)
2021-12-29 21:19:03,240 Val Step[1000/1563], Loss: 0.7992 (1.5556), Acc@1: 0.9375 (0.6544), Acc@5: 0.9375 (0.8738)
2021-12-29 21:19:04,557 Val Step[1050/1563], Loss: 0.4556 (1.5708), Acc@1: 0.9688 (0.6514), Acc@5: 0.9688 (0.8716)
2021-12-29 21:19:05,889 Val Step[1100/1563], Loss: 1.4547 (1.5906), Acc@1: 0.6875 (0.6471), Acc@5: 0.9688 (0.8682)
2021-12-29 21:19:07,294 Val Step[1150/1563], Loss: 1.3639 (1.6103), Acc@1: 0.7500 (0.6440), Acc@5: 0.8125 (0.8650)
2021-12-29 21:19:08,632 Val Step[1200/1563], Loss: 1.5109 (1.6297), Acc@1: 0.6875 (0.6401), Acc@5: 0.8125 (0.8621)
2021-12-29 21:19:09,955 Val Step[1250/1563], Loss: 0.6587 (1.6431), Acc@1: 0.8750 (0.6382), Acc@5: 0.9375 (0.8596)
2021-12-29 21:19:11,271 Val Step[1300/1563], Loss: 1.3208 (1.6565), Acc@1: 0.7188 (0.6351), Acc@5: 0.8750 (0.8577)
2021-12-29 21:19:12,651 Val Step[1350/1563], Loss: 2.2268 (1.6786), Acc@1: 0.4375 (0.6306), Acc@5: 0.7812 (0.8538)
2021-12-29 21:19:13,949 Val Step[1400/1563], Loss: 1.5636 (1.6874), Acc@1: 0.7500 (0.6285), Acc@5: 0.8750 (0.8526)
2021-12-29 21:19:15,254 Val Step[1450/1563], Loss: 2.0059 (1.6926), Acc@1: 0.4375 (0.6276), Acc@5: 0.8750 (0.8520)
2021-12-29 21:19:16,569 Val Step[1500/1563], Loss: 1.5080 (1.6800), Acc@1: 0.6250 (0.6302), Acc@5: 0.9062 (0.8537)
2021-12-29 21:19:17,868 Val Step[1550/1563], Loss: 1.2174 (1.6772), Acc@1: 0.8438 (0.6304), Acc@5: 0.9062 (0.8539)
2021-12-29 21:19:18,720 ----- Epoch[082/310], Validation Loss: 1.6750, Validation Acc@1: 0.6308, Validation Acc@5: 0.8542, time: 93.91
2021-12-29 21:19:18,721 ----- Epoch[082/310], Train Loss: 4.2031, Train Acc: 0.2920, time: 1570.46, Best Val(epoch82) Acc@1: 0.6308
2021-12-29 21:19:19,137 Max accuracy so far: 0.6308 at epoch_82
2021-12-29 21:19:19,138 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-29 21:19:19,138 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-29 21:19:19,189 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-29 21:19:19,423 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 21:19:19,424 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 21:19:19,535 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 21:19:19,535 Now training epoch 83. LR=0.000824
2021-12-29 21:20:29,152 Epoch[083/310], Step[0000/1251], Loss: 4.3041(4.3041), Acc: 0.3721(0.3721)
2021-12-29 21:21:28,573 Epoch[083/310], Step[0050/1251], Loss: 4.8781(4.2569), Acc: 0.2119(0.2857)
2021-12-29 21:22:26,830 Epoch[083/310], Step[0100/1251], Loss: 4.0590(4.2337), Acc: 0.3135(0.2947)
2021-12-29 21:23:26,532 Epoch[083/310], Step[0150/1251], Loss: 4.2800(4.2227), Acc: 0.3350(0.2938)
2021-12-29 21:24:24,577 Epoch[083/310], Step[0200/1251], Loss: 4.2076(4.2170), Acc: 0.3887(0.2939)
2021-12-29 21:25:23,629 Epoch[083/310], Step[0250/1251], Loss: 4.0604(4.2090), Acc: 0.3350(0.2905)
2021-12-29 21:26:21,797 Epoch[083/310], Step[0300/1251], Loss: 3.9289(4.2079), Acc: 0.3535(0.2939)
2021-12-29 21:27:22,224 Epoch[083/310], Step[0350/1251], Loss: 4.0995(4.2115), Acc: 0.1719(0.2937)
2021-12-29 21:28:21,688 Epoch[083/310], Step[0400/1251], Loss: 4.1723(4.2183), Acc: 0.3369(0.2903)
2021-12-29 21:29:21,511 Epoch[083/310], Step[0450/1251], Loss: 3.6442(4.2117), Acc: 0.4697(0.2908)
2021-12-29 21:30:21,439 Epoch[083/310], Step[0500/1251], Loss: 4.1122(4.2095), Acc: 0.3701(0.2895)
2021-12-29 21:31:21,616 Epoch[083/310], Step[0550/1251], Loss: 3.8454(4.2068), Acc: 0.3506(0.2906)
2021-12-29 21:32:21,470 Epoch[083/310], Step[0600/1251], Loss: 4.2612(4.2078), Acc: 0.3496(0.2916)
2021-12-29 21:33:22,227 Epoch[083/310], Step[0650/1251], Loss: 4.5103(4.2090), Acc: 0.1768(0.2899)
2021-12-29 21:34:22,572 Epoch[083/310], Step[0700/1251], Loss: 3.7547(4.2114), Acc: 0.4434(0.2880)
2021-12-29 21:35:22,907 Epoch[083/310], Step[0750/1251], Loss: 4.0250(4.2132), Acc: 0.3818(0.2893)
2021-12-29 21:36:22,901 Epoch[083/310], Step[0800/1251], Loss: 4.1589(4.2140), Acc: 0.3730(0.2910)
2021-12-29 21:37:23,122 Epoch[083/310], Step[0850/1251], Loss: 3.9967(4.2173), Acc: 0.2100(0.2915)
2021-12-29 21:38:23,955 Epoch[083/310], Step[0900/1251], Loss: 4.1904(4.2154), Acc: 0.3721(0.2924)
2021-12-29 21:39:24,759 Epoch[083/310], Step[0950/1251], Loss: 4.2526(4.2128), Acc: 0.1670(0.2933)
2021-12-29 21:40:25,479 Epoch[083/310], Step[1000/1251], Loss: 4.0399(4.2152), Acc: 0.2207(0.2924)
2021-12-29 21:41:26,252 Epoch[083/310], Step[1050/1251], Loss: 4.2187(4.2137), Acc: 0.2100(0.2930)
2021-12-29 21:42:26,857 Epoch[083/310], Step[1100/1251], Loss: 4.1838(4.2123), Acc: 0.3555(0.2929)
2021-12-29 21:43:27,244 Epoch[083/310], Step[1150/1251], Loss: 3.9467(4.2122), Acc: 0.1328(0.2926)
2021-12-29 21:44:26,516 Epoch[083/310], Step[1200/1251], Loss: 4.4891(4.2107), Acc: 0.1826(0.2922)
2021-12-29 21:45:28,046 Epoch[083/310], Step[1250/1251], Loss: 4.4842(4.2088), Acc: 0.1514(0.2930)
2021-12-29 21:45:29,764 ----- Epoch[083/310], Train Loss: 4.2088, Train Acc: 0.2930, time: 1570.23, Best Val(epoch82) Acc@1: 0.6308
2021-12-29 21:45:29,944 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 21:45:29,944 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 21:45:30,053 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 21:45:30,054 Now training epoch 84. LR=0.000820
2021-12-29 21:46:40,813 Epoch[084/310], Step[0000/1251], Loss: 3.9948(3.9948), Acc: 0.3574(0.3574)
2021-12-29 21:47:37,912 Epoch[084/310], Step[0050/1251], Loss: 3.9858(4.2701), Acc: 0.2061(0.2828)
2021-12-29 21:48:36,364 Epoch[084/310], Step[0100/1251], Loss: 4.5379(4.2471), Acc: 0.3252(0.2882)
2021-12-29 21:49:36,238 Epoch[084/310], Step[0150/1251], Loss: 3.5383(4.2310), Acc: 0.3955(0.2921)
2021-12-29 21:50:36,838 Epoch[084/310], Step[0200/1251], Loss: 4.2341(4.2448), Acc: 0.2441(0.2851)
2021-12-29 21:51:37,425 Epoch[084/310], Step[0250/1251], Loss: 3.4942(4.2515), Acc: 0.3760(0.2839)
2021-12-29 21:52:36,802 Epoch[084/310], Step[0300/1251], Loss: 3.9777(4.2412), Acc: 0.3223(0.2837)
2021-12-29 21:53:37,369 Epoch[084/310], Step[0350/1251], Loss: 4.5272(4.2392), Acc: 0.2646(0.2864)
2021-12-29 21:54:37,255 Epoch[084/310], Step[0400/1251], Loss: 4.4772(4.2293), Acc: 0.3750(0.2871)
2021-12-29 21:55:36,860 Epoch[084/310], Step[0450/1251], Loss: 3.9555(4.2307), Acc: 0.4531(0.2882)
2021-12-29 21:56:35,982 Epoch[084/310], Step[0500/1251], Loss: 4.7167(4.2247), Acc: 0.2764(0.2899)
2021-12-29 21:57:33,973 Epoch[084/310], Step[0550/1251], Loss: 4.2241(4.2231), Acc: 0.2158(0.2917)
2021-12-29 21:58:33,341 Epoch[084/310], Step[0600/1251], Loss: 3.7393(4.2243), Acc: 0.2930(0.2919)
2021-12-29 21:59:33,209 Epoch[084/310], Step[0650/1251], Loss: 4.1570(4.2265), Acc: 0.4072(0.2903)
2021-12-29 22:00:31,838 Epoch[084/310], Step[0700/1251], Loss: 4.6927(4.2309), Acc: 0.3486(0.2898)
2021-12-29 22:01:31,001 Epoch[084/310], Step[0750/1251], Loss: 4.0380(4.2248), Acc: 0.3887(0.2886)
2021-12-29 22:02:29,403 Epoch[084/310], Step[0800/1251], Loss: 3.8515(4.2247), Acc: 0.1846(0.2892)
2021-12-29 22:03:28,301 Epoch[084/310], Step[0850/1251], Loss: 4.3716(4.2217), Acc: 0.2939(0.2902)
2021-12-29 22:04:27,243 Epoch[084/310], Step[0900/1251], Loss: 4.0568(4.2177), Acc: 0.4141(0.2911)
2021-12-29 22:05:27,957 Epoch[084/310], Step[0950/1251], Loss: 4.5333(4.2170), Acc: 0.2812(0.2908)
2021-12-29 22:06:27,872 Epoch[084/310], Step[1000/1251], Loss: 4.8131(4.2190), Acc: 0.2402(0.2892)
2021-12-29 22:07:28,333 Epoch[084/310], Step[1050/1251], Loss: 3.9231(4.2195), Acc: 0.3213(0.2883)
2021-12-29 22:08:29,128 Epoch[084/310], Step[1100/1251], Loss: 4.6825(4.2205), Acc: 0.2852(0.2878)
2021-12-29 22:09:29,130 Epoch[084/310], Step[1150/1251], Loss: 4.3248(4.2185), Acc: 0.2109(0.2878)
2021-12-29 22:10:29,375 Epoch[084/310], Step[1200/1251], Loss: 3.7236(4.2175), Acc: 0.3682(0.2894)
2021-12-29 22:11:29,647 Epoch[084/310], Step[1250/1251], Loss: 4.6279(4.2196), Acc: 0.2266(0.2890)
2021-12-29 22:11:31,409 ----- Validation after Epoch: 84
2021-12-29 22:12:24,063 Val Step[0000/1563], Loss: 0.9434 (0.9434), Acc@1: 0.8438 (0.8438), Acc@5: 0.9062 (0.9062)
2021-12-29 22:12:25,399 Val Step[0050/1563], Loss: 2.8172 (1.0080), Acc@1: 0.3125 (0.7935), Acc@5: 0.7188 (0.9381)
2021-12-29 22:12:26,770 Val Step[0100/1563], Loss: 2.0310 (1.3993), Acc@1: 0.4375 (0.6897), Acc@5: 0.8125 (0.8846)
2021-12-29 22:12:28,190 Val Step[0150/1563], Loss: 0.8784 (1.3011), Acc@1: 0.7812 (0.7136), Acc@5: 0.9062 (0.8969)
2021-12-29 22:12:29,560 Val Step[0200/1563], Loss: 1.3776 (1.3183), Acc@1: 0.7188 (0.7110), Acc@5: 0.9062 (0.8943)
2021-12-29 22:12:30,898 Val Step[0250/1563], Loss: 1.4127 (1.2556), Acc@1: 0.5938 (0.7239), Acc@5: 1.0000 (0.9031)
2021-12-29 22:12:32,234 Val Step[0300/1563], Loss: 1.6132 (1.3264), Acc@1: 0.5938 (0.7022), Acc@5: 0.9062 (0.8982)
2021-12-29 22:12:33,536 Val Step[0350/1563], Loss: 1.2588 (1.3334), Acc@1: 0.6562 (0.6973), Acc@5: 0.9062 (0.9019)
2021-12-29 22:12:34,876 Val Step[0400/1563], Loss: 1.4306 (1.3353), Acc@1: 0.6875 (0.6925), Acc@5: 0.9062 (0.9043)
2021-12-29 22:12:36,327 Val Step[0450/1563], Loss: 1.3583 (1.3347), Acc@1: 0.5938 (0.6897), Acc@5: 0.9688 (0.9059)
2021-12-29 22:12:37,668 Val Step[0500/1563], Loss: 0.5282 (1.3297), Acc@1: 0.8750 (0.6915), Acc@5: 1.0000 (0.9067)
2021-12-29 22:12:39,121 Val Step[0550/1563], Loss: 1.1841 (1.3057), Acc@1: 0.7500 (0.6981), Acc@5: 0.9688 (0.9094)
2021-12-29 22:12:40,458 Val Step[0600/1563], Loss: 1.3562 (1.3074), Acc@1: 0.6875 (0.6985), Acc@5: 0.8750 (0.9089)
2021-12-29 22:12:41,806 Val Step[0650/1563], Loss: 0.7544 (1.3317), Acc@1: 0.8438 (0.6932), Acc@5: 0.9688 (0.9049)
2021-12-29 22:12:43,239 Val Step[0700/1563], Loss: 1.1929 (1.3706), Acc@1: 0.7812 (0.6862), Acc@5: 0.8438 (0.8987)
2021-12-29 22:12:44,642 Val Step[0750/1563], Loss: 1.7083 (1.4108), Acc@1: 0.5938 (0.6788), Acc@5: 0.8438 (0.8926)
2021-12-29 22:12:45,983 Val Step[0800/1563], Loss: 1.2942 (1.4573), Acc@1: 0.7188 (0.6683), Acc@5: 0.9062 (0.8858)
2021-12-29 22:12:47,363 Val Step[0850/1563], Loss: 2.1722 (1.4876), Acc@1: 0.5000 (0.6626), Acc@5: 0.7500 (0.8817)
2021-12-29 22:12:48,812 Val Step[0900/1563], Loss: 0.6580 (1.4887), Acc@1: 0.8750 (0.6646), Acc@5: 0.9375 (0.8806)
2021-12-29 22:12:50,375 Val Step[0950/1563], Loss: 1.5261 (1.5154), Acc@1: 0.5938 (0.6591), Acc@5: 0.9062 (0.8767)
2021-12-29 22:12:51,735 Val Step[1000/1563], Loss: 0.9642 (1.5450), Acc@1: 0.8438 (0.6524), Acc@5: 0.9688 (0.8725)
2021-12-29 22:12:53,104 Val Step[1050/1563], Loss: 0.3547 (1.5641), Acc@1: 0.9688 (0.6482), Acc@5: 0.9688 (0.8697)
2021-12-29 22:12:54,472 Val Step[1100/1563], Loss: 1.2148 (1.5843), Acc@1: 0.7500 (0.6441), Acc@5: 0.8750 (0.8665)
2021-12-29 22:12:55,951 Val Step[1150/1563], Loss: 1.3563 (1.6033), Acc@1: 0.7812 (0.6408), Acc@5: 0.7812 (0.8637)
2021-12-29 22:12:57,369 Val Step[1200/1563], Loss: 1.5285 (1.6223), Acc@1: 0.8125 (0.6376), Acc@5: 0.8438 (0.8604)
2021-12-29 22:12:58,798 Val Step[1250/1563], Loss: 0.7206 (1.6383), Acc@1: 0.8750 (0.6350), Acc@5: 0.9375 (0.8576)
2021-12-29 22:13:00,289 Val Step[1300/1563], Loss: 1.4545 (1.6509), Acc@1: 0.7500 (0.6323), Acc@5: 0.8438 (0.8560)
2021-12-29 22:13:01,726 Val Step[1350/1563], Loss: 2.0330 (1.6755), Acc@1: 0.4375 (0.6269), Acc@5: 0.8125 (0.8522)
2021-12-29 22:13:03,190 Val Step[1400/1563], Loss: 1.4552 (1.6846), Acc@1: 0.7812 (0.6250), Acc@5: 0.8438 (0.8506)
2021-12-29 22:13:04,636 Val Step[1450/1563], Loss: 1.7984 (1.6905), Acc@1: 0.6562 (0.6236), Acc@5: 0.9062 (0.8499)
2021-12-29 22:13:06,115 Val Step[1500/1563], Loss: 1.5598 (1.6792), Acc@1: 0.5625 (0.6259), Acc@5: 0.9062 (0.8516)
2021-12-29 22:13:07,589 Val Step[1550/1563], Loss: 1.1695 (1.6766), Acc@1: 0.7812 (0.6263), Acc@5: 0.9062 (0.8517)
2021-12-29 22:13:08,430 ----- Epoch[084/310], Validation Loss: 1.6737, Validation Acc@1: 0.6269, Validation Acc@5: 0.8520, time: 97.02
2021-12-29 22:13:08,430 ----- Epoch[084/310], Train Loss: 4.2196, Train Acc: 0.2890, time: 1561.35, Best Val(epoch82) Acc@1: 0.6308
2021-12-29 22:13:08,624 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 22:13:08,624 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 22:13:08,728 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 22:13:08,729 Now training epoch 85. LR=0.000816
2021-12-29 22:14:20,263 Epoch[085/310], Step[0000/1251], Loss: 4.1297(4.1297), Acc: 0.3076(0.3076)
2021-12-29 22:15:19,687 Epoch[085/310], Step[0050/1251], Loss: 3.8894(4.1954), Acc: 0.4121(0.2898)
2021-12-29 22:16:18,167 Epoch[085/310], Step[0100/1251], Loss: 4.0096(4.2270), Acc: 0.3145(0.2806)
2021-12-29 22:17:17,805 Epoch[085/310], Step[0150/1251], Loss: 4.7314(4.2134), Acc: 0.1426(0.2803)
2021-12-29 22:18:16,502 Epoch[085/310], Step[0200/1251], Loss: 4.5303(4.2139), Acc: 0.2324(0.2854)
2021-12-29 22:19:16,450 Epoch[085/310], Step[0250/1251], Loss: 4.2060(4.2206), Acc: 0.1455(0.2846)
2021-12-29 22:20:15,221 Epoch[085/310], Step[0300/1251], Loss: 4.6783(4.2125), Acc: 0.2578(0.2844)
2021-12-29 22:21:13,120 Epoch[085/310], Step[0350/1251], Loss: 3.3339(4.2088), Acc: 0.3887(0.2865)
2021-12-29 22:22:12,295 Epoch[085/310], Step[0400/1251], Loss: 4.2371(4.2140), Acc: 0.2061(0.2854)
2021-12-29 22:23:13,362 Epoch[085/310], Step[0450/1251], Loss: 4.5774(4.2110), Acc: 0.2432(0.2882)
2021-12-29 22:24:13,034 Epoch[085/310], Step[0500/1251], Loss: 4.6156(4.2086), Acc: 0.3281(0.2898)
2021-12-29 22:25:13,207 Epoch[085/310], Step[0550/1251], Loss: 4.2436(4.2106), Acc: 0.3711(0.2899)
2021-12-29 22:26:14,803 Epoch[085/310], Step[0600/1251], Loss: 3.9606(4.2133), Acc: 0.2744(0.2891)
2021-12-29 22:27:15,771 Epoch[085/310], Step[0650/1251], Loss: 4.5017(4.2135), Acc: 0.2852(0.2881)
2021-12-29 22:28:15,735 Epoch[085/310], Step[0700/1251], Loss: 4.3150(4.2106), Acc: 0.4102(0.2895)
2021-12-29 22:29:16,185 Epoch[085/310], Step[0750/1251], Loss: 3.8331(4.2135), Acc: 0.3096(0.2885)
2021-12-29 22:30:16,475 Epoch[085/310], Step[0800/1251], Loss: 3.5108(4.2116), Acc: 0.4717(0.2894)
2021-12-29 22:31:16,013 Epoch[085/310], Step[0850/1251], Loss: 4.0356(4.2095), Acc: 0.2617(0.2900)
2021-12-29 22:32:14,461 Epoch[085/310], Step[0900/1251], Loss: 3.9905(4.2093), Acc: 0.2676(0.2900)
2021-12-29 22:33:12,886 Epoch[085/310], Step[0950/1251], Loss: 4.3530(4.2110), Acc: 0.1758(0.2892)
2021-12-29 22:34:13,776 Epoch[085/310], Step[1000/1251], Loss: 4.3768(4.2112), Acc: 0.3955(0.2890)
2021-12-29 22:35:12,957 Epoch[085/310], Step[1050/1251], Loss: 4.0022(4.2062), Acc: 0.3750(0.2902)
2021-12-29 22:36:13,857 Epoch[085/310], Step[1100/1251], Loss: 4.6670(4.1996), Acc: 0.3018(0.2917)
2021-12-29 22:37:12,905 Epoch[085/310], Step[1150/1251], Loss: 4.3042(4.1976), Acc: 0.2461(0.2925)
2021-12-29 22:38:14,546 Epoch[085/310], Step[1200/1251], Loss: 4.3566(4.1995), Acc: 0.2715(0.2923)
2021-12-29 22:39:15,989 Epoch[085/310], Step[1250/1251], Loss: 4.5450(4.2011), Acc: 0.3418(0.2931)
2021-12-29 22:39:17,802 ----- Epoch[085/310], Train Loss: 4.2011, Train Acc: 0.2931, time: 1569.07, Best Val(epoch82) Acc@1: 0.6308
2021-12-29 22:39:17,988 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 22:39:17,989 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 22:39:18,340 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 22:39:18,341 Now training epoch 86. LR=0.000812
2021-12-29 22:40:27,677 Epoch[086/310], Step[0000/1251], Loss: 4.6434(4.6434), Acc: 0.1875(0.1875)
2021-12-29 22:41:26,487 Epoch[086/310], Step[0050/1251], Loss: 4.3388(4.1542), Acc: 0.4170(0.3077)
2021-12-29 22:42:25,830 Epoch[086/310], Step[0100/1251], Loss: 4.0502(4.1552), Acc: 0.3848(0.3012)
2021-12-29 22:43:25,416 Epoch[086/310], Step[0150/1251], Loss: 4.0177(4.1508), Acc: 0.1943(0.3015)
2021-12-29 22:44:24,235 Epoch[086/310], Step[0200/1251], Loss: 3.8348(4.1496), Acc: 0.3076(0.2971)
2021-12-29 22:45:23,738 Epoch[086/310], Step[0250/1251], Loss: 4.1746(4.1531), Acc: 0.2939(0.2977)
2021-12-29 22:46:22,611 Epoch[086/310], Step[0300/1251], Loss: 4.3770(4.1488), Acc: 0.3574(0.2973)
2021-12-29 22:47:20,614 Epoch[086/310], Step[0350/1251], Loss: 4.2203(4.1593), Acc: 0.1416(0.2982)
2021-12-29 22:48:21,376 Epoch[086/310], Step[0400/1251], Loss: 4.7361(4.1683), Acc: 0.1250(0.2953)
2021-12-29 22:49:21,513 Epoch[086/310], Step[0450/1251], Loss: 4.0453(4.1757), Acc: 0.1445(0.2931)
2021-12-29 22:50:20,901 Epoch[086/310], Step[0500/1251], Loss: 4.3201(4.1758), Acc: 0.3525(0.2935)
2021-12-29 22:51:20,946 Epoch[086/310], Step[0550/1251], Loss: 3.8957(4.1688), Acc: 0.4385(0.2938)
2021-12-29 22:52:20,894 Epoch[086/310], Step[0600/1251], Loss: 4.1963(4.1687), Acc: 0.1406(0.2956)
2021-12-29 22:53:21,308 Epoch[086/310], Step[0650/1251], Loss: 4.1157(4.1776), Acc: 0.3037(0.2952)
2021-12-29 22:54:22,790 Epoch[086/310], Step[0700/1251], Loss: 4.3500(4.1785), Acc: 0.1797(0.2955)
2021-12-29 22:55:24,376 Epoch[086/310], Step[0750/1251], Loss: 4.3621(4.1853), Acc: 0.2324(0.2926)
2021-12-29 22:56:25,006 Epoch[086/310], Step[0800/1251], Loss: 3.8540(4.1863), Acc: 0.3271(0.2922)
2021-12-29 22:57:24,516 Epoch[086/310], Step[0850/1251], Loss: 4.3776(4.1877), Acc: 0.1338(0.2920)
2021-12-29 22:58:25,670 Epoch[086/310], Step[0900/1251], Loss: 4.2984(4.1904), Acc: 0.3008(0.2919)
2021-12-29 22:59:25,776 Epoch[086/310], Step[0950/1251], Loss: 3.6881(4.1858), Acc: 0.2588(0.2922)
2021-12-29 23:00:26,561 Epoch[086/310], Step[1000/1251], Loss: 4.2094(4.1872), Acc: 0.3369(0.2923)
2021-12-29 23:01:27,413 Epoch[086/310], Step[1050/1251], Loss: 3.9160(4.1854), Acc: 0.4336(0.2927)
2021-12-29 23:02:29,039 Epoch[086/310], Step[1100/1251], Loss: 4.5767(4.1892), Acc: 0.3672(0.2920)
2021-12-29 23:03:30,851 Epoch[086/310], Step[1150/1251], Loss: 4.5348(4.1902), Acc: 0.2617(0.2919)
2021-12-29 23:04:32,462 Epoch[086/310], Step[1200/1251], Loss: 4.2415(4.1868), Acc: 0.4365(0.2921)
2021-12-29 23:05:33,524 Epoch[086/310], Step[1250/1251], Loss: 4.5688(4.1889), Acc: 0.2324(0.2920)
2021-12-29 23:05:35,237 ----- Validation after Epoch: 86
2021-12-29 23:06:28,015 Val Step[0000/1563], Loss: 0.7960 (0.7960), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-29 23:06:29,480 Val Step[0050/1563], Loss: 2.1396 (0.9892), Acc@1: 0.5312 (0.7874), Acc@5: 0.8750 (0.9387)
2021-12-29 23:06:30,970 Val Step[0100/1563], Loss: 1.8291 (1.3489), Acc@1: 0.5938 (0.6937), Acc@5: 0.8125 (0.8963)
2021-12-29 23:06:32,443 Val Step[0150/1563], Loss: 0.7462 (1.2722), Acc@1: 0.9062 (0.7142), Acc@5: 0.9688 (0.9038)
2021-12-29 23:06:33,790 Val Step[0200/1563], Loss: 1.3111 (1.3017), Acc@1: 0.7188 (0.7104), Acc@5: 0.9062 (0.9003)
2021-12-29 23:06:35,142 Val Step[0250/1563], Loss: 1.0285 (1.2437), Acc@1: 0.7812 (0.7247), Acc@5: 0.9688 (0.9085)
2021-12-29 23:06:36,465 Val Step[0300/1563], Loss: 1.7344 (1.3136), Acc@1: 0.5000 (0.7039), Acc@5: 0.9062 (0.9016)
2021-12-29 23:06:37,806 Val Step[0350/1563], Loss: 1.4361 (1.3197), Acc@1: 0.6562 (0.6996), Acc@5: 0.9062 (0.9039)
2021-12-29 23:06:39,164 Val Step[0400/1563], Loss: 1.7061 (1.3276), Acc@1: 0.6250 (0.6940), Acc@5: 0.8438 (0.9052)
2021-12-29 23:06:40,517 Val Step[0450/1563], Loss: 1.3336 (1.3336), Acc@1: 0.5312 (0.6900), Acc@5: 0.9688 (0.9058)
2021-12-29 23:06:41,912 Val Step[0500/1563], Loss: 0.4398 (1.3242), Acc@1: 0.9375 (0.6937), Acc@5: 1.0000 (0.9064)
2021-12-29 23:06:43,366 Val Step[0550/1563], Loss: 0.9573 (1.2979), Acc@1: 0.7812 (0.7011), Acc@5: 0.9375 (0.9094)
2021-12-29 23:06:44,851 Val Step[0600/1563], Loss: 0.9461 (1.2987), Acc@1: 0.8125 (0.7021), Acc@5: 0.8750 (0.9099)
2021-12-29 23:06:46,156 Val Step[0650/1563], Loss: 0.7090 (1.3172), Acc@1: 0.9375 (0.6994), Acc@5: 0.9688 (0.9067)
2021-12-29 23:06:47,486 Val Step[0700/1563], Loss: 1.7903 (1.3592), Acc@1: 0.6562 (0.6910), Acc@5: 0.8438 (0.9007)
2021-12-29 23:06:48,911 Val Step[0750/1563], Loss: 1.9953 (1.4027), Acc@1: 0.5938 (0.6825), Acc@5: 0.7500 (0.8940)
2021-12-29 23:06:50,240 Val Step[0800/1563], Loss: 1.3158 (1.4484), Acc@1: 0.7500 (0.6730), Acc@5: 0.9062 (0.8873)
2021-12-29 23:06:51,680 Val Step[0850/1563], Loss: 1.7803 (1.4805), Acc@1: 0.5312 (0.6668), Acc@5: 0.8750 (0.8827)
2021-12-29 23:06:53,038 Val Step[0900/1563], Loss: 0.6601 (1.4852), Acc@1: 0.9062 (0.6679), Acc@5: 0.9688 (0.8814)
2021-12-29 23:06:54,523 Val Step[0950/1563], Loss: 1.9948 (1.5135), Acc@1: 0.6562 (0.6622), Acc@5: 0.8125 (0.8770)
2021-12-29 23:06:55,834 Val Step[1000/1563], Loss: 0.8009 (1.5405), Acc@1: 0.9062 (0.6566), Acc@5: 0.9688 (0.8731)
2021-12-29 23:06:57,161 Val Step[1050/1563], Loss: 0.3226 (1.5585), Acc@1: 0.9688 (0.6532), Acc@5: 0.9688 (0.8708)
2021-12-29 23:06:58,479 Val Step[1100/1563], Loss: 1.5839 (1.5790), Acc@1: 0.6250 (0.6495), Acc@5: 0.8750 (0.8678)
2021-12-29 23:06:59,850 Val Step[1150/1563], Loss: 1.7625 (1.5997), Acc@1: 0.7188 (0.6460), Acc@5: 0.7812 (0.8648)
2021-12-29 23:07:01,166 Val Step[1200/1563], Loss: 1.7261 (1.6188), Acc@1: 0.7500 (0.6426), Acc@5: 0.8438 (0.8618)
2021-12-29 23:07:02,471 Val Step[1250/1563], Loss: 1.2326 (1.6373), Acc@1: 0.7500 (0.6397), Acc@5: 0.8750 (0.8587)
2021-12-29 23:07:03,851 Val Step[1300/1563], Loss: 1.2810 (1.6485), Acc@1: 0.7812 (0.6374), Acc@5: 0.8438 (0.8575)
2021-12-29 23:07:05,160 Val Step[1350/1563], Loss: 2.2418 (1.6688), Acc@1: 0.3750 (0.6334), Acc@5: 0.7500 (0.8541)
2021-12-29 23:07:06,466 Val Step[1400/1563], Loss: 1.3693 (1.6801), Acc@1: 0.7500 (0.6313), Acc@5: 0.9062 (0.8524)
2021-12-29 23:07:07,782 Val Step[1450/1563], Loss: 1.7619 (1.6865), Acc@1: 0.5000 (0.6301), Acc@5: 0.8750 (0.8520)
2021-12-29 23:07:09,070 Val Step[1500/1563], Loss: 1.6464 (1.6741), Acc@1: 0.6562 (0.6325), Acc@5: 0.8750 (0.8537)
2021-12-29 23:07:10,416 Val Step[1550/1563], Loss: 0.9771 (1.6705), Acc@1: 0.8750 (0.6330), Acc@5: 0.9062 (0.8540)
2021-12-29 23:07:11,257 ----- Epoch[086/310], Validation Loss: 1.6680, Validation Acc@1: 0.6334, Validation Acc@5: 0.8544, time: 96.02
2021-12-29 23:07:11,257 ----- Epoch[086/310], Train Loss: 4.1889, Train Acc: 0.2920, time: 1576.89, Best Val(epoch86) Acc@1: 0.6334
2021-12-29 23:07:11,446 Max accuracy so far: 0.6334 at epoch_86
2021-12-29 23:07:11,447 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-29 23:07:11,447 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-29 23:07:11,554 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-29 23:07:11,932 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 23:07:11,933 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 23:07:12,066 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 23:07:12,066 Now training epoch 87. LR=0.000807
2021-12-29 23:08:27,271 Epoch[087/310], Step[0000/1251], Loss: 4.1797(4.1797), Acc: 0.2852(0.2852)
2021-12-29 23:09:25,423 Epoch[087/310], Step[0050/1251], Loss: 4.0073(4.1222), Acc: 0.2471(0.3184)
2021-12-29 23:10:25,009 Epoch[087/310], Step[0100/1251], Loss: 4.1236(4.1627), Acc: 0.4482(0.3016)
2021-12-29 23:11:24,097 Epoch[087/310], Step[0150/1251], Loss: 3.7661(4.1893), Acc: 0.4785(0.2912)
2021-12-29 23:12:23,338 Epoch[087/310], Step[0200/1251], Loss: 4.0742(4.1910), Acc: 0.4121(0.2931)
2021-12-29 23:13:22,605 Epoch[087/310], Step[0250/1251], Loss: 4.3776(4.1965), Acc: 0.2617(0.2904)
2021-12-29 23:14:22,904 Epoch[087/310], Step[0300/1251], Loss: 4.3059(4.2094), Acc: 0.1387(0.2868)
2021-12-29 23:15:21,385 Epoch[087/310], Step[0350/1251], Loss: 4.4043(4.2090), Acc: 0.2363(0.2867)
2021-12-29 23:16:21,621 Epoch[087/310], Step[0400/1251], Loss: 4.2450(4.2060), Acc: 0.2773(0.2881)
2021-12-29 23:17:21,562 Epoch[087/310], Step[0450/1251], Loss: 3.6051(4.2009), Acc: 0.2734(0.2906)
2021-12-29 23:18:19,713 Epoch[087/310], Step[0500/1251], Loss: 4.9995(4.1928), Acc: 0.1885(0.2927)
2021-12-29 23:19:19,097 Epoch[087/310], Step[0550/1251], Loss: 4.7279(4.2026), Acc: 0.2812(0.2920)
2021-12-29 23:20:19,882 Epoch[087/310], Step[0600/1251], Loss: 4.3994(4.2039), Acc: 0.3203(0.2888)
2021-12-29 23:21:19,606 Epoch[087/310], Step[0650/1251], Loss: 4.0828(4.2044), Acc: 0.1074(0.2882)
2021-12-29 23:22:19,525 Epoch[087/310], Step[0700/1251], Loss: 3.6496(4.2068), Acc: 0.2559(0.2870)
2021-12-29 23:23:18,980 Epoch[087/310], Step[0750/1251], Loss: 4.1411(4.2078), Acc: 0.1484(0.2884)
2021-12-29 23:24:19,700 Epoch[087/310], Step[0800/1251], Loss: 3.7423(4.2069), Acc: 0.4678(0.2892)
2021-12-29 23:25:20,079 Epoch[087/310], Step[0850/1251], Loss: 4.1811(4.2046), Acc: 0.2656(0.2897)
2021-12-29 23:26:21,262 Epoch[087/310], Step[0900/1251], Loss: 3.9506(4.2069), Acc: 0.3291(0.2887)
2021-12-29 23:27:22,907 Epoch[087/310], Step[0950/1251], Loss: 4.3880(4.2060), Acc: 0.1934(0.2877)
2021-12-29 23:28:22,583 Epoch[087/310], Step[1000/1251], Loss: 4.6426(4.2034), Acc: 0.3672(0.2894)
2021-12-29 23:29:22,929 Epoch[087/310], Step[1050/1251], Loss: 4.3626(4.2066), Acc: 0.2930(0.2892)
2021-12-29 23:30:22,426 Epoch[087/310], Step[1100/1251], Loss: 4.1796(4.2051), Acc: 0.3887(0.2882)
2021-12-29 23:31:23,821 Epoch[087/310], Step[1150/1251], Loss: 3.8663(4.2086), Acc: 0.3672(0.2887)
2021-12-29 23:32:25,214 Epoch[087/310], Step[1200/1251], Loss: 4.0445(4.2063), Acc: 0.4141(0.2886)
2021-12-29 23:33:25,879 Epoch[087/310], Step[1250/1251], Loss: 4.6036(4.2055), Acc: 0.1748(0.2883)
2021-12-29 23:33:27,825 ----- Epoch[087/310], Train Loss: 4.2055, Train Acc: 0.2883, time: 1575.75, Best Val(epoch86) Acc@1: 0.6334
2021-12-29 23:33:28,003 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-29 23:33:28,004 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-29 23:33:28,109 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-29 23:33:28,110 Now training epoch 88. LR=0.000803
2021-12-29 23:34:44,871 Epoch[088/310], Step[0000/1251], Loss: 3.8063(3.8063), Acc: 0.1904(0.1904)
2021-12-29 23:35:43,662 Epoch[088/310], Step[0050/1251], Loss: 4.0772(4.1467), Acc: 0.3457(0.3142)
2021-12-29 23:36:42,734 Epoch[088/310], Step[0100/1251], Loss: 3.7750(4.1316), Acc: 0.1748(0.2969)
2021-12-29 23:37:41,991 Epoch[088/310], Step[0150/1251], Loss: 4.3568(4.1453), Acc: 0.3535(0.2895)
2021-12-29 23:38:40,793 Epoch[088/310], Step[0200/1251], Loss: 4.1025(4.1395), Acc: 0.2949(0.2948)
2021-12-29 23:39:40,546 Epoch[088/310], Step[0250/1251], Loss: 4.5066(4.1602), Acc: 0.1699(0.2943)
2021-12-29 23:40:40,018 Epoch[088/310], Step[0300/1251], Loss: 4.1112(4.1593), Acc: 0.2012(0.2903)
2021-12-29 23:41:39,571 Epoch[088/310], Step[0350/1251], Loss: 3.8818(4.1736), Acc: 0.4219(0.2897)
2021-12-29 23:42:39,447 Epoch[088/310], Step[0400/1251], Loss: 3.9652(4.1777), Acc: 0.2461(0.2897)
2021-12-29 23:43:38,433 Epoch[088/310], Step[0450/1251], Loss: 4.0465(4.1835), Acc: 0.4004(0.2906)
2021-12-29 23:44:38,481 Epoch[088/310], Step[0500/1251], Loss: 4.9099(4.1809), Acc: 0.1865(0.2928)
2021-12-29 23:45:39,238 Epoch[088/310], Step[0550/1251], Loss: 4.6737(4.1841), Acc: 0.3018(0.2933)
2021-12-29 23:46:40,525 Epoch[088/310], Step[0600/1251], Loss: 3.8874(4.1893), Acc: 0.2646(0.2935)
2021-12-29 23:47:42,345 Epoch[088/310], Step[0650/1251], Loss: 4.2409(4.1911), Acc: 0.2285(0.2930)
2021-12-29 23:48:43,769 Epoch[088/310], Step[0700/1251], Loss: 4.2116(4.1894), Acc: 0.3770(0.2929)
2021-12-29 23:49:43,999 Epoch[088/310], Step[0750/1251], Loss: 4.4043(4.1910), Acc: 0.2344(0.2932)
2021-12-29 23:50:45,645 Epoch[088/310], Step[0800/1251], Loss: 4.1345(4.1933), Acc: 0.1504(0.2910)
2021-12-29 23:51:44,201 Epoch[088/310], Step[0850/1251], Loss: 4.1923(4.1880), Acc: 0.2988(0.2905)
2021-12-29 23:52:44,439 Epoch[088/310], Step[0900/1251], Loss: 3.8776(4.1917), Acc: 0.4443(0.2901)
2021-12-29 23:53:44,972 Epoch[088/310], Step[0950/1251], Loss: 4.1488(4.1908), Acc: 0.3389(0.2895)
2021-12-29 23:54:45,888 Epoch[088/310], Step[1000/1251], Loss: 4.3421(4.1897), Acc: 0.2920(0.2896)
2021-12-29 23:55:46,479 Epoch[088/310], Step[1050/1251], Loss: 4.4270(4.1884), Acc: 0.3545(0.2900)
2021-12-29 23:56:46,257 Epoch[088/310], Step[1100/1251], Loss: 4.3452(4.1898), Acc: 0.3535(0.2894)
2021-12-29 23:57:46,823 Epoch[088/310], Step[1150/1251], Loss: 3.9059(4.1863), Acc: 0.4619(0.2900)
2021-12-29 23:58:46,665 Epoch[088/310], Step[1200/1251], Loss: 4.1203(4.1848), Acc: 0.2588(0.2899)
2021-12-29 23:59:47,405 Epoch[088/310], Step[1250/1251], Loss: 4.5638(4.1815), Acc: 0.3604(0.2909)
2021-12-29 23:59:49,100 ----- Validation after Epoch: 88
2021-12-30 00:00:40,697 Val Step[0000/1563], Loss: 0.9395 (0.9395), Acc@1: 0.8125 (0.8125), Acc@5: 0.9688 (0.9688)
2021-12-30 00:00:42,241 Val Step[0050/1563], Loss: 2.5194 (1.0253), Acc@1: 0.3750 (0.7880), Acc@5: 0.7812 (0.9350)
2021-12-30 00:00:43,569 Val Step[0100/1563], Loss: 2.0362 (1.3619), Acc@1: 0.5000 (0.6980), Acc@5: 0.8125 (0.8979)
2021-12-30 00:00:44,978 Val Step[0150/1563], Loss: 0.9375 (1.3097), Acc@1: 0.8438 (0.7094), Acc@5: 0.9375 (0.9036)
2021-12-30 00:00:46,305 Val Step[0200/1563], Loss: 1.4150 (1.3247), Acc@1: 0.6875 (0.7128), Acc@5: 0.9375 (0.9002)
2021-12-30 00:00:47,622 Val Step[0250/1563], Loss: 1.0623 (1.2687), Acc@1: 0.7500 (0.7251), Acc@5: 0.9688 (0.9084)
2021-12-30 00:00:49,007 Val Step[0300/1563], Loss: 1.3526 (1.3375), Acc@1: 0.6875 (0.7051), Acc@5: 0.9375 (0.9006)
2021-12-30 00:00:50,386 Val Step[0350/1563], Loss: 1.4355 (1.3387), Acc@1: 0.6875 (0.7010), Acc@5: 0.9375 (0.9042)
2021-12-30 00:00:51,755 Val Step[0400/1563], Loss: 1.6191 (1.3334), Acc@1: 0.6562 (0.6972), Acc@5: 0.9062 (0.9070)
2021-12-30 00:00:53,208 Val Step[0450/1563], Loss: 1.3440 (1.3380), Acc@1: 0.5312 (0.6944), Acc@5: 0.9688 (0.9085)
2021-12-30 00:00:54,629 Val Step[0500/1563], Loss: 0.5378 (1.3339), Acc@1: 0.9375 (0.6960), Acc@5: 1.0000 (0.9087)
2021-12-30 00:00:56,054 Val Step[0550/1563], Loss: 1.1656 (1.3054), Acc@1: 0.7500 (0.7038), Acc@5: 0.9688 (0.9120)
2021-12-30 00:00:57,362 Val Step[0600/1563], Loss: 0.9363 (1.3076), Acc@1: 0.8438 (0.7053), Acc@5: 0.9375 (0.9110)
2021-12-30 00:00:58,728 Val Step[0650/1563], Loss: 1.1462 (1.3368), Acc@1: 0.8125 (0.7004), Acc@5: 0.9062 (0.9067)
2021-12-30 00:01:00,115 Val Step[0700/1563], Loss: 1.3983 (1.3796), Acc@1: 0.7188 (0.6907), Acc@5: 0.9062 (0.8999)
2021-12-30 00:01:01,458 Val Step[0750/1563], Loss: 1.8608 (1.4145), Acc@1: 0.6562 (0.6843), Acc@5: 0.7500 (0.8944)
2021-12-30 00:01:02,918 Val Step[0800/1563], Loss: 1.1609 (1.4592), Acc@1: 0.7812 (0.6736), Acc@5: 0.9375 (0.8885)
2021-12-30 00:01:04,251 Val Step[0850/1563], Loss: 1.8535 (1.4916), Acc@1: 0.5625 (0.6669), Acc@5: 0.9062 (0.8842)
2021-12-30 00:01:05,647 Val Step[0900/1563], Loss: 0.8999 (1.4922), Acc@1: 0.8438 (0.6683), Acc@5: 0.9375 (0.8835)
2021-12-30 00:01:07,053 Val Step[0950/1563], Loss: 1.4249 (1.5167), Acc@1: 0.6875 (0.6637), Acc@5: 0.8125 (0.8790)
2021-12-30 00:01:08,379 Val Step[1000/1563], Loss: 0.8360 (1.5465), Acc@1: 0.9062 (0.6572), Acc@5: 1.0000 (0.8747)
2021-12-30 00:01:09,732 Val Step[1050/1563], Loss: 0.7124 (1.5647), Acc@1: 0.9062 (0.6535), Acc@5: 0.9688 (0.8716)
2021-12-30 00:01:11,151 Val Step[1100/1563], Loss: 1.4085 (1.5814), Acc@1: 0.6875 (0.6508), Acc@5: 0.8750 (0.8691)
2021-12-30 00:01:12,490 Val Step[1150/1563], Loss: 1.5543 (1.5992), Acc@1: 0.7188 (0.6479), Acc@5: 0.8438 (0.8664)
2021-12-30 00:01:13,811 Val Step[1200/1563], Loss: 1.6274 (1.6162), Acc@1: 0.7500 (0.6450), Acc@5: 0.8438 (0.8633)
2021-12-30 00:01:15,146 Val Step[1250/1563], Loss: 0.8892 (1.6323), Acc@1: 0.8438 (0.6423), Acc@5: 0.9375 (0.8607)
2021-12-30 00:01:16,548 Val Step[1300/1563], Loss: 1.0775 (1.6445), Acc@1: 0.8125 (0.6395), Acc@5: 0.8750 (0.8593)
2021-12-30 00:01:18,014 Val Step[1350/1563], Loss: 2.1339 (1.6626), Acc@1: 0.4375 (0.6359), Acc@5: 0.7500 (0.8564)
2021-12-30 00:01:19,366 Val Step[1400/1563], Loss: 1.6675 (1.6738), Acc@1: 0.7188 (0.6339), Acc@5: 0.8438 (0.8544)
2021-12-30 00:01:20,776 Val Step[1450/1563], Loss: 1.8752 (1.6793), Acc@1: 0.5000 (0.6326), Acc@5: 0.9375 (0.8542)
2021-12-30 00:01:22,270 Val Step[1500/1563], Loss: 1.5753 (1.6688), Acc@1: 0.5625 (0.6350), Acc@5: 0.9062 (0.8560)
2021-12-30 00:01:23,591 Val Step[1550/1563], Loss: 1.0702 (1.6696), Acc@1: 0.8750 (0.6345), Acc@5: 0.9062 (0.8556)
2021-12-30 00:01:24,411 ----- Epoch[088/310], Validation Loss: 1.6674, Validation Acc@1: 0.6349, Validation Acc@5: 0.8559, time: 95.31
2021-12-30 00:01:24,411 ----- Epoch[088/310], Train Loss: 4.1815, Train Acc: 0.2909, time: 1580.99, Best Val(epoch88) Acc@1: 0.6349
2021-12-30 00:01:24,595 Max accuracy so far: 0.6349 at epoch_88
2021-12-30 00:01:24,596 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 00:01:24,596 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 00:01:24,700 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 00:01:25,176 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 00:01:25,177 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 00:01:25,263 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 00:01:25,264 Now training epoch 89. LR=0.000799
2021-12-30 00:02:40,574 Epoch[089/310], Step[0000/1251], Loss: 4.0560(4.0560), Acc: 0.3955(0.3955)
2021-12-30 00:03:39,713 Epoch[089/310], Step[0050/1251], Loss: 4.1834(4.1080), Acc: 0.2852(0.2857)
2021-12-30 00:04:37,836 Epoch[089/310], Step[0100/1251], Loss: 3.3505(4.1311), Acc: 0.2500(0.2859)
2021-12-30 00:05:36,881 Epoch[089/310], Step[0150/1251], Loss: 4.1466(4.1213), Acc: 0.4023(0.2895)
2021-12-30 00:06:35,905 Epoch[089/310], Step[0200/1251], Loss: 4.3409(4.1404), Acc: 0.1631(0.2917)
2021-12-30 00:07:36,485 Epoch[089/310], Step[0250/1251], Loss: 4.2573(4.1495), Acc: 0.2139(0.2901)
2021-12-30 00:08:35,289 Epoch[089/310], Step[0300/1251], Loss: 3.7785(4.1487), Acc: 0.3691(0.2920)
2021-12-30 00:09:34,347 Epoch[089/310], Step[0350/1251], Loss: 3.7414(4.1502), Acc: 0.2002(0.2949)
2021-12-30 00:10:34,283 Epoch[089/310], Step[0400/1251], Loss: 4.1358(4.1591), Acc: 0.1445(0.2941)
2021-12-30 00:11:32,755 Epoch[089/310], Step[0450/1251], Loss: 4.5784(4.1607), Acc: 0.1299(0.2949)
2021-12-30 00:12:32,576 Epoch[089/310], Step[0500/1251], Loss: 4.0481(4.1542), Acc: 0.0762(0.2946)
2021-12-30 00:13:34,069 Epoch[089/310], Step[0550/1251], Loss: 3.5966(4.1581), Acc: 0.4639(0.2946)
2021-12-30 00:14:34,372 Epoch[089/310], Step[0600/1251], Loss: 4.8179(4.1705), Acc: 0.2617(0.2917)
2021-12-30 00:15:33,997 Epoch[089/310], Step[0650/1251], Loss: 4.1147(4.1714), Acc: 0.2422(0.2940)
2021-12-30 00:16:33,485 Epoch[089/310], Step[0700/1251], Loss: 4.2329(4.1751), Acc: 0.2402(0.2937)
2021-12-30 00:17:32,964 Epoch[089/310], Step[0750/1251], Loss: 4.1529(4.1790), Acc: 0.3604(0.2923)
2021-12-30 00:18:33,375 Epoch[089/310], Step[0800/1251], Loss: 4.0625(4.1785), Acc: 0.1807(0.2923)
2021-12-30 00:19:34,205 Epoch[089/310], Step[0850/1251], Loss: 4.3551(4.1793), Acc: 0.2871(0.2922)
2021-12-30 00:20:34,131 Epoch[089/310], Step[0900/1251], Loss: 4.0109(4.1830), Acc: 0.3672(0.2924)
2021-12-30 00:21:34,831 Epoch[089/310], Step[0950/1251], Loss: 3.9789(4.1848), Acc: 0.4580(0.2932)
2021-12-30 00:22:34,350 Epoch[089/310], Step[1000/1251], Loss: 3.8542(4.1847), Acc: 0.2451(0.2935)
2021-12-30 00:23:32,832 Epoch[089/310], Step[1050/1251], Loss: 4.2746(4.1854), Acc: 0.1963(0.2931)
2021-12-30 00:24:33,163 Epoch[089/310], Step[1100/1251], Loss: 4.5830(4.1861), Acc: 0.2676(0.2935)
2021-12-30 00:25:34,005 Epoch[089/310], Step[1150/1251], Loss: 4.3640(4.1831), Acc: 0.3408(0.2942)
2021-12-30 00:26:33,603 Epoch[089/310], Step[1200/1251], Loss: 4.1536(4.1827), Acc: 0.3789(0.2945)
2021-12-30 00:27:31,331 Epoch[089/310], Step[1250/1251], Loss: 4.4750(4.1838), Acc: 0.2090(0.2944)
2021-12-30 00:27:32,950 ----- Epoch[089/310], Train Loss: 4.1838, Train Acc: 0.2944, time: 1567.68, Best Val(epoch88) Acc@1: 0.6349
2021-12-30 00:27:33,146 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 00:27:33,147 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 00:27:33,248 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 00:27:33,248 Now training epoch 90. LR=0.000795
2021-12-30 00:28:47,614 Epoch[090/310], Step[0000/1251], Loss: 4.6387(4.6387), Acc: 0.2637(0.2637)
2021-12-30 00:29:46,129 Epoch[090/310], Step[0050/1251], Loss: 3.7820(4.1670), Acc: 0.2959(0.2884)
2021-12-30 00:30:44,588 Epoch[090/310], Step[0100/1251], Loss: 4.6026(4.1492), Acc: 0.2900(0.2991)
2021-12-30 00:31:44,887 Epoch[090/310], Step[0150/1251], Loss: 3.5761(4.1459), Acc: 0.2500(0.3027)
2021-12-30 00:32:44,625 Epoch[090/310], Step[0200/1251], Loss: 4.3084(4.1574), Acc: 0.3770(0.2991)
2021-12-30 00:33:43,817 Epoch[090/310], Step[0250/1251], Loss: 4.1736(4.1685), Acc: 0.4229(0.2994)
2021-12-30 00:34:44,260 Epoch[090/310], Step[0300/1251], Loss: 4.1547(4.1628), Acc: 0.2637(0.2984)
2021-12-30 00:35:44,778 Epoch[090/310], Step[0350/1251], Loss: 4.0970(4.1673), Acc: 0.2246(0.2962)
2021-12-30 00:36:45,679 Epoch[090/310], Step[0400/1251], Loss: 3.8142(4.1631), Acc: 0.2930(0.2950)
2021-12-30 00:37:45,486 Epoch[090/310], Step[0450/1251], Loss: 4.9340(4.1695), Acc: 0.2295(0.2927)
2021-12-30 00:38:45,602 Epoch[090/310], Step[0500/1251], Loss: 4.3260(4.1737), Acc: 0.2295(0.2950)
2021-12-30 00:39:45,332 Epoch[090/310], Step[0550/1251], Loss: 4.5676(4.1743), Acc: 0.3223(0.2941)
2021-12-30 00:40:46,265 Epoch[090/310], Step[0600/1251], Loss: 3.8887(4.1786), Acc: 0.2842(0.2936)
2021-12-30 00:41:45,529 Epoch[090/310], Step[0650/1251], Loss: 4.5950(4.1769), Acc: 0.3379(0.2947)
2021-12-30 00:42:45,607 Epoch[090/310], Step[0700/1251], Loss: 4.2437(4.1792), Acc: 0.3135(0.2942)
2021-12-30 00:43:45,006 Epoch[090/310], Step[0750/1251], Loss: 4.9584(4.1761), Acc: 0.2148(0.2960)
2021-12-30 00:44:45,076 Epoch[090/310], Step[0800/1251], Loss: 4.1509(4.1792), Acc: 0.4121(0.2964)
2021-12-30 00:45:45,634 Epoch[090/310], Step[0850/1251], Loss: 3.5460(4.1791), Acc: 0.3428(0.2963)
2021-12-30 00:46:46,596 Epoch[090/310], Step[0900/1251], Loss: 3.8418(4.1762), Acc: 0.4326(0.2963)
2021-12-30 00:47:46,069 Epoch[090/310], Step[0950/1251], Loss: 4.1461(4.1763), Acc: 0.2695(0.2963)
2021-12-30 00:48:46,326 Epoch[090/310], Step[1000/1251], Loss: 4.6056(4.1757), Acc: 0.2656(0.2962)
2021-12-30 00:49:46,604 Epoch[090/310], Step[1050/1251], Loss: 3.9716(4.1731), Acc: 0.2051(0.2964)
2021-12-30 00:50:45,473 Epoch[090/310], Step[1100/1251], Loss: 4.3871(4.1745), Acc: 0.3418(0.2959)
2021-12-30 00:51:46,657 Epoch[090/310], Step[1150/1251], Loss: 4.3972(4.1735), Acc: 0.1758(0.2962)
2021-12-30 00:52:47,411 Epoch[090/310], Step[1200/1251], Loss: 4.2159(4.1732), Acc: 0.2461(0.2968)
2021-12-30 00:53:47,589 Epoch[090/310], Step[1250/1251], Loss: 4.3460(4.1742), Acc: 0.2539(0.2966)
2021-12-30 00:53:49,164 ----- Validation after Epoch: 90
2021-12-30 00:54:37,890 Val Step[0000/1563], Loss: 0.8089 (0.8089), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2021-12-30 00:54:39,340 Val Step[0050/1563], Loss: 2.6819 (1.0342), Acc@1: 0.3750 (0.8015), Acc@5: 0.7812 (0.9412)
2021-12-30 00:54:40,649 Val Step[0100/1563], Loss: 1.9341 (1.3961), Acc@1: 0.4688 (0.7002), Acc@5: 0.8438 (0.8967)
2021-12-30 00:54:41,948 Val Step[0150/1563], Loss: 0.7923 (1.3255), Acc@1: 0.8750 (0.7159), Acc@5: 0.9375 (0.9038)
2021-12-30 00:54:43,220 Val Step[0200/1563], Loss: 1.2404 (1.3497), Acc@1: 0.6562 (0.7166), Acc@5: 0.9375 (0.8996)
2021-12-30 00:54:44,519 Val Step[0250/1563], Loss: 1.3968 (1.3001), Acc@1: 0.5938 (0.7285), Acc@5: 0.9688 (0.9079)
2021-12-30 00:54:45,856 Val Step[0300/1563], Loss: 1.4485 (1.3609), Acc@1: 0.6250 (0.7085), Acc@5: 0.8750 (0.9043)
2021-12-30 00:54:47,317 Val Step[0350/1563], Loss: 1.5922 (1.3650), Acc@1: 0.6562 (0.7050), Acc@5: 0.8750 (0.9064)
2021-12-30 00:54:48,698 Val Step[0400/1563], Loss: 1.5489 (1.3695), Acc@1: 0.7500 (0.7002), Acc@5: 0.9375 (0.9073)
2021-12-30 00:54:49,975 Val Step[0450/1563], Loss: 1.4498 (1.3803), Acc@1: 0.5000 (0.6946), Acc@5: 0.9688 (0.9081)
2021-12-30 00:54:51,239 Val Step[0500/1563], Loss: 0.5059 (1.3728), Acc@1: 0.9375 (0.6967), Acc@5: 1.0000 (0.9089)
2021-12-30 00:54:52,668 Val Step[0550/1563], Loss: 1.4566 (1.3485), Acc@1: 0.6250 (0.7046), Acc@5: 0.9688 (0.9113)
2021-12-30 00:54:53,943 Val Step[0600/1563], Loss: 1.0326 (1.3545), Acc@1: 0.7812 (0.7039), Acc@5: 0.9375 (0.9107)
2021-12-30 00:54:55,226 Val Step[0650/1563], Loss: 0.8675 (1.3779), Acc@1: 0.8125 (0.6994), Acc@5: 1.0000 (0.9063)
2021-12-30 00:54:56,510 Val Step[0700/1563], Loss: 1.5996 (1.4169), Acc@1: 0.6250 (0.6919), Acc@5: 0.8750 (0.9003)
2021-12-30 00:54:57,767 Val Step[0750/1563], Loss: 1.8549 (1.4580), Acc@1: 0.7188 (0.6843), Acc@5: 0.8438 (0.8940)
2021-12-30 00:54:59,038 Val Step[0800/1563], Loss: 1.3254 (1.4980), Acc@1: 0.7500 (0.6746), Acc@5: 0.8750 (0.8885)
2021-12-30 00:55:00,333 Val Step[0850/1563], Loss: 1.8832 (1.5264), Acc@1: 0.5625 (0.6682), Acc@5: 0.7812 (0.8840)
2021-12-30 00:55:01,588 Val Step[0900/1563], Loss: 0.8865 (1.5280), Acc@1: 0.8750 (0.6694), Acc@5: 0.9375 (0.8824)
2021-12-30 00:55:02,973 Val Step[0950/1563], Loss: 1.6920 (1.5543), Acc@1: 0.6250 (0.6642), Acc@5: 0.8438 (0.8777)
2021-12-30 00:55:04,222 Val Step[1000/1563], Loss: 1.0137 (1.5840), Acc@1: 0.9062 (0.6577), Acc@5: 0.9688 (0.8734)
2021-12-30 00:55:05,476 Val Step[1050/1563], Loss: 0.4544 (1.5985), Acc@1: 0.9688 (0.6553), Acc@5: 0.9688 (0.8709)
2021-12-30 00:55:06,758 Val Step[1100/1563], Loss: 1.3846 (1.6145), Acc@1: 0.7188 (0.6517), Acc@5: 0.8125 (0.8681)
2021-12-30 00:55:08,029 Val Step[1150/1563], Loss: 1.7997 (1.6342), Acc@1: 0.7188 (0.6479), Acc@5: 0.7812 (0.8652)
2021-12-30 00:55:09,368 Val Step[1200/1563], Loss: 1.7143 (1.6547), Acc@1: 0.6562 (0.6438), Acc@5: 0.8438 (0.8618)
2021-12-30 00:55:10,804 Val Step[1250/1563], Loss: 1.3783 (1.6663), Acc@1: 0.7188 (0.6425), Acc@5: 0.9062 (0.8599)
2021-12-30 00:55:12,118 Val Step[1300/1563], Loss: 1.2119 (1.6790), Acc@1: 0.7812 (0.6402), Acc@5: 0.8750 (0.8585)
2021-12-30 00:55:13,470 Val Step[1350/1563], Loss: 3.2231 (1.7014), Acc@1: 0.1875 (0.6354), Acc@5: 0.5938 (0.8547)
2021-12-30 00:55:14,996 Val Step[1400/1563], Loss: 1.8714 (1.7129), Acc@1: 0.6562 (0.6331), Acc@5: 0.7812 (0.8524)
2021-12-30 00:55:16,363 Val Step[1450/1563], Loss: 1.8113 (1.7191), Acc@1: 0.5938 (0.6319), Acc@5: 0.9062 (0.8518)
2021-12-30 00:55:17,658 Val Step[1500/1563], Loss: 1.9501 (1.7062), Acc@1: 0.5938 (0.6345), Acc@5: 0.8438 (0.8536)
2021-12-30 00:55:18,929 Val Step[1550/1563], Loss: 1.0426 (1.7027), Acc@1: 0.8750 (0.6344), Acc@5: 0.9062 (0.8543)
2021-12-30 00:55:19,672 ----- Epoch[090/310], Validation Loss: 1.7011, Validation Acc@1: 0.6346, Validation Acc@5: 0.8545, time: 90.50
2021-12-30 00:55:19,672 ----- Epoch[090/310], Train Loss: 4.1742, Train Acc: 0.2966, time: 1575.91, Best Val(epoch88) Acc@1: 0.6349
2021-12-30 00:55:19,836 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-90-Loss-4.183012914314545.pdparams
2021-12-30 00:55:19,836 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-90-Loss-4.183012914314545.pdopt
2021-12-30 00:55:19,877 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-90-Loss-4.183012914314545-EMA.pdparams
2021-12-30 00:55:19,877 Now training epoch 91. LR=0.000791
2021-12-30 00:56:30,718 Epoch[091/310], Step[0000/1251], Loss: 3.8492(3.8492), Acc: 0.3594(0.3594)
2021-12-30 00:57:29,272 Epoch[091/310], Step[0050/1251], Loss: 3.9424(4.2144), Acc: 0.2949(0.3097)
2021-12-30 00:58:27,539 Epoch[091/310], Step[0100/1251], Loss: 4.4996(4.1829), Acc: 0.1357(0.3070)
2021-12-30 00:59:24,630 Epoch[091/310], Step[0150/1251], Loss: 4.1206(4.1506), Acc: 0.2998(0.3113)
2021-12-30 01:00:23,713 Epoch[091/310], Step[0200/1251], Loss: 4.3268(4.1382), Acc: 0.2930(0.3073)
2021-12-30 01:01:23,613 Epoch[091/310], Step[0250/1251], Loss: 4.1417(4.1401), Acc: 0.4072(0.3072)
2021-12-30 01:02:21,845 Epoch[091/310], Step[0300/1251], Loss: 3.9699(4.1426), Acc: 0.2783(0.3085)
2021-12-30 01:03:19,538 Epoch[091/310], Step[0350/1251], Loss: 3.9710(4.1496), Acc: 0.2432(0.3081)
2021-12-30 01:04:17,942 Epoch[091/310], Step[0400/1251], Loss: 3.6585(4.1492), Acc: 0.3164(0.3084)
2021-12-30 01:05:15,679 Epoch[091/310], Step[0450/1251], Loss: 4.1440(4.1595), Acc: 0.1982(0.3056)
2021-12-30 01:06:14,737 Epoch[091/310], Step[0500/1251], Loss: 4.1686(4.1669), Acc: 0.2676(0.3051)
2021-12-30 01:07:14,090 Epoch[091/310], Step[0550/1251], Loss: 3.7632(4.1655), Acc: 0.1543(0.3049)
2021-12-30 01:08:11,911 Epoch[091/310], Step[0600/1251], Loss: 3.7093(4.1624), Acc: 0.3447(0.3048)
2021-12-30 01:09:10,534 Epoch[091/310], Step[0650/1251], Loss: 4.1269(4.1685), Acc: 0.0801(0.3033)
2021-12-30 01:10:09,415 Epoch[091/310], Step[0700/1251], Loss: 4.7637(4.1725), Acc: 0.2354(0.3016)
2021-12-30 01:11:08,455 Epoch[091/310], Step[0750/1251], Loss: 4.1670(4.1725), Acc: 0.1963(0.3000)
2021-12-30 01:12:08,380 Epoch[091/310], Step[0800/1251], Loss: 4.1214(4.1703), Acc: 0.4023(0.2995)
2021-12-30 01:13:07,899 Epoch[091/310], Step[0850/1251], Loss: 3.8632(4.1733), Acc: 0.2539(0.2993)
2021-12-30 01:14:08,360 Epoch[091/310], Step[0900/1251], Loss: 4.6591(4.1779), Acc: 0.2129(0.2987)
2021-12-30 01:15:08,245 Epoch[091/310], Step[0950/1251], Loss: 4.4856(4.1763), Acc: 0.0840(0.2985)
2021-12-30 01:16:07,453 Epoch[091/310], Step[1000/1251], Loss: 4.5404(4.1755), Acc: 0.4053(0.2982)
2021-12-30 01:17:06,943 Epoch[091/310], Step[1050/1251], Loss: 4.2335(4.1722), Acc: 0.1943(0.2982)
2021-12-30 01:18:07,032 Epoch[091/310], Step[1100/1251], Loss: 4.4856(4.1726), Acc: 0.2598(0.2977)
2021-12-30 01:19:04,672 Epoch[091/310], Step[1150/1251], Loss: 3.9362(4.1734), Acc: 0.4736(0.2980)
2021-12-30 01:20:04,791 Epoch[091/310], Step[1200/1251], Loss: 4.6280(4.1736), Acc: 0.3457(0.2985)
2021-12-30 01:21:03,738 Epoch[091/310], Step[1250/1251], Loss: 4.3884(4.1744), Acc: 0.2598(0.2981)
2021-12-30 01:21:05,257 ----- Epoch[091/310], Train Loss: 4.1744, Train Acc: 0.2981, time: 1545.38, Best Val(epoch88) Acc@1: 0.6349
2021-12-30 01:21:05,445 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 01:21:05,445 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 01:21:05,529 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 01:21:05,530 Now training epoch 92. LR=0.000786
2021-12-30 01:22:12,647 Epoch[092/310], Step[0000/1251], Loss: 4.0834(4.0834), Acc: 0.4336(0.4336)
2021-12-30 01:23:10,534 Epoch[092/310], Step[0050/1251], Loss: 4.6295(4.1628), Acc: 0.2578(0.2743)
2021-12-30 01:24:07,509 Epoch[092/310], Step[0100/1251], Loss: 3.9620(4.2007), Acc: 0.2197(0.2815)
2021-12-30 01:25:05,958 Epoch[092/310], Step[0150/1251], Loss: 3.3076(4.1938), Acc: 0.2920(0.2824)
2021-12-30 01:26:04,536 Epoch[092/310], Step[0200/1251], Loss: 4.5027(4.1715), Acc: 0.1738(0.2883)
2021-12-30 01:27:02,684 Epoch[092/310], Step[0250/1251], Loss: 4.1006(4.1704), Acc: 0.3857(0.2926)
2021-12-30 01:28:01,221 Epoch[092/310], Step[0300/1251], Loss: 4.3867(4.1689), Acc: 0.3721(0.2950)
2021-12-30 01:28:58,863 Epoch[092/310], Step[0350/1251], Loss: 3.5770(4.1671), Acc: 0.2061(0.2947)
2021-12-30 01:29:56,971 Epoch[092/310], Step[0400/1251], Loss: 4.0270(4.1732), Acc: 0.0264(0.2919)
2021-12-30 01:30:54,757 Epoch[092/310], Step[0450/1251], Loss: 3.9833(4.1764), Acc: 0.2656(0.2929)
2021-12-30 01:31:53,511 Epoch[092/310], Step[0500/1251], Loss: 3.7433(4.1735), Acc: 0.3174(0.2936)
2021-12-30 01:32:51,781 Epoch[092/310], Step[0550/1251], Loss: 4.2582(4.1747), Acc: 0.3477(0.2934)
2021-12-30 01:33:51,762 Epoch[092/310], Step[0600/1251], Loss: 4.2220(4.1733), Acc: 0.3828(0.2927)
2021-12-30 01:34:50,640 Epoch[092/310], Step[0650/1251], Loss: 3.9722(4.1758), Acc: 0.3418(0.2935)
2021-12-30 01:35:49,356 Epoch[092/310], Step[0700/1251], Loss: 3.9267(4.1748), Acc: 0.2773(0.2950)
2021-12-30 01:36:47,990 Epoch[092/310], Step[0750/1251], Loss: 3.9074(4.1780), Acc: 0.2871(0.2944)
2021-12-30 01:37:47,838 Epoch[092/310], Step[0800/1251], Loss: 4.2674(4.1759), Acc: 0.3965(0.2942)
2021-12-30 01:38:45,983 Epoch[092/310], Step[0850/1251], Loss: 4.4094(4.1754), Acc: 0.2793(0.2947)
2021-12-30 01:39:45,688 Epoch[092/310], Step[0900/1251], Loss: 4.2818(4.1762), Acc: 0.1865(0.2926)
2021-12-30 01:40:44,467 Epoch[092/310], Step[0950/1251], Loss: 4.1513(4.1839), Acc: 0.2129(0.2910)
2021-12-30 01:41:43,821 Epoch[092/310], Step[1000/1251], Loss: 4.4417(4.1843), Acc: 0.1270(0.2909)
2021-12-30 01:42:43,426 Epoch[092/310], Step[1050/1251], Loss: 3.7753(4.1856), Acc: 0.4883(0.2902)
2021-12-30 01:43:42,281 Epoch[092/310], Step[1100/1251], Loss: 4.4282(4.1887), Acc: 0.2881(0.2902)
2021-12-30 01:44:40,646 Epoch[092/310], Step[1150/1251], Loss: 4.6032(4.1869), Acc: 0.1758(0.2899)
2021-12-30 01:45:38,586 Epoch[092/310], Step[1200/1251], Loss: 4.2006(4.1890), Acc: 0.2979(0.2901)
2021-12-30 01:46:35,933 Epoch[092/310], Step[1250/1251], Loss: 4.3420(4.1865), Acc: 0.2627(0.2906)
2021-12-30 01:46:37,383 ----- Validation after Epoch: 92
2021-12-30 01:47:31,248 Val Step[0000/1563], Loss: 0.6080 (0.6080), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2021-12-30 01:47:32,620 Val Step[0050/1563], Loss: 2.7891 (1.1031), Acc@1: 0.3750 (0.7727), Acc@5: 0.7500 (0.9240)
2021-12-30 01:47:33,890 Val Step[0100/1563], Loss: 1.9227 (1.4465), Acc@1: 0.4688 (0.6776), Acc@5: 0.8750 (0.8840)
2021-12-30 01:47:35,178 Val Step[0150/1563], Loss: 0.8554 (1.3415), Acc@1: 0.8438 (0.7047), Acc@5: 0.9062 (0.8953)
2021-12-30 01:47:36,452 Val Step[0200/1563], Loss: 1.2053 (1.3623), Acc@1: 0.7500 (0.7040), Acc@5: 0.9375 (0.8937)
2021-12-30 01:47:37,731 Val Step[0250/1563], Loss: 1.0615 (1.2812), Acc@1: 0.7500 (0.7229), Acc@5: 0.9688 (0.9046)
2021-12-30 01:47:38,984 Val Step[0300/1563], Loss: 1.1927 (1.3406), Acc@1: 0.7188 (0.7045), Acc@5: 0.9062 (0.9007)
2021-12-30 01:47:40,246 Val Step[0350/1563], Loss: 1.7113 (1.3554), Acc@1: 0.5312 (0.6988), Acc@5: 0.9062 (0.9021)
2021-12-30 01:47:41,509 Val Step[0400/1563], Loss: 1.2836 (1.3594), Acc@1: 0.8125 (0.6932), Acc@5: 0.9688 (0.9034)
2021-12-30 01:47:42,958 Val Step[0450/1563], Loss: 1.1693 (1.3658), Acc@1: 0.7188 (0.6892), Acc@5: 0.9688 (0.9034)
2021-12-30 01:47:44,388 Val Step[0500/1563], Loss: 0.5527 (1.3590), Acc@1: 0.8750 (0.6906), Acc@5: 1.0000 (0.9056)
2021-12-30 01:47:45,950 Val Step[0550/1563], Loss: 1.4015 (1.3244), Acc@1: 0.6250 (0.6996), Acc@5: 0.9375 (0.9094)
2021-12-30 01:47:47,382 Val Step[0600/1563], Loss: 0.8180 (1.3227), Acc@1: 0.7812 (0.7012), Acc@5: 0.9688 (0.9095)
2021-12-30 01:47:48,801 Val Step[0650/1563], Loss: 0.9659 (1.3433), Acc@1: 0.8125 (0.6971), Acc@5: 0.9375 (0.9062)
2021-12-30 01:47:50,247 Val Step[0700/1563], Loss: 1.8476 (1.3843), Acc@1: 0.6250 (0.6882), Acc@5: 0.8438 (0.9005)
2021-12-30 01:47:51,719 Val Step[0750/1563], Loss: 1.7792 (1.4243), Acc@1: 0.6875 (0.6809), Acc@5: 0.7812 (0.8941)
2021-12-30 01:47:53,124 Val Step[0800/1563], Loss: 1.2589 (1.4733), Acc@1: 0.7188 (0.6697), Acc@5: 0.9375 (0.8869)
2021-12-30 01:47:54,545 Val Step[0850/1563], Loss: 1.7926 (1.5046), Acc@1: 0.5312 (0.6628), Acc@5: 0.8750 (0.8826)
2021-12-30 01:47:55,869 Val Step[0900/1563], Loss: 0.5024 (1.5037), Acc@1: 0.9688 (0.6649), Acc@5: 0.9688 (0.8819)
2021-12-30 01:47:57,237 Val Step[0950/1563], Loss: 1.5383 (1.5308), Acc@1: 0.7812 (0.6597), Acc@5: 0.8438 (0.8775)
2021-12-30 01:47:58,498 Val Step[1000/1563], Loss: 0.6676 (1.5554), Acc@1: 0.9375 (0.6548), Acc@5: 0.9688 (0.8736)
2021-12-30 01:47:59,774 Val Step[1050/1563], Loss: 0.3997 (1.5715), Acc@1: 0.9375 (0.6516), Acc@5: 0.9688 (0.8713)
2021-12-30 01:48:01,039 Val Step[1100/1563], Loss: 1.8047 (1.5903), Acc@1: 0.6250 (0.6478), Acc@5: 0.8750 (0.8680)
2021-12-30 01:48:02,322 Val Step[1150/1563], Loss: 1.6886 (1.6103), Acc@1: 0.7188 (0.6444), Acc@5: 0.7812 (0.8647)
2021-12-30 01:48:03,635 Val Step[1200/1563], Loss: 1.5793 (1.6284), Acc@1: 0.7188 (0.6407), Acc@5: 0.8438 (0.8616)
2021-12-30 01:48:04,942 Val Step[1250/1563], Loss: 1.4313 (1.6453), Acc@1: 0.7500 (0.6383), Acc@5: 0.8750 (0.8588)
2021-12-30 01:48:06,205 Val Step[1300/1563], Loss: 1.1427 (1.6548), Acc@1: 0.8125 (0.6362), Acc@5: 0.8750 (0.8575)
2021-12-30 01:48:07,552 Val Step[1350/1563], Loss: 2.0909 (1.6726), Acc@1: 0.4062 (0.6327), Acc@5: 0.9062 (0.8543)
2021-12-30 01:48:08,808 Val Step[1400/1563], Loss: 1.4529 (1.6824), Acc@1: 0.7812 (0.6304), Acc@5: 0.9062 (0.8530)
2021-12-30 01:48:10,063 Val Step[1450/1563], Loss: 1.9904 (1.6904), Acc@1: 0.5625 (0.6290), Acc@5: 0.8438 (0.8520)
2021-12-30 01:48:11,322 Val Step[1500/1563], Loss: 1.9372 (1.6786), Acc@1: 0.5312 (0.6315), Acc@5: 0.8750 (0.8537)
2021-12-30 01:48:12,600 Val Step[1550/1563], Loss: 1.0831 (1.6780), Acc@1: 0.8750 (0.6313), Acc@5: 0.9062 (0.8538)
2021-12-30 01:48:13,355 ----- Epoch[092/310], Validation Loss: 1.6763, Validation Acc@1: 0.6316, Validation Acc@5: 0.8540, time: 95.97
2021-12-30 01:48:13,355 ----- Epoch[092/310], Train Loss: 4.1865, Train Acc: 0.2906, time: 1531.85, Best Val(epoch88) Acc@1: 0.6349
2021-12-30 01:48:13,543 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 01:48:13,543 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 01:48:13,648 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 01:48:13,649 Now training epoch 93. LR=0.000782
2021-12-30 01:49:31,179 Epoch[093/310], Step[0000/1251], Loss: 4.4475(4.4475), Acc: 0.1865(0.1865)
2021-12-30 01:50:29,694 Epoch[093/310], Step[0050/1251], Loss: 3.7279(4.1489), Acc: 0.2666(0.3028)
2021-12-30 01:51:29,739 Epoch[093/310], Step[0100/1251], Loss: 3.3079(4.1534), Acc: 0.3975(0.3048)
2021-12-30 01:52:29,656 Epoch[093/310], Step[0150/1251], Loss: 3.9705(4.1378), Acc: 0.2373(0.3025)
2021-12-30 01:53:28,405 Epoch[093/310], Step[0200/1251], Loss: 4.5514(4.1448), Acc: 0.2754(0.3064)
2021-12-30 01:54:28,396 Epoch[093/310], Step[0250/1251], Loss: 3.9835(4.1540), Acc: 0.3193(0.3028)
2021-12-30 01:55:27,819 Epoch[093/310], Step[0300/1251], Loss: 4.2422(4.1585), Acc: 0.3115(0.3026)
2021-12-30 01:56:27,454 Epoch[093/310], Step[0350/1251], Loss: 3.6999(4.1517), Acc: 0.1924(0.3035)
2021-12-30 01:57:26,356 Epoch[093/310], Step[0400/1251], Loss: 4.2299(4.1574), Acc: 0.3750(0.3012)
2021-12-30 01:58:25,657 Epoch[093/310], Step[0450/1251], Loss: 3.8459(4.1549), Acc: 0.2871(0.3002)
2021-12-30 01:59:24,702 Epoch[093/310], Step[0500/1251], Loss: 4.0696(4.1595), Acc: 0.4268(0.3017)
2021-12-30 02:00:24,578 Epoch[093/310], Step[0550/1251], Loss: 4.8308(4.1611), Acc: 0.2207(0.3017)
2021-12-30 02:01:25,426 Epoch[093/310], Step[0600/1251], Loss: 4.7879(4.1606), Acc: 0.2676(0.3014)
2021-12-30 02:02:25,829 Epoch[093/310], Step[0650/1251], Loss: 3.8083(4.1603), Acc: 0.4580(0.3014)
2021-12-30 02:03:25,937 Epoch[093/310], Step[0700/1251], Loss: 3.8304(4.1608), Acc: 0.2607(0.3011)
2021-12-30 02:04:25,420 Epoch[093/310], Step[0750/1251], Loss: 4.2592(4.1648), Acc: 0.1367(0.3009)
2021-12-30 02:05:25,998 Epoch[093/310], Step[0800/1251], Loss: 3.9870(4.1670), Acc: 0.0205(0.2991)
2021-12-30 02:06:26,326 Epoch[093/310], Step[0850/1251], Loss: 4.5314(4.1653), Acc: 0.2764(0.2989)
2021-12-30 02:07:25,670 Epoch[093/310], Step[0900/1251], Loss: 4.3704(4.1606), Acc: 0.1533(0.2978)
2021-12-30 02:08:24,845 Epoch[093/310], Step[0950/1251], Loss: 4.9729(4.1633), Acc: 0.2188(0.2973)
2021-12-30 02:09:24,368 Epoch[093/310], Step[1000/1251], Loss: 4.1188(4.1626), Acc: 0.3125(0.2966)
2021-12-30 02:10:22,668 Epoch[093/310], Step[1050/1251], Loss: 4.3005(4.1648), Acc: 0.3760(0.2969)
2021-12-30 02:11:23,265 Epoch[093/310], Step[1100/1251], Loss: 4.2261(4.1679), Acc: 0.3848(0.2965)
2021-12-30 02:12:23,347 Epoch[093/310], Step[1150/1251], Loss: 4.2906(4.1673), Acc: 0.3564(0.2964)
2021-12-30 02:13:22,561 Epoch[093/310], Step[1200/1251], Loss: 3.8676(4.1664), Acc: 0.4395(0.2969)
2021-12-30 02:14:22,109 Epoch[093/310], Step[1250/1251], Loss: 4.6035(4.1674), Acc: 0.2725(0.2965)
2021-12-30 02:14:23,600 ----- Epoch[093/310], Train Loss: 4.1674, Train Acc: 0.2965, time: 1569.95, Best Val(epoch88) Acc@1: 0.6349
2021-12-30 02:14:23,779 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 02:14:23,779 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 02:14:23,888 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 02:14:23,888 Now training epoch 94. LR=0.000778
2021-12-30 02:15:30,254 Epoch[094/310], Step[0000/1251], Loss: 3.8872(3.8872), Acc: 0.3486(0.3486)
2021-12-30 02:16:28,213 Epoch[094/310], Step[0050/1251], Loss: 4.1158(4.1238), Acc: 0.2930(0.3010)
2021-12-30 02:17:26,660 Epoch[094/310], Step[0100/1251], Loss: 4.1962(4.1867), Acc: 0.4209(0.2927)
2021-12-30 02:18:25,208 Epoch[094/310], Step[0150/1251], Loss: 3.7718(4.1765), Acc: 0.4697(0.2915)
2021-12-30 02:19:23,257 Epoch[094/310], Step[0200/1251], Loss: 3.9961(4.1779), Acc: 0.3809(0.2944)
2021-12-30 02:20:22,403 Epoch[094/310], Step[0250/1251], Loss: 3.6925(4.1725), Acc: 0.4814(0.2992)
2021-12-30 02:21:21,179 Epoch[094/310], Step[0300/1251], Loss: 3.9690(4.1633), Acc: 0.3164(0.3001)
2021-12-30 02:22:19,851 Epoch[094/310], Step[0350/1251], Loss: 4.4470(4.1636), Acc: 0.2734(0.2975)
2021-12-30 02:23:17,964 Epoch[094/310], Step[0400/1251], Loss: 3.5678(4.1732), Acc: 0.3613(0.2974)
2021-12-30 02:24:17,309 Epoch[094/310], Step[0450/1251], Loss: 4.2952(4.1721), Acc: 0.2676(0.2948)
2021-12-30 02:25:17,635 Epoch[094/310], Step[0500/1251], Loss: 4.2415(4.1801), Acc: 0.3223(0.2949)
2021-12-30 02:26:16,073 Epoch[094/310], Step[0550/1251], Loss: 4.1549(4.1801), Acc: 0.3789(0.2961)
2021-12-30 02:27:15,579 Epoch[094/310], Step[0600/1251], Loss: 4.4590(4.1810), Acc: 0.2598(0.2955)
2021-12-30 02:28:15,438 Epoch[094/310], Step[0650/1251], Loss: 4.0195(4.1764), Acc: 0.2100(0.2952)
2021-12-30 02:29:14,328 Epoch[094/310], Step[0700/1251], Loss: 4.3359(4.1711), Acc: 0.3447(0.2944)
2021-12-30 02:30:12,100 Epoch[094/310], Step[0750/1251], Loss: 3.9852(4.1676), Acc: 0.4043(0.2932)
2021-12-30 02:31:10,573 Epoch[094/310], Step[0800/1251], Loss: 3.8019(4.1693), Acc: 0.1934(0.2927)
2021-12-30 02:32:09,116 Epoch[094/310], Step[0850/1251], Loss: 4.3411(4.1715), Acc: 0.3115(0.2923)
2021-12-30 02:33:08,593 Epoch[094/310], Step[0900/1251], Loss: 4.4862(4.1715), Acc: 0.3105(0.2933)
2021-12-30 02:34:06,361 Epoch[094/310], Step[0950/1251], Loss: 4.8103(4.1718), Acc: 0.2354(0.2921)
2021-12-30 02:35:03,563 Epoch[094/310], Step[1000/1251], Loss: 4.5373(4.1749), Acc: 0.1709(0.2923)
2021-12-30 02:36:02,681 Epoch[094/310], Step[1050/1251], Loss: 4.4480(4.1728), Acc: 0.1963(0.2927)
2021-12-30 02:37:01,672 Epoch[094/310], Step[1100/1251], Loss: 4.3897(4.1721), Acc: 0.2510(0.2931)
2021-12-30 02:38:00,913 Epoch[094/310], Step[1150/1251], Loss: 4.0416(4.1684), Acc: 0.4463(0.2933)
2021-12-30 02:39:00,929 Epoch[094/310], Step[1200/1251], Loss: 4.3245(4.1703), Acc: 0.1846(0.2937)
2021-12-30 02:40:00,530 Epoch[094/310], Step[1250/1251], Loss: 4.3683(4.1709), Acc: 0.2119(0.2934)
2021-12-30 02:40:02,064 ----- Validation after Epoch: 94
2021-12-30 02:40:54,207 Val Step[0000/1563], Loss: 0.8625 (0.8625), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-30 02:40:55,542 Val Step[0050/1563], Loss: 2.9355 (1.0791), Acc@1: 0.2500 (0.7862), Acc@5: 0.7188 (0.9289)
2021-12-30 02:40:56,837 Val Step[0100/1563], Loss: 2.0086 (1.3972), Acc@1: 0.4688 (0.6946), Acc@5: 0.8438 (0.8951)
2021-12-30 02:40:58,125 Val Step[0150/1563], Loss: 0.7612 (1.3112), Acc@1: 0.8438 (0.7148), Acc@5: 0.9688 (0.9017)
2021-12-30 02:40:59,401 Val Step[0200/1563], Loss: 1.3500 (1.3282), Acc@1: 0.7500 (0.7177), Acc@5: 0.9375 (0.8988)
2021-12-30 02:41:00,701 Val Step[0250/1563], Loss: 0.9867 (1.2661), Acc@1: 0.8438 (0.7298), Acc@5: 1.0000 (0.9070)
2021-12-30 02:41:01,976 Val Step[0300/1563], Loss: 1.3482 (1.3264), Acc@1: 0.6562 (0.7081), Acc@5: 0.9688 (0.9034)
2021-12-30 02:41:03,313 Val Step[0350/1563], Loss: 1.2818 (1.3240), Acc@1: 0.7812 (0.7067), Acc@5: 0.9375 (0.9072)
2021-12-30 02:41:04,741 Val Step[0400/1563], Loss: 1.2021 (1.3245), Acc@1: 0.7188 (0.7015), Acc@5: 0.9688 (0.9085)
2021-12-30 02:41:06,187 Val Step[0450/1563], Loss: 0.8635 (1.3343), Acc@1: 0.7500 (0.6960), Acc@5: 1.0000 (0.9091)
2021-12-30 02:41:07,632 Val Step[0500/1563], Loss: 0.5058 (1.3261), Acc@1: 0.9062 (0.6983), Acc@5: 1.0000 (0.9102)
2021-12-30 02:41:09,114 Val Step[0550/1563], Loss: 1.0744 (1.2992), Acc@1: 0.7812 (0.7072), Acc@5: 0.9375 (0.9128)
2021-12-30 02:41:10,550 Val Step[0600/1563], Loss: 0.7635 (1.2980), Acc@1: 0.8438 (0.7087), Acc@5: 0.9688 (0.9126)
2021-12-30 02:41:11,991 Val Step[0650/1563], Loss: 0.9482 (1.3173), Acc@1: 0.8125 (0.7058), Acc@5: 1.0000 (0.9096)
2021-12-30 02:41:13,405 Val Step[0700/1563], Loss: 1.6739 (1.3591), Acc@1: 0.7188 (0.6966), Acc@5: 0.8750 (0.9038)
2021-12-30 02:41:14,831 Val Step[0750/1563], Loss: 1.6991 (1.3989), Acc@1: 0.6875 (0.6882), Acc@5: 0.8125 (0.8976)
2021-12-30 02:41:16,317 Val Step[0800/1563], Loss: 1.2204 (1.4417), Acc@1: 0.7188 (0.6785), Acc@5: 0.9688 (0.8920)
2021-12-30 02:41:17,729 Val Step[0850/1563], Loss: 1.8149 (1.4719), Acc@1: 0.5312 (0.6723), Acc@5: 0.8750 (0.8872)
2021-12-30 02:41:19,204 Val Step[0900/1563], Loss: 0.6833 (1.4728), Acc@1: 0.9062 (0.6738), Acc@5: 0.9375 (0.8863)
2021-12-30 02:41:20,671 Val Step[0950/1563], Loss: 1.4801 (1.4966), Acc@1: 0.6875 (0.6692), Acc@5: 0.8438 (0.8819)
2021-12-30 02:41:22,090 Val Step[1000/1563], Loss: 0.8269 (1.5241), Acc@1: 0.8750 (0.6627), Acc@5: 0.9688 (0.8777)
2021-12-30 02:41:23,457 Val Step[1050/1563], Loss: 0.4843 (1.5391), Acc@1: 0.9688 (0.6592), Acc@5: 0.9688 (0.8758)
2021-12-30 02:41:24,770 Val Step[1100/1563], Loss: 1.5407 (1.5548), Acc@1: 0.5625 (0.6559), Acc@5: 0.8438 (0.8735)
2021-12-30 02:41:26,036 Val Step[1150/1563], Loss: 1.7358 (1.5743), Acc@1: 0.7188 (0.6524), Acc@5: 0.7812 (0.8703)
2021-12-30 02:41:27,322 Val Step[1200/1563], Loss: 1.3304 (1.5915), Acc@1: 0.7812 (0.6490), Acc@5: 0.8438 (0.8670)
2021-12-30 02:41:28,648 Val Step[1250/1563], Loss: 1.0443 (1.6063), Acc@1: 0.8438 (0.6462), Acc@5: 0.9062 (0.8642)
2021-12-30 02:41:29,928 Val Step[1300/1563], Loss: 1.6701 (1.6197), Acc@1: 0.6875 (0.6438), Acc@5: 0.8438 (0.8627)
2021-12-30 02:41:31,187 Val Step[1350/1563], Loss: 2.1611 (1.6436), Acc@1: 0.3438 (0.6392), Acc@5: 0.7812 (0.8588)
2021-12-30 02:41:32,457 Val Step[1400/1563], Loss: 1.4252 (1.6522), Acc@1: 0.7188 (0.6370), Acc@5: 0.8750 (0.8574)
2021-12-30 02:41:33,763 Val Step[1450/1563], Loss: 1.5451 (1.6587), Acc@1: 0.7500 (0.6359), Acc@5: 0.9375 (0.8567)
2021-12-30 02:41:35,093 Val Step[1500/1563], Loss: 2.0757 (1.6493), Acc@1: 0.5000 (0.6380), Acc@5: 0.9062 (0.8581)
2021-12-30 02:41:36,417 Val Step[1550/1563], Loss: 1.0908 (1.6484), Acc@1: 0.8750 (0.6387), Acc@5: 0.9062 (0.8583)
2021-12-30 02:41:37,206 ----- Epoch[094/310], Validation Loss: 1.6461, Validation Acc@1: 0.6391, Validation Acc@5: 0.8587, time: 95.14
2021-12-30 02:41:37,206 ----- Epoch[094/310], Train Loss: 4.1709, Train Acc: 0.2934, time: 1538.17, Best Val(epoch94) Acc@1: 0.6391
2021-12-30 02:41:37,391 Max accuracy so far: 0.6391 at epoch_94
2021-12-30 02:41:37,392 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 02:41:37,392 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 02:41:37,502 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 02:41:37,880 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 02:41:37,880 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 02:41:38,013 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 02:41:38,014 Now training epoch 95. LR=0.000773
2021-12-30 02:42:43,604 Epoch[095/310], Step[0000/1251], Loss: 4.0912(4.0912), Acc: 0.3154(0.3154)
2021-12-30 02:43:42,183 Epoch[095/310], Step[0050/1251], Loss: 3.8819(4.1443), Acc: 0.1631(0.2942)
2021-12-30 02:44:40,111 Epoch[095/310], Step[0100/1251], Loss: 4.3164(4.1250), Acc: 0.3428(0.3110)
2021-12-30 02:45:39,429 Epoch[095/310], Step[0150/1251], Loss: 4.1335(4.1400), Acc: 0.2549(0.3104)
2021-12-30 02:46:36,958 Epoch[095/310], Step[0200/1251], Loss: 4.4162(4.1594), Acc: 0.3457(0.3037)
2021-12-30 02:47:35,695 Epoch[095/310], Step[0250/1251], Loss: 4.3712(4.1790), Acc: 0.3691(0.3044)
2021-12-30 02:48:35,722 Epoch[095/310], Step[0300/1251], Loss: 4.0381(4.1711), Acc: 0.2695(0.3073)
2021-12-30 02:49:34,159 Epoch[095/310], Step[0350/1251], Loss: 4.6307(4.1696), Acc: 0.1025(0.3074)
2021-12-30 02:50:32,094 Epoch[095/310], Step[0400/1251], Loss: 4.2335(4.1699), Acc: 0.4385(0.3086)
2021-12-30 02:51:31,914 Epoch[095/310], Step[0450/1251], Loss: 4.5084(4.1702), Acc: 0.2656(0.3085)
2021-12-30 02:52:30,614 Epoch[095/310], Step[0500/1251], Loss: 4.0532(4.1712), Acc: 0.3877(0.3061)
2021-12-30 02:53:27,963 Epoch[095/310], Step[0550/1251], Loss: 4.2627(4.1619), Acc: 0.3945(0.3075)
2021-12-30 02:54:27,350 Epoch[095/310], Step[0600/1251], Loss: 4.4341(4.1613), Acc: 0.2383(0.3070)
2021-12-30 02:55:26,760 Epoch[095/310], Step[0650/1251], Loss: 4.2323(4.1604), Acc: 0.0693(0.3054)
2021-12-30 02:56:27,023 Epoch[095/310], Step[0700/1251], Loss: 4.4884(4.1651), Acc: 0.2490(0.3040)
2021-12-30 02:57:26,954 Epoch[095/310], Step[0750/1251], Loss: 4.2008(4.1657), Acc: 0.1230(0.3034)
2021-12-30 02:58:27,045 Epoch[095/310], Step[0800/1251], Loss: 4.3601(4.1637), Acc: 0.3447(0.3045)
2021-12-30 02:59:27,230 Epoch[095/310], Step[0850/1251], Loss: 4.1937(4.1644), Acc: 0.3691(0.3040)
2021-12-30 03:00:25,324 Epoch[095/310], Step[0900/1251], Loss: 4.7387(4.1580), Acc: 0.2852(0.3050)
2021-12-30 03:01:23,852 Epoch[095/310], Step[0950/1251], Loss: 3.9035(4.1566), Acc: 0.2197(0.3043)
2021-12-30 03:02:22,108 Epoch[095/310], Step[1000/1251], Loss: 4.0796(4.1563), Acc: 0.1543(0.3042)
2021-12-30 03:03:21,772 Epoch[095/310], Step[1050/1251], Loss: 3.9716(4.1576), Acc: 0.2988(0.3037)
2021-12-30 03:04:21,880 Epoch[095/310], Step[1100/1251], Loss: 4.2858(4.1590), Acc: 0.3311(0.3035)
2021-12-30 03:05:21,230 Epoch[095/310], Step[1150/1251], Loss: 4.9578(4.1614), Acc: 0.2588(0.3022)
2021-12-30 03:06:21,873 Epoch[095/310], Step[1200/1251], Loss: 4.5539(4.1604), Acc: 0.2324(0.3020)
2021-12-30 03:07:20,597 Epoch[095/310], Step[1250/1251], Loss: 4.0881(4.1595), Acc: 0.2637(0.3018)
2021-12-30 03:07:22,108 ----- Epoch[095/310], Train Loss: 4.1595, Train Acc: 0.3018, time: 1544.09, Best Val(epoch94) Acc@1: 0.6391
2021-12-30 03:07:22,284 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 03:07:22,285 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 03:07:22,388 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 03:07:22,389 Now training epoch 96. LR=0.000769
2021-12-30 03:08:37,892 Epoch[096/310], Step[0000/1251], Loss: 3.8277(3.8277), Acc: 0.2578(0.2578)
2021-12-30 03:09:34,958 Epoch[096/310], Step[0050/1251], Loss: 4.1994(4.0108), Acc: 0.3867(0.2997)
2021-12-30 03:10:32,003 Epoch[096/310], Step[0100/1251], Loss: 4.2112(4.0885), Acc: 0.3555(0.3059)
2021-12-30 03:11:30,515 Epoch[096/310], Step[0150/1251], Loss: 3.8418(4.1005), Acc: 0.1494(0.3073)
2021-12-30 03:12:29,203 Epoch[096/310], Step[0200/1251], Loss: 3.8775(4.1156), Acc: 0.2861(0.3006)
2021-12-30 03:13:26,999 Epoch[096/310], Step[0250/1251], Loss: 4.3247(4.1200), Acc: 0.1504(0.3024)
2021-12-30 03:14:25,258 Epoch[096/310], Step[0300/1251], Loss: 4.1141(4.1379), Acc: 0.3320(0.3049)
2021-12-30 03:15:22,292 Epoch[096/310], Step[0350/1251], Loss: 4.3005(4.1366), Acc: 0.2803(0.3058)
2021-12-30 03:16:20,651 Epoch[096/310], Step[0400/1251], Loss: 4.1900(4.1504), Acc: 0.1680(0.3024)
2021-12-30 03:17:18,758 Epoch[096/310], Step[0450/1251], Loss: 3.7104(4.1595), Acc: 0.4375(0.3021)
2021-12-30 03:18:17,196 Epoch[096/310], Step[0500/1251], Loss: 3.8626(4.1581), Acc: 0.3408(0.3008)
2021-12-30 03:19:15,090 Epoch[096/310], Step[0550/1251], Loss: 4.3955(4.1650), Acc: 0.2305(0.3003)
2021-12-30 03:20:13,301 Epoch[096/310], Step[0600/1251], Loss: 4.2209(4.1658), Acc: 0.2441(0.2991)
2021-12-30 03:21:12,158 Epoch[096/310], Step[0650/1251], Loss: 4.4772(4.1636), Acc: 0.2168(0.2996)
2021-12-30 03:22:11,240 Epoch[096/310], Step[0700/1251], Loss: 3.6512(4.1593), Acc: 0.1387(0.2993)
2021-12-30 03:23:09,365 Epoch[096/310], Step[0750/1251], Loss: 4.3391(4.1543), Acc: 0.1992(0.2984)
2021-12-30 03:24:07,347 Epoch[096/310], Step[0800/1251], Loss: 3.3142(4.1526), Acc: 0.2549(0.2988)
2021-12-30 03:25:07,069 Epoch[096/310], Step[0850/1251], Loss: 4.1074(4.1563), Acc: 0.2881(0.2980)
2021-12-30 03:26:06,070 Epoch[096/310], Step[0900/1251], Loss: 4.4339(4.1575), Acc: 0.3291(0.2980)
2021-12-30 03:27:05,193 Epoch[096/310], Step[0950/1251], Loss: 3.9135(4.1583), Acc: 0.4580(0.2971)
2021-12-30 03:28:03,402 Epoch[096/310], Step[1000/1251], Loss: 4.4033(4.1583), Acc: 0.3037(0.2973)
2021-12-30 03:29:01,673 Epoch[096/310], Step[1050/1251], Loss: 4.7807(4.1588), Acc: 0.3350(0.2972)
2021-12-30 03:30:00,197 Epoch[096/310], Step[1100/1251], Loss: 4.4061(4.1599), Acc: 0.2666(0.2978)
2021-12-30 03:30:59,167 Epoch[096/310], Step[1150/1251], Loss: 3.4917(4.1571), Acc: 0.4854(0.2979)
2021-12-30 03:31:56,757 Epoch[096/310], Step[1200/1251], Loss: 3.7087(4.1563), Acc: 0.2314(0.2990)
2021-12-30 03:32:54,806 Epoch[096/310], Step[1250/1251], Loss: 3.8481(4.1567), Acc: 0.2832(0.2986)
2021-12-30 03:32:56,280 ----- Validation after Epoch: 96
2021-12-30 03:33:46,751 Val Step[0000/1563], Loss: 0.7768 (0.7768), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-30 03:33:48,260 Val Step[0050/1563], Loss: 2.7807 (1.0578), Acc@1: 0.3750 (0.7825), Acc@5: 0.8125 (0.9234)
2021-12-30 03:33:49,580 Val Step[0100/1563], Loss: 2.3696 (1.3634), Acc@1: 0.3750 (0.6989), Acc@5: 0.8438 (0.8877)
2021-12-30 03:33:50,943 Val Step[0150/1563], Loss: 0.9038 (1.2945), Acc@1: 0.8438 (0.7167), Acc@5: 0.9375 (0.8963)
2021-12-30 03:33:52,221 Val Step[0200/1563], Loss: 0.9836 (1.3210), Acc@1: 0.7812 (0.7138), Acc@5: 0.9375 (0.8949)
2021-12-30 03:33:53,649 Val Step[0250/1563], Loss: 1.0632 (1.2474), Acc@1: 0.7500 (0.7285), Acc@5: 0.9375 (0.9053)
2021-12-30 03:33:55,005 Val Step[0300/1563], Loss: 1.7074 (1.3038), Acc@1: 0.4688 (0.7074), Acc@5: 0.8750 (0.8996)
2021-12-30 03:33:56,281 Val Step[0350/1563], Loss: 1.7423 (1.3097), Acc@1: 0.5625 (0.7019), Acc@5: 0.9062 (0.9023)
2021-12-30 03:33:57,549 Val Step[0400/1563], Loss: 1.1443 (1.3115), Acc@1: 0.7188 (0.6947), Acc@5: 0.9688 (0.9044)
2021-12-30 03:33:58,926 Val Step[0450/1563], Loss: 1.1636 (1.3147), Acc@1: 0.6250 (0.6914), Acc@5: 0.9688 (0.9057)
2021-12-30 03:34:00,394 Val Step[0500/1563], Loss: 0.4546 (1.3108), Acc@1: 0.9375 (0.6944), Acc@5: 1.0000 (0.9067)
2021-12-30 03:34:01,881 Val Step[0550/1563], Loss: 0.9563 (1.2863), Acc@1: 0.7188 (0.7014), Acc@5: 0.9688 (0.9096)
2021-12-30 03:34:03,295 Val Step[0600/1563], Loss: 1.1596 (1.2873), Acc@1: 0.7812 (0.7027), Acc@5: 0.9375 (0.9095)
2021-12-30 03:34:04,636 Val Step[0650/1563], Loss: 0.9732 (1.3095), Acc@1: 0.7188 (0.6976), Acc@5: 0.9688 (0.9069)
2021-12-30 03:34:05,973 Val Step[0700/1563], Loss: 2.0003 (1.3482), Acc@1: 0.5938 (0.6892), Acc@5: 0.8125 (0.9008)
2021-12-30 03:34:07,229 Val Step[0750/1563], Loss: 1.6317 (1.3850), Acc@1: 0.6250 (0.6826), Acc@5: 0.8438 (0.8956)
2021-12-30 03:34:08,589 Val Step[0800/1563], Loss: 1.2365 (1.4270), Acc@1: 0.7812 (0.6733), Acc@5: 0.9375 (0.8899)
2021-12-30 03:34:09,828 Val Step[0850/1563], Loss: 1.6939 (1.4538), Acc@1: 0.5625 (0.6678), Acc@5: 0.8750 (0.8863)
2021-12-30 03:34:11,213 Val Step[0900/1563], Loss: 0.7128 (1.4579), Acc@1: 0.8750 (0.6688), Acc@5: 0.9375 (0.8846)
2021-12-30 03:34:12,642 Val Step[0950/1563], Loss: 1.6076 (1.4823), Acc@1: 0.6250 (0.6645), Acc@5: 0.8750 (0.8809)
2021-12-30 03:34:13,923 Val Step[1000/1563], Loss: 0.9103 (1.5081), Acc@1: 0.8125 (0.6591), Acc@5: 0.9375 (0.8766)
2021-12-30 03:34:15,240 Val Step[1050/1563], Loss: 0.5564 (1.5229), Acc@1: 0.9375 (0.6563), Acc@5: 0.9688 (0.8746)
2021-12-30 03:34:16,555 Val Step[1100/1563], Loss: 1.5517 (1.5424), Acc@1: 0.7500 (0.6529), Acc@5: 0.8438 (0.8714)
2021-12-30 03:34:17,860 Val Step[1150/1563], Loss: 1.5023 (1.5671), Acc@1: 0.7500 (0.6482), Acc@5: 0.7812 (0.8678)
2021-12-30 03:34:19,159 Val Step[1200/1563], Loss: 1.4978 (1.5835), Acc@1: 0.8125 (0.6453), Acc@5: 0.8438 (0.8648)
2021-12-30 03:34:20,502 Val Step[1250/1563], Loss: 1.2685 (1.6008), Acc@1: 0.7500 (0.6426), Acc@5: 0.9062 (0.8621)
2021-12-30 03:34:21,782 Val Step[1300/1563], Loss: 1.5924 (1.6127), Acc@1: 0.6875 (0.6398), Acc@5: 0.8438 (0.8607)
2021-12-30 03:34:23,055 Val Step[1350/1563], Loss: 2.3555 (1.6344), Acc@1: 0.3750 (0.6353), Acc@5: 0.7188 (0.8569)
2021-12-30 03:34:24,370 Val Step[1400/1563], Loss: 1.5086 (1.6443), Acc@1: 0.6875 (0.6329), Acc@5: 0.9375 (0.8555)
2021-12-30 03:34:25,685 Val Step[1450/1563], Loss: 1.6930 (1.6509), Acc@1: 0.6250 (0.6315), Acc@5: 0.9375 (0.8550)
2021-12-30 03:34:27,068 Val Step[1500/1563], Loss: 1.7829 (1.6392), Acc@1: 0.6250 (0.6343), Acc@5: 0.9062 (0.8566)
2021-12-30 03:34:28,513 Val Step[1550/1563], Loss: 1.2611 (1.6372), Acc@1: 0.8438 (0.6346), Acc@5: 0.9062 (0.8572)
2021-12-30 03:34:29,301 ----- Epoch[096/310], Validation Loss: 1.6353, Validation Acc@1: 0.6349, Validation Acc@5: 0.8574, time: 93.02
2021-12-30 03:34:29,301 ----- Epoch[096/310], Train Loss: 4.1567, Train Acc: 0.2986, time: 1533.89, Best Val(epoch94) Acc@1: 0.6391
2021-12-30 03:34:29,489 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 03:34:29,490 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 03:34:29,598 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 03:34:29,599 Now training epoch 97. LR=0.000765
2021-12-30 03:35:37,624 Epoch[097/310], Step[0000/1251], Loss: 4.2553(4.2553), Acc: 0.2715(0.2715)
2021-12-30 03:36:35,677 Epoch[097/310], Step[0050/1251], Loss: 4.3853(4.1777), Acc: 0.3154(0.2915)
2021-12-30 03:37:34,585 Epoch[097/310], Step[0100/1251], Loss: 4.0175(4.1652), Acc: 0.1377(0.2898)
2021-12-30 03:38:34,954 Epoch[097/310], Step[0150/1251], Loss: 4.0342(4.1610), Acc: 0.2646(0.2903)
2021-12-30 03:39:33,925 Epoch[097/310], Step[0200/1251], Loss: 4.5475(4.1606), Acc: 0.1895(0.2947)
2021-12-30 03:40:33,334 Epoch[097/310], Step[0250/1251], Loss: 4.3550(4.1585), Acc: 0.3604(0.2903)
2021-12-30 03:41:32,497 Epoch[097/310], Step[0300/1251], Loss: 4.2537(4.1616), Acc: 0.4062(0.2948)
2021-12-30 03:42:32,002 Epoch[097/310], Step[0350/1251], Loss: 3.5898(4.1528), Acc: 0.2061(0.2933)
2021-12-30 03:43:31,281 Epoch[097/310], Step[0400/1251], Loss: 4.3544(4.1488), Acc: 0.3730(0.2936)
2021-12-30 03:44:31,101 Epoch[097/310], Step[0450/1251], Loss: 4.0811(4.1492), Acc: 0.2744(0.2951)
2021-12-30 03:45:31,177 Epoch[097/310], Step[0500/1251], Loss: 4.0960(4.1491), Acc: 0.2578(0.2955)
2021-12-30 03:46:30,157 Epoch[097/310], Step[0550/1251], Loss: 4.1537(4.1536), Acc: 0.3545(0.2973)
2021-12-30 03:47:30,062 Epoch[097/310], Step[0600/1251], Loss: 4.2128(4.1540), Acc: 0.1699(0.2966)
2021-12-30 03:48:29,484 Epoch[097/310], Step[0650/1251], Loss: 4.3375(4.1526), Acc: 0.1631(0.2960)
2021-12-30 03:49:28,566 Epoch[097/310], Step[0700/1251], Loss: 3.9834(4.1505), Acc: 0.4814(0.2968)
2021-12-30 03:50:27,547 Epoch[097/310], Step[0750/1251], Loss: 3.8581(4.1494), Acc: 0.4658(0.2991)
2021-12-30 03:51:25,888 Epoch[097/310], Step[0800/1251], Loss: 3.9850(4.1500), Acc: 0.4375(0.2981)
2021-12-30 03:52:25,250 Epoch[097/310], Step[0850/1251], Loss: 4.0639(4.1493), Acc: 0.3506(0.2994)
2021-12-30 03:53:24,562 Epoch[097/310], Step[0900/1251], Loss: 4.3295(4.1452), Acc: 0.3643(0.3000)
2021-12-30 03:54:23,934 Epoch[097/310], Step[0950/1251], Loss: 3.7431(4.1444), Acc: 0.4854(0.3008)
2021-12-30 03:55:24,037 Epoch[097/310], Step[1000/1251], Loss: 3.6925(4.1447), Acc: 0.2734(0.2996)
2021-12-30 03:56:24,476 Epoch[097/310], Step[1050/1251], Loss: 4.1512(4.1481), Acc: 0.2383(0.2988)
2021-12-30 03:57:24,040 Epoch[097/310], Step[1100/1251], Loss: 4.3437(4.1519), Acc: 0.2793(0.2981)
2021-12-30 03:58:23,419 Epoch[097/310], Step[1150/1251], Loss: 3.4597(4.1512), Acc: 0.3887(0.2969)
2021-12-30 03:59:22,990 Epoch[097/310], Step[1200/1251], Loss: 4.6490(4.1541), Acc: 0.2051(0.2973)
2021-12-30 04:00:23,510 Epoch[097/310], Step[1250/1251], Loss: 4.1254(4.1556), Acc: 0.4160(0.2964)
2021-12-30 04:00:25,785 ----- Epoch[097/310], Train Loss: 4.1556, Train Acc: 0.2964, time: 1556.18, Best Val(epoch94) Acc@1: 0.6391
2021-12-30 04:00:25,960 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 04:00:25,961 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 04:00:26,061 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 04:00:26,062 Now training epoch 98. LR=0.000760
2021-12-30 04:01:34,157 Epoch[098/310], Step[0000/1251], Loss: 4.0620(4.0620), Acc: 0.1963(0.1963)
2021-12-30 04:02:32,389 Epoch[098/310], Step[0050/1251], Loss: 4.3701(4.2001), Acc: 0.3994(0.3045)
2021-12-30 04:03:31,002 Epoch[098/310], Step[0100/1251], Loss: 4.0808(4.1868), Acc: 0.4121(0.2871)
2021-12-30 04:04:30,010 Epoch[098/310], Step[0150/1251], Loss: 3.8600(4.1421), Acc: 0.3281(0.2981)
2021-12-30 04:05:28,357 Epoch[098/310], Step[0200/1251], Loss: 3.9497(4.1346), Acc: 0.2871(0.3022)
2021-12-30 04:06:28,418 Epoch[098/310], Step[0250/1251], Loss: 3.7644(4.1328), Acc: 0.0928(0.2986)
2021-12-30 04:07:27,539 Epoch[098/310], Step[0300/1251], Loss: 3.8107(4.1352), Acc: 0.3184(0.2969)
2021-12-30 04:08:26,425 Epoch[098/310], Step[0350/1251], Loss: 3.3500(4.1341), Acc: 0.3945(0.2983)
2021-12-30 04:09:26,861 Epoch[098/310], Step[0400/1251], Loss: 3.9047(4.1310), Acc: 0.3750(0.3012)
2021-12-30 04:10:27,204 Epoch[098/310], Step[0450/1251], Loss: 4.1604(4.1319), Acc: 0.3145(0.3020)
2021-12-30 04:11:26,484 Epoch[098/310], Step[0500/1251], Loss: 4.4749(4.1354), Acc: 0.3330(0.3012)
2021-12-30 04:12:25,442 Epoch[098/310], Step[0550/1251], Loss: 3.8829(4.1383), Acc: 0.4453(0.2999)
2021-12-30 04:13:25,851 Epoch[098/310], Step[0600/1251], Loss: 4.4607(4.1420), Acc: 0.3145(0.2995)
2021-12-30 04:14:23,868 Epoch[098/310], Step[0650/1251], Loss: 4.1157(4.1417), Acc: 0.1650(0.2997)
2021-12-30 04:15:23,033 Epoch[098/310], Step[0700/1251], Loss: 4.3279(4.1366), Acc: 0.2725(0.2997)
2021-12-30 04:16:22,539 Epoch[098/310], Step[0750/1251], Loss: 4.4323(4.1381), Acc: 0.2412(0.3004)
2021-12-30 04:17:21,452 Epoch[098/310], Step[0800/1251], Loss: 4.4446(4.1351), Acc: 0.3477(0.3000)
2021-12-30 04:18:19,029 Epoch[098/310], Step[0850/1251], Loss: 4.0884(4.1324), Acc: 0.3750(0.2998)
2021-12-30 04:19:18,283 Epoch[098/310], Step[0900/1251], Loss: 4.6573(4.1363), Acc: 0.3350(0.2986)
2021-12-30 04:20:15,927 Epoch[098/310], Step[0950/1251], Loss: 3.8492(4.1363), Acc: 0.4053(0.2979)
2021-12-30 04:21:15,190 Epoch[098/310], Step[1000/1251], Loss: 3.6176(4.1370), Acc: 0.4736(0.2980)
2021-12-30 04:22:14,325 Epoch[098/310], Step[1050/1251], Loss: 4.3667(4.1358), Acc: 0.1387(0.2975)
2021-12-30 04:23:13,121 Epoch[098/310], Step[1100/1251], Loss: 3.9226(4.1384), Acc: 0.4492(0.2976)
2021-12-30 04:24:10,918 Epoch[098/310], Step[1150/1251], Loss: 4.4790(4.1377), Acc: 0.2676(0.2990)
2021-12-30 04:25:09,723 Epoch[098/310], Step[1200/1251], Loss: 3.7864(4.1389), Acc: 0.3301(0.2991)
2021-12-30 04:26:08,661 Epoch[098/310], Step[1250/1251], Loss: 3.9441(4.1427), Acc: 0.3242(0.2985)
2021-12-30 04:26:10,158 ----- Validation after Epoch: 98
2021-12-30 04:26:58,624 Val Step[0000/1563], Loss: 0.7298 (0.7298), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-30 04:26:59,930 Val Step[0050/1563], Loss: 2.8275 (0.9836), Acc@1: 0.3750 (0.7923), Acc@5: 0.6875 (0.9375)
2021-12-30 04:27:01,199 Val Step[0100/1563], Loss: 2.2201 (1.3367), Acc@1: 0.3125 (0.6934), Acc@5: 0.7812 (0.9004)
2021-12-30 04:27:02,579 Val Step[0150/1563], Loss: 0.6023 (1.2534), Acc@1: 0.8750 (0.7142), Acc@5: 0.9688 (0.9073)
2021-12-30 04:27:03,880 Val Step[0200/1563], Loss: 1.3789 (1.2962), Acc@1: 0.6250 (0.7121), Acc@5: 0.9375 (0.9007)
2021-12-30 04:27:05,161 Val Step[0250/1563], Loss: 1.1990 (1.2212), Acc@1: 0.7188 (0.7301), Acc@5: 0.9375 (0.9099)
2021-12-30 04:27:06,457 Val Step[0300/1563], Loss: 1.8713 (1.2924), Acc@1: 0.5000 (0.7085), Acc@5: 0.7812 (0.9031)
2021-12-30 04:27:07,868 Val Step[0350/1563], Loss: 2.0017 (1.3029), Acc@1: 0.4375 (0.7036), Acc@5: 0.8750 (0.9054)
2021-12-30 04:27:09,249 Val Step[0400/1563], Loss: 1.3240 (1.3113), Acc@1: 0.7500 (0.6968), Acc@5: 0.9062 (0.9060)
2021-12-30 04:27:10,749 Val Step[0450/1563], Loss: 1.1523 (1.3085), Acc@1: 0.6250 (0.6957), Acc@5: 0.9688 (0.9083)
2021-12-30 04:27:12,182 Val Step[0500/1563], Loss: 0.6685 (1.3004), Acc@1: 0.8125 (0.6980), Acc@5: 1.0000 (0.9100)
2021-12-30 04:27:13,663 Val Step[0550/1563], Loss: 0.8501 (1.2774), Acc@1: 0.7812 (0.7054), Acc@5: 0.9375 (0.9123)
2021-12-30 04:27:15,083 Val Step[0600/1563], Loss: 0.7165 (1.2786), Acc@1: 0.8750 (0.7056), Acc@5: 0.9688 (0.9123)
2021-12-30 04:27:16,501 Val Step[0650/1563], Loss: 1.4380 (1.3002), Acc@1: 0.6875 (0.7009), Acc@5: 0.9375 (0.9093)
2021-12-30 04:27:17,845 Val Step[0700/1563], Loss: 1.2603 (1.3370), Acc@1: 0.7500 (0.6933), Acc@5: 0.9062 (0.9039)
2021-12-30 04:27:19,196 Val Step[0750/1563], Loss: 2.1286 (1.3787), Acc@1: 0.5000 (0.6849), Acc@5: 0.7500 (0.8980)
2021-12-30 04:27:20,465 Val Step[0800/1563], Loss: 1.3975 (1.4195), Acc@1: 0.6875 (0.6756), Acc@5: 0.9062 (0.8922)
2021-12-30 04:27:21,726 Val Step[0850/1563], Loss: 1.6153 (1.4519), Acc@1: 0.5625 (0.6691), Acc@5: 0.9062 (0.8877)
2021-12-30 04:27:22,999 Val Step[0900/1563], Loss: 0.4804 (1.4544), Acc@1: 0.9375 (0.6706), Acc@5: 1.0000 (0.8867)
2021-12-30 04:27:24,375 Val Step[0950/1563], Loss: 1.9203 (1.4806), Acc@1: 0.6250 (0.6660), Acc@5: 0.7812 (0.8828)
2021-12-30 04:27:25,654 Val Step[1000/1563], Loss: 0.8482 (1.5087), Acc@1: 0.8438 (0.6603), Acc@5: 0.9688 (0.8783)
2021-12-30 04:27:26,952 Val Step[1050/1563], Loss: 0.5158 (1.5260), Acc@1: 0.9375 (0.6573), Acc@5: 0.9688 (0.8760)
2021-12-30 04:27:28,282 Val Step[1100/1563], Loss: 1.6175 (1.5448), Acc@1: 0.6875 (0.6543), Acc@5: 0.8125 (0.8729)
2021-12-30 04:27:29,558 Val Step[1150/1563], Loss: 1.8350 (1.5660), Acc@1: 0.6875 (0.6504), Acc@5: 0.7812 (0.8695)
2021-12-30 04:27:30,825 Val Step[1200/1563], Loss: 1.5371 (1.5809), Acc@1: 0.8125 (0.6479), Acc@5: 0.8438 (0.8670)
2021-12-30 04:27:32,102 Val Step[1250/1563], Loss: 1.1205 (1.5959), Acc@1: 0.8125 (0.6457), Acc@5: 0.9062 (0.8644)
2021-12-30 04:27:33,380 Val Step[1300/1563], Loss: 1.2905 (1.6088), Acc@1: 0.7500 (0.6428), Acc@5: 0.8750 (0.8627)
2021-12-30 04:27:34,652 Val Step[1350/1563], Loss: 2.0792 (1.6296), Acc@1: 0.4062 (0.6384), Acc@5: 0.8125 (0.8592)
2021-12-30 04:27:35,991 Val Step[1400/1563], Loss: 1.3732 (1.6375), Acc@1: 0.7500 (0.6367), Acc@5: 0.9062 (0.8577)
2021-12-30 04:27:37,403 Val Step[1450/1563], Loss: 1.9700 (1.6434), Acc@1: 0.5000 (0.6352), Acc@5: 0.8125 (0.8567)
2021-12-30 04:27:38,751 Val Step[1500/1563], Loss: 1.9396 (1.6294), Acc@1: 0.5000 (0.6379), Acc@5: 0.8750 (0.8590)
2021-12-30 04:27:40,017 Val Step[1550/1563], Loss: 1.0158 (1.6270), Acc@1: 0.8750 (0.6385), Acc@5: 0.9062 (0.8592)
2021-12-30 04:27:40,758 ----- Epoch[098/310], Validation Loss: 1.6250, Validation Acc@1: 0.6388, Validation Acc@5: 0.8595, time: 90.60
2021-12-30 04:27:40,759 ----- Epoch[098/310], Train Loss: 4.1427, Train Acc: 0.2985, time: 1544.09, Best Val(epoch94) Acc@1: 0.6391
2021-12-30 04:27:40,945 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 04:27:40,945 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 04:27:41,053 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 04:27:41,054 Now training epoch 99. LR=0.000756
2021-12-30 04:28:50,259 Epoch[099/310], Step[0000/1251], Loss: 4.2230(4.2230), Acc: 0.3486(0.3486)
2021-12-30 04:29:48,055 Epoch[099/310], Step[0050/1251], Loss: 4.2327(4.1463), Acc: 0.2227(0.2990)
2021-12-30 04:30:45,556 Epoch[099/310], Step[0100/1251], Loss: 4.3978(4.1648), Acc: 0.2441(0.2942)
2021-12-30 04:31:43,477 Epoch[099/310], Step[0150/1251], Loss: 4.5103(4.1706), Acc: 0.3730(0.2948)
2021-12-30 04:32:41,476 Epoch[099/310], Step[0200/1251], Loss: 3.2925(4.1703), Acc: 0.3320(0.2956)
2021-12-30 04:33:38,833 Epoch[099/310], Step[0250/1251], Loss: 3.9330(4.1786), Acc: 0.4170(0.2963)
2021-12-30 04:34:37,350 Epoch[099/310], Step[0300/1251], Loss: 4.2007(4.1723), Acc: 0.4199(0.2946)
2021-12-30 04:35:35,822 Epoch[099/310], Step[0350/1251], Loss: 4.0382(4.1691), Acc: 0.1875(0.2947)
2021-12-30 04:36:34,176 Epoch[099/310], Step[0400/1251], Loss: 4.2327(4.1697), Acc: 0.4180(0.2971)
2021-12-30 04:37:32,599 Epoch[099/310], Step[0450/1251], Loss: 4.0330(4.1776), Acc: 0.4639(0.2976)
2021-12-30 04:38:31,956 Epoch[099/310], Step[0500/1251], Loss: 4.1706(4.1736), Acc: 0.3301(0.2980)
2021-12-30 04:39:31,155 Epoch[099/310], Step[0550/1251], Loss: 4.1011(4.1686), Acc: 0.4180(0.2972)
2021-12-30 04:40:29,871 Epoch[099/310], Step[0600/1251], Loss: 4.3676(4.1641), Acc: 0.1641(0.2977)
2021-12-30 04:41:26,464 Epoch[099/310], Step[0650/1251], Loss: 4.8156(4.1589), Acc: 0.2314(0.2982)
2021-12-30 04:42:23,955 Epoch[099/310], Step[0700/1251], Loss: 3.8351(4.1593), Acc: 0.3086(0.2963)
2021-12-30 04:43:20,977 Epoch[099/310], Step[0750/1251], Loss: 4.2603(4.1583), Acc: 0.3350(0.2956)
2021-12-30 04:44:19,451 Epoch[099/310], Step[0800/1251], Loss: 4.8254(4.1625), Acc: 0.2363(0.2956)
2021-12-30 04:45:17,314 Epoch[099/310], Step[0850/1251], Loss: 3.8580(4.1647), Acc: 0.3105(0.2958)
2021-12-30 04:46:16,140 Epoch[099/310], Step[0900/1251], Loss: 4.0655(4.1643), Acc: 0.1787(0.2955)
2021-12-30 04:47:14,702 Epoch[099/310], Step[0950/1251], Loss: 3.8266(4.1629), Acc: 0.4941(0.2962)
2021-12-30 04:48:13,540 Epoch[099/310], Step[1000/1251], Loss: 4.9091(4.1599), Acc: 0.2305(0.2960)
2021-12-30 04:49:11,697 Epoch[099/310], Step[1050/1251], Loss: 3.8428(4.1607), Acc: 0.3613(0.2962)
2021-12-30 04:50:12,168 Epoch[099/310], Step[1100/1251], Loss: 4.1008(4.1622), Acc: 0.1787(0.2952)
2021-12-30 04:51:10,676 Epoch[099/310], Step[1150/1251], Loss: 4.6035(4.1658), Acc: 0.1680(0.2954)
2021-12-30 04:52:10,396 Epoch[099/310], Step[1200/1251], Loss: 3.6693(4.1637), Acc: 0.1055(0.2955)
2021-12-30 04:53:10,366 Epoch[099/310], Step[1250/1251], Loss: 3.4969(4.1661), Acc: 0.2793(0.2953)
2021-12-30 04:53:11,811 ----- Epoch[099/310], Train Loss: 4.1661, Train Acc: 0.2953, time: 1530.75, Best Val(epoch94) Acc@1: 0.6391
2021-12-30 04:53:11,992 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 04:53:11,993 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 04:53:12,083 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 04:53:12,084 Now training epoch 100. LR=0.000751
2021-12-30 04:54:16,733 Epoch[100/310], Step[0000/1251], Loss: 4.1634(4.1634), Acc: 0.3906(0.3906)
2021-12-30 04:55:15,453 Epoch[100/310], Step[0050/1251], Loss: 4.1043(4.1985), Acc: 0.2139(0.3061)
2021-12-30 04:56:13,132 Epoch[100/310], Step[0100/1251], Loss: 4.3809(4.1797), Acc: 0.3760(0.3012)
2021-12-30 04:57:10,301 Epoch[100/310], Step[0150/1251], Loss: 4.3071(4.1572), Acc: 0.1943(0.3017)
2021-12-30 04:58:08,530 Epoch[100/310], Step[0200/1251], Loss: 4.2552(4.1511), Acc: 0.1553(0.2952)
2021-12-30 04:59:06,674 Epoch[100/310], Step[0250/1251], Loss: 4.0223(4.1432), Acc: 0.3379(0.2954)
2021-12-30 05:00:04,511 Epoch[100/310], Step[0300/1251], Loss: 3.9770(4.1360), Acc: 0.3535(0.2985)
2021-12-30 05:01:03,256 Epoch[100/310], Step[0350/1251], Loss: 3.9739(4.1332), Acc: 0.3818(0.2993)
2021-12-30 05:02:01,580 Epoch[100/310], Step[0400/1251], Loss: 3.7819(4.1329), Acc: 0.4561(0.2987)
2021-12-30 05:03:00,253 Epoch[100/310], Step[0450/1251], Loss: 4.1002(4.1463), Acc: 0.3193(0.2990)
2021-12-30 05:03:58,598 Epoch[100/310], Step[0500/1251], Loss: 4.2790(4.1476), Acc: 0.3174(0.2946)
2021-12-30 05:04:56,874 Epoch[100/310], Step[0550/1251], Loss: 4.0973(4.1517), Acc: 0.2676(0.2942)
2021-12-30 05:05:56,151 Epoch[100/310], Step[0600/1251], Loss: 4.7064(4.1509), Acc: 0.2002(0.2945)
2021-12-30 05:06:56,026 Epoch[100/310], Step[0650/1251], Loss: 4.0925(4.1531), Acc: 0.1240(0.2937)
2021-12-30 05:07:54,691 Epoch[100/310], Step[0700/1251], Loss: 3.9652(4.1530), Acc: 0.1318(0.2934)
2021-12-30 05:08:54,172 Epoch[100/310], Step[0750/1251], Loss: 4.2467(4.1494), Acc: 0.2607(0.2948)
2021-12-30 05:09:54,025 Epoch[100/310], Step[0800/1251], Loss: 3.8127(4.1507), Acc: 0.4766(0.2949)
2021-12-30 05:10:53,547 Epoch[100/310], Step[0850/1251], Loss: 3.9230(4.1540), Acc: 0.3291(0.2951)
2021-12-30 05:11:53,148 Epoch[100/310], Step[0900/1251], Loss: 4.3108(4.1541), Acc: 0.4199(0.2956)
2021-12-30 05:12:52,647 Epoch[100/310], Step[0950/1251], Loss: 4.1506(4.1589), Acc: 0.2021(0.2943)
2021-12-30 05:13:52,744 Epoch[100/310], Step[1000/1251], Loss: 4.0801(4.1555), Acc: 0.3672(0.2935)
2021-12-30 05:14:52,162 Epoch[100/310], Step[1050/1251], Loss: 3.9413(4.1575), Acc: 0.3223(0.2938)
2021-12-30 05:15:52,028 Epoch[100/310], Step[1100/1251], Loss: 3.7373(4.1578), Acc: 0.3516(0.2932)
2021-12-30 05:16:52,688 Epoch[100/310], Step[1150/1251], Loss: 3.9127(4.1572), Acc: 0.1973(0.2938)
2021-12-30 05:17:52,940 Epoch[100/310], Step[1200/1251], Loss: 4.3552(4.1568), Acc: 0.1797(0.2941)
2021-12-30 05:18:51,765 Epoch[100/310], Step[1250/1251], Loss: 4.6353(4.1539), Acc: 0.3516(0.2955)
2021-12-30 05:18:53,337 ----- Validation after Epoch: 100
2021-12-30 05:19:42,348 Val Step[0000/1563], Loss: 1.1239 (1.1239), Acc@1: 0.8125 (0.8125), Acc@5: 0.9375 (0.9375)
2021-12-30 05:19:43,722 Val Step[0050/1563], Loss: 2.6174 (0.9610), Acc@1: 0.3750 (0.7966), Acc@5: 0.8438 (0.9393)
2021-12-30 05:19:45,008 Val Step[0100/1563], Loss: 2.1824 (1.3169), Acc@1: 0.3750 (0.6974), Acc@5: 0.8125 (0.8994)
2021-12-30 05:19:46,287 Val Step[0150/1563], Loss: 0.7675 (1.2345), Acc@1: 0.8438 (0.7177), Acc@5: 0.9062 (0.9050)
2021-12-30 05:19:47,544 Val Step[0200/1563], Loss: 1.3543 (1.2441), Acc@1: 0.7188 (0.7201), Acc@5: 0.9375 (0.9036)
2021-12-30 05:19:48,915 Val Step[0250/1563], Loss: 0.9029 (1.1866), Acc@1: 0.7812 (0.7339), Acc@5: 0.9375 (0.9112)
2021-12-30 05:19:50,191 Val Step[0300/1563], Loss: 1.3604 (1.2578), Acc@1: 0.6875 (0.7122), Acc@5: 0.8750 (0.9045)
2021-12-30 05:19:51,495 Val Step[0350/1563], Loss: 1.1744 (1.2659), Acc@1: 0.7812 (0.7083), Acc@5: 0.9062 (0.9064)
2021-12-30 05:19:52,837 Val Step[0400/1563], Loss: 1.2466 (1.2652), Acc@1: 0.7188 (0.7035), Acc@5: 0.9375 (0.9088)
2021-12-30 05:19:54,138 Val Step[0450/1563], Loss: 0.9679 (1.2666), Acc@1: 0.7188 (0.7004), Acc@5: 1.0000 (0.9106)
2021-12-30 05:19:55,434 Val Step[0500/1563], Loss: 0.5385 (1.2555), Acc@1: 0.8750 (0.7028), Acc@5: 1.0000 (0.9126)
2021-12-30 05:19:56,825 Val Step[0550/1563], Loss: 0.9636 (1.2311), Acc@1: 0.7500 (0.7102), Acc@5: 0.9375 (0.9156)
2021-12-30 05:19:58,134 Val Step[0600/1563], Loss: 0.6344 (1.2337), Acc@1: 0.8750 (0.7100), Acc@5: 0.9688 (0.9150)
2021-12-30 05:19:59,601 Val Step[0650/1563], Loss: 0.9061 (1.2578), Acc@1: 0.8438 (0.7055), Acc@5: 1.0000 (0.9112)
2021-12-30 05:20:00,915 Val Step[0700/1563], Loss: 1.2735 (1.3002), Acc@1: 0.7188 (0.6963), Acc@5: 0.9062 (0.9054)
2021-12-30 05:20:02,206 Val Step[0750/1563], Loss: 1.9191 (1.3373), Acc@1: 0.5312 (0.6893), Acc@5: 0.7500 (0.9003)
2021-12-30 05:20:03,528 Val Step[0800/1563], Loss: 1.2190 (1.3785), Acc@1: 0.6875 (0.6808), Acc@5: 0.9688 (0.8949)
2021-12-30 05:20:04,854 Val Step[0850/1563], Loss: 1.7923 (1.4088), Acc@1: 0.5625 (0.6744), Acc@5: 0.8750 (0.8912)
2021-12-30 05:20:06,193 Val Step[0900/1563], Loss: 0.6894 (1.4139), Acc@1: 0.8750 (0.6754), Acc@5: 0.9375 (0.8895)
2021-12-30 05:20:07,673 Val Step[0950/1563], Loss: 1.8458 (1.4409), Acc@1: 0.6250 (0.6703), Acc@5: 0.7812 (0.8855)
2021-12-30 05:20:09,080 Val Step[1000/1563], Loss: 0.7108 (1.4716), Acc@1: 0.9375 (0.6641), Acc@5: 0.9688 (0.8807)
2021-12-30 05:20:10,474 Val Step[1050/1563], Loss: 0.5027 (1.4875), Acc@1: 0.9375 (0.6614), Acc@5: 0.9688 (0.8788)
2021-12-30 05:20:11,890 Val Step[1100/1563], Loss: 1.3468 (1.5070), Acc@1: 0.6875 (0.6574), Acc@5: 0.8438 (0.8757)
2021-12-30 05:20:13,185 Val Step[1150/1563], Loss: 1.6324 (1.5249), Acc@1: 0.7188 (0.6549), Acc@5: 0.7812 (0.8728)
2021-12-30 05:20:14,456 Val Step[1200/1563], Loss: 1.5586 (1.5437), Acc@1: 0.7188 (0.6517), Acc@5: 0.8438 (0.8697)
2021-12-30 05:20:15,739 Val Step[1250/1563], Loss: 1.0054 (1.5586), Acc@1: 0.8438 (0.6494), Acc@5: 0.9062 (0.8671)
2021-12-30 05:20:17,010 Val Step[1300/1563], Loss: 1.1834 (1.5710), Acc@1: 0.7812 (0.6471), Acc@5: 0.8750 (0.8655)
2021-12-30 05:20:18,327 Val Step[1350/1563], Loss: 2.1710 (1.5941), Acc@1: 0.4062 (0.6418), Acc@5: 0.7500 (0.8619)
2021-12-30 05:20:19,621 Val Step[1400/1563], Loss: 1.4934 (1.6022), Acc@1: 0.6562 (0.6399), Acc@5: 0.8438 (0.8611)
2021-12-30 05:20:20,875 Val Step[1450/1563], Loss: 1.7333 (1.6098), Acc@1: 0.6250 (0.6383), Acc@5: 0.9062 (0.8602)
2021-12-30 05:20:22,149 Val Step[1500/1563], Loss: 2.1152 (1.5974), Acc@1: 0.4375 (0.6407), Acc@5: 0.9062 (0.8621)
2021-12-30 05:20:23,483 Val Step[1550/1563], Loss: 1.1517 (1.5948), Acc@1: 0.8750 (0.6411), Acc@5: 0.9062 (0.8623)
2021-12-30 05:20:24,225 ----- Epoch[100/310], Validation Loss: 1.5922, Validation Acc@1: 0.6417, Validation Acc@5: 0.8626, time: 90.88
2021-12-30 05:20:24,225 ----- Epoch[100/310], Train Loss: 4.1539, Train Acc: 0.2955, time: 1541.25, Best Val(epoch100) Acc@1: 0.6417
2021-12-30 05:20:24,413 Max accuracy so far: 0.6417 at epoch_100
2021-12-30 05:20:24,414 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 05:20:24,414 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 05:20:24,524 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 05:20:24,650 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-100-Loss-4.173319120963605.pdparams
2021-12-30 05:20:24,650 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-100-Loss-4.173319120963605.pdopt
2021-12-30 05:20:24,691 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-100-Loss-4.173319120963605-EMA.pdparams
2021-12-30 05:20:24,691 Now training epoch 101. LR=0.000747
2021-12-30 05:21:35,833 Epoch[101/310], Step[0000/1251], Loss: 3.7503(3.7503), Acc: 0.4199(0.4199)
2021-12-30 05:22:34,177 Epoch[101/310], Step[0050/1251], Loss: 4.7060(4.1695), Acc: 0.2617(0.3105)
2021-12-30 05:23:32,919 Epoch[101/310], Step[0100/1251], Loss: 4.3160(4.1462), Acc: 0.2578(0.3155)
2021-12-30 05:24:30,444 Epoch[101/310], Step[0150/1251], Loss: 3.4844(4.1486), Acc: 0.5195(0.3098)
2021-12-30 05:25:28,818 Epoch[101/310], Step[0200/1251], Loss: 4.2875(4.1480), Acc: 0.3926(0.3070)
2021-12-30 05:26:25,949 Epoch[101/310], Step[0250/1251], Loss: 4.5544(4.1426), Acc: 0.2920(0.3055)
2021-12-30 05:27:24,639 Epoch[101/310], Step[0300/1251], Loss: 4.8770(4.1446), Acc: 0.1641(0.3053)
2021-12-30 05:28:22,563 Epoch[101/310], Step[0350/1251], Loss: 4.0811(4.1481), Acc: 0.3037(0.3041)
2021-12-30 05:29:19,699 Epoch[101/310], Step[0400/1251], Loss: 4.4615(4.1403), Acc: 0.2988(0.3036)
2021-12-30 05:30:18,580 Epoch[101/310], Step[0450/1251], Loss: 4.3306(4.1379), Acc: 0.4062(0.3027)
2021-12-30 05:31:17,608 Epoch[101/310], Step[0500/1251], Loss: 4.5688(4.1418), Acc: 0.2363(0.3004)
2021-12-30 05:32:16,753 Epoch[101/310], Step[0550/1251], Loss: 3.7223(4.1447), Acc: 0.4678(0.3012)
2021-12-30 05:33:15,840 Epoch[101/310], Step[0600/1251], Loss: 4.3184(4.1477), Acc: 0.3574(0.2992)
2021-12-30 05:34:14,142 Epoch[101/310], Step[0650/1251], Loss: 4.2684(4.1479), Acc: 0.0645(0.2979)
2021-12-30 05:35:11,363 Epoch[101/310], Step[0700/1251], Loss: 3.9131(4.1496), Acc: 0.3711(0.2973)
2021-12-30 05:36:09,254 Epoch[101/310], Step[0750/1251], Loss: 4.1416(4.1512), Acc: 0.1406(0.2978)
2021-12-30 05:37:07,555 Epoch[101/310], Step[0800/1251], Loss: 4.3895(4.1561), Acc: 0.2393(0.2964)
2021-12-30 05:38:06,247 Epoch[101/310], Step[0850/1251], Loss: 4.2790(4.1566), Acc: 0.1406(0.2956)
2021-12-30 05:39:04,815 Epoch[101/310], Step[0900/1251], Loss: 4.3450(4.1562), Acc: 0.2656(0.2958)
2021-12-30 05:40:03,794 Epoch[101/310], Step[0950/1251], Loss: 4.3839(4.1548), Acc: 0.2910(0.2971)
2021-12-30 05:41:00,543 Epoch[101/310], Step[1000/1251], Loss: 4.1122(4.1532), Acc: 0.1611(0.2973)
2021-12-30 05:41:59,225 Epoch[101/310], Step[1050/1251], Loss: 3.7234(4.1543), Acc: 0.3467(0.2978)
2021-12-30 05:42:57,807 Epoch[101/310], Step[1100/1251], Loss: 4.3567(4.1544), Acc: 0.4033(0.2976)
2021-12-30 05:43:57,467 Epoch[101/310], Step[1150/1251], Loss: 3.6216(4.1514), Acc: 0.3975(0.2988)
2021-12-30 05:44:55,846 Epoch[101/310], Step[1200/1251], Loss: 4.4490(4.1498), Acc: 0.3193(0.2986)
2021-12-30 05:45:55,139 Epoch[101/310], Step[1250/1251], Loss: 4.1702(4.1498), Acc: 0.3408(0.2994)
2021-12-30 05:45:57,069 ----- Epoch[101/310], Train Loss: 4.1498, Train Acc: 0.2994, time: 1532.37, Best Val(epoch100) Acc@1: 0.6417
2021-12-30 05:45:57,280 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 05:45:57,281 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 05:45:57,369 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 05:45:57,370 Now training epoch 102. LR=0.000742
2021-12-30 05:47:05,285 Epoch[102/310], Step[0000/1251], Loss: 4.2770(4.2770), Acc: 0.1465(0.1465)
2021-12-30 05:48:04,435 Epoch[102/310], Step[0050/1251], Loss: 4.5935(4.1341), Acc: 0.1465(0.2917)
2021-12-30 05:49:02,363 Epoch[102/310], Step[0100/1251], Loss: 4.3573(4.1715), Acc: 0.2520(0.2941)
2021-12-30 05:50:00,101 Epoch[102/310], Step[0150/1251], Loss: 4.3913(4.1402), Acc: 0.2559(0.2928)
2021-12-30 05:50:59,404 Epoch[102/310], Step[0200/1251], Loss: 4.4003(4.1422), Acc: 0.2373(0.2918)
2021-12-30 05:51:59,318 Epoch[102/310], Step[0250/1251], Loss: 4.1800(4.1352), Acc: 0.2773(0.2966)
2021-12-30 05:52:58,955 Epoch[102/310], Step[0300/1251], Loss: 4.5198(4.1402), Acc: 0.3633(0.2970)
2021-12-30 05:53:59,082 Epoch[102/310], Step[0350/1251], Loss: 3.8685(4.1289), Acc: 0.3301(0.2967)
2021-12-30 05:54:58,778 Epoch[102/310], Step[0400/1251], Loss: 3.6626(4.1236), Acc: 0.4971(0.2976)
2021-12-30 05:55:58,590 Epoch[102/310], Step[0450/1251], Loss: 3.7203(4.1229), Acc: 0.2646(0.2976)
2021-12-30 05:56:58,803 Epoch[102/310], Step[0500/1251], Loss: 3.7338(4.1223), Acc: 0.3438(0.2985)
2021-12-30 05:57:57,798 Epoch[102/310], Step[0550/1251], Loss: 4.1609(4.1222), Acc: 0.3203(0.3005)
2021-12-30 05:58:56,359 Epoch[102/310], Step[0600/1251], Loss: 4.3067(4.1253), Acc: 0.2559(0.3010)
2021-12-30 05:59:55,537 Epoch[102/310], Step[0650/1251], Loss: 4.2172(4.1299), Acc: 0.2305(0.3016)
2021-12-30 06:00:53,360 Epoch[102/310], Step[0700/1251], Loss: 4.1049(4.1280), Acc: 0.4219(0.2998)
2021-12-30 06:01:52,709 Epoch[102/310], Step[0750/1251], Loss: 4.3514(4.1288), Acc: 0.2510(0.2992)
2021-12-30 06:02:50,080 Epoch[102/310], Step[0800/1251], Loss: 4.2479(4.1301), Acc: 0.2461(0.3002)
2021-12-30 06:03:48,224 Epoch[102/310], Step[0850/1251], Loss: 4.1393(4.1289), Acc: 0.3252(0.3007)
2021-12-30 06:04:45,909 Epoch[102/310], Step[0900/1251], Loss: 4.0320(4.1278), Acc: 0.2695(0.3002)
2021-12-30 06:05:43,749 Epoch[102/310], Step[0950/1251], Loss: 3.9819(4.1331), Acc: 0.2383(0.2995)
2021-12-30 06:06:41,825 Epoch[102/310], Step[1000/1251], Loss: 4.2431(4.1360), Acc: 0.4014(0.2998)
2021-12-30 06:07:41,013 Epoch[102/310], Step[1050/1251], Loss: 4.4955(4.1376), Acc: 0.2207(0.2999)
2021-12-30 06:08:40,110 Epoch[102/310], Step[1100/1251], Loss: 3.8545(4.1381), Acc: 0.3301(0.3000)
2021-12-30 06:09:38,278 Epoch[102/310], Step[1150/1251], Loss: 4.2045(4.1353), Acc: 0.3623(0.2999)
2021-12-30 06:10:37,786 Epoch[102/310], Step[1200/1251], Loss: 3.8863(4.1372), Acc: 0.4697(0.2997)
2021-12-30 06:11:37,362 Epoch[102/310], Step[1250/1251], Loss: 3.8280(4.1350), Acc: 0.3242(0.3006)
2021-12-30 06:11:38,844 ----- Validation after Epoch: 102
2021-12-30 06:12:28,392 Val Step[0000/1563], Loss: 0.6630 (0.6630), Acc@1: 0.9688 (0.9688), Acc@5: 0.9688 (0.9688)
2021-12-30 06:12:29,706 Val Step[0050/1563], Loss: 2.3453 (0.9951), Acc@1: 0.4062 (0.7953), Acc@5: 0.8438 (0.9412)
2021-12-30 06:12:31,003 Val Step[0100/1563], Loss: 2.3275 (1.3578), Acc@1: 0.3750 (0.6965), Acc@5: 0.8125 (0.9044)
2021-12-30 06:12:32,415 Val Step[0150/1563], Loss: 0.8579 (1.2791), Acc@1: 0.8438 (0.7185), Acc@5: 0.9375 (0.9087)
2021-12-30 06:12:33,813 Val Step[0200/1563], Loss: 1.1214 (1.2946), Acc@1: 0.7500 (0.7200), Acc@5: 0.9062 (0.9050)
2021-12-30 06:12:35,197 Val Step[0250/1563], Loss: 1.0109 (1.2315), Acc@1: 0.6875 (0.7349), Acc@5: 0.9688 (0.9141)
2021-12-30 06:12:36,565 Val Step[0300/1563], Loss: 1.4976 (1.2972), Acc@1: 0.6875 (0.7174), Acc@5: 0.9375 (0.9066)
2021-12-30 06:12:37,980 Val Step[0350/1563], Loss: 1.4801 (1.2989), Acc@1: 0.6250 (0.7131), Acc@5: 0.9062 (0.9101)
2021-12-30 06:12:39,361 Val Step[0400/1563], Loss: 1.3612 (1.3102), Acc@1: 0.7500 (0.7053), Acc@5: 0.9062 (0.9101)
2021-12-30 06:12:40,753 Val Step[0450/1563], Loss: 0.9833 (1.3127), Acc@1: 0.7188 (0.7014), Acc@5: 0.9688 (0.9113)
2021-12-30 06:12:42,172 Val Step[0500/1563], Loss: 0.6125 (1.3052), Acc@1: 0.8750 (0.7034), Acc@5: 1.0000 (0.9123)
2021-12-30 06:12:43,563 Val Step[0550/1563], Loss: 1.0798 (1.2846), Acc@1: 0.7500 (0.7100), Acc@5: 0.9375 (0.9141)
2021-12-30 06:12:44,898 Val Step[0600/1563], Loss: 0.8635 (1.2852), Acc@1: 0.8438 (0.7110), Acc@5: 0.9375 (0.9138)
2021-12-30 06:12:46,171 Val Step[0650/1563], Loss: 0.8545 (1.3061), Acc@1: 0.8438 (0.7069), Acc@5: 1.0000 (0.9108)
2021-12-30 06:12:47,452 Val Step[0700/1563], Loss: 1.2946 (1.3434), Acc@1: 0.7812 (0.6985), Acc@5: 0.9062 (0.9052)
2021-12-30 06:12:48,721 Val Step[0750/1563], Loss: 1.5995 (1.3816), Acc@1: 0.6562 (0.6905), Acc@5: 0.8438 (0.8995)
2021-12-30 06:12:49,985 Val Step[0800/1563], Loss: 1.0328 (1.4233), Acc@1: 0.7188 (0.6812), Acc@5: 0.9688 (0.8939)
2021-12-30 06:12:51,246 Val Step[0850/1563], Loss: 1.4232 (1.4498), Acc@1: 0.6875 (0.6754), Acc@5: 0.9062 (0.8898)
2021-12-30 06:12:52,553 Val Step[0900/1563], Loss: 0.4642 (1.4513), Acc@1: 0.9375 (0.6766), Acc@5: 1.0000 (0.8887)
2021-12-30 06:12:53,926 Val Step[0950/1563], Loss: 1.5292 (1.4745), Acc@1: 0.6562 (0.6721), Acc@5: 0.8750 (0.8853)
2021-12-30 06:12:55,342 Val Step[1000/1563], Loss: 0.7302 (1.5012), Acc@1: 0.9375 (0.6661), Acc@5: 0.9375 (0.8811)
2021-12-30 06:12:56,766 Val Step[1050/1563], Loss: 0.5133 (1.5199), Acc@1: 0.9688 (0.6625), Acc@5: 0.9688 (0.8786)
2021-12-30 06:12:58,196 Val Step[1100/1563], Loss: 1.3967 (1.5369), Acc@1: 0.7188 (0.6595), Acc@5: 0.9375 (0.8757)
2021-12-30 06:12:59,635 Val Step[1150/1563], Loss: 1.3114 (1.5552), Acc@1: 0.7812 (0.6562), Acc@5: 0.8125 (0.8727)
2021-12-30 06:13:01,050 Val Step[1200/1563], Loss: 1.8332 (1.5760), Acc@1: 0.7812 (0.6523), Acc@5: 0.8438 (0.8692)
2021-12-30 06:13:02,512 Val Step[1250/1563], Loss: 0.9162 (1.5915), Acc@1: 0.8125 (0.6498), Acc@5: 0.9062 (0.8661)
2021-12-30 06:13:03,941 Val Step[1300/1563], Loss: 1.1344 (1.6006), Acc@1: 0.8438 (0.6482), Acc@5: 0.8750 (0.8652)
2021-12-30 06:13:05,350 Val Step[1350/1563], Loss: 2.2546 (1.6219), Acc@1: 0.3750 (0.6437), Acc@5: 0.7500 (0.8619)
2021-12-30 06:13:06,756 Val Step[1400/1563], Loss: 1.2488 (1.6286), Acc@1: 0.7188 (0.6418), Acc@5: 0.9062 (0.8609)
2021-12-30 06:13:08,179 Val Step[1450/1563], Loss: 1.4226 (1.6377), Acc@1: 0.7188 (0.6397), Acc@5: 0.9375 (0.8596)
2021-12-30 06:13:09,604 Val Step[1500/1563], Loss: 1.9979 (1.6277), Acc@1: 0.4375 (0.6417), Acc@5: 0.8750 (0.8610)
2021-12-30 06:13:11,045 Val Step[1550/1563], Loss: 1.1936 (1.6272), Acc@1: 0.8750 (0.6419), Acc@5: 0.9375 (0.8609)
2021-12-30 06:13:11,819 ----- Epoch[102/310], Validation Loss: 1.6248, Validation Acc@1: 0.6424, Validation Acc@5: 0.8612, time: 92.97
2021-12-30 06:13:11,820 ----- Epoch[102/310], Train Loss: 4.1350, Train Acc: 0.3006, time: 1541.47, Best Val(epoch102) Acc@1: 0.6424
2021-12-30 06:13:12,004 Max accuracy so far: 0.6424 at epoch_102
2021-12-30 06:13:12,005 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 06:13:12,005 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 06:13:12,110 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 06:13:12,500 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 06:13:12,500 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 06:13:12,632 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 06:13:12,633 Now training epoch 103. LR=0.000738
2021-12-30 06:14:17,615 Epoch[103/310], Step[0000/1251], Loss: 4.4141(4.4141), Acc: 0.2646(0.2646)
2021-12-30 06:15:14,325 Epoch[103/310], Step[0050/1251], Loss: 3.6005(4.0831), Acc: 0.4883(0.3061)
2021-12-30 06:16:10,797 Epoch[103/310], Step[0100/1251], Loss: 4.5850(4.1237), Acc: 0.1885(0.3071)
2021-12-30 06:17:08,230 Epoch[103/310], Step[0150/1251], Loss: 4.2952(4.1546), Acc: 0.2637(0.2978)
2021-12-30 06:18:06,324 Epoch[103/310], Step[0200/1251], Loss: 4.4182(4.1626), Acc: 0.3291(0.2974)
2021-12-30 06:19:04,592 Epoch[103/310], Step[0250/1251], Loss: 4.3111(4.1625), Acc: 0.2959(0.2957)
2021-12-30 06:20:02,686 Epoch[103/310], Step[0300/1251], Loss: 4.1697(4.1470), Acc: 0.2734(0.2972)
2021-12-30 06:20:59,375 Epoch[103/310], Step[0350/1251], Loss: 3.8778(4.1467), Acc: 0.2549(0.2939)
2021-12-30 06:21:58,509 Epoch[103/310], Step[0400/1251], Loss: 4.1005(4.1475), Acc: 0.1895(0.2930)
2021-12-30 06:22:57,079 Epoch[103/310], Step[0450/1251], Loss: 3.7777(4.1416), Acc: 0.3027(0.2923)
2021-12-30 06:23:55,228 Epoch[103/310], Step[0500/1251], Loss: 4.0289(4.1396), Acc: 0.3613(0.2921)
2021-12-30 06:24:54,794 Epoch[103/310], Step[0550/1251], Loss: 4.0504(4.1380), Acc: 0.2900(0.2917)
2021-12-30 06:25:52,835 Epoch[103/310], Step[0600/1251], Loss: 3.6001(4.1363), Acc: 0.3281(0.2928)
2021-12-30 06:26:50,129 Epoch[103/310], Step[0650/1251], Loss: 4.2975(4.1305), Acc: 0.3428(0.2943)
2021-12-30 06:27:49,262 Epoch[103/310], Step[0700/1251], Loss: 3.9659(4.1311), Acc: 0.4561(0.2949)
2021-12-30 06:28:48,531 Epoch[103/310], Step[0750/1251], Loss: 3.8219(4.1268), Acc: 0.3828(0.2948)
2021-12-30 06:29:47,411 Epoch[103/310], Step[0800/1251], Loss: 4.3065(4.1295), Acc: 0.4102(0.2939)
2021-12-30 06:30:46,013 Epoch[103/310], Step[0850/1251], Loss: 4.0430(4.1306), Acc: 0.2412(0.2948)
2021-12-30 06:31:44,832 Epoch[103/310], Step[0900/1251], Loss: 4.0343(4.1264), Acc: 0.3193(0.2956)
2021-12-30 06:32:44,127 Epoch[103/310], Step[0950/1251], Loss: 3.8740(4.1277), Acc: 0.4521(0.2958)
2021-12-30 06:33:43,804 Epoch[103/310], Step[1000/1251], Loss: 4.2218(4.1257), Acc: 0.2373(0.2965)
2021-12-30 06:34:42,575 Epoch[103/310], Step[1050/1251], Loss: 3.7381(4.1266), Acc: 0.1885(0.2967)
2021-12-30 06:35:41,387 Epoch[103/310], Step[1100/1251], Loss: 4.5617(4.1274), Acc: 0.2559(0.2965)
2021-12-30 06:36:41,242 Epoch[103/310], Step[1150/1251], Loss: 4.3882(4.1259), Acc: 0.2236(0.2963)
2021-12-30 06:37:40,331 Epoch[103/310], Step[1200/1251], Loss: 4.5235(4.1276), Acc: 0.2178(0.2965)
2021-12-30 06:38:39,953 Epoch[103/310], Step[1250/1251], Loss: 4.1688(4.1261), Acc: 0.3740(0.2970)
2021-12-30 06:38:42,119 ----- Epoch[103/310], Train Loss: 4.1261, Train Acc: 0.2970, time: 1529.48, Best Val(epoch102) Acc@1: 0.6424
2021-12-30 06:38:42,300 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 06:38:42,300 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 06:38:42,404 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 06:38:42,404 Now training epoch 104. LR=0.000733
2021-12-30 06:39:50,545 Epoch[104/310], Step[0000/1251], Loss: 3.9407(3.9407), Acc: 0.2832(0.2832)
2021-12-30 06:40:48,424 Epoch[104/310], Step[0050/1251], Loss: 3.5945(4.1155), Acc: 0.3564(0.2907)
2021-12-30 06:41:46,062 Epoch[104/310], Step[0100/1251], Loss: 4.8622(4.1193), Acc: 0.1611(0.2947)
2021-12-30 06:42:43,584 Epoch[104/310], Step[0150/1251], Loss: 4.0596(4.1328), Acc: 0.2510(0.2933)
2021-12-30 06:43:41,429 Epoch[104/310], Step[0200/1251], Loss: 3.9123(4.1133), Acc: 0.2324(0.2936)
2021-12-30 06:44:39,313 Epoch[104/310], Step[0250/1251], Loss: 4.3860(4.1212), Acc: 0.1904(0.2970)
2021-12-30 06:45:36,824 Epoch[104/310], Step[0300/1251], Loss: 4.2031(4.1226), Acc: 0.4219(0.2987)
2021-12-30 06:46:34,879 Epoch[104/310], Step[0350/1251], Loss: 3.8079(4.1191), Acc: 0.1045(0.2987)
2021-12-30 06:47:33,181 Epoch[104/310], Step[0400/1251], Loss: 4.2538(4.1220), Acc: 0.1855(0.3005)
2021-12-30 06:48:31,805 Epoch[104/310], Step[0450/1251], Loss: 3.4611(4.1200), Acc: 0.3916(0.3001)
2021-12-30 06:49:30,186 Epoch[104/310], Step[0500/1251], Loss: 3.9077(4.1230), Acc: 0.3379(0.3005)
2021-12-30 06:50:28,954 Epoch[104/310], Step[0550/1251], Loss: 3.5242(4.1228), Acc: 0.4170(0.2993)
2021-12-30 06:51:26,941 Epoch[104/310], Step[0600/1251], Loss: 4.0769(4.1215), Acc: 0.3525(0.2976)
2021-12-30 06:52:24,236 Epoch[104/310], Step[0650/1251], Loss: 4.1218(4.1217), Acc: 0.2578(0.2972)
2021-12-30 06:53:21,950 Epoch[104/310], Step[0700/1251], Loss: 4.4392(4.1252), Acc: 0.4180(0.2978)
2021-12-30 06:54:19,481 Epoch[104/310], Step[0750/1251], Loss: 4.3531(4.1272), Acc: 0.2666(0.2970)
2021-12-30 06:55:17,690 Epoch[104/310], Step[0800/1251], Loss: 4.0705(4.1280), Acc: 0.3584(0.2959)
2021-12-30 06:56:16,506 Epoch[104/310], Step[0850/1251], Loss: 4.2310(4.1294), Acc: 0.2236(0.2963)
2021-12-30 06:57:13,957 Epoch[104/310], Step[0900/1251], Loss: 4.3008(4.1317), Acc: 0.2236(0.2962)
2021-12-30 06:58:11,098 Epoch[104/310], Step[0950/1251], Loss: 3.7878(4.1269), Acc: 0.4502(0.2975)
2021-12-30 06:59:08,492 Epoch[104/310], Step[1000/1251], Loss: 4.6278(4.1287), Acc: 0.2559(0.2976)
2021-12-30 07:00:07,401 Epoch[104/310], Step[1050/1251], Loss: 3.9288(4.1265), Acc: 0.4541(0.2985)
2021-12-30 07:01:05,522 Epoch[104/310], Step[1100/1251], Loss: 4.5951(4.1255), Acc: 0.3340(0.2981)
2021-12-30 07:02:03,816 Epoch[104/310], Step[1150/1251], Loss: 3.8795(4.1282), Acc: 0.3389(0.2975)
2021-12-30 07:03:01,092 Epoch[104/310], Step[1200/1251], Loss: 4.5317(4.1304), Acc: 0.2900(0.2976)
2021-12-30 07:03:59,310 Epoch[104/310], Step[1250/1251], Loss: 4.1584(4.1276), Acc: 0.3799(0.2984)
2021-12-30 07:04:00,776 ----- Validation after Epoch: 104
2021-12-30 07:04:48,929 Val Step[0000/1563], Loss: 0.8552 (0.8552), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-30 07:04:50,241 Val Step[0050/1563], Loss: 2.1877 (0.9807), Acc@1: 0.4688 (0.8107), Acc@5: 0.8438 (0.9387)
2021-12-30 07:04:51,504 Val Step[0100/1563], Loss: 2.3065 (1.3912), Acc@1: 0.4375 (0.6940), Acc@5: 0.7188 (0.8926)
2021-12-30 07:04:52,850 Val Step[0150/1563], Loss: 0.6870 (1.3079), Acc@1: 0.8750 (0.7156), Acc@5: 0.9375 (0.8994)
2021-12-30 07:04:54,186 Val Step[0200/1563], Loss: 1.4867 (1.3371), Acc@1: 0.6562 (0.7163), Acc@5: 0.9062 (0.8961)
2021-12-30 07:04:55,548 Val Step[0250/1563], Loss: 0.9824 (1.2736), Acc@1: 0.8125 (0.7318), Acc@5: 0.9375 (0.9059)
2021-12-30 07:04:56,815 Val Step[0300/1563], Loss: 1.5439 (1.3327), Acc@1: 0.5938 (0.7127), Acc@5: 0.9062 (0.9007)
2021-12-30 07:04:58,185 Val Step[0350/1563], Loss: 1.3291 (1.3386), Acc@1: 0.7188 (0.7075), Acc@5: 0.8750 (0.9034)
2021-12-30 07:04:59,541 Val Step[0400/1563], Loss: 1.5907 (1.3399), Acc@1: 0.5938 (0.7026), Acc@5: 0.9375 (0.9062)
2021-12-30 07:05:00,965 Val Step[0450/1563], Loss: 1.0847 (1.3336), Acc@1: 0.7188 (0.7016), Acc@5: 1.0000 (0.9086)
2021-12-30 07:05:02,370 Val Step[0500/1563], Loss: 0.9430 (1.3230), Acc@1: 0.6875 (0.7040), Acc@5: 0.9688 (0.9105)
2021-12-30 07:05:03,741 Val Step[0550/1563], Loss: 1.0585 (1.2915), Acc@1: 0.7188 (0.7126), Acc@5: 0.9375 (0.9138)
2021-12-30 07:05:05,098 Val Step[0600/1563], Loss: 0.7377 (1.2941), Acc@1: 0.8438 (0.7117), Acc@5: 0.9688 (0.9134)
2021-12-30 07:05:06,361 Val Step[0650/1563], Loss: 1.2183 (1.3182), Acc@1: 0.7500 (0.7069), Acc@5: 0.9688 (0.9100)
2021-12-30 07:05:07,683 Val Step[0700/1563], Loss: 1.2750 (1.3574), Acc@1: 0.7500 (0.6987), Acc@5: 0.9062 (0.9046)
2021-12-30 07:05:09,016 Val Step[0750/1563], Loss: 2.0835 (1.3995), Acc@1: 0.5938 (0.6897), Acc@5: 0.7812 (0.8986)
2021-12-30 07:05:10,305 Val Step[0800/1563], Loss: 1.1585 (1.4399), Acc@1: 0.6875 (0.6805), Acc@5: 0.9688 (0.8928)
2021-12-30 07:05:11,626 Val Step[0850/1563], Loss: 1.7035 (1.4692), Acc@1: 0.5625 (0.6744), Acc@5: 0.8750 (0.8891)
2021-12-30 07:05:12,907 Val Step[0900/1563], Loss: 0.7144 (1.4693), Acc@1: 0.8750 (0.6767), Acc@5: 0.9375 (0.8884)
2021-12-30 07:05:14,348 Val Step[0950/1563], Loss: 2.2559 (1.4968), Acc@1: 0.4688 (0.6710), Acc@5: 0.7500 (0.8840)
2021-12-30 07:05:15,960 Val Step[1000/1563], Loss: 1.0159 (1.5236), Acc@1: 0.9375 (0.6653), Acc@5: 0.9688 (0.8797)
2021-12-30 07:05:17,373 Val Step[1050/1563], Loss: 0.5065 (1.5395), Acc@1: 0.9062 (0.6616), Acc@5: 0.9688 (0.8775)
2021-12-30 07:05:18,800 Val Step[1100/1563], Loss: 1.8075 (1.5581), Acc@1: 0.6562 (0.6578), Acc@5: 0.8125 (0.8746)
2021-12-30 07:05:20,233 Val Step[1150/1563], Loss: 1.4953 (1.5747), Acc@1: 0.7500 (0.6549), Acc@5: 0.7812 (0.8718)
2021-12-30 07:05:21,695 Val Step[1200/1563], Loss: 1.5865 (1.5912), Acc@1: 0.7500 (0.6517), Acc@5: 0.8750 (0.8689)
2021-12-30 07:05:23,151 Val Step[1250/1563], Loss: 1.2252 (1.6073), Acc@1: 0.7500 (0.6493), Acc@5: 0.9062 (0.8659)
2021-12-30 07:05:24,577 Val Step[1300/1563], Loss: 1.2305 (1.6192), Acc@1: 0.7500 (0.6471), Acc@5: 0.8750 (0.8642)
2021-12-30 07:05:25,997 Val Step[1350/1563], Loss: 2.0332 (1.6393), Acc@1: 0.5312 (0.6434), Acc@5: 0.7188 (0.8605)
2021-12-30 07:05:27,425 Val Step[1400/1563], Loss: 1.2968 (1.6471), Acc@1: 0.7188 (0.6415), Acc@5: 0.8750 (0.8594)
2021-12-30 07:05:28,808 Val Step[1450/1563], Loss: 1.7583 (1.6526), Acc@1: 0.5938 (0.6402), Acc@5: 0.9062 (0.8589)
2021-12-30 07:05:30,063 Val Step[1500/1563], Loss: 1.5819 (1.6428), Acc@1: 0.6250 (0.6422), Acc@5: 0.9688 (0.8606)
2021-12-30 07:05:31,318 Val Step[1550/1563], Loss: 1.0483 (1.6405), Acc@1: 0.8750 (0.6426), Acc@5: 0.9062 (0.8609)
2021-12-30 07:05:32,068 ----- Epoch[104/310], Validation Loss: 1.6378, Validation Acc@1: 0.6432, Validation Acc@5: 0.8613, time: 91.29
2021-12-30 07:05:32,069 ----- Epoch[104/310], Train Loss: 4.1276, Train Acc: 0.2984, time: 1518.37, Best Val(epoch104) Acc@1: 0.6432
2021-12-30 07:05:32,255 Max accuracy so far: 0.6432 at epoch_104
2021-12-30 07:05:32,255 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 07:05:32,255 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 07:05:32,363 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 07:05:32,742 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 07:05:32,742 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 07:05:32,875 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 07:05:32,875 Now training epoch 105. LR=0.000728
2021-12-30 07:06:39,325 Epoch[105/310], Step[0000/1251], Loss: 4.1563(4.1563), Acc: 0.3682(0.3682)
2021-12-30 07:07:37,458 Epoch[105/310], Step[0050/1251], Loss: 4.0054(4.0509), Acc: 0.2383(0.2976)
2021-12-30 07:08:34,972 Epoch[105/310], Step[0100/1251], Loss: 3.8938(4.0988), Acc: 0.3389(0.2964)
2021-12-30 07:09:32,676 Epoch[105/310], Step[0150/1251], Loss: 3.6214(4.1125), Acc: 0.2041(0.3038)
2021-12-30 07:10:30,668 Epoch[105/310], Step[0200/1251], Loss: 3.5915(4.1119), Acc: 0.3818(0.3090)
2021-12-30 07:11:29,153 Epoch[105/310], Step[0250/1251], Loss: 3.8874(4.1085), Acc: 0.3125(0.3045)
2021-12-30 07:12:29,400 Epoch[105/310], Step[0300/1251], Loss: 3.7491(4.1074), Acc: 0.3555(0.3056)
2021-12-30 07:13:29,071 Epoch[105/310], Step[0350/1251], Loss: 3.9093(4.1116), Acc: 0.3828(0.3067)
2021-12-30 07:14:27,736 Epoch[105/310], Step[0400/1251], Loss: 3.8735(4.0991), Acc: 0.3623(0.3090)
2021-12-30 07:15:27,031 Epoch[105/310], Step[0450/1251], Loss: 3.9923(4.0950), Acc: 0.2900(0.3076)
2021-12-30 07:16:26,738 Epoch[105/310], Step[0500/1251], Loss: 4.0993(4.0874), Acc: 0.3682(0.3078)
2021-12-30 07:17:26,175 Epoch[105/310], Step[0550/1251], Loss: 3.7314(4.0854), Acc: 0.4189(0.3084)
2021-12-30 07:18:26,147 Epoch[105/310], Step[0600/1251], Loss: 3.6481(4.0944), Acc: 0.4775(0.3068)
2021-12-30 07:19:25,793 Epoch[105/310], Step[0650/1251], Loss: 3.9019(4.0958), Acc: 0.2676(0.3074)
2021-12-30 07:20:24,190 Epoch[105/310], Step[0700/1251], Loss: 4.2240(4.1034), Acc: 0.3213(0.3070)
2021-12-30 07:21:22,708 Epoch[105/310], Step[0750/1251], Loss: 4.0365(4.1133), Acc: 0.4482(0.3071)
2021-12-30 07:22:21,935 Epoch[105/310], Step[0800/1251], Loss: 4.1167(4.1149), Acc: 0.2422(0.3071)
2021-12-30 07:23:21,163 Epoch[105/310], Step[0850/1251], Loss: 4.2534(4.1152), Acc: 0.3828(0.3065)
2021-12-30 07:24:19,438 Epoch[105/310], Step[0900/1251], Loss: 4.1526(4.1140), Acc: 0.2539(0.3078)
2021-12-30 07:25:16,171 Epoch[105/310], Step[0950/1251], Loss: 4.1050(4.1177), Acc: 0.4307(0.3087)
2021-12-30 07:26:13,501 Epoch[105/310], Step[1000/1251], Loss: 3.9535(4.1178), Acc: 0.4785(0.3096)
2021-12-30 07:27:12,340 Epoch[105/310], Step[1050/1251], Loss: 4.1805(4.1162), Acc: 0.2314(0.3082)
2021-12-30 07:28:10,842 Epoch[105/310], Step[1100/1251], Loss: 4.3977(4.1176), Acc: 0.2920(0.3077)
2021-12-30 07:29:10,066 Epoch[105/310], Step[1150/1251], Loss: 3.9133(4.1152), Acc: 0.4873(0.3080)
2021-12-30 07:30:07,982 Epoch[105/310], Step[1200/1251], Loss: 4.4925(4.1171), Acc: 0.2744(0.3084)
2021-12-30 07:31:07,049 Epoch[105/310], Step[1250/1251], Loss: 4.1824(4.1176), Acc: 0.2588(0.3084)
2021-12-30 07:31:08,515 ----- Epoch[105/310], Train Loss: 4.1176, Train Acc: 0.3084, time: 1535.64, Best Val(epoch104) Acc@1: 0.6432
2021-12-30 07:31:08,693 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 07:31:08,694 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 07:31:08,804 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 07:31:08,805 Now training epoch 106. LR=0.000724
2021-12-30 07:32:16,718 Epoch[106/310], Step[0000/1251], Loss: 3.1051(3.1051), Acc: 0.2842(0.2842)
2021-12-30 07:33:14,916 Epoch[106/310], Step[0050/1251], Loss: 3.7968(4.0034), Acc: 0.2324(0.3062)
2021-12-30 07:34:12,497 Epoch[106/310], Step[0100/1251], Loss: 4.7579(4.0737), Acc: 0.3145(0.3080)
2021-12-30 07:35:09,130 Epoch[106/310], Step[0150/1251], Loss: 3.7566(4.0816), Acc: 0.2617(0.3048)
2021-12-30 07:36:06,432 Epoch[106/310], Step[0200/1251], Loss: 4.3864(4.1041), Acc: 0.3564(0.3024)
2021-12-30 07:37:04,724 Epoch[106/310], Step[0250/1251], Loss: 4.5006(4.1028), Acc: 0.1201(0.3047)
2021-12-30 07:38:02,763 Epoch[106/310], Step[0300/1251], Loss: 3.5536(4.1068), Acc: 0.5225(0.3043)
2021-12-30 07:39:01,614 Epoch[106/310], Step[0350/1251], Loss: 4.0131(4.1091), Acc: 0.3467(0.3034)
2021-12-30 07:39:58,961 Epoch[106/310], Step[0400/1251], Loss: 4.4761(4.1228), Acc: 0.2148(0.3030)
2021-12-30 07:40:58,075 Epoch[106/310], Step[0450/1251], Loss: 4.6314(4.1249), Acc: 0.1992(0.3029)
2021-12-30 07:41:56,442 Epoch[106/310], Step[0500/1251], Loss: 3.7629(4.1265), Acc: 0.3115(0.3052)
2021-12-30 07:42:55,073 Epoch[106/310], Step[0550/1251], Loss: 3.5438(4.1253), Acc: 0.0049(0.3037)
2021-12-30 07:43:55,181 Epoch[106/310], Step[0600/1251], Loss: 3.8198(4.1333), Acc: 0.2188(0.3029)
2021-12-30 07:44:54,605 Epoch[106/310], Step[0650/1251], Loss: 3.8214(4.1318), Acc: 0.4395(0.3024)
2021-12-30 07:45:53,675 Epoch[106/310], Step[0700/1251], Loss: 4.4958(4.1349), Acc: 0.1475(0.3030)
2021-12-30 07:46:53,070 Epoch[106/310], Step[0750/1251], Loss: 3.9220(4.1382), Acc: 0.4580(0.3014)
2021-12-30 07:47:51,943 Epoch[106/310], Step[0800/1251], Loss: 3.8219(4.1365), Acc: 0.4775(0.3027)
2021-12-30 07:48:51,247 Epoch[106/310], Step[0850/1251], Loss: 3.7050(4.1398), Acc: 0.3574(0.3028)
2021-12-30 07:49:49,714 Epoch[106/310], Step[0900/1251], Loss: 4.2795(4.1398), Acc: 0.2637(0.3036)
2021-12-30 07:50:48,376 Epoch[106/310], Step[0950/1251], Loss: 4.0081(4.1366), Acc: 0.3008(0.3046)
2021-12-30 07:51:48,582 Epoch[106/310], Step[1000/1251], Loss: 3.8470(4.1368), Acc: 0.3604(0.3038)
2021-12-30 07:52:46,431 Epoch[106/310], Step[1050/1251], Loss: 4.3015(4.1371), Acc: 0.1523(0.3047)
2021-12-30 07:53:45,664 Epoch[106/310], Step[1100/1251], Loss: 4.2618(4.1321), Acc: 0.3643(0.3054)
2021-12-30 07:54:45,224 Epoch[106/310], Step[1150/1251], Loss: 3.8213(4.1315), Acc: 0.4365(0.3052)
2021-12-30 07:55:45,183 Epoch[106/310], Step[1200/1251], Loss: 4.4811(4.1333), Acc: 0.2607(0.3054)
2021-12-30 07:56:45,629 Epoch[106/310], Step[1250/1251], Loss: 3.9804(4.1308), Acc: 0.4697(0.3057)
2021-12-30 07:56:47,775 ----- Validation after Epoch: 106
2021-12-30 07:57:35,581 Val Step[0000/1563], Loss: 1.0169 (1.0169), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2021-12-30 07:57:36,997 Val Step[0050/1563], Loss: 2.8036 (1.0504), Acc@1: 0.4062 (0.7868), Acc@5: 0.7188 (0.9326)
2021-12-30 07:57:38,353 Val Step[0100/1563], Loss: 1.8018 (1.3681), Acc@1: 0.5938 (0.6974), Acc@5: 0.8750 (0.8982)
2021-12-30 07:57:39,688 Val Step[0150/1563], Loss: 0.5633 (1.2610), Acc@1: 0.8750 (0.7231), Acc@5: 1.0000 (0.9083)
2021-12-30 07:57:41,030 Val Step[0200/1563], Loss: 1.2320 (1.2710), Acc@1: 0.7188 (0.7275), Acc@5: 0.9062 (0.9072)
2021-12-30 07:57:42,461 Val Step[0250/1563], Loss: 0.8840 (1.2126), Acc@1: 0.7812 (0.7415), Acc@5: 1.0000 (0.9140)
2021-12-30 07:57:43,767 Val Step[0300/1563], Loss: 1.3759 (1.2808), Acc@1: 0.6562 (0.7186), Acc@5: 0.9688 (0.9074)
2021-12-30 07:57:45,100 Val Step[0350/1563], Loss: 1.3669 (1.2966), Acc@1: 0.7812 (0.7119), Acc@5: 0.9062 (0.9092)
2021-12-30 07:57:46,377 Val Step[0400/1563], Loss: 1.5363 (1.3055), Acc@1: 0.7188 (0.7057), Acc@5: 0.9062 (0.9098)
2021-12-30 07:57:47,652 Val Step[0450/1563], Loss: 1.1111 (1.3044), Acc@1: 0.6562 (0.7038), Acc@5: 1.0000 (0.9119)
2021-12-30 07:57:49,001 Val Step[0500/1563], Loss: 0.4593 (1.2950), Acc@1: 0.8438 (0.7060), Acc@5: 1.0000 (0.9139)
2021-12-30 07:57:50,356 Val Step[0550/1563], Loss: 1.1116 (1.2704), Acc@1: 0.6562 (0.7129), Acc@5: 1.0000 (0.9161)
2021-12-30 07:57:51,665 Val Step[0600/1563], Loss: 1.0908 (1.2713), Acc@1: 0.7812 (0.7131), Acc@5: 0.9062 (0.9156)
2021-12-30 07:57:52,969 Val Step[0650/1563], Loss: 1.1617 (1.2945), Acc@1: 0.6562 (0.7074), Acc@5: 0.9688 (0.9124)
2021-12-30 07:57:54,231 Val Step[0700/1563], Loss: 1.5540 (1.3324), Acc@1: 0.7500 (0.6995), Acc@5: 0.8438 (0.9068)
2021-12-30 07:57:55,518 Val Step[0750/1563], Loss: 1.5928 (1.3690), Acc@1: 0.6875 (0.6925), Acc@5: 0.8750 (0.9008)
2021-12-30 07:57:56,844 Val Step[0800/1563], Loss: 1.2146 (1.4130), Acc@1: 0.7812 (0.6839), Acc@5: 0.9688 (0.8942)
2021-12-30 07:57:58,134 Val Step[0850/1563], Loss: 1.7825 (1.4420), Acc@1: 0.5625 (0.6772), Acc@5: 0.8750 (0.8902)
2021-12-30 07:57:59,406 Val Step[0900/1563], Loss: 0.4048 (1.4432), Acc@1: 0.9375 (0.6785), Acc@5: 0.9688 (0.8894)
2021-12-30 07:58:00,780 Val Step[0950/1563], Loss: 1.8292 (1.4657), Acc@1: 0.6562 (0.6744), Acc@5: 0.8125 (0.8857)
2021-12-30 07:58:02,058 Val Step[1000/1563], Loss: 0.6426 (1.4950), Acc@1: 0.8750 (0.6678), Acc@5: 1.0000 (0.8813)
2021-12-30 07:58:03,465 Val Step[1050/1563], Loss: 0.5079 (1.5109), Acc@1: 0.9688 (0.6648), Acc@5: 0.9688 (0.8789)
2021-12-30 07:58:04,819 Val Step[1100/1563], Loss: 1.1615 (1.5298), Acc@1: 0.7188 (0.6610), Acc@5: 0.9062 (0.8753)
2021-12-30 07:58:06,195 Val Step[1150/1563], Loss: 1.4680 (1.5486), Acc@1: 0.6875 (0.6578), Acc@5: 0.8125 (0.8726)
2021-12-30 07:58:07,575 Val Step[1200/1563], Loss: 1.3725 (1.5660), Acc@1: 0.7812 (0.6543), Acc@5: 0.8750 (0.8699)
2021-12-30 07:58:08,882 Val Step[1250/1563], Loss: 1.0217 (1.5813), Acc@1: 0.7812 (0.6515), Acc@5: 0.9062 (0.8675)
2021-12-30 07:58:10,210 Val Step[1300/1563], Loss: 1.5034 (1.5932), Acc@1: 0.6875 (0.6486), Acc@5: 0.8438 (0.8661)
2021-12-30 07:58:11,544 Val Step[1350/1563], Loss: 2.0688 (1.6132), Acc@1: 0.4062 (0.6440), Acc@5: 0.8438 (0.8628)
2021-12-30 07:58:12,880 Val Step[1400/1563], Loss: 1.4394 (1.6212), Acc@1: 0.6875 (0.6423), Acc@5: 0.8438 (0.8616)
2021-12-30 07:58:14,249 Val Step[1450/1563], Loss: 2.0076 (1.6280), Acc@1: 0.4688 (0.6410), Acc@5: 0.8750 (0.8612)
2021-12-30 07:58:15,675 Val Step[1500/1563], Loss: 2.0297 (1.6168), Acc@1: 0.5312 (0.6433), Acc@5: 0.9062 (0.8630)
2021-12-30 07:58:16,971 Val Step[1550/1563], Loss: 1.1725 (1.6160), Acc@1: 0.8750 (0.6436), Acc@5: 0.9062 (0.8631)
2021-12-30 07:58:17,731 ----- Epoch[106/310], Validation Loss: 1.6139, Validation Acc@1: 0.6441, Validation Acc@5: 0.8633, time: 89.95
2021-12-30 07:58:17,731 ----- Epoch[106/310], Train Loss: 4.1308, Train Acc: 0.3057, time: 1538.97, Best Val(epoch106) Acc@1: 0.6441
2021-12-30 07:58:17,917 Max accuracy so far: 0.6441 at epoch_106
2021-12-30 07:58:17,917 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 07:58:17,917 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 07:58:18,017 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 07:58:18,399 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 07:58:18,400 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 07:58:18,529 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 07:58:18,529 Now training epoch 107. LR=0.000719
2021-12-30 07:59:25,393 Epoch[107/310], Step[0000/1251], Loss: 4.0734(4.0734), Acc: 0.3838(0.3838)
2021-12-30 08:00:23,609 Epoch[107/310], Step[0050/1251], Loss: 4.3510(4.0759), Acc: 0.1992(0.3284)
2021-12-30 08:01:21,285 Epoch[107/310], Step[0100/1251], Loss: 4.2020(4.1026), Acc: 0.4229(0.3177)
2021-12-30 08:02:19,434 Epoch[107/310], Step[0150/1251], Loss: 3.4258(4.1173), Acc: 0.3770(0.3132)
2021-12-30 08:03:17,541 Epoch[107/310], Step[0200/1251], Loss: 4.0777(4.1213), Acc: 0.2461(0.3069)
2021-12-30 08:04:15,518 Epoch[107/310], Step[0250/1251], Loss: 4.2883(4.1137), Acc: 0.1836(0.3032)
2021-12-30 08:05:12,504 Epoch[107/310], Step[0300/1251], Loss: 3.7806(4.1166), Acc: 0.3740(0.3070)
2021-12-30 08:06:11,229 Epoch[107/310], Step[0350/1251], Loss: 4.3304(4.1194), Acc: 0.3975(0.3064)
2021-12-30 08:07:09,873 Epoch[107/310], Step[0400/1251], Loss: 3.3909(4.1165), Acc: 0.1387(0.3060)
2021-12-30 08:08:07,760 Epoch[107/310], Step[0450/1251], Loss: 4.4199(4.1253), Acc: 0.2744(0.3043)
2021-12-30 08:09:06,598 Epoch[107/310], Step[0500/1251], Loss: 3.8426(4.1336), Acc: 0.3281(0.3035)
2021-12-30 08:10:05,197 Epoch[107/310], Step[0550/1251], Loss: 4.1647(4.1300), Acc: 0.2725(0.3047)
2021-12-30 08:11:03,713 Epoch[107/310], Step[0600/1251], Loss: 3.7867(4.1285), Acc: 0.2822(0.3073)
2021-12-30 08:12:02,750 Epoch[107/310], Step[0650/1251], Loss: 3.7013(4.1296), Acc: 0.0938(0.3060)
2021-12-30 08:13:03,364 Epoch[107/310], Step[0700/1251], Loss: 4.2826(4.1314), Acc: 0.2754(0.3067)
2021-12-30 08:14:02,844 Epoch[107/310], Step[0750/1251], Loss: 4.1486(4.1260), Acc: 0.4287(0.3070)
2021-12-30 08:15:02,330 Epoch[107/310], Step[0800/1251], Loss: 3.9579(4.1244), Acc: 0.1719(0.3058)
2021-12-30 08:16:01,338 Epoch[107/310], Step[0850/1251], Loss: 4.4478(4.1242), Acc: 0.2021(0.3053)
2021-12-30 08:17:02,107 Epoch[107/310], Step[0900/1251], Loss: 4.2918(4.1270), Acc: 0.2686(0.3050)
2021-12-30 08:18:01,210 Epoch[107/310], Step[0950/1251], Loss: 4.1668(4.1267), Acc: 0.3184(0.3049)
2021-12-30 08:19:00,828 Epoch[107/310], Step[1000/1251], Loss: 4.0581(4.1267), Acc: 0.4404(0.3044)
2021-12-30 08:20:00,230 Epoch[107/310], Step[1050/1251], Loss: 4.1564(4.1271), Acc: 0.3516(0.3044)
2021-12-30 08:20:59,897 Epoch[107/310], Step[1100/1251], Loss: 4.2574(4.1276), Acc: 0.3506(0.3040)
2021-12-30 08:21:59,427 Epoch[107/310], Step[1150/1251], Loss: 4.3723(4.1274), Acc: 0.2588(0.3043)
2021-12-30 08:22:56,460 Epoch[107/310], Step[1200/1251], Loss: 4.6619(4.1298), Acc: 0.3398(0.3041)
2021-12-30 08:23:54,280 Epoch[107/310], Step[1250/1251], Loss: 4.0178(4.1314), Acc: 0.4531(0.3041)
2021-12-30 08:23:55,760 ----- Epoch[107/310], Train Loss: 4.1314, Train Acc: 0.3041, time: 1537.23, Best Val(epoch106) Acc@1: 0.6441
2021-12-30 08:23:55,937 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 08:23:55,937 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 08:23:56,040 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 08:23:56,040 Now training epoch 108. LR=0.000714
2021-12-30 08:25:04,749 Epoch[108/310], Step[0000/1251], Loss: 4.5902(4.5902), Acc: 0.1455(0.1455)
2021-12-30 08:26:03,454 Epoch[108/310], Step[0050/1251], Loss: 4.1584(4.1298), Acc: 0.2871(0.3039)
2021-12-30 08:27:02,645 Epoch[108/310], Step[0100/1251], Loss: 4.5334(4.1024), Acc: 0.3994(0.2988)
2021-12-30 08:28:01,878 Epoch[108/310], Step[0150/1251], Loss: 4.5846(4.1019), Acc: 0.3018(0.3016)
2021-12-30 08:29:00,264 Epoch[108/310], Step[0200/1251], Loss: 4.1705(4.1034), Acc: 0.3164(0.3047)
2021-12-30 08:29:58,241 Epoch[108/310], Step[0250/1251], Loss: 4.2134(4.0935), Acc: 0.3838(0.3080)
2021-12-30 08:30:56,450 Epoch[108/310], Step[0300/1251], Loss: 4.1048(4.0896), Acc: 0.1855(0.3092)
2021-12-30 08:31:56,223 Epoch[108/310], Step[0350/1251], Loss: 4.1728(4.0884), Acc: 0.1904(0.3108)
2021-12-30 08:32:54,058 Epoch[108/310], Step[0400/1251], Loss: 3.8266(4.0844), Acc: 0.1787(0.3114)
2021-12-30 08:33:53,116 Epoch[108/310], Step[0450/1251], Loss: 3.9398(4.0814), Acc: 0.2451(0.3108)
2021-12-30 08:34:52,143 Epoch[108/310], Step[0500/1251], Loss: 4.4172(4.0838), Acc: 0.2793(0.3098)
2021-12-30 08:35:52,379 Epoch[108/310], Step[0550/1251], Loss: 4.5406(4.0872), Acc: 0.2393(0.3097)
2021-12-30 08:36:50,965 Epoch[108/310], Step[0600/1251], Loss: 3.9894(4.0911), Acc: 0.3086(0.3091)
2021-12-30 08:37:50,689 Epoch[108/310], Step[0650/1251], Loss: 3.3430(4.0926), Acc: 0.0039(0.3073)
2021-12-30 08:38:49,653 Epoch[108/310], Step[0700/1251], Loss: 4.2657(4.1014), Acc: 0.2900(0.3069)
2021-12-30 08:39:48,712 Epoch[108/310], Step[0750/1251], Loss: 3.9745(4.1042), Acc: 0.0312(0.3065)
2021-12-30 08:40:48,389 Epoch[108/310], Step[0800/1251], Loss: 4.1848(4.1036), Acc: 0.2822(0.3067)
2021-12-30 08:41:47,759 Epoch[108/310], Step[0850/1251], Loss: 4.0650(4.1066), Acc: 0.4189(0.3066)
2021-12-30 08:42:46,382 Epoch[108/310], Step[0900/1251], Loss: 4.1676(4.1048), Acc: 0.4365(0.3081)
2021-12-30 08:43:46,109 Epoch[108/310], Step[0950/1251], Loss: 4.7392(4.1054), Acc: 0.3516(0.3074)
2021-12-30 08:44:43,603 Epoch[108/310], Step[1000/1251], Loss: 4.3373(4.1035), Acc: 0.3525(0.3075)
2021-12-30 08:45:42,473 Epoch[108/310], Step[1050/1251], Loss: 4.2453(4.1036), Acc: 0.3662(0.3076)
2021-12-30 08:46:41,279 Epoch[108/310], Step[1100/1251], Loss: 3.3539(4.1027), Acc: 0.3936(0.3070)
2021-12-30 08:47:39,951 Epoch[108/310], Step[1150/1251], Loss: 3.2683(4.1040), Acc: 0.2871(0.3069)
2021-12-30 08:48:40,397 Epoch[108/310], Step[1200/1251], Loss: 4.4052(4.1076), Acc: 0.1758(0.3059)
2021-12-30 08:49:39,879 Epoch[108/310], Step[1250/1251], Loss: 4.2569(4.1088), Acc: 0.3486(0.3059)
2021-12-30 08:49:41,377 ----- Validation after Epoch: 108
2021-12-30 08:50:32,691 Val Step[0000/1563], Loss: 0.7722 (0.7722), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2021-12-30 08:50:34,064 Val Step[0050/1563], Loss: 2.7080 (1.0161), Acc@1: 0.4688 (0.7868), Acc@5: 0.7812 (0.9357)
2021-12-30 08:50:35,374 Val Step[0100/1563], Loss: 2.0945 (1.3243), Acc@1: 0.3438 (0.7070), Acc@5: 0.8750 (0.9013)
2021-12-30 08:50:36,678 Val Step[0150/1563], Loss: 0.7468 (1.2554), Acc@1: 0.8438 (0.7223), Acc@5: 0.9688 (0.9060)
2021-12-30 08:50:37,977 Val Step[0200/1563], Loss: 1.4624 (1.2763), Acc@1: 0.6562 (0.7225), Acc@5: 0.8438 (0.9031)
2021-12-30 08:50:39,273 Val Step[0250/1563], Loss: 1.0000 (1.1986), Acc@1: 0.8125 (0.7415), Acc@5: 1.0000 (0.9132)
2021-12-30 08:50:40,631 Val Step[0300/1563], Loss: 1.2311 (1.2618), Acc@1: 0.6875 (0.7189), Acc@5: 0.9375 (0.9076)
2021-12-30 08:50:42,114 Val Step[0350/1563], Loss: 1.6040 (1.2684), Acc@1: 0.5938 (0.7135), Acc@5: 0.9062 (0.9106)
2021-12-30 08:50:43,538 Val Step[0400/1563], Loss: 1.6356 (1.2664), Acc@1: 0.6562 (0.7088), Acc@5: 0.9062 (0.9130)
2021-12-30 08:50:44,974 Val Step[0450/1563], Loss: 0.8569 (1.2729), Acc@1: 0.7500 (0.7064), Acc@5: 0.9688 (0.9134)
2021-12-30 08:50:46,381 Val Step[0500/1563], Loss: 0.4786 (1.2621), Acc@1: 0.9375 (0.7100), Acc@5: 1.0000 (0.9151)
2021-12-30 08:50:47,874 Val Step[0550/1563], Loss: 1.2587 (1.2366), Acc@1: 0.7812 (0.7169), Acc@5: 0.9062 (0.9178)
2021-12-30 08:50:49,304 Val Step[0600/1563], Loss: 0.7932 (1.2354), Acc@1: 0.8438 (0.7182), Acc@5: 0.9688 (0.9177)
2021-12-30 08:50:50,852 Val Step[0650/1563], Loss: 0.8641 (1.2566), Acc@1: 0.9062 (0.7135), Acc@5: 0.9375 (0.9147)
2021-12-30 08:50:52,279 Val Step[0700/1563], Loss: 1.3737 (1.2937), Acc@1: 0.7500 (0.7058), Acc@5: 0.9062 (0.9100)
2021-12-30 08:50:53,712 Val Step[0750/1563], Loss: 1.4876 (1.3334), Acc@1: 0.6875 (0.6979), Acc@5: 0.8125 (0.9040)
2021-12-30 08:50:55,124 Val Step[0800/1563], Loss: 1.2474 (1.3764), Acc@1: 0.7188 (0.6886), Acc@5: 0.9062 (0.8974)
2021-12-30 08:50:56,502 Val Step[0850/1563], Loss: 1.8118 (1.4057), Acc@1: 0.5312 (0.6823), Acc@5: 0.7812 (0.8924)
2021-12-30 08:50:57,830 Val Step[0900/1563], Loss: 0.5744 (1.4082), Acc@1: 0.8750 (0.6829), Acc@5: 0.9688 (0.8914)
2021-12-30 08:50:59,322 Val Step[0950/1563], Loss: 1.7253 (1.4317), Acc@1: 0.5625 (0.6776), Acc@5: 0.8438 (0.8876)
2021-12-30 08:51:00,670 Val Step[1000/1563], Loss: 0.8274 (1.4587), Acc@1: 0.8750 (0.6718), Acc@5: 0.9688 (0.8833)
2021-12-30 08:51:01,956 Val Step[1050/1563], Loss: 0.4402 (1.4717), Acc@1: 0.9688 (0.6692), Acc@5: 0.9688 (0.8818)
2021-12-30 08:51:03,234 Val Step[1100/1563], Loss: 1.2236 (1.4921), Acc@1: 0.7188 (0.6647), Acc@5: 0.9375 (0.8784)
2021-12-30 08:51:04,505 Val Step[1150/1563], Loss: 1.6551 (1.5101), Acc@1: 0.6875 (0.6609), Acc@5: 0.7500 (0.8760)
2021-12-30 08:51:05,826 Val Step[1200/1563], Loss: 1.7127 (1.5269), Acc@1: 0.7188 (0.6570), Acc@5: 0.8438 (0.8733)
2021-12-30 08:51:07,155 Val Step[1250/1563], Loss: 0.9548 (1.5431), Acc@1: 0.7812 (0.6541), Acc@5: 0.9062 (0.8704)
2021-12-30 08:51:08,539 Val Step[1300/1563], Loss: 1.0646 (1.5557), Acc@1: 0.8125 (0.6513), Acc@5: 0.8750 (0.8686)
2021-12-30 08:51:09,926 Val Step[1350/1563], Loss: 2.5884 (1.5745), Acc@1: 0.2500 (0.6474), Acc@5: 0.7500 (0.8658)
2021-12-30 08:51:11,322 Val Step[1400/1563], Loss: 1.4412 (1.5818), Acc@1: 0.7188 (0.6461), Acc@5: 0.9062 (0.8648)
2021-12-30 08:51:12,747 Val Step[1450/1563], Loss: 1.8223 (1.5899), Acc@1: 0.5312 (0.6443), Acc@5: 0.9062 (0.8640)
2021-12-30 08:51:14,048 Val Step[1500/1563], Loss: 2.0054 (1.5794), Acc@1: 0.5625 (0.6463), Acc@5: 0.9062 (0.8655)
2021-12-30 08:51:15,353 Val Step[1550/1563], Loss: 1.0929 (1.5785), Acc@1: 0.8750 (0.6464), Acc@5: 0.9062 (0.8656)
2021-12-30 08:51:16,100 ----- Epoch[108/310], Validation Loss: 1.5763, Validation Acc@1: 0.6469, Validation Acc@5: 0.8657, time: 94.72
2021-12-30 08:51:16,100 ----- Epoch[108/310], Train Loss: 4.1088, Train Acc: 0.3059, time: 1545.33, Best Val(epoch108) Acc@1: 0.6469
2021-12-30 08:51:16,285 Max accuracy so far: 0.6469 at epoch_108
2021-12-30 08:51:16,285 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 08:51:16,285 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 08:51:16,391 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 08:51:16,764 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 08:51:16,764 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 08:51:16,911 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 08:51:16,912 Now training epoch 109. LR=0.000710
2021-12-30 08:52:22,012 Epoch[109/310], Step[0000/1251], Loss: 3.9054(3.9054), Acc: 0.4678(0.4678)
2021-12-30 08:53:19,453 Epoch[109/310], Step[0050/1251], Loss: 4.0674(3.9987), Acc: 0.4219(0.3138)
2021-12-30 08:54:16,329 Epoch[109/310], Step[0100/1251], Loss: 4.1880(4.0306), Acc: 0.3975(0.3144)
2021-12-30 08:55:13,308 Epoch[109/310], Step[0150/1251], Loss: 4.1164(4.0800), Acc: 0.1758(0.3091)
2021-12-30 08:56:09,463 Epoch[109/310], Step[0200/1251], Loss: 3.6257(4.0776), Acc: 0.4297(0.3137)
2021-12-30 08:57:06,296 Epoch[109/310], Step[0250/1251], Loss: 4.7995(4.0837), Acc: 0.1777(0.3073)
2021-12-30 08:58:03,237 Epoch[109/310], Step[0300/1251], Loss: 3.9451(4.0870), Acc: 0.3799(0.3051)
2021-12-30 08:59:01,020 Epoch[109/310], Step[0350/1251], Loss: 4.1944(4.0881), Acc: 0.3691(0.3045)
2021-12-30 08:59:58,271 Epoch[109/310], Step[0400/1251], Loss: 4.2176(4.0880), Acc: 0.3809(0.3070)
2021-12-30 09:00:56,307 Epoch[109/310], Step[0450/1251], Loss: 4.0285(4.0894), Acc: 0.1621(0.3059)
2021-12-30 09:01:53,723 Epoch[109/310], Step[0500/1251], Loss: 4.5298(4.0985), Acc: 0.3418(0.3049)
2021-12-30 09:02:51,449 Epoch[109/310], Step[0550/1251], Loss: 4.2199(4.1013), Acc: 0.1631(0.3040)
2021-12-30 09:03:50,044 Epoch[109/310], Step[0600/1251], Loss: 4.0935(4.1044), Acc: 0.4268(0.3051)
2021-12-30 09:04:47,434 Epoch[109/310], Step[0650/1251], Loss: 4.4101(4.1054), Acc: 0.2344(0.3058)
2021-12-30 09:05:47,641 Epoch[109/310], Step[0700/1251], Loss: 4.3474(4.1066), Acc: 0.3564(0.3051)
2021-12-30 09:06:47,765 Epoch[109/310], Step[0750/1251], Loss: 4.4879(4.1155), Acc: 0.2178(0.3044)
2021-12-30 09:07:46,890 Epoch[109/310], Step[0800/1251], Loss: 4.4960(4.1143), Acc: 0.3037(0.3044)
2021-12-30 09:08:47,709 Epoch[109/310], Step[0850/1251], Loss: 4.6301(4.1123), Acc: 0.1875(0.3048)
2021-12-30 09:09:48,331 Epoch[109/310], Step[0900/1251], Loss: 4.6612(4.1138), Acc: 0.2695(0.3047)
2021-12-30 09:10:48,853 Epoch[109/310], Step[0950/1251], Loss: 4.4907(4.1144), Acc: 0.3516(0.3045)
2021-12-30 09:11:48,532 Epoch[109/310], Step[1000/1251], Loss: 3.7572(4.1140), Acc: 0.4541(0.3043)
2021-12-30 09:12:49,093 Epoch[109/310], Step[1050/1251], Loss: 3.9641(4.1148), Acc: 0.1543(0.3035)
2021-12-30 09:13:50,119 Epoch[109/310], Step[1100/1251], Loss: 3.8821(4.1170), Acc: 0.1729(0.3034)
2021-12-30 09:14:50,387 Epoch[109/310], Step[1150/1251], Loss: 4.3823(4.1173), Acc: 0.3594(0.3039)
2021-12-30 09:15:50,705 Epoch[109/310], Step[1200/1251], Loss: 4.7560(4.1182), Acc: 0.2109(0.3036)
2021-12-30 09:16:50,087 Epoch[109/310], Step[1250/1251], Loss: 3.6452(4.1182), Acc: 0.4873(0.3032)
2021-12-30 09:16:51,996 ----- Epoch[109/310], Train Loss: 4.1182, Train Acc: 0.3032, time: 1535.08, Best Val(epoch108) Acc@1: 0.6469
2021-12-30 09:16:52,208 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 09:16:52,209 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 09:16:52,332 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 09:16:52,333 Now training epoch 110. LR=0.000705
2021-12-30 09:17:57,137 Epoch[110/310], Step[0000/1251], Loss: 4.5229(4.5229), Acc: 0.2422(0.2422)
2021-12-30 09:18:55,752 Epoch[110/310], Step[0050/1251], Loss: 4.2927(4.1417), Acc: 0.2412(0.3108)
2021-12-30 09:19:54,712 Epoch[110/310], Step[0100/1251], Loss: 4.0190(4.1168), Acc: 0.3926(0.3114)
2021-12-30 09:20:53,014 Epoch[110/310], Step[0150/1251], Loss: 4.4465(4.1267), Acc: 0.1855(0.3095)
2021-12-30 09:21:50,336 Epoch[110/310], Step[0200/1251], Loss: 4.0015(4.1322), Acc: 0.3789(0.3108)
2021-12-30 09:22:49,516 Epoch[110/310], Step[0250/1251], Loss: 3.8857(4.1358), Acc: 0.4648(0.3054)
2021-12-30 09:23:48,351 Epoch[110/310], Step[0300/1251], Loss: 4.0892(4.1433), Acc: 0.3994(0.3065)
2021-12-30 09:24:45,538 Epoch[110/310], Step[0350/1251], Loss: 3.6885(4.1400), Acc: 0.3447(0.3028)
2021-12-30 09:25:45,410 Epoch[110/310], Step[0400/1251], Loss: 4.2002(4.1405), Acc: 0.3672(0.3012)
2021-12-30 09:26:44,850 Epoch[110/310], Step[0450/1251], Loss: 4.0579(4.1371), Acc: 0.3223(0.3010)
2021-12-30 09:27:44,482 Epoch[110/310], Step[0500/1251], Loss: 3.6219(4.1358), Acc: 0.3740(0.3025)
2021-12-30 09:28:42,806 Epoch[110/310], Step[0550/1251], Loss: 3.5905(4.1355), Acc: 0.5234(0.3021)
2021-12-30 09:29:42,776 Epoch[110/310], Step[0600/1251], Loss: 4.1752(4.1370), Acc: 0.3428(0.3023)
2021-12-30 09:30:42,998 Epoch[110/310], Step[0650/1251], Loss: 4.5525(4.1400), Acc: 0.3145(0.3020)
2021-12-30 09:31:40,276 Epoch[110/310], Step[0700/1251], Loss: 4.3164(4.1362), Acc: 0.3242(0.3027)
2021-12-30 09:32:38,836 Epoch[110/310], Step[0750/1251], Loss: 3.8824(4.1329), Acc: 0.4307(0.3022)
2021-12-30 09:33:37,664 Epoch[110/310], Step[0800/1251], Loss: 3.6931(4.1339), Acc: 0.4697(0.3004)
2021-12-30 09:34:35,692 Epoch[110/310], Step[0850/1251], Loss: 4.6467(4.1315), Acc: 0.2441(0.3003)
2021-12-30 09:35:34,861 Epoch[110/310], Step[0900/1251], Loss: 3.3044(4.1291), Acc: 0.3867(0.3000)
2021-12-30 09:36:34,773 Epoch[110/310], Step[0950/1251], Loss: 4.2302(4.1296), Acc: 0.3486(0.3008)
2021-12-30 09:37:31,563 Epoch[110/310], Step[1000/1251], Loss: 4.2303(4.1322), Acc: 0.4141(0.3010)
2021-12-30 09:38:30,353 Epoch[110/310], Step[1050/1251], Loss: 3.4969(4.1302), Acc: 0.2891(0.3008)
2021-12-30 09:39:28,539 Epoch[110/310], Step[1100/1251], Loss: 4.6155(4.1262), Acc: 0.2432(0.3008)
2021-12-30 09:40:27,967 Epoch[110/310], Step[1150/1251], Loss: 4.1947(4.1231), Acc: 0.3418(0.3011)
2021-12-30 09:41:25,914 Epoch[110/310], Step[1200/1251], Loss: 4.0627(4.1218), Acc: 0.2041(0.3017)
2021-12-30 09:42:23,413 Epoch[110/310], Step[1250/1251], Loss: 4.1468(4.1208), Acc: 0.4287(0.3014)
2021-12-30 09:42:25,362 ----- Validation after Epoch: 110
2021-12-30 09:45:56,035 Val Step[0000/1563], Loss: 0.8395 (0.8395), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-30 09:45:57,815 Val Step[0050/1563], Loss: 2.2976 (0.9818), Acc@1: 0.4062 (0.7874), Acc@5: 0.8438 (0.9381)
2021-12-30 09:45:59,160 Val Step[0100/1563], Loss: 2.1566 (1.3325), Acc@1: 0.4375 (0.6993), Acc@5: 0.8125 (0.8998)
2021-12-30 09:46:00,596 Val Step[0150/1563], Loss: 0.6199 (1.2461), Acc@1: 0.8750 (0.7192), Acc@5: 1.0000 (0.9071)
2021-12-30 09:46:02,079 Val Step[0200/1563], Loss: 1.2377 (1.2457), Acc@1: 0.7812 (0.7271), Acc@5: 0.9062 (0.9055)
2021-12-30 09:46:03,477 Val Step[0250/1563], Loss: 0.9832 (1.1866), Acc@1: 0.7500 (0.7394), Acc@5: 0.9688 (0.9136)
2021-12-30 09:46:04,903 Val Step[0300/1563], Loss: 1.6356 (1.2528), Acc@1: 0.5000 (0.7212), Acc@5: 0.9375 (0.9072)
2021-12-30 09:46:06,278 Val Step[0350/1563], Loss: 1.2198 (1.2598), Acc@1: 0.7188 (0.7157), Acc@5: 0.9375 (0.9095)
2021-12-30 09:46:07,702 Val Step[0400/1563], Loss: 1.1685 (1.2574), Acc@1: 0.7812 (0.7119), Acc@5: 0.9688 (0.9119)
2021-12-30 09:46:09,096 Val Step[0450/1563], Loss: 1.7063 (1.2590), Acc@1: 0.1250 (0.7090), Acc@5: 1.0000 (0.9137)
2021-12-30 09:46:10,412 Val Step[0500/1563], Loss: 0.5217 (1.2533), Acc@1: 0.8438 (0.7097), Acc@5: 1.0000 (0.9152)
2021-12-30 09:46:11,805 Val Step[0550/1563], Loss: 1.0738 (1.2283), Acc@1: 0.6875 (0.7164), Acc@5: 0.9688 (0.9180)
2021-12-30 09:46:13,154 Val Step[0600/1563], Loss: 0.7111 (1.2315), Acc@1: 0.8438 (0.7163), Acc@5: 0.9375 (0.9173)
2021-12-30 09:46:14,706 Val Step[0650/1563], Loss: 0.7165 (1.2501), Acc@1: 0.9062 (0.7122), Acc@5: 0.9688 (0.9146)
2021-12-30 09:46:16,149 Val Step[0700/1563], Loss: 1.2808 (1.2933), Acc@1: 0.7500 (0.7033), Acc@5: 0.9375 (0.9088)
2021-12-30 09:46:17,598 Val Step[0750/1563], Loss: 2.0452 (1.3353), Acc@1: 0.5312 (0.6958), Acc@5: 0.8125 (0.9029)
2021-12-30 09:46:19,034 Val Step[0800/1563], Loss: 1.2117 (1.3788), Acc@1: 0.7188 (0.6858), Acc@5: 0.9688 (0.8972)
2021-12-30 09:46:20,376 Val Step[0850/1563], Loss: 1.4674 (1.4108), Acc@1: 0.6875 (0.6797), Acc@5: 0.9375 (0.8931)
2021-12-30 09:46:21,829 Val Step[0900/1563], Loss: 0.5224 (1.4123), Acc@1: 0.9062 (0.6813), Acc@5: 0.9688 (0.8921)
2021-12-30 09:46:23,321 Val Step[0950/1563], Loss: 1.5313 (1.4378), Acc@1: 0.6875 (0.6764), Acc@5: 0.8750 (0.8887)
2021-12-30 09:46:24,691 Val Step[1000/1563], Loss: 0.6074 (1.4687), Acc@1: 0.9375 (0.6698), Acc@5: 1.0000 (0.8840)
2021-12-30 09:46:26,060 Val Step[1050/1563], Loss: 0.7081 (1.4869), Acc@1: 0.8750 (0.6656), Acc@5: 0.9688 (0.8817)
2021-12-30 09:46:27,431 Val Step[1100/1563], Loss: 1.3180 (1.5045), Acc@1: 0.7188 (0.6622), Acc@5: 0.8750 (0.8784)
2021-12-30 09:46:28,823 Val Step[1150/1563], Loss: 1.5504 (1.5233), Acc@1: 0.7188 (0.6583), Acc@5: 0.8125 (0.8757)
2021-12-30 09:46:30,350 Val Step[1200/1563], Loss: 1.6268 (1.5424), Acc@1: 0.7812 (0.6552), Acc@5: 0.8438 (0.8725)
2021-12-30 09:46:31,800 Val Step[1250/1563], Loss: 1.3528 (1.5564), Acc@1: 0.7812 (0.6530), Acc@5: 0.8750 (0.8699)
2021-12-30 09:46:33,214 Val Step[1300/1563], Loss: 1.0550 (1.5709), Acc@1: 0.7812 (0.6502), Acc@5: 0.8750 (0.8680)
2021-12-30 09:46:34,590 Val Step[1350/1563], Loss: 2.3338 (1.5904), Acc@1: 0.3750 (0.6460), Acc@5: 0.7812 (0.8649)
2021-12-30 09:46:36,019 Val Step[1400/1563], Loss: 1.2881 (1.5992), Acc@1: 0.7500 (0.6440), Acc@5: 0.9375 (0.8637)
2021-12-30 09:46:37,522 Val Step[1450/1563], Loss: 1.8601 (1.6056), Acc@1: 0.5000 (0.6428), Acc@5: 0.8750 (0.8630)
2021-12-30 09:46:39,034 Val Step[1500/1563], Loss: 1.9215 (1.5954), Acc@1: 0.5312 (0.6447), Acc@5: 0.8750 (0.8647)
2021-12-30 09:46:40,633 Val Step[1550/1563], Loss: 0.9961 (1.5932), Acc@1: 0.8750 (0.6451), Acc@5: 0.9062 (0.8650)
2021-12-30 09:46:41,337 ----- Epoch[110/310], Validation Loss: 1.5908, Validation Acc@1: 0.6457, Validation Acc@5: 0.8651, time: 255.97
2021-12-30 09:46:41,337 ----- Epoch[110/310], Train Loss: 4.1208, Train Acc: 0.3014, time: 1533.02, Best Val(epoch108) Acc@1: 0.6469
2021-12-30 09:46:41,514 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-110-Loss-4.122869782215305.pdparams
2021-12-30 09:46:41,514 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-110-Loss-4.122869782215305.pdopt
2021-12-30 09:46:41,558 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-110-Loss-4.122869782215305-EMA.pdparams
2021-12-30 09:46:41,558 Now training epoch 111. LR=0.000700
2021-12-30 09:50:31,996 Epoch[111/310], Step[0000/1251], Loss: 4.5903(4.5903), Acc: 0.2861(0.2861)
2021-12-30 09:51:32,432 Epoch[111/310], Step[0050/1251], Loss: 4.1902(4.1134), Acc: 0.1631(0.2983)
2021-12-30 09:52:33,715 Epoch[111/310], Step[0100/1251], Loss: 4.2458(4.1027), Acc: 0.1338(0.2916)
2021-12-30 09:53:33,421 Epoch[111/310], Step[0150/1251], Loss: 4.4580(4.0942), Acc: 0.3789(0.3056)
2021-12-30 09:54:33,905 Epoch[111/310], Step[0200/1251], Loss: 4.2746(4.0881), Acc: 0.2842(0.3018)
2021-12-30 09:55:33,737 Epoch[111/310], Step[0250/1251], Loss: 4.1948(4.0950), Acc: 0.1533(0.3030)
2021-12-30 09:56:34,015 Epoch[111/310], Step[0300/1251], Loss: 4.6852(4.1098), Acc: 0.3730(0.3025)
2021-12-30 09:57:32,567 Epoch[111/310], Step[0350/1251], Loss: 4.0207(4.1056), Acc: 0.4385(0.3021)
2021-12-30 09:58:34,467 Epoch[111/310], Step[0400/1251], Loss: 4.7657(4.1152), Acc: 0.2656(0.3020)
2021-12-30 09:59:35,071 Epoch[111/310], Step[0450/1251], Loss: 4.2752(4.1169), Acc: 0.3672(0.3046)
2021-12-30 10:00:36,541 Epoch[111/310], Step[0500/1251], Loss: 4.2819(4.1100), Acc: 0.2930(0.3037)
2021-12-30 10:01:37,825 Epoch[111/310], Step[0550/1251], Loss: 4.5916(4.1140), Acc: 0.1562(0.3026)
2021-12-30 10:02:38,838 Epoch[111/310], Step[0600/1251], Loss: 4.5280(4.1118), Acc: 0.2637(0.3044)
2021-12-30 10:03:40,602 Epoch[111/310], Step[0650/1251], Loss: 4.5087(4.1143), Acc: 0.2900(0.3038)
2021-12-30 10:04:40,182 Epoch[111/310], Step[0700/1251], Loss: 4.0896(4.1143), Acc: 0.2871(0.3051)
2021-12-30 10:05:41,298 Epoch[111/310], Step[0750/1251], Loss: 3.9707(4.1096), Acc: 0.4121(0.3063)
2021-12-30 10:06:41,253 Epoch[111/310], Step[0800/1251], Loss: 3.5277(4.1082), Acc: 0.4072(0.3075)
2021-12-30 10:07:42,154 Epoch[111/310], Step[0850/1251], Loss: 4.1438(4.1063), Acc: 0.4375(0.3079)
2021-12-30 10:08:42,943 Epoch[111/310], Step[0900/1251], Loss: 3.6823(4.1093), Acc: 0.4170(0.3078)
2021-12-30 10:09:44,016 Epoch[111/310], Step[0950/1251], Loss: 3.9201(4.1122), Acc: 0.4463(0.3069)
2021-12-30 10:10:45,666 Epoch[111/310], Step[1000/1251], Loss: 4.2909(4.1147), Acc: 0.3271(0.3066)
2021-12-30 10:11:46,787 Epoch[111/310], Step[1050/1251], Loss: 4.1573(4.1153), Acc: 0.4238(0.3075)
2021-12-30 10:12:48,029 Epoch[111/310], Step[1100/1251], Loss: 3.7572(4.1156), Acc: 0.1523(0.3071)
2021-12-30 10:13:50,149 Epoch[111/310], Step[1150/1251], Loss: 3.6146(4.1167), Acc: 0.4404(0.3079)
2021-12-30 10:14:51,240 Epoch[111/310], Step[1200/1251], Loss: 4.6166(4.1179), Acc: 0.2666(0.3084)
2021-12-30 10:15:50,834 Epoch[111/310], Step[1250/1251], Loss: 4.1304(4.1179), Acc: 0.2529(0.3090)
2021-12-30 10:15:52,909 ----- Epoch[111/310], Train Loss: 4.1179, Train Acc: 0.3090, time: 1751.35, Best Val(epoch108) Acc@1: 0.6469
2021-12-30 10:15:53,086 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 10:15:53,086 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 10:15:53,189 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 10:15:53,190 Now training epoch 112. LR=0.000695
2021-12-30 10:17:09,991 Epoch[112/310], Step[0000/1251], Loss: 4.4537(4.4537), Acc: 0.3105(0.3105)
2021-12-30 10:18:09,268 Epoch[112/310], Step[0050/1251], Loss: 3.9966(4.1157), Acc: 0.2793(0.2954)
2021-12-30 10:19:09,088 Epoch[112/310], Step[0100/1251], Loss: 4.4793(4.1087), Acc: 0.2588(0.3078)
2021-12-30 10:20:07,997 Epoch[112/310], Step[0150/1251], Loss: 4.6238(4.1264), Acc: 0.1855(0.3008)
2021-12-30 10:21:07,176 Epoch[112/310], Step[0200/1251], Loss: 3.7943(4.1139), Acc: 0.3311(0.3030)
2021-12-30 10:22:06,119 Epoch[112/310], Step[0250/1251], Loss: 3.8638(4.1057), Acc: 0.3457(0.3057)
2021-12-30 10:23:06,434 Epoch[112/310], Step[0300/1251], Loss: 3.5048(4.1061), Acc: 0.3789(0.3068)
2021-12-30 10:24:05,674 Epoch[112/310], Step[0350/1251], Loss: 4.5194(4.1012), Acc: 0.3105(0.3078)
2021-12-30 10:25:05,030 Epoch[112/310], Step[0400/1251], Loss: 4.2887(4.0983), Acc: 0.2910(0.3087)
2021-12-30 10:26:03,892 Epoch[112/310], Step[0450/1251], Loss: 4.4815(4.1015), Acc: 0.2939(0.3070)
2021-12-30 10:27:02,326 Epoch[112/310], Step[0500/1251], Loss: 3.7989(4.1031), Acc: 0.2148(0.3061)
2021-12-30 10:28:00,608 Epoch[112/310], Step[0550/1251], Loss: 4.1164(4.1076), Acc: 0.3975(0.3070)
2021-12-30 10:29:00,945 Epoch[112/310], Step[0600/1251], Loss: 3.8540(4.1065), Acc: 0.1201(0.3072)
2021-12-30 10:30:01,197 Epoch[112/310], Step[0650/1251], Loss: 4.0874(4.1013), Acc: 0.3535(0.3071)
2021-12-30 10:31:00,405 Epoch[112/310], Step[0700/1251], Loss: 3.9607(4.1033), Acc: 0.1670(0.3056)
2021-12-30 10:32:00,539 Epoch[112/310], Step[0750/1251], Loss: 4.2442(4.1046), Acc: 0.3496(0.3059)
2021-12-30 10:33:00,902 Epoch[112/310], Step[0800/1251], Loss: 4.2948(4.1047), Acc: 0.1768(0.3067)
2021-12-30 10:34:00,726 Epoch[112/310], Step[0850/1251], Loss: 4.3366(4.1061), Acc: 0.3154(0.3066)
2021-12-30 10:35:00,260 Epoch[112/310], Step[0900/1251], Loss: 4.3782(4.1067), Acc: 0.3965(0.3070)
2021-12-30 10:35:59,440 Epoch[112/310], Step[0950/1251], Loss: 4.6180(4.1128), Acc: 0.2344(0.3056)
2021-12-30 10:36:59,461 Epoch[112/310], Step[1000/1251], Loss: 4.4806(4.1132), Acc: 0.2881(0.3057)
2021-12-30 10:37:59,684 Epoch[112/310], Step[1050/1251], Loss: 3.8237(4.1120), Acc: 0.4600(0.3051)
2021-12-30 10:38:59,234 Epoch[112/310], Step[1100/1251], Loss: 4.1124(4.1141), Acc: 0.3330(0.3045)
2021-12-30 10:39:59,256 Epoch[112/310], Step[1150/1251], Loss: 3.9247(4.1161), Acc: 0.2979(0.3046)
2021-12-30 10:40:58,419 Epoch[112/310], Step[1200/1251], Loss: 4.1916(4.1134), Acc: 0.3818(0.3047)
2021-12-30 10:41:58,026 Epoch[112/310], Step[1250/1251], Loss: 4.2540(4.1143), Acc: 0.0859(0.3040)
2021-12-30 10:41:59,756 ----- Validation after Epoch: 112
2021-12-30 10:42:52,942 Val Step[0000/1563], Loss: 0.8968 (0.8968), Acc@1: 0.8438 (0.8438), Acc@5: 0.9688 (0.9688)
2021-12-30 10:42:54,317 Val Step[0050/1563], Loss: 2.3200 (1.0284), Acc@1: 0.4375 (0.8051), Acc@5: 0.8750 (0.9375)
2021-12-30 10:42:55,808 Val Step[0100/1563], Loss: 1.9987 (1.3440), Acc@1: 0.4688 (0.7129), Acc@5: 0.8125 (0.8998)
2021-12-30 10:42:57,098 Val Step[0150/1563], Loss: 0.7876 (1.2724), Acc@1: 0.8750 (0.7297), Acc@5: 0.9062 (0.9040)
2021-12-30 10:42:58,538 Val Step[0200/1563], Loss: 1.4504 (1.2925), Acc@1: 0.6875 (0.7279), Acc@5: 0.8438 (0.9014)
2021-12-30 10:42:59,816 Val Step[0250/1563], Loss: 1.4665 (1.2181), Acc@1: 0.7188 (0.7443), Acc@5: 0.9375 (0.9112)
2021-12-30 10:43:01,229 Val Step[0300/1563], Loss: 1.4629 (1.2854), Acc@1: 0.5938 (0.7237), Acc@5: 0.9688 (0.9068)
2021-12-30 10:43:02,510 Val Step[0350/1563], Loss: 1.4637 (1.2865), Acc@1: 0.6250 (0.7179), Acc@5: 0.9062 (0.9105)
2021-12-30 10:43:03,786 Val Step[0400/1563], Loss: 1.4878 (1.2895), Acc@1: 0.6875 (0.7142), Acc@5: 0.9375 (0.9129)
2021-12-30 10:43:05,088 Val Step[0450/1563], Loss: 1.4131 (1.2995), Acc@1: 0.4062 (0.7089), Acc@5: 0.9688 (0.9133)
2021-12-30 10:43:06,392 Val Step[0500/1563], Loss: 0.6566 (1.2933), Acc@1: 0.8750 (0.7114), Acc@5: 1.0000 (0.9144)
2021-12-30 10:43:07,938 Val Step[0550/1563], Loss: 1.1241 (1.2721), Acc@1: 0.8438 (0.7181), Acc@5: 0.9375 (0.9163)
2021-12-30 10:43:09,413 Val Step[0600/1563], Loss: 0.9372 (1.2788), Acc@1: 0.8125 (0.7174), Acc@5: 0.9688 (0.9160)
2021-12-30 10:43:10,844 Val Step[0650/1563], Loss: 0.8887 (1.2998), Acc@1: 0.7500 (0.7132), Acc@5: 1.0000 (0.9130)
2021-12-30 10:43:12,314 Val Step[0700/1563], Loss: 1.7837 (1.3376), Acc@1: 0.7188 (0.7051), Acc@5: 0.8750 (0.9080)
2021-12-30 10:43:13,793 Val Step[0750/1563], Loss: 2.0502 (1.3719), Acc@1: 0.5625 (0.6982), Acc@5: 0.7188 (0.9026)
2021-12-30 10:43:15,127 Val Step[0800/1563], Loss: 1.3986 (1.4135), Acc@1: 0.7188 (0.6885), Acc@5: 0.9062 (0.8960)
2021-12-30 10:43:16,467 Val Step[0850/1563], Loss: 1.7255 (1.4433), Acc@1: 0.6562 (0.6816), Acc@5: 0.8125 (0.8922)
2021-12-30 10:43:17,783 Val Step[0900/1563], Loss: 0.6191 (1.4435), Acc@1: 0.8750 (0.6826), Acc@5: 0.9688 (0.8912)
2021-12-30 10:43:19,222 Val Step[0950/1563], Loss: 1.6568 (1.4673), Acc@1: 0.6562 (0.6775), Acc@5: 0.8750 (0.8875)
2021-12-30 10:43:20,549 Val Step[1000/1563], Loss: 0.4948 (1.4938), Acc@1: 0.9688 (0.6714), Acc@5: 1.0000 (0.8835)
2021-12-30 10:43:21,852 Val Step[1050/1563], Loss: 0.4189 (1.5115), Acc@1: 0.9688 (0.6673), Acc@5: 0.9688 (0.8808)
2021-12-30 10:43:23,179 Val Step[1100/1563], Loss: 2.2971 (1.5305), Acc@1: 0.6250 (0.6637), Acc@5: 0.7188 (0.8780)
2021-12-30 10:43:24,478 Val Step[1150/1563], Loss: 1.7896 (1.5487), Acc@1: 0.6875 (0.6607), Acc@5: 0.7500 (0.8752)
2021-12-30 10:43:25,819 Val Step[1200/1563], Loss: 1.7065 (1.5641), Acc@1: 0.6875 (0.6584), Acc@5: 0.8438 (0.8723)
2021-12-30 10:43:27,138 Val Step[1250/1563], Loss: 1.1971 (1.5803), Acc@1: 0.7812 (0.6559), Acc@5: 0.9062 (0.8695)
2021-12-30 10:43:28,413 Val Step[1300/1563], Loss: 1.0769 (1.5947), Acc@1: 0.8438 (0.6529), Acc@5: 0.8750 (0.8676)
2021-12-30 10:43:29,726 Val Step[1350/1563], Loss: 2.4018 (1.6125), Acc@1: 0.3438 (0.6494), Acc@5: 0.7188 (0.8648)
2021-12-30 10:43:31,062 Val Step[1400/1563], Loss: 1.6687 (1.6234), Acc@1: 0.7188 (0.6466), Acc@5: 0.8750 (0.8630)
2021-12-30 10:43:32,356 Val Step[1450/1563], Loss: 2.1336 (1.6307), Acc@1: 0.4375 (0.6451), Acc@5: 0.8125 (0.8621)
2021-12-30 10:43:33,651 Val Step[1500/1563], Loss: 2.0233 (1.6195), Acc@1: 0.5000 (0.6478), Acc@5: 0.8750 (0.8639)
2021-12-30 10:43:35,100 Val Step[1550/1563], Loss: 0.9133 (1.6168), Acc@1: 0.8750 (0.6483), Acc@5: 0.9062 (0.8641)
2021-12-30 10:43:35,948 ----- Epoch[112/310], Validation Loss: 1.6145, Validation Acc@1: 0.6488, Validation Acc@5: 0.8644, time: 96.19
2021-12-30 10:43:35,948 ----- Epoch[112/310], Train Loss: 4.1143, Train Acc: 0.3040, time: 1566.56, Best Val(epoch112) Acc@1: 0.6488
2021-12-30 10:43:36,135 Max accuracy so far: 0.6488 at epoch_112
2021-12-30 10:43:36,136 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 10:43:36,136 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 10:43:36,261 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 10:43:36,659 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 10:43:36,922 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 10:43:36,982 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 10:43:36,982 Now training epoch 113. LR=0.000690
2021-12-30 10:44:52,069 Epoch[113/310], Step[0000/1251], Loss: 4.2955(4.2955), Acc: 0.3896(0.3896)
2021-12-30 10:45:50,406 Epoch[113/310], Step[0050/1251], Loss: 4.5595(4.1352), Acc: 0.3105(0.3210)
2021-12-30 10:46:50,128 Epoch[113/310], Step[0100/1251], Loss: 3.8624(4.1272), Acc: 0.3398(0.3113)
2021-12-30 10:47:49,130 Epoch[113/310], Step[0150/1251], Loss: 4.2225(4.1255), Acc: 0.1572(0.3035)
2021-12-30 10:48:47,886 Epoch[113/310], Step[0200/1251], Loss: 4.4825(4.1286), Acc: 0.3506(0.3066)
2021-12-30 10:49:46,499 Epoch[113/310], Step[0250/1251], Loss: 4.4861(4.1380), Acc: 0.2314(0.3065)
2021-12-30 10:50:46,338 Epoch[113/310], Step[0300/1251], Loss: 3.4294(4.1301), Acc: 0.4111(0.3065)
2021-12-30 10:51:45,580 Epoch[113/310], Step[0350/1251], Loss: 3.2575(4.1272), Acc: 0.4004(0.3077)
2021-12-30 10:52:44,583 Epoch[113/310], Step[0400/1251], Loss: 3.5380(4.1234), Acc: 0.2949(0.3100)
2021-12-30 10:53:43,648 Epoch[113/310], Step[0450/1251], Loss: 4.0995(4.1226), Acc: 0.3291(0.3098)
2021-12-30 10:54:42,990 Epoch[113/310], Step[0500/1251], Loss: 4.1035(4.1184), Acc: 0.3213(0.3104)
2021-12-30 10:55:42,478 Epoch[113/310], Step[0550/1251], Loss: 4.4523(4.1189), Acc: 0.3496(0.3116)
2021-12-30 10:56:42,102 Epoch[113/310], Step[0600/1251], Loss: 4.5614(4.1138), Acc: 0.1621(0.3098)
2021-12-30 10:57:41,014 Epoch[113/310], Step[0650/1251], Loss: 4.3730(4.1190), Acc: 0.3486(0.3097)
2021-12-30 10:58:41,684 Epoch[113/310], Step[0700/1251], Loss: 4.1607(4.1207), Acc: 0.3594(0.3099)
2021-12-30 10:59:40,846 Epoch[113/310], Step[0750/1251], Loss: 4.5653(4.1217), Acc: 0.1592(0.3092)
2021-12-30 11:00:40,330 Epoch[113/310], Step[0800/1251], Loss: 4.1537(4.1192), Acc: 0.2217(0.3079)
2021-12-30 11:01:41,426 Epoch[113/310], Step[0850/1251], Loss: 4.4627(4.1187), Acc: 0.2217(0.3063)
2021-12-30 11:02:40,961 Epoch[113/310], Step[0900/1251], Loss: 4.1858(4.1177), Acc: 0.2246(0.3071)
2021-12-30 11:03:41,691 Epoch[113/310], Step[0950/1251], Loss: 4.2170(4.1207), Acc: 0.1846(0.3061)
2021-12-30 11:04:42,260 Epoch[113/310], Step[1000/1251], Loss: 4.3322(4.1217), Acc: 0.3174(0.3051)
2021-12-30 11:05:42,086 Epoch[113/310], Step[1050/1251], Loss: 4.4812(4.1197), Acc: 0.2695(0.3060)
2021-12-30 11:06:42,227 Epoch[113/310], Step[1100/1251], Loss: 4.2938(4.1217), Acc: 0.3262(0.3048)
2021-12-30 11:07:42,894 Epoch[113/310], Step[1150/1251], Loss: 4.1568(4.1196), Acc: 0.1855(0.3058)
2021-12-30 11:08:44,336 Epoch[113/310], Step[1200/1251], Loss: 3.7766(4.1151), Acc: 0.3467(0.3061)
2021-12-30 11:09:43,071 Epoch[113/310], Step[1250/1251], Loss: 4.0263(4.1156), Acc: 0.4453(0.3068)
2021-12-30 11:09:44,629 ----- Epoch[113/310], Train Loss: 4.1156, Train Acc: 0.3068, time: 1567.64, Best Val(epoch112) Acc@1: 0.6488
2021-12-30 11:09:44,810 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 11:09:44,810 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 11:09:44,913 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 11:09:44,914 Now training epoch 114. LR=0.000686
2021-12-30 11:11:01,132 Epoch[114/310], Step[0000/1251], Loss: 3.7092(3.7092), Acc: 0.4922(0.4922)
2021-12-30 11:12:02,361 Epoch[114/310], Step[0050/1251], Loss: 4.3468(4.0212), Acc: 0.2617(0.3380)
2021-12-30 11:13:01,484 Epoch[114/310], Step[0100/1251], Loss: 4.4852(4.0793), Acc: 0.3369(0.3192)
2021-12-30 11:14:00,510 Epoch[114/310], Step[0150/1251], Loss: 4.4575(4.1073), Acc: 0.2217(0.3131)
2021-12-30 11:15:00,144 Epoch[114/310], Step[0200/1251], Loss: 4.0621(4.1087), Acc: 0.1982(0.3127)
2021-12-30 11:15:59,837 Epoch[114/310], Step[0250/1251], Loss: 4.2257(4.0974), Acc: 0.3096(0.3144)
2021-12-30 11:17:00,037 Epoch[114/310], Step[0300/1251], Loss: 4.9390(4.1006), Acc: 0.2041(0.3101)
2021-12-30 11:17:58,335 Epoch[114/310], Step[0350/1251], Loss: 4.0359(4.1067), Acc: 0.4111(0.3079)
2021-12-30 11:18:58,030 Epoch[114/310], Step[0400/1251], Loss: 4.2490(4.1074), Acc: 0.3721(0.3095)
2021-12-30 11:19:57,928 Epoch[114/310], Step[0450/1251], Loss: 4.0726(4.1083), Acc: 0.3428(0.3093)
2021-12-30 11:20:58,337 Epoch[114/310], Step[0500/1251], Loss: 3.9766(4.1027), Acc: 0.4082(0.3068)
2021-12-30 11:21:57,497 Epoch[114/310], Step[0550/1251], Loss: 4.5257(4.1078), Acc: 0.3252(0.3078)
2021-12-30 11:22:58,278 Epoch[114/310], Step[0600/1251], Loss: 4.1932(4.1121), Acc: 0.3262(0.3072)
2021-12-30 11:23:59,083 Epoch[114/310], Step[0650/1251], Loss: 4.1160(4.1106), Acc: 0.4277(0.3084)
2021-12-30 11:24:59,790 Epoch[114/310], Step[0700/1251], Loss: 4.2327(4.1068), Acc: 0.3213(0.3084)
2021-12-30 11:25:59,983 Epoch[114/310], Step[0750/1251], Loss: 4.6447(4.1015), Acc: 0.2471(0.3081)
2021-12-30 11:26:59,058 Epoch[114/310], Step[0800/1251], Loss: 4.0130(4.0998), Acc: 0.1592(0.3077)
2021-12-30 11:27:58,822 Epoch[114/310], Step[0850/1251], Loss: 3.8950(4.0970), Acc: 0.1514(0.3064)
2021-12-30 11:28:59,952 Epoch[114/310], Step[0900/1251], Loss: 4.5418(4.0992), Acc: 0.1641(0.3062)
2021-12-30 11:30:01,443 Epoch[114/310], Step[0950/1251], Loss: 4.1513(4.0981), Acc: 0.4326(0.3050)
2021-12-30 11:31:02,683 Epoch[114/310], Step[1000/1251], Loss: 4.2782(4.0966), Acc: 0.0840(0.3034)
2021-12-30 11:32:02,775 Epoch[114/310], Step[1050/1251], Loss: 4.2816(4.1004), Acc: 0.3506(0.3029)
2021-12-30 11:33:03,010 Epoch[114/310], Step[1100/1251], Loss: 4.2613(4.1009), Acc: 0.2178(0.3025)
2021-12-30 11:34:02,546 Epoch[114/310], Step[1150/1251], Loss: 4.3967(4.1041), Acc: 0.3145(0.3023)
2021-12-30 11:35:01,428 Epoch[114/310], Step[1200/1251], Loss: 4.1935(4.1069), Acc: 0.2227(0.3021)
2021-12-30 11:36:00,611 Epoch[114/310], Step[1250/1251], Loss: 4.4114(4.1041), Acc: 0.2461(0.3027)
2021-12-30 11:36:02,212 ----- Validation after Epoch: 114
2021-12-30 11:36:54,309 Val Step[0000/1563], Loss: 0.8609 (0.8609), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-30 11:36:55,665 Val Step[0050/1563], Loss: 2.2546 (0.9886), Acc@1: 0.4375 (0.8039), Acc@5: 0.8125 (0.9461)
2021-12-30 11:36:56,966 Val Step[0100/1563], Loss: 2.3042 (1.3319), Acc@1: 0.3438 (0.7088), Acc@5: 0.8125 (0.9066)
2021-12-30 11:36:58,274 Val Step[0150/1563], Loss: 0.5703 (1.2545), Acc@1: 0.8438 (0.7250), Acc@5: 0.9688 (0.9116)
2021-12-30 11:36:59,574 Val Step[0200/1563], Loss: 1.2762 (1.2778), Acc@1: 0.7500 (0.7237), Acc@5: 0.9688 (0.9081)
2021-12-30 11:37:00,866 Val Step[0250/1563], Loss: 1.3793 (1.2129), Acc@1: 0.6562 (0.7394), Acc@5: 1.0000 (0.9177)
2021-12-30 11:37:02,154 Val Step[0300/1563], Loss: 1.2435 (1.2764), Acc@1: 0.7188 (0.7183), Acc@5: 0.9688 (0.9127)
2021-12-30 11:37:03,460 Val Step[0350/1563], Loss: 1.4359 (1.2968), Acc@1: 0.6562 (0.7091), Acc@5: 0.9062 (0.9136)
2021-12-30 11:37:04,777 Val Step[0400/1563], Loss: 1.3335 (1.3039), Acc@1: 0.7188 (0.7039), Acc@5: 0.9688 (0.9139)
2021-12-30 11:37:06,095 Val Step[0450/1563], Loss: 1.2337 (1.3072), Acc@1: 0.5938 (0.7026), Acc@5: 0.9688 (0.9144)
2021-12-30 11:37:07,381 Val Step[0500/1563], Loss: 0.3871 (1.2927), Acc@1: 0.9375 (0.7055), Acc@5: 1.0000 (0.9165)
2021-12-30 11:37:08,864 Val Step[0550/1563], Loss: 1.0205 (1.2633), Acc@1: 0.7188 (0.7139), Acc@5: 0.9062 (0.9188)
2021-12-30 11:37:10,322 Val Step[0600/1563], Loss: 0.6212 (1.2678), Acc@1: 0.8750 (0.7131), Acc@5: 0.9688 (0.9182)
2021-12-30 11:37:11,643 Val Step[0650/1563], Loss: 1.1877 (1.2886), Acc@1: 0.7188 (0.7091), Acc@5: 1.0000 (0.9147)
2021-12-30 11:37:12,982 Val Step[0700/1563], Loss: 1.4394 (1.3216), Acc@1: 0.7188 (0.7017), Acc@5: 0.8438 (0.9106)
2021-12-30 11:37:14,403 Val Step[0750/1563], Loss: 1.6904 (1.3567), Acc@1: 0.6562 (0.6955), Acc@5: 0.7812 (0.9050)
2021-12-30 11:37:15,693 Val Step[0800/1563], Loss: 1.2683 (1.3978), Acc@1: 0.6562 (0.6859), Acc@5: 0.9375 (0.8990)
2021-12-30 11:37:17,002 Val Step[0850/1563], Loss: 1.3172 (1.4252), Acc@1: 0.6875 (0.6804), Acc@5: 0.9688 (0.8951)
2021-12-30 11:37:18,274 Val Step[0900/1563], Loss: 0.4302 (1.4285), Acc@1: 0.9062 (0.6815), Acc@5: 1.0000 (0.8937)
2021-12-30 11:37:19,660 Val Step[0950/1563], Loss: 1.7836 (1.4525), Acc@1: 0.5938 (0.6778), Acc@5: 0.8750 (0.8896)
2021-12-30 11:37:20,976 Val Step[1000/1563], Loss: 0.6361 (1.4782), Acc@1: 0.9375 (0.6719), Acc@5: 1.0000 (0.8855)
2021-12-30 11:37:22,277 Val Step[1050/1563], Loss: 0.5179 (1.4974), Acc@1: 0.9688 (0.6674), Acc@5: 0.9688 (0.8831)
2021-12-30 11:37:23,568 Val Step[1100/1563], Loss: 1.1447 (1.5133), Acc@1: 0.7812 (0.6643), Acc@5: 0.9062 (0.8805)
2021-12-30 11:37:24,949 Val Step[1150/1563], Loss: 1.6227 (1.5283), Acc@1: 0.6562 (0.6621), Acc@5: 0.7500 (0.8776)
2021-12-30 11:37:26,262 Val Step[1200/1563], Loss: 1.3846 (1.5477), Acc@1: 0.7812 (0.6582), Acc@5: 0.8750 (0.8740)
2021-12-30 11:37:27,534 Val Step[1250/1563], Loss: 0.9766 (1.5632), Acc@1: 0.8750 (0.6559), Acc@5: 0.9062 (0.8713)
2021-12-30 11:37:28,921 Val Step[1300/1563], Loss: 1.2092 (1.5750), Acc@1: 0.7188 (0.6538), Acc@5: 0.8750 (0.8697)
2021-12-30 11:37:30,309 Val Step[1350/1563], Loss: 2.3213 (1.5957), Acc@1: 0.4062 (0.6495), Acc@5: 0.7812 (0.8663)
2021-12-30 11:37:31,572 Val Step[1400/1563], Loss: 1.5313 (1.6070), Acc@1: 0.6562 (0.6468), Acc@5: 0.9062 (0.8648)
2021-12-30 11:37:32,919 Val Step[1450/1563], Loss: 1.6195 (1.6131), Acc@1: 0.6875 (0.6454), Acc@5: 0.9062 (0.8639)
2021-12-30 11:37:34,192 Val Step[1500/1563], Loss: 2.5109 (1.6003), Acc@1: 0.3438 (0.6480), Acc@5: 0.7500 (0.8657)
2021-12-30 11:37:35,456 Val Step[1550/1563], Loss: 1.0136 (1.6014), Acc@1: 0.8750 (0.6476), Acc@5: 0.9062 (0.8656)
2021-12-30 11:37:36,305 ----- Epoch[114/310], Validation Loss: 1.5988, Validation Acc@1: 0.6482, Validation Acc@5: 0.8658, time: 94.09
2021-12-30 11:37:36,305 ----- Epoch[114/310], Train Loss: 4.1041, Train Acc: 0.3027, time: 1577.29, Best Val(epoch112) Acc@1: 0.6488
2021-12-30 11:37:36,489 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 11:37:36,489 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 11:37:36,595 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 11:37:36,883 Now training epoch 115. LR=0.000681
2021-12-30 11:38:47,457 Epoch[115/310], Step[0000/1251], Loss: 3.7722(3.7722), Acc: 0.2549(0.2549)
2021-12-30 11:39:44,871 Epoch[115/310], Step[0050/1251], Loss: 3.6350(4.0694), Acc: 0.4795(0.3077)
2021-12-30 11:40:44,170 Epoch[115/310], Step[0100/1251], Loss: 4.3765(4.0420), Acc: 0.2695(0.3148)
2021-12-30 11:41:43,822 Epoch[115/310], Step[0150/1251], Loss: 4.4659(4.0661), Acc: 0.3838(0.3071)
2021-12-30 11:42:42,612 Epoch[115/310], Step[0200/1251], Loss: 4.1781(4.0520), Acc: 0.4580(0.3107)
2021-12-30 11:43:41,382 Epoch[115/310], Step[0250/1251], Loss: 4.1637(4.0689), Acc: 0.3623(0.3079)
2021-12-30 11:44:41,833 Epoch[115/310], Step[0300/1251], Loss: 4.7025(4.0794), Acc: 0.1494(0.3069)
2021-12-30 11:45:41,337 Epoch[115/310], Step[0350/1251], Loss: 4.4925(4.0824), Acc: 0.1094(0.3074)
2021-12-30 11:46:42,306 Epoch[115/310], Step[0400/1251], Loss: 4.5342(4.0839), Acc: 0.2686(0.3100)
2021-12-30 11:47:42,912 Epoch[115/310], Step[0450/1251], Loss: 4.0261(4.0932), Acc: 0.3125(0.3077)
2021-12-30 11:48:43,058 Epoch[115/310], Step[0500/1251], Loss: 4.2334(4.0907), Acc: 0.2383(0.3093)
2021-12-30 11:49:43,274 Epoch[115/310], Step[0550/1251], Loss: 3.9036(4.0950), Acc: 0.3203(0.3060)
2021-12-30 11:50:42,162 Epoch[115/310], Step[0600/1251], Loss: 3.6597(4.0865), Acc: 0.3545(0.3056)
2021-12-30 11:51:41,432 Epoch[115/310], Step[0650/1251], Loss: 4.3478(4.0907), Acc: 0.2461(0.3048)
2021-12-30 11:52:41,317 Epoch[115/310], Step[0700/1251], Loss: 3.9465(4.0925), Acc: 0.3389(0.3042)
2021-12-30 11:53:38,802 Epoch[115/310], Step[0750/1251], Loss: 4.2280(4.0916), Acc: 0.3525(0.3057)
2021-12-30 11:54:37,599 Epoch[115/310], Step[0800/1251], Loss: 4.1664(4.0947), Acc: 0.3115(0.3057)
2021-12-30 11:55:36,171 Epoch[115/310], Step[0850/1251], Loss: 4.3843(4.0973), Acc: 0.1738(0.3052)
2021-12-30 11:56:36,195 Epoch[115/310], Step[0900/1251], Loss: 4.2181(4.0967), Acc: 0.1738(0.3057)
2021-12-30 11:57:36,507 Epoch[115/310], Step[0950/1251], Loss: 3.7435(4.0940), Acc: 0.4561(0.3062)
2021-12-30 11:58:36,702 Epoch[115/310], Step[1000/1251], Loss: 3.5771(4.0957), Acc: 0.3711(0.3063)
2021-12-30 11:59:36,258 Epoch[115/310], Step[1050/1251], Loss: 3.9600(4.0958), Acc: 0.3535(0.3062)
2021-12-30 12:00:35,996 Epoch[115/310], Step[1100/1251], Loss: 4.0831(4.0948), Acc: 0.0664(0.3058)
2021-12-30 12:01:34,117 Epoch[115/310], Step[1150/1251], Loss: 4.1029(4.0991), Acc: 0.3359(0.3061)
2021-12-30 12:02:32,933 Epoch[115/310], Step[1200/1251], Loss: 4.1772(4.0955), Acc: 0.2568(0.3066)
2021-12-30 12:03:32,126 Epoch[115/310], Step[1250/1251], Loss: 4.2888(4.0933), Acc: 0.1738(0.3062)
2021-12-30 12:03:33,852 ----- Epoch[115/310], Train Loss: 4.0933, Train Acc: 0.3062, time: 1556.97, Best Val(epoch112) Acc@1: 0.6488
2021-12-30 12:03:34,038 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 12:03:34,039 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 12:03:34,124 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 12:03:34,124 Now training epoch 116. LR=0.000676
2021-12-30 12:04:52,462 Epoch[116/310], Step[0000/1251], Loss: 4.2432(4.2432), Acc: 0.2842(0.2842)
2021-12-30 12:05:50,753 Epoch[116/310], Step[0050/1251], Loss: 4.0909(4.1483), Acc: 0.3545(0.2886)
2021-12-30 12:06:49,205 Epoch[116/310], Step[0100/1251], Loss: 4.6041(4.1545), Acc: 0.2979(0.2997)
2021-12-30 12:07:48,336 Epoch[116/310], Step[0150/1251], Loss: 4.6516(4.1462), Acc: 0.2852(0.2972)
2021-12-30 12:08:47,433 Epoch[116/310], Step[0200/1251], Loss: 3.9951(4.1347), Acc: 0.3271(0.2991)
2021-12-30 12:09:47,174 Epoch[116/310], Step[0250/1251], Loss: 4.2163(4.1397), Acc: 0.2725(0.3025)
2021-12-30 12:10:46,672 Epoch[116/310], Step[0300/1251], Loss: 4.4413(4.1297), Acc: 0.3379(0.3075)
2021-12-30 12:11:45,660 Epoch[116/310], Step[0350/1251], Loss: 4.3252(4.1300), Acc: 0.1162(0.3068)
2021-12-30 12:12:44,824 Epoch[116/310], Step[0400/1251], Loss: 3.9405(4.1225), Acc: 0.2461(0.3076)
2021-12-30 12:13:44,283 Epoch[116/310], Step[0450/1251], Loss: 4.1007(4.1181), Acc: 0.3340(0.3066)
2021-12-30 12:14:45,001 Epoch[116/310], Step[0500/1251], Loss: 3.9725(4.1178), Acc: 0.3545(0.3050)
2021-12-30 12:15:45,673 Epoch[116/310], Step[0550/1251], Loss: 4.3436(4.1140), Acc: 0.1816(0.3039)
2021-12-30 12:16:46,426 Epoch[116/310], Step[0600/1251], Loss: 4.0700(4.1117), Acc: 0.3193(0.3044)
2021-12-30 12:17:46,236 Epoch[116/310], Step[0650/1251], Loss: 4.3800(4.1184), Acc: 0.4092(0.3047)
2021-12-30 12:18:45,851 Epoch[116/310], Step[0700/1251], Loss: 4.2274(4.1131), Acc: 0.4277(0.3048)
2021-12-30 12:19:45,073 Epoch[116/310], Step[0750/1251], Loss: 4.1936(4.1087), Acc: 0.3389(0.3067)
2021-12-30 12:20:45,018 Epoch[116/310], Step[0800/1251], Loss: 3.9319(4.1034), Acc: 0.4365(0.3078)
2021-12-30 12:21:44,733 Epoch[116/310], Step[0850/1251], Loss: 4.4497(4.1075), Acc: 0.2402(0.3073)
2021-12-30 12:22:45,093 Epoch[116/310], Step[0900/1251], Loss: 3.9280(4.1093), Acc: 0.2578(0.3068)
2021-12-30 12:23:45,190 Epoch[116/310], Step[0950/1251], Loss: 4.4993(4.1115), Acc: 0.3789(0.3066)
2021-12-30 12:24:45,866 Epoch[116/310], Step[1000/1251], Loss: 4.2217(4.1110), Acc: 0.2637(0.3060)
2021-12-30 12:25:47,073 Epoch[116/310], Step[1050/1251], Loss: 3.9717(4.1114), Acc: 0.2617(0.3060)
2021-12-30 12:26:47,904 Epoch[116/310], Step[1100/1251], Loss: 3.7200(4.1133), Acc: 0.4180(0.3061)
2021-12-30 12:27:47,033 Epoch[116/310], Step[1150/1251], Loss: 4.4682(4.1113), Acc: 0.2773(0.3059)
2021-12-30 12:28:46,591 Epoch[116/310], Step[1200/1251], Loss: 3.8877(4.1096), Acc: 0.3896(0.3057)
2021-12-30 12:29:44,907 Epoch[116/310], Step[1250/1251], Loss: 4.3036(4.1080), Acc: 0.2236(0.3060)
2021-12-30 12:29:47,522 ----- Validation after Epoch: 116
2021-12-30 12:30:40,010 Val Step[0000/1563], Loss: 0.7748 (0.7748), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-30 12:30:41,393 Val Step[0050/1563], Loss: 2.0522 (0.9226), Acc@1: 0.4688 (0.8137), Acc@5: 0.8750 (0.9424)
2021-12-30 12:30:42,823 Val Step[0100/1563], Loss: 2.2474 (1.2474), Acc@1: 0.2812 (0.7209), Acc@5: 0.8438 (0.9112)
2021-12-30 12:30:44,241 Val Step[0150/1563], Loss: 0.7630 (1.1762), Acc@1: 0.8438 (0.7355), Acc@5: 0.9688 (0.9158)
2021-12-30 12:30:45,630 Val Step[0200/1563], Loss: 1.7045 (1.2131), Acc@1: 0.5625 (0.7320), Acc@5: 0.8750 (0.9106)
2021-12-30 12:30:46,965 Val Step[0250/1563], Loss: 0.9228 (1.1591), Acc@1: 0.8750 (0.7448), Acc@5: 1.0000 (0.9182)
2021-12-30 12:30:48,281 Val Step[0300/1563], Loss: 1.1470 (1.2166), Acc@1: 0.6875 (0.7253), Acc@5: 0.9062 (0.9131)
2021-12-30 12:30:49,740 Val Step[0350/1563], Loss: 1.3965 (1.2297), Acc@1: 0.6875 (0.7184), Acc@5: 0.9375 (0.9152)
2021-12-30 12:30:51,136 Val Step[0400/1563], Loss: 1.2014 (1.2313), Acc@1: 0.7188 (0.7136), Acc@5: 0.9688 (0.9165)
2021-12-30 12:30:52,523 Val Step[0450/1563], Loss: 1.0817 (1.2417), Acc@1: 0.6562 (0.7104), Acc@5: 1.0000 (0.9163)
2021-12-30 12:30:53,873 Val Step[0500/1563], Loss: 0.5044 (1.2353), Acc@1: 0.8750 (0.7131), Acc@5: 1.0000 (0.9169)
2021-12-30 12:30:55,316 Val Step[0550/1563], Loss: 1.1877 (1.2124), Acc@1: 0.5938 (0.7193), Acc@5: 0.9375 (0.9193)
2021-12-30 12:30:56,713 Val Step[0600/1563], Loss: 0.7245 (1.2133), Acc@1: 0.8750 (0.7208), Acc@5: 0.9688 (0.9191)
2021-12-30 12:30:58,061 Val Step[0650/1563], Loss: 0.8248 (1.2323), Acc@1: 0.8125 (0.7167), Acc@5: 1.0000 (0.9165)
2021-12-30 12:30:59,360 Val Step[0700/1563], Loss: 1.1849 (1.2677), Acc@1: 0.7500 (0.7084), Acc@5: 0.9375 (0.9113)
2021-12-30 12:31:00,712 Val Step[0750/1563], Loss: 1.5426 (1.3108), Acc@1: 0.6562 (0.6994), Acc@5: 0.8438 (0.9051)
2021-12-30 12:31:02,069 Val Step[0800/1563], Loss: 1.5126 (1.3561), Acc@1: 0.6562 (0.6895), Acc@5: 0.9062 (0.8989)
2021-12-30 12:31:03,341 Val Step[0850/1563], Loss: 1.8555 (1.3849), Acc@1: 0.5625 (0.6834), Acc@5: 0.8438 (0.8948)
2021-12-30 12:31:04,623 Val Step[0900/1563], Loss: 0.9803 (1.3888), Acc@1: 0.8125 (0.6840), Acc@5: 0.9062 (0.8935)
2021-12-30 12:31:06,087 Val Step[0950/1563], Loss: 1.5344 (1.4153), Acc@1: 0.6875 (0.6791), Acc@5: 0.8750 (0.8892)
2021-12-30 12:31:07,375 Val Step[1000/1563], Loss: 0.6650 (1.4425), Acc@1: 0.8750 (0.6734), Acc@5: 0.9375 (0.8853)
2021-12-30 12:31:08,652 Val Step[1050/1563], Loss: 0.5242 (1.4608), Acc@1: 0.9062 (0.6694), Acc@5: 0.9688 (0.8827)
2021-12-30 12:31:09,930 Val Step[1100/1563], Loss: 1.5492 (1.4770), Acc@1: 0.6875 (0.6666), Acc@5: 0.7812 (0.8797)
2021-12-30 12:31:11,297 Val Step[1150/1563], Loss: 1.4943 (1.4962), Acc@1: 0.6875 (0.6631), Acc@5: 0.8125 (0.8765)
2021-12-30 12:31:12,736 Val Step[1200/1563], Loss: 2.0198 (1.5145), Acc@1: 0.6875 (0.6596), Acc@5: 0.8438 (0.8734)
2021-12-30 12:31:14,022 Val Step[1250/1563], Loss: 0.8378 (1.5318), Acc@1: 0.9062 (0.6566), Acc@5: 0.9062 (0.8707)
2021-12-30 12:31:15,316 Val Step[1300/1563], Loss: 1.0728 (1.5442), Acc@1: 0.7500 (0.6533), Acc@5: 0.8438 (0.8691)
2021-12-30 12:31:16,606 Val Step[1350/1563], Loss: 2.5068 (1.5642), Acc@1: 0.2500 (0.6494), Acc@5: 0.7188 (0.8657)
2021-12-30 12:31:17,889 Val Step[1400/1563], Loss: 1.2993 (1.5737), Acc@1: 0.7812 (0.6476), Acc@5: 0.8750 (0.8645)
2021-12-30 12:31:19,185 Val Step[1450/1563], Loss: 1.6412 (1.5794), Acc@1: 0.6250 (0.6462), Acc@5: 0.9375 (0.8639)
2021-12-30 12:31:20,459 Val Step[1500/1563], Loss: 1.7821 (1.5669), Acc@1: 0.5938 (0.6489), Acc@5: 0.9062 (0.8657)
2021-12-30 12:31:21,721 Val Step[1550/1563], Loss: 0.9077 (1.5647), Acc@1: 0.8750 (0.6491), Acc@5: 0.9062 (0.8660)
2021-12-30 12:31:22,503 ----- Epoch[116/310], Validation Loss: 1.5618, Validation Acc@1: 0.6497, Validation Acc@5: 0.8662, time: 94.98
2021-12-30 12:31:22,503 ----- Epoch[116/310], Train Loss: 4.1080, Train Acc: 0.3060, time: 1573.39, Best Val(epoch116) Acc@1: 0.6497
2021-12-30 12:31:22,689 Max accuracy so far: 0.6497 at epoch_116
2021-12-30 12:31:22,689 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 12:31:22,689 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 12:31:22,798 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 12:31:23,177 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 12:31:23,178 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 12:31:23,303 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 12:31:23,304 Now training epoch 117. LR=0.000671
2021-12-30 12:32:35,637 Epoch[117/310], Step[0000/1251], Loss: 4.3434(4.3434), Acc: 0.3760(0.3760)
2021-12-30 12:33:36,019 Epoch[117/310], Step[0050/1251], Loss: 4.2740(4.1341), Acc: 0.2705(0.2817)
2021-12-30 12:34:36,164 Epoch[117/310], Step[0100/1251], Loss: 4.7954(4.1232), Acc: 0.2441(0.2956)
2021-12-30 12:35:36,114 Epoch[117/310], Step[0150/1251], Loss: 4.3100(4.0779), Acc: 0.3750(0.2983)
2021-12-30 12:36:35,626 Epoch[117/310], Step[0200/1251], Loss: 3.3343(4.0636), Acc: 0.2637(0.2985)
2021-12-30 12:37:36,710 Epoch[117/310], Step[0250/1251], Loss: 4.2914(4.0616), Acc: 0.2168(0.3014)
2021-12-30 12:38:37,069 Epoch[117/310], Step[0300/1251], Loss: 4.0374(4.0690), Acc: 0.2910(0.3025)
2021-12-30 12:39:36,827 Epoch[117/310], Step[0350/1251], Loss: 4.0849(4.0790), Acc: 0.3184(0.3056)
2021-12-30 12:40:36,589 Epoch[117/310], Step[0400/1251], Loss: 3.5317(4.0746), Acc: 0.4092(0.3086)
2021-12-30 12:41:35,184 Epoch[117/310], Step[0450/1251], Loss: 3.8097(4.0715), Acc: 0.1963(0.3089)
2021-12-30 12:42:35,565 Epoch[117/310], Step[0500/1251], Loss: 4.6138(4.0766), Acc: 0.2344(0.3073)
2021-12-30 12:43:36,105 Epoch[117/310], Step[0550/1251], Loss: 3.5167(4.0725), Acc: 0.5029(0.3102)
2021-12-30 12:44:36,145 Epoch[117/310], Step[0600/1251], Loss: 3.5281(4.0714), Acc: 0.1260(0.3107)
2021-12-30 12:45:36,295 Epoch[117/310], Step[0650/1251], Loss: 4.7290(4.0727), Acc: 0.2100(0.3106)
2021-12-30 12:46:37,085 Epoch[117/310], Step[0700/1251], Loss: 3.9363(4.0775), Acc: 0.4834(0.3094)
2021-12-30 12:47:35,901 Epoch[117/310], Step[0750/1251], Loss: 4.1448(4.0781), Acc: 0.4277(0.3090)
2021-12-30 12:48:35,722 Epoch[117/310], Step[0800/1251], Loss: 3.9149(4.0824), Acc: 0.3301(0.3095)
2021-12-30 12:49:35,762 Epoch[117/310], Step[0850/1251], Loss: 4.2995(4.0899), Acc: 0.2568(0.3087)
2021-12-30 12:50:36,040 Epoch[117/310], Step[0900/1251], Loss: 4.1007(4.0940), Acc: 0.3926(0.3091)
2021-12-30 12:51:36,084 Epoch[117/310], Step[0950/1251], Loss: 4.7128(4.0890), Acc: 0.2979(0.3104)
2021-12-30 12:52:36,138 Epoch[117/310], Step[1000/1251], Loss: 3.7514(4.0934), Acc: 0.3584(0.3106)
2021-12-30 12:53:36,772 Epoch[117/310], Step[1050/1251], Loss: 4.3061(4.0957), Acc: 0.2695(0.3106)
2021-12-30 12:54:37,148 Epoch[117/310], Step[1100/1251], Loss: 4.1386(4.0965), Acc: 0.2627(0.3114)
2021-12-30 12:55:37,793 Epoch[117/310], Step[1150/1251], Loss: 4.2425(4.0967), Acc: 0.1807(0.3112)
2021-12-30 12:56:37,670 Epoch[117/310], Step[1200/1251], Loss: 4.4061(4.0963), Acc: 0.2998(0.3111)
2021-12-30 12:57:39,449 Epoch[117/310], Step[1250/1251], Loss: 3.7997(4.0959), Acc: 0.4648(0.3109)
2021-12-30 12:57:41,573 ----- Epoch[117/310], Train Loss: 4.0959, Train Acc: 0.3109, time: 1578.27, Best Val(epoch116) Acc@1: 0.6497
2021-12-30 12:57:41,753 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 12:57:41,753 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 12:57:41,858 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 12:57:41,859 Now training epoch 118. LR=0.000666
2021-12-30 12:58:53,615 Epoch[118/310], Step[0000/1251], Loss: 4.3481(4.3481), Acc: 0.2607(0.2607)
2021-12-30 12:59:54,046 Epoch[118/310], Step[0050/1251], Loss: 4.3116(4.0831), Acc: 0.3867(0.3002)
2021-12-30 13:00:54,206 Epoch[118/310], Step[0100/1251], Loss: 3.8658(4.1121), Acc: 0.1318(0.3003)
2021-12-30 13:01:53,237 Epoch[118/310], Step[0150/1251], Loss: 4.5923(4.0984), Acc: 0.0439(0.3037)
2021-12-30 13:02:52,587 Epoch[118/310], Step[0200/1251], Loss: 4.3457(4.0858), Acc: 0.2988(0.3045)
2021-12-30 13:03:51,251 Epoch[118/310], Step[0250/1251], Loss: 3.9313(4.0805), Acc: 0.4248(0.3061)
2021-12-30 13:04:51,035 Epoch[118/310], Step[0300/1251], Loss: 3.4304(4.1065), Acc: 0.3945(0.3050)
2021-12-30 13:05:50,867 Epoch[118/310], Step[0350/1251], Loss: 4.1660(4.0966), Acc: 0.3926(0.3045)
2021-12-30 13:06:50,763 Epoch[118/310], Step[0400/1251], Loss: 4.1653(4.0949), Acc: 0.3252(0.3074)
2021-12-30 13:07:49,061 Epoch[118/310], Step[0450/1251], Loss: 4.7351(4.0963), Acc: 0.2900(0.3078)
2021-12-30 13:08:47,285 Epoch[118/310], Step[0500/1251], Loss: 4.2189(4.0928), Acc: 0.3652(0.3068)
2021-12-30 13:09:45,024 Epoch[118/310], Step[0550/1251], Loss: 4.2653(4.0938), Acc: 0.3320(0.3093)
2021-12-30 13:10:44,346 Epoch[118/310], Step[0600/1251], Loss: 3.9895(4.0913), Acc: 0.3555(0.3097)
2021-12-30 13:11:43,648 Epoch[118/310], Step[0650/1251], Loss: 3.8985(4.0892), Acc: 0.3896(0.3115)
2021-12-30 13:12:43,185 Epoch[118/310], Step[0700/1251], Loss: 4.3406(4.0900), Acc: 0.1270(0.3107)
2021-12-30 13:13:43,049 Epoch[118/310], Step[0750/1251], Loss: 4.2252(4.0872), Acc: 0.3691(0.3104)
2021-12-30 13:14:42,535 Epoch[118/310], Step[0800/1251], Loss: 4.7685(4.0904), Acc: 0.1162(0.3091)
2021-12-30 13:15:42,355 Epoch[118/310], Step[0850/1251], Loss: 4.9210(4.0932), Acc: 0.2373(0.3076)
2021-12-30 13:16:41,223 Epoch[118/310], Step[0900/1251], Loss: 4.4586(4.0920), Acc: 0.3896(0.3086)
2021-12-30 13:17:41,005 Epoch[118/310], Step[0950/1251], Loss: 4.1188(4.0874), Acc: 0.4209(0.3090)
2021-12-30 13:18:40,422 Epoch[118/310], Step[1000/1251], Loss: 4.1844(4.0870), Acc: 0.3115(0.3089)
2021-12-30 13:19:40,300 Epoch[118/310], Step[1050/1251], Loss: 4.6122(4.0906), Acc: 0.2607(0.3083)
2021-12-30 13:20:40,234 Epoch[118/310], Step[1100/1251], Loss: 4.0412(4.0944), Acc: 0.4004(0.3074)
2021-12-30 13:21:39,414 Epoch[118/310], Step[1150/1251], Loss: 3.8771(4.0928), Acc: 0.1885(0.3089)
2021-12-30 13:22:39,771 Epoch[118/310], Step[1200/1251], Loss: 3.5473(4.0914), Acc: 0.4521(0.3087)
2021-12-30 13:23:39,600 Epoch[118/310], Step[1250/1251], Loss: 4.4961(4.0939), Acc: 0.2607(0.3083)
2021-12-30 13:23:41,214 ----- Validation after Epoch: 118
2021-12-30 13:24:31,292 Val Step[0000/1563], Loss: 0.6821 (0.6821), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-30 13:24:32,721 Val Step[0050/1563], Loss: 2.6202 (0.9835), Acc@1: 0.3750 (0.7880), Acc@5: 0.8125 (0.9412)
2021-12-30 13:24:34,094 Val Step[0100/1563], Loss: 2.2303 (1.3162), Acc@1: 0.4062 (0.7024), Acc@5: 0.8125 (0.9022)
2021-12-30 13:24:35,363 Val Step[0150/1563], Loss: 0.9218 (1.2360), Acc@1: 0.8125 (0.7227), Acc@5: 0.9375 (0.9089)
2021-12-30 13:24:36,713 Val Step[0200/1563], Loss: 1.0613 (1.2506), Acc@1: 0.7812 (0.7262), Acc@5: 0.9062 (0.9066)
2021-12-30 13:24:38,084 Val Step[0250/1563], Loss: 0.9833 (1.1838), Acc@1: 0.7812 (0.7414), Acc@5: 1.0000 (0.9153)
2021-12-30 13:24:39,481 Val Step[0300/1563], Loss: 1.3921 (1.2381), Acc@1: 0.6875 (0.7231), Acc@5: 0.9062 (0.9110)
2021-12-30 13:24:40,846 Val Step[0350/1563], Loss: 1.4280 (1.2527), Acc@1: 0.5938 (0.7162), Acc@5: 0.9375 (0.9127)
2021-12-30 13:24:42,151 Val Step[0400/1563], Loss: 1.3588 (1.2585), Acc@1: 0.6875 (0.7113), Acc@5: 0.9062 (0.9131)
2021-12-30 13:24:43,488 Val Step[0450/1563], Loss: 1.0312 (1.2603), Acc@1: 0.6875 (0.7087), Acc@5: 1.0000 (0.9142)
2021-12-30 13:24:44,905 Val Step[0500/1563], Loss: 0.5328 (1.2500), Acc@1: 0.8750 (0.7106), Acc@5: 1.0000 (0.9162)
2021-12-30 13:24:46,370 Val Step[0550/1563], Loss: 1.0476 (1.2214), Acc@1: 0.7188 (0.7185), Acc@5: 0.9688 (0.9193)
2021-12-30 13:24:47,718 Val Step[0600/1563], Loss: 0.8148 (1.2238), Acc@1: 0.8125 (0.7181), Acc@5: 0.9062 (0.9186)
2021-12-30 13:24:49,111 Val Step[0650/1563], Loss: 0.8752 (1.2488), Acc@1: 0.7812 (0.7137), Acc@5: 1.0000 (0.9151)
2021-12-30 13:24:50,442 Val Step[0700/1563], Loss: 1.3531 (1.2844), Acc@1: 0.7812 (0.7067), Acc@5: 0.9062 (0.9098)
2021-12-30 13:24:51,861 Val Step[0750/1563], Loss: 1.8964 (1.3230), Acc@1: 0.5938 (0.6992), Acc@5: 0.7812 (0.9035)
2021-12-30 13:24:53,284 Val Step[0800/1563], Loss: 1.2435 (1.3679), Acc@1: 0.6875 (0.6884), Acc@5: 0.9375 (0.8972)
2021-12-30 13:24:54,631 Val Step[0850/1563], Loss: 1.6078 (1.3974), Acc@1: 0.6562 (0.6830), Acc@5: 0.9062 (0.8933)
2021-12-30 13:24:55,993 Val Step[0900/1563], Loss: 0.4940 (1.4017), Acc@1: 0.9062 (0.6836), Acc@5: 0.9688 (0.8924)
2021-12-30 13:24:57,371 Val Step[0950/1563], Loss: 1.6142 (1.4254), Acc@1: 0.6250 (0.6794), Acc@5: 0.8750 (0.8888)
2021-12-30 13:24:58,718 Val Step[1000/1563], Loss: 0.5803 (1.4527), Acc@1: 0.9688 (0.6731), Acc@5: 1.0000 (0.8847)
2021-12-30 13:25:00,093 Val Step[1050/1563], Loss: 0.4090 (1.4675), Acc@1: 0.9688 (0.6706), Acc@5: 0.9688 (0.8831)
2021-12-30 13:25:01,448 Val Step[1100/1563], Loss: 1.4614 (1.4847), Acc@1: 0.6875 (0.6671), Acc@5: 0.8750 (0.8804)
2021-12-30 13:25:02,782 Val Step[1150/1563], Loss: 1.6232 (1.5042), Acc@1: 0.7188 (0.6638), Acc@5: 0.7812 (0.8770)
2021-12-30 13:25:04,117 Val Step[1200/1563], Loss: 1.1871 (1.5230), Acc@1: 0.7812 (0.6596), Acc@5: 0.8750 (0.8743)
2021-12-30 13:25:05,428 Val Step[1250/1563], Loss: 1.1468 (1.5392), Acc@1: 0.8125 (0.6578), Acc@5: 0.9062 (0.8717)
2021-12-30 13:25:06,710 Val Step[1300/1563], Loss: 1.3006 (1.5515), Acc@1: 0.7500 (0.6557), Acc@5: 0.8750 (0.8699)
2021-12-30 13:25:08,017 Val Step[1350/1563], Loss: 1.9811 (1.5708), Acc@1: 0.5000 (0.6518), Acc@5: 0.8125 (0.8664)
2021-12-30 13:25:09,382 Val Step[1400/1563], Loss: 1.2472 (1.5824), Acc@1: 0.7188 (0.6492), Acc@5: 0.9375 (0.8648)
2021-12-30 13:25:10,779 Val Step[1450/1563], Loss: 1.6100 (1.5897), Acc@1: 0.5938 (0.6479), Acc@5: 0.9375 (0.8643)
2021-12-30 13:25:12,243 Val Step[1500/1563], Loss: 1.9105 (1.5777), Acc@1: 0.5312 (0.6506), Acc@5: 0.8750 (0.8661)
2021-12-30 13:25:13,562 Val Step[1550/1563], Loss: 1.0424 (1.5773), Acc@1: 0.8750 (0.6504), Acc@5: 0.9062 (0.8663)
2021-12-30 13:25:14,473 ----- Epoch[118/310], Validation Loss: 1.5748, Validation Acc@1: 0.6509, Validation Acc@5: 0.8665, time: 93.26
2021-12-30 13:25:14,473 ----- Epoch[118/310], Train Loss: 4.0939, Train Acc: 0.3083, time: 1559.35, Best Val(epoch118) Acc@1: 0.6509
2021-12-30 13:25:14,660 Max accuracy so far: 0.6509 at epoch_118
2021-12-30 13:25:14,661 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 13:25:14,661 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 13:25:14,769 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 13:25:15,154 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 13:25:15,154 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 13:25:15,290 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 13:25:15,291 Now training epoch 119. LR=0.000661
2021-12-30 13:26:25,390 Epoch[119/310], Step[0000/1251], Loss: 3.6777(3.6777), Acc: 0.5186(0.5186)
2021-12-30 13:27:25,269 Epoch[119/310], Step[0050/1251], Loss: 4.0244(4.0651), Acc: 0.2939(0.3257)
2021-12-30 13:28:25,594 Epoch[119/310], Step[0100/1251], Loss: 3.9066(4.0691), Acc: 0.4932(0.3188)
2021-12-30 13:29:24,127 Epoch[119/310], Step[0150/1251], Loss: 3.9849(4.0443), Acc: 0.4395(0.3224)
2021-12-30 13:30:24,274 Epoch[119/310], Step[0200/1251], Loss: 3.0984(4.0443), Acc: 0.2783(0.3217)
2021-12-30 13:31:24,837 Epoch[119/310], Step[0250/1251], Loss: 4.2001(4.0410), Acc: 0.1123(0.3200)
2021-12-30 13:32:24,234 Epoch[119/310], Step[0300/1251], Loss: 4.0184(4.0436), Acc: 0.2041(0.3196)
2021-12-30 13:33:25,015 Epoch[119/310], Step[0350/1251], Loss: 4.0244(4.0569), Acc: 0.1611(0.3158)
2021-12-30 13:34:24,870 Epoch[119/310], Step[0400/1251], Loss: 4.2863(4.0592), Acc: 0.3838(0.3160)
2021-12-30 13:35:25,481 Epoch[119/310], Step[0450/1251], Loss: 3.9447(4.0656), Acc: 0.3213(0.3140)
2021-12-30 13:36:26,466 Epoch[119/310], Step[0500/1251], Loss: 4.0342(4.0651), Acc: 0.4277(0.3120)
2021-12-30 13:37:26,217 Epoch[119/310], Step[0550/1251], Loss: 3.4701(4.0673), Acc: 0.2383(0.3123)
2021-12-30 13:38:25,554 Epoch[119/310], Step[0600/1251], Loss: 3.8687(4.0690), Acc: 0.3359(0.3100)
2021-12-30 13:39:26,698 Epoch[119/310], Step[0650/1251], Loss: 4.5767(4.0708), Acc: 0.1699(0.3083)
2021-12-30 13:40:27,033 Epoch[119/310], Step[0700/1251], Loss: 3.5258(4.0689), Acc: 0.4404(0.3084)
2021-12-30 13:41:25,996 Epoch[119/310], Step[0750/1251], Loss: 3.9949(4.0705), Acc: 0.3145(0.3080)
2021-12-30 13:42:25,777 Epoch[119/310], Step[0800/1251], Loss: 3.8299(4.0705), Acc: 0.4727(0.3092)
2021-12-30 13:43:24,882 Epoch[119/310], Step[0850/1251], Loss: 3.7045(4.0714), Acc: 0.3701(0.3090)
2021-12-30 13:44:23,536 Epoch[119/310], Step[0900/1251], Loss: 3.7064(4.0728), Acc: 0.3906(0.3091)
2021-12-30 13:45:24,037 Epoch[119/310], Step[0950/1251], Loss: 4.2687(4.0768), Acc: 0.3076(0.3085)
2021-12-30 13:46:23,831 Epoch[119/310], Step[1000/1251], Loss: 4.2938(4.0756), Acc: 0.2451(0.3086)
2021-12-30 13:47:22,719 Epoch[119/310], Step[1050/1251], Loss: 4.0790(4.0783), Acc: 0.3721(0.3090)
2021-12-30 13:48:23,474 Epoch[119/310], Step[1100/1251], Loss: 4.0911(4.0781), Acc: 0.2715(0.3081)
2021-12-30 13:49:23,304 Epoch[119/310], Step[1150/1251], Loss: 4.0230(4.0761), Acc: 0.3369(0.3082)
2021-12-30 13:50:24,303 Epoch[119/310], Step[1200/1251], Loss: 3.7964(4.0758), Acc: 0.3408(0.3080)
2021-12-30 13:51:23,642 Epoch[119/310], Step[1250/1251], Loss: 3.7407(4.0719), Acc: 0.4873(0.3095)
2021-12-30 13:51:25,192 ----- Epoch[119/310], Train Loss: 4.0719, Train Acc: 0.3095, time: 1569.90, Best Val(epoch118) Acc@1: 0.6509
2021-12-30 13:51:25,372 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 13:51:25,372 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 13:51:25,491 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 13:51:25,492 Now training epoch 120. LR=0.000656
2021-12-30 13:52:36,138 Epoch[120/310], Step[0000/1251], Loss: 4.5111(4.5111), Acc: 0.2812(0.2812)
2021-12-30 13:53:35,382 Epoch[120/310], Step[0050/1251], Loss: 4.2744(4.1550), Acc: 0.1719(0.2987)
2021-12-30 13:54:35,351 Epoch[120/310], Step[0100/1251], Loss: 4.4393(4.1155), Acc: 0.3184(0.3077)
2021-12-30 13:55:32,434 Epoch[120/310], Step[0150/1251], Loss: 4.0565(4.1037), Acc: 0.2939(0.3146)
2021-12-30 13:56:31,908 Epoch[120/310], Step[0200/1251], Loss: 4.1207(4.0921), Acc: 0.4199(0.3093)
2021-12-30 13:57:31,078 Epoch[120/310], Step[0250/1251], Loss: 4.5763(4.0849), Acc: 0.2822(0.3111)
2021-12-30 13:58:30,339 Epoch[120/310], Step[0300/1251], Loss: 4.2835(4.0848), Acc: 0.3750(0.3105)
2021-12-30 13:59:29,527 Epoch[120/310], Step[0350/1251], Loss: 3.8457(4.0727), Acc: 0.2178(0.3092)
2021-12-30 14:00:29,796 Epoch[120/310], Step[0400/1251], Loss: 3.8497(4.0694), Acc: 0.3135(0.3093)
2021-12-30 14:01:28,838 Epoch[120/310], Step[0450/1251], Loss: 4.1874(4.0789), Acc: 0.1787(0.3088)
2021-12-30 14:02:26,549 Epoch[120/310], Step[0500/1251], Loss: 4.0608(4.0826), Acc: 0.4248(0.3097)
2021-12-30 14:03:25,882 Epoch[120/310], Step[0550/1251], Loss: 3.9187(4.0816), Acc: 0.4258(0.3091)
2021-12-30 14:04:24,258 Epoch[120/310], Step[0600/1251], Loss: 4.2254(4.0782), Acc: 0.4297(0.3110)
2021-12-30 14:05:23,263 Epoch[120/310], Step[0650/1251], Loss: 4.0930(4.0820), Acc: 0.4570(0.3098)
2021-12-30 14:06:22,405 Epoch[120/310], Step[0700/1251], Loss: 4.1858(4.0842), Acc: 0.2158(0.3098)
2021-12-30 14:07:22,541 Epoch[120/310], Step[0750/1251], Loss: 4.5301(4.0843), Acc: 0.3242(0.3087)
2021-12-30 14:08:21,445 Epoch[120/310], Step[0800/1251], Loss: 4.0390(4.0844), Acc: 0.2812(0.3095)
2021-12-30 14:09:20,772 Epoch[120/310], Step[0850/1251], Loss: 4.5661(4.0891), Acc: 0.3408(0.3084)
2021-12-30 14:10:19,354 Epoch[120/310], Step[0900/1251], Loss: 4.0565(4.0872), Acc: 0.4160(0.3091)
2021-12-30 14:11:18,182 Epoch[120/310], Step[0950/1251], Loss: 3.8517(4.0859), Acc: 0.3936(0.3103)
2021-12-30 14:12:17,359 Epoch[120/310], Step[1000/1251], Loss: 4.1839(4.0826), Acc: 0.1494(0.3095)
2021-12-30 14:13:16,985 Epoch[120/310], Step[1050/1251], Loss: 4.5616(4.0822), Acc: 0.1582(0.3096)
2021-12-30 14:14:16,994 Epoch[120/310], Step[1100/1251], Loss: 4.1838(4.0828), Acc: 0.1270(0.3089)
2021-12-30 14:15:15,891 Epoch[120/310], Step[1150/1251], Loss: 4.2452(4.0800), Acc: 0.2949(0.3080)
2021-12-30 14:16:15,460 Epoch[120/310], Step[1200/1251], Loss: 4.2269(4.0760), Acc: 0.2969(0.3082)
2021-12-30 14:17:13,767 Epoch[120/310], Step[1250/1251], Loss: 4.5683(4.0760), Acc: 0.3330(0.3084)
2021-12-30 14:17:15,363 ----- Validation after Epoch: 120
2021-12-30 14:18:05,240 Val Step[0000/1563], Loss: 0.7897 (0.7897), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-30 14:18:06,550 Val Step[0050/1563], Loss: 2.4325 (1.0194), Acc@1: 0.3438 (0.7984), Acc@5: 0.8438 (0.9350)
2021-12-30 14:18:07,806 Val Step[0100/1563], Loss: 2.3578 (1.3173), Acc@1: 0.2500 (0.7027), Acc@5: 0.8438 (0.9032)
2021-12-30 14:18:09,091 Val Step[0150/1563], Loss: 0.6811 (1.2523), Acc@1: 0.8438 (0.7260), Acc@5: 0.9688 (0.9096)
2021-12-30 14:18:10,357 Val Step[0200/1563], Loss: 1.7126 (1.2720), Acc@1: 0.5625 (0.7247), Acc@5: 0.9062 (0.9080)
2021-12-30 14:18:11,782 Val Step[0250/1563], Loss: 0.9079 (1.2147), Acc@1: 0.8125 (0.7385), Acc@5: 1.0000 (0.9153)
2021-12-30 14:18:13,129 Val Step[0300/1563], Loss: 1.3310 (1.2868), Acc@1: 0.6875 (0.7170), Acc@5: 0.9688 (0.9096)
2021-12-30 14:18:14,534 Val Step[0350/1563], Loss: 0.9879 (1.3016), Acc@1: 0.7812 (0.7105), Acc@5: 0.9375 (0.9107)
2021-12-30 14:18:15,865 Val Step[0400/1563], Loss: 0.9532 (1.3095), Acc@1: 0.8125 (0.7050), Acc@5: 0.9688 (0.9115)
2021-12-30 14:18:17,201 Val Step[0450/1563], Loss: 1.0145 (1.3140), Acc@1: 0.7500 (0.7027), Acc@5: 0.9688 (0.9132)
2021-12-30 14:18:18,581 Val Step[0500/1563], Loss: 0.6642 (1.3020), Acc@1: 0.8750 (0.7058), Acc@5: 1.0000 (0.9148)
2021-12-30 14:18:19,986 Val Step[0550/1563], Loss: 0.9326 (1.2759), Acc@1: 0.8438 (0.7136), Acc@5: 0.9688 (0.9173)
2021-12-30 14:18:21,310 Val Step[0600/1563], Loss: 0.9479 (1.2771), Acc@1: 0.8125 (0.7140), Acc@5: 0.9062 (0.9170)
2021-12-30 14:18:22,678 Val Step[0650/1563], Loss: 0.6900 (1.2954), Acc@1: 0.8750 (0.7108), Acc@5: 1.0000 (0.9136)
2021-12-30 14:18:24,110 Val Step[0700/1563], Loss: 1.5911 (1.3317), Acc@1: 0.6875 (0.7032), Acc@5: 0.8125 (0.9082)
2021-12-30 14:18:25,548 Val Step[0750/1563], Loss: 1.6352 (1.3706), Acc@1: 0.7188 (0.6949), Acc@5: 0.8438 (0.9024)
2021-12-30 14:18:27,037 Val Step[0800/1563], Loss: 1.3019 (1.4165), Acc@1: 0.7188 (0.6843), Acc@5: 0.9375 (0.8962)
2021-12-30 14:18:28,412 Val Step[0850/1563], Loss: 1.5118 (1.4464), Acc@1: 0.6562 (0.6783), Acc@5: 0.9062 (0.8921)
2021-12-30 14:18:29,703 Val Step[0900/1563], Loss: 0.5352 (1.4473), Acc@1: 0.9062 (0.6793), Acc@5: 0.9688 (0.8911)
2021-12-30 14:18:31,151 Val Step[0950/1563], Loss: 1.7693 (1.4700), Acc@1: 0.6875 (0.6754), Acc@5: 0.7812 (0.8874)
2021-12-30 14:18:32,436 Val Step[1000/1563], Loss: 0.6515 (1.4947), Acc@1: 0.9375 (0.6704), Acc@5: 1.0000 (0.8838)
2021-12-30 14:18:33,706 Val Step[1050/1563], Loss: 0.5579 (1.5108), Acc@1: 0.9688 (0.6673), Acc@5: 0.9688 (0.8815)
2021-12-30 14:18:34,979 Val Step[1100/1563], Loss: 1.3397 (1.5296), Acc@1: 0.7500 (0.6643), Acc@5: 0.9062 (0.8785)
2021-12-30 14:18:36,277 Val Step[1150/1563], Loss: 1.7889 (1.5477), Acc@1: 0.6875 (0.6608), Acc@5: 0.7812 (0.8758)
2021-12-30 14:18:37,627 Val Step[1200/1563], Loss: 1.4525 (1.5642), Acc@1: 0.7812 (0.6573), Acc@5: 0.8438 (0.8731)
2021-12-30 14:18:38,894 Val Step[1250/1563], Loss: 0.9123 (1.5808), Acc@1: 0.8750 (0.6548), Acc@5: 0.9062 (0.8703)
2021-12-30 14:18:40,176 Val Step[1300/1563], Loss: 1.2036 (1.5928), Acc@1: 0.7812 (0.6527), Acc@5: 0.8750 (0.8687)
2021-12-30 14:18:41,454 Val Step[1350/1563], Loss: 2.6347 (1.6124), Acc@1: 0.4062 (0.6485), Acc@5: 0.7500 (0.8657)
2021-12-30 14:18:42,765 Val Step[1400/1563], Loss: 1.3475 (1.6209), Acc@1: 0.6875 (0.6463), Acc@5: 0.9062 (0.8643)
2021-12-30 14:18:44,238 Val Step[1450/1563], Loss: 1.7546 (1.6282), Acc@1: 0.5938 (0.6450), Acc@5: 0.9062 (0.8635)
2021-12-30 14:18:45,703 Val Step[1500/1563], Loss: 1.9251 (1.6153), Acc@1: 0.5312 (0.6475), Acc@5: 0.8750 (0.8655)
2021-12-30 14:18:47,116 Val Step[1550/1563], Loss: 1.0164 (1.6150), Acc@1: 0.8750 (0.6476), Acc@5: 0.9062 (0.8658)
2021-12-30 14:18:47,899 ----- Epoch[120/310], Validation Loss: 1.6128, Validation Acc@1: 0.6479, Validation Acc@5: 0.8660, time: 92.53
2021-12-30 14:18:47,899 ----- Epoch[120/310], Train Loss: 4.0760, Train Acc: 0.3084, time: 1549.87, Best Val(epoch118) Acc@1: 0.6509
2021-12-30 14:18:48,065 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-120-Loss-4.077287445251319.pdparams
2021-12-30 14:18:48,066 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-120-Loss-4.077287445251319.pdopt
2021-12-30 14:18:48,107 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-120-Loss-4.077287445251319-EMA.pdparams
2021-12-30 14:18:48,108 Now training epoch 121. LR=0.000651
2021-12-30 14:20:02,523 Epoch[121/310], Step[0000/1251], Loss: 3.8067(3.8067), Acc: 0.3428(0.3428)
2021-12-30 14:21:01,463 Epoch[121/310], Step[0050/1251], Loss: 3.7839(4.0588), Acc: 0.2236(0.3070)
2021-12-30 14:22:00,646 Epoch[121/310], Step[0100/1251], Loss: 4.2484(4.0721), Acc: 0.2490(0.2997)
2021-12-30 14:23:01,612 Epoch[121/310], Step[0150/1251], Loss: 4.3471(4.0703), Acc: 0.2354(0.3038)
2021-12-30 14:24:00,376 Epoch[121/310], Step[0200/1251], Loss: 4.1080(4.0771), Acc: 0.3506(0.3075)
2021-12-30 14:25:00,908 Epoch[121/310], Step[0250/1251], Loss: 3.5166(4.0632), Acc: 0.2549(0.3073)
2021-12-30 14:26:00,769 Epoch[121/310], Step[0300/1251], Loss: 4.0960(4.0501), Acc: 0.2832(0.3109)
2021-12-30 14:26:58,987 Epoch[121/310], Step[0350/1251], Loss: 4.2718(4.0454), Acc: 0.3154(0.3109)
2021-12-30 14:27:58,114 Epoch[121/310], Step[0400/1251], Loss: 3.9870(4.0523), Acc: 0.1768(0.3079)
2021-12-30 14:28:56,778 Epoch[121/310], Step[0450/1251], Loss: 4.4921(4.0581), Acc: 0.2920(0.3081)
2021-12-30 14:29:54,197 Epoch[121/310], Step[0500/1251], Loss: 4.2886(4.0674), Acc: 0.2881(0.3065)
2021-12-30 14:30:53,167 Epoch[121/310], Step[0550/1251], Loss: 4.0607(4.0676), Acc: 0.3359(0.3080)
2021-12-30 14:31:52,192 Epoch[121/310], Step[0600/1251], Loss: 3.9731(4.0702), Acc: 0.3125(0.3074)
2021-12-30 14:32:51,617 Epoch[121/310], Step[0650/1251], Loss: 4.4885(4.0676), Acc: 0.2666(0.3080)
2021-12-30 14:33:51,811 Epoch[121/310], Step[0700/1251], Loss: 3.9029(4.0726), Acc: 0.3008(0.3072)
2021-12-30 14:34:52,959 Epoch[121/310], Step[0750/1251], Loss: 4.0101(4.0708), Acc: 0.4814(0.3077)
2021-12-30 14:35:53,394 Epoch[121/310], Step[0800/1251], Loss: 4.2866(4.0668), Acc: 0.3203(0.3089)
2021-12-30 14:36:53,948 Epoch[121/310], Step[0850/1251], Loss: 3.8992(4.0629), Acc: 0.1885(0.3094)
2021-12-30 14:37:55,383 Epoch[121/310], Step[0900/1251], Loss: 4.4951(4.0690), Acc: 0.2744(0.3092)
2021-12-30 14:38:56,169 Epoch[121/310], Step[0950/1251], Loss: 4.1453(4.0714), Acc: 0.4102(0.3082)
2021-12-30 14:39:55,754 Epoch[121/310], Step[1000/1251], Loss: 3.8551(4.0705), Acc: 0.4756(0.3086)
2021-12-30 14:40:54,412 Epoch[121/310], Step[1050/1251], Loss: 4.3274(4.0717), Acc: 0.2275(0.3096)
2021-12-30 14:41:54,215 Epoch[121/310], Step[1100/1251], Loss: 3.7899(4.0698), Acc: 0.4072(0.3096)
2021-12-30 14:42:54,906 Epoch[121/310], Step[1150/1251], Loss: 4.0860(4.0744), Acc: 0.1875(0.3090)
2021-12-30 14:43:54,295 Epoch[121/310], Step[1200/1251], Loss: 4.4317(4.0761), Acc: 0.3018(0.3082)
2021-12-30 14:44:54,186 Epoch[121/310], Step[1250/1251], Loss: 4.1876(4.0802), Acc: 0.4287(0.3078)
2021-12-30 14:44:55,740 ----- Epoch[121/310], Train Loss: 4.0802, Train Acc: 0.3078, time: 1567.63, Best Val(epoch118) Acc@1: 0.6509
2021-12-30 14:44:55,918 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 14:44:55,919 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 14:44:56,024 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 14:44:56,025 Now training epoch 122. LR=0.000646
2021-12-30 14:46:03,979 Epoch[122/310], Step[0000/1251], Loss: 4.3199(4.3199), Acc: 0.3193(0.3193)
2021-12-30 14:47:04,823 Epoch[122/310], Step[0050/1251], Loss: 3.8633(4.0148), Acc: 0.2900(0.3188)
2021-12-30 14:48:04,131 Epoch[122/310], Step[0100/1251], Loss: 4.1875(4.0600), Acc: 0.3291(0.3211)
2021-12-30 14:49:02,936 Epoch[122/310], Step[0150/1251], Loss: 4.2420(4.0554), Acc: 0.2246(0.3161)
2021-12-30 14:50:01,861 Epoch[122/310], Step[0200/1251], Loss: 3.9305(4.0678), Acc: 0.4248(0.3155)
2021-12-30 14:51:00,359 Epoch[122/310], Step[0250/1251], Loss: 3.8854(4.0598), Acc: 0.3184(0.3171)
2021-12-30 14:51:59,439 Epoch[122/310], Step[0300/1251], Loss: 3.9978(4.0692), Acc: 0.4150(0.3153)
2021-12-30 14:52:58,402 Epoch[122/310], Step[0350/1251], Loss: 3.8181(4.0633), Acc: 0.4902(0.3165)
2021-12-30 14:53:58,861 Epoch[122/310], Step[0400/1251], Loss: 4.0098(4.0624), Acc: 0.0684(0.3138)
2021-12-30 14:55:00,910 Epoch[122/310], Step[0450/1251], Loss: 3.8093(4.0653), Acc: 0.2559(0.3137)
2021-12-30 14:56:02,448 Epoch[122/310], Step[0500/1251], Loss: 3.8999(4.0684), Acc: 0.3730(0.3123)
2021-12-30 14:57:03,212 Epoch[122/310], Step[0550/1251], Loss: 4.3146(4.0707), Acc: 0.2939(0.3116)
2021-12-30 14:58:02,913 Epoch[122/310], Step[0600/1251], Loss: 3.9338(4.0694), Acc: 0.2285(0.3123)
2021-12-30 14:59:02,833 Epoch[122/310], Step[0650/1251], Loss: 4.2549(4.0753), Acc: 0.2686(0.3112)
2021-12-30 15:00:02,199 Epoch[122/310], Step[0700/1251], Loss: 4.1874(4.0694), Acc: 0.2158(0.3114)
2021-12-30 15:01:02,616 Epoch[122/310], Step[0750/1251], Loss: 4.0967(4.0683), Acc: 0.3203(0.3127)
2021-12-30 15:02:02,434 Epoch[122/310], Step[0800/1251], Loss: 4.1998(4.0703), Acc: 0.1699(0.3114)
2021-12-30 15:03:04,218 Epoch[122/310], Step[0850/1251], Loss: 3.9958(4.0698), Acc: 0.3213(0.3108)
2021-12-30 15:04:05,321 Epoch[122/310], Step[0900/1251], Loss: 3.5231(4.0659), Acc: 0.4268(0.3101)
2021-12-30 15:05:05,757 Epoch[122/310], Step[0950/1251], Loss: 3.8229(4.0648), Acc: 0.2773(0.3097)
2021-12-30 15:06:06,587 Epoch[122/310], Step[1000/1251], Loss: 4.1969(4.0610), Acc: 0.3564(0.3092)
2021-12-30 15:07:07,636 Epoch[122/310], Step[1050/1251], Loss: 3.9173(4.0622), Acc: 0.2119(0.3086)
2021-12-30 15:08:09,108 Epoch[122/310], Step[1100/1251], Loss: 4.3346(4.0657), Acc: 0.2822(0.3075)
2021-12-30 15:09:10,466 Epoch[122/310], Step[1150/1251], Loss: 4.1100(4.0637), Acc: 0.4355(0.3082)
2021-12-30 15:10:11,281 Epoch[122/310], Step[1200/1251], Loss: 4.1852(4.0661), Acc: 0.2832(0.3080)
2021-12-30 15:11:11,643 Epoch[122/310], Step[1250/1251], Loss: 3.5576(4.0677), Acc: 0.5176(0.3087)
2021-12-30 15:11:13,194 ----- Validation after Epoch: 122
2021-12-30 15:12:02,997 Val Step[0000/1563], Loss: 0.6105 (0.6105), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2021-12-30 15:12:04,325 Val Step[0050/1563], Loss: 2.0559 (0.9793), Acc@1: 0.4375 (0.7966), Acc@5: 0.8438 (0.9320)
2021-12-30 15:12:05,632 Val Step[0100/1563], Loss: 2.1046 (1.3004), Acc@1: 0.4688 (0.7138), Acc@5: 0.8125 (0.9038)
2021-12-30 15:12:06,948 Val Step[0150/1563], Loss: 0.7562 (1.2117), Acc@1: 0.8438 (0.7312), Acc@5: 0.9688 (0.9118)
2021-12-30 15:12:08,317 Val Step[0200/1563], Loss: 1.2542 (1.2269), Acc@1: 0.7500 (0.7313), Acc@5: 0.9062 (0.9072)
2021-12-30 15:12:09,660 Val Step[0250/1563], Loss: 1.0167 (1.1619), Acc@1: 0.7812 (0.7455), Acc@5: 0.9375 (0.9157)
2021-12-30 15:12:10,965 Val Step[0300/1563], Loss: 1.4543 (1.2093), Acc@1: 0.5938 (0.7284), Acc@5: 0.9062 (0.9134)
2021-12-30 15:12:12,266 Val Step[0350/1563], Loss: 1.3427 (1.2147), Acc@1: 0.7500 (0.7254), Acc@5: 0.8750 (0.9145)
2021-12-30 15:12:13,638 Val Step[0400/1563], Loss: 1.3300 (1.2250), Acc@1: 0.7188 (0.7193), Acc@5: 0.9688 (0.9151)
2021-12-30 15:12:14,941 Val Step[0450/1563], Loss: 1.2818 (1.2280), Acc@1: 0.4688 (0.7149), Acc@5: 1.0000 (0.9166)
2021-12-30 15:12:16,270 Val Step[0500/1563], Loss: 0.5093 (1.2207), Acc@1: 0.8750 (0.7163), Acc@5: 1.0000 (0.9180)
2021-12-30 15:12:17,789 Val Step[0550/1563], Loss: 1.0405 (1.1990), Acc@1: 0.6875 (0.7220), Acc@5: 0.9375 (0.9202)
2021-12-30 15:12:19,177 Val Step[0600/1563], Loss: 0.7452 (1.2002), Acc@1: 0.8438 (0.7230), Acc@5: 0.9375 (0.9201)
2021-12-30 15:12:20,489 Val Step[0650/1563], Loss: 0.7586 (1.2254), Acc@1: 0.8438 (0.7174), Acc@5: 1.0000 (0.9164)
2021-12-30 15:12:21,768 Val Step[0700/1563], Loss: 1.4372 (1.2666), Acc@1: 0.6875 (0.7083), Acc@5: 0.8125 (0.9105)
2021-12-30 15:12:23,055 Val Step[0750/1563], Loss: 1.4531 (1.3095), Acc@1: 0.7500 (0.7000), Acc@5: 0.8750 (0.9043)
2021-12-30 15:12:24,344 Val Step[0800/1563], Loss: 1.4226 (1.3583), Acc@1: 0.7188 (0.6893), Acc@5: 0.9375 (0.8980)
2021-12-30 15:12:25,667 Val Step[0850/1563], Loss: 1.7419 (1.3871), Acc@1: 0.5625 (0.6834), Acc@5: 0.9062 (0.8940)
2021-12-30 15:12:27,053 Val Step[0900/1563], Loss: 0.3783 (1.3895), Acc@1: 0.9375 (0.6844), Acc@5: 1.0000 (0.8928)
2021-12-30 15:12:28,522 Val Step[0950/1563], Loss: 1.7308 (1.4119), Acc@1: 0.6562 (0.6803), Acc@5: 0.8750 (0.8895)
2021-12-30 15:12:29,868 Val Step[1000/1563], Loss: 0.7512 (1.4413), Acc@1: 0.9375 (0.6731), Acc@5: 1.0000 (0.8854)
2021-12-30 15:12:31,310 Val Step[1050/1563], Loss: 0.3739 (1.4574), Acc@1: 0.9688 (0.6700), Acc@5: 0.9688 (0.8832)
2021-12-30 15:12:32,702 Val Step[1100/1563], Loss: 0.9697 (1.4732), Acc@1: 0.8125 (0.6674), Acc@5: 0.9375 (0.8805)
2021-12-30 15:12:34,155 Val Step[1150/1563], Loss: 1.5378 (1.4893), Acc@1: 0.7188 (0.6649), Acc@5: 0.7812 (0.8778)
2021-12-30 15:12:35,576 Val Step[1200/1563], Loss: 1.6369 (1.5061), Acc@1: 0.7188 (0.6616), Acc@5: 0.8750 (0.8751)
2021-12-30 15:12:37,002 Val Step[1250/1563], Loss: 0.8418 (1.5199), Acc@1: 0.8750 (0.6594), Acc@5: 0.9062 (0.8728)
2021-12-30 15:12:38,450 Val Step[1300/1563], Loss: 1.0486 (1.5322), Acc@1: 0.7812 (0.6571), Acc@5: 0.8750 (0.8711)
2021-12-30 15:12:39,854 Val Step[1350/1563], Loss: 2.1764 (1.5533), Acc@1: 0.4375 (0.6529), Acc@5: 0.7812 (0.8681)
2021-12-30 15:12:41,290 Val Step[1400/1563], Loss: 1.6439 (1.5638), Acc@1: 0.6875 (0.6509), Acc@5: 0.8438 (0.8662)
2021-12-30 15:12:42,750 Val Step[1450/1563], Loss: 1.8040 (1.5707), Acc@1: 0.6250 (0.6493), Acc@5: 0.9062 (0.8657)
2021-12-30 15:12:44,157 Val Step[1500/1563], Loss: 2.1600 (1.5587), Acc@1: 0.4688 (0.6522), Acc@5: 0.8750 (0.8675)
2021-12-30 15:12:45,572 Val Step[1550/1563], Loss: 1.2762 (1.5603), Acc@1: 0.8125 (0.6518), Acc@5: 0.8750 (0.8674)
2021-12-30 15:12:46,377 ----- Epoch[122/310], Validation Loss: 1.5587, Validation Acc@1: 0.6521, Validation Acc@5: 0.8676, time: 93.18
2021-12-30 15:12:46,377 ----- Epoch[122/310], Train Loss: 4.0677, Train Acc: 0.3087, time: 1577.17, Best Val(epoch122) Acc@1: 0.6521
2021-12-30 15:12:46,563 Max accuracy so far: 0.6521 at epoch_122
2021-12-30 15:12:46,563 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 15:12:46,563 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 15:12:46,655 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 15:12:47,058 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 15:12:47,058 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 15:12:47,202 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 15:12:47,203 Now training epoch 123. LR=0.000641
2021-12-30 15:13:57,418 Epoch[123/310], Step[0000/1251], Loss: 3.8100(3.8100), Acc: 0.3252(0.3252)
2021-12-30 15:14:57,585 Epoch[123/310], Step[0050/1251], Loss: 4.1145(4.0428), Acc: 0.3008(0.3051)
2021-12-30 15:15:56,646 Epoch[123/310], Step[0100/1251], Loss: 4.3571(4.0726), Acc: 0.3389(0.3142)
2021-12-30 15:16:55,365 Epoch[123/310], Step[0150/1251], Loss: 4.0088(4.0649), Acc: 0.3574(0.3144)
2021-12-30 15:17:54,870 Epoch[123/310], Step[0200/1251], Loss: 4.0889(4.0611), Acc: 0.4023(0.3167)
2021-12-30 15:18:53,895 Epoch[123/310], Step[0250/1251], Loss: 3.6147(4.0739), Acc: 0.2305(0.3133)
2021-12-30 15:19:52,194 Epoch[123/310], Step[0300/1251], Loss: 4.2110(4.0638), Acc: 0.3857(0.3165)
2021-12-30 15:20:51,845 Epoch[123/310], Step[0350/1251], Loss: 3.5272(4.0664), Acc: 0.3271(0.3164)
2021-12-30 15:21:51,797 Epoch[123/310], Step[0400/1251], Loss: 3.9327(4.0680), Acc: 0.3691(0.3118)
2021-12-30 15:22:52,606 Epoch[123/310], Step[0450/1251], Loss: 4.1755(4.0625), Acc: 0.2090(0.3129)
2021-12-30 15:23:53,853 Epoch[123/310], Step[0500/1251], Loss: 4.2783(4.0643), Acc: 0.2441(0.3130)
2021-12-30 15:24:54,638 Epoch[123/310], Step[0550/1251], Loss: 3.4428(4.0655), Acc: 0.4316(0.3125)
2021-12-30 15:25:55,354 Epoch[123/310], Step[0600/1251], Loss: 4.1154(4.0723), Acc: 0.3389(0.3109)
2021-12-30 15:26:56,034 Epoch[123/310], Step[0650/1251], Loss: 3.7923(4.0737), Acc: 0.2021(0.3105)
2021-12-30 15:27:56,968 Epoch[123/310], Step[0700/1251], Loss: 3.7502(4.0701), Acc: 0.1494(0.3101)
2021-12-30 15:28:57,933 Epoch[123/310], Step[0750/1251], Loss: 3.5010(4.0720), Acc: 0.4395(0.3103)
2021-12-30 15:29:58,486 Epoch[123/310], Step[0800/1251], Loss: 3.8034(4.0768), Acc: 0.4531(0.3095)
2021-12-30 15:30:58,718 Epoch[123/310], Step[0850/1251], Loss: 4.0909(4.0745), Acc: 0.3789(0.3101)
2021-12-30 15:31:58,682 Epoch[123/310], Step[0900/1251], Loss: 4.3438(4.0735), Acc: 0.1328(0.3095)
2021-12-30 15:32:59,294 Epoch[123/310], Step[0950/1251], Loss: 3.9214(4.0766), Acc: 0.3252(0.3100)
2021-12-30 15:33:59,998 Epoch[123/310], Step[1000/1251], Loss: 3.8769(4.0792), Acc: 0.3711(0.3100)
2021-12-30 15:35:01,093 Epoch[123/310], Step[1050/1251], Loss: 3.6537(4.0780), Acc: 0.3584(0.3087)
2021-12-30 15:36:00,823 Epoch[123/310], Step[1100/1251], Loss: 3.8067(4.0783), Acc: 0.4678(0.3098)
2021-12-30 15:37:02,252 Epoch[123/310], Step[1150/1251], Loss: 3.9024(4.0752), Acc: 0.2695(0.3097)
2021-12-30 15:38:02,637 Epoch[123/310], Step[1200/1251], Loss: 4.2013(4.0769), Acc: 0.2217(0.3091)
2021-12-30 15:39:02,477 Epoch[123/310], Step[1250/1251], Loss: 4.0239(4.0782), Acc: 0.3242(0.3088)
2021-12-30 15:39:04,212 ----- Epoch[123/310], Train Loss: 4.0782, Train Acc: 0.3088, time: 1577.01, Best Val(epoch122) Acc@1: 0.6521
2021-12-30 15:39:04,391 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 15:39:04,392 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 15:39:04,497 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 15:39:04,498 Now training epoch 124. LR=0.000636
2021-12-30 15:40:13,398 Epoch[124/310], Step[0000/1251], Loss: 3.8649(3.8649), Acc: 0.4365(0.4365)
2021-12-30 15:41:13,553 Epoch[124/310], Step[0050/1251], Loss: 3.6627(4.1041), Acc: 0.4844(0.2855)
2021-12-30 15:42:12,533 Epoch[124/310], Step[0100/1251], Loss: 3.3253(4.0187), Acc: 0.4111(0.3026)
2021-12-30 15:43:12,455 Epoch[124/310], Step[0150/1251], Loss: 3.3190(4.0044), Acc: 0.3877(0.3131)
2021-12-30 15:44:13,627 Epoch[124/310], Step[0200/1251], Loss: 3.9830(4.0029), Acc: 0.3525(0.3113)
2021-12-30 15:45:12,896 Epoch[124/310], Step[0250/1251], Loss: 3.5655(3.9981), Acc: 0.3848(0.3150)
2021-12-30 15:46:12,014 Epoch[124/310], Step[0300/1251], Loss: 3.7578(4.0141), Acc: 0.4717(0.3132)
2021-12-30 15:47:11,231 Epoch[124/310], Step[0350/1251], Loss: 3.8353(4.0144), Acc: 0.2578(0.3117)
2021-12-30 15:48:10,012 Epoch[124/310], Step[0400/1251], Loss: 3.6622(4.0279), Acc: 0.3594(0.3138)
2021-12-30 15:49:10,207 Epoch[124/310], Step[0450/1251], Loss: 4.1609(4.0331), Acc: 0.1885(0.3151)
2021-12-30 15:50:11,602 Epoch[124/310], Step[0500/1251], Loss: 4.2516(4.0346), Acc: 0.2344(0.3134)
2021-12-30 15:51:12,853 Epoch[124/310], Step[0550/1251], Loss: 4.3938(4.0387), Acc: 0.3447(0.3119)
2021-12-30 15:52:14,688 Epoch[124/310], Step[0600/1251], Loss: 4.1383(4.0425), Acc: 0.3691(0.3125)
2021-12-30 15:53:16,531 Epoch[124/310], Step[0650/1251], Loss: 4.0491(4.0479), Acc: 0.4062(0.3116)
2021-12-30 15:54:15,504 Epoch[124/310], Step[0700/1251], Loss: 3.7373(4.0476), Acc: 0.2754(0.3123)
2021-12-30 15:55:16,510 Epoch[124/310], Step[0750/1251], Loss: 3.8909(4.0494), Acc: 0.3945(0.3114)
2021-12-30 15:56:17,186 Epoch[124/310], Step[0800/1251], Loss: 4.4787(4.0460), Acc: 0.3115(0.3115)
2021-12-30 15:57:18,079 Epoch[124/310], Step[0850/1251], Loss: 3.8703(4.0493), Acc: 0.4004(0.3115)
2021-12-30 15:58:19,782 Epoch[124/310], Step[0900/1251], Loss: 3.2461(4.0514), Acc: 0.4189(0.3105)
2021-12-30 15:59:21,213 Epoch[124/310], Step[0950/1251], Loss: 4.1965(4.0516), Acc: 0.4326(0.3100)
2021-12-30 16:00:22,063 Epoch[124/310], Step[1000/1251], Loss: 4.7098(4.0525), Acc: 0.2988(0.3107)
2021-12-30 16:01:23,456 Epoch[124/310], Step[1050/1251], Loss: 3.8230(4.0537), Acc: 0.1582(0.3108)
2021-12-30 16:02:25,301 Epoch[124/310], Step[1100/1251], Loss: 3.6474(4.0550), Acc: 0.3486(0.3102)
2021-12-30 16:03:24,925 Epoch[124/310], Step[1150/1251], Loss: 4.5411(4.0555), Acc: 0.2969(0.3110)
2021-12-30 16:04:24,073 Epoch[124/310], Step[1200/1251], Loss: 4.6904(4.0566), Acc: 0.2959(0.3105)
2021-12-30 16:05:24,792 Epoch[124/310], Step[1250/1251], Loss: 4.3936(4.0578), Acc: 0.3545(0.3107)
2021-12-30 16:05:26,441 ----- Validation after Epoch: 124
2021-12-30 16:06:18,842 Val Step[0000/1563], Loss: 0.6904 (0.6904), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-30 16:06:20,253 Val Step[0050/1563], Loss: 2.1495 (0.9617), Acc@1: 0.5000 (0.7996), Acc@5: 0.8125 (0.9412)
2021-12-30 16:06:21,559 Val Step[0100/1563], Loss: 2.2168 (1.2862), Acc@1: 0.5000 (0.7104), Acc@5: 0.8125 (0.9059)
2021-12-30 16:06:22,963 Val Step[0150/1563], Loss: 0.5786 (1.2105), Acc@1: 0.8750 (0.7274), Acc@5: 0.9688 (0.9125)
2021-12-30 16:06:24,461 Val Step[0200/1563], Loss: 1.1111 (1.2276), Acc@1: 0.7812 (0.7292), Acc@5: 0.9062 (0.9073)
2021-12-30 16:06:25,873 Val Step[0250/1563], Loss: 1.0906 (1.1574), Acc@1: 0.8125 (0.7451), Acc@5: 0.9688 (0.9173)
2021-12-30 16:06:27,270 Val Step[0300/1563], Loss: 1.3209 (1.2272), Acc@1: 0.6875 (0.7220), Acc@5: 0.9062 (0.9109)
2021-12-30 16:06:28,675 Val Step[0350/1563], Loss: 1.3656 (1.2411), Acc@1: 0.6875 (0.7169), Acc@5: 0.9375 (0.9127)
2021-12-30 16:06:29,970 Val Step[0400/1563], Loss: 1.3697 (1.2500), Acc@1: 0.6875 (0.7112), Acc@5: 0.9375 (0.9130)
2021-12-30 16:06:31,397 Val Step[0450/1563], Loss: 1.0324 (1.2548), Acc@1: 0.6562 (0.7082), Acc@5: 1.0000 (0.9144)
2021-12-30 16:06:32,882 Val Step[0500/1563], Loss: 0.3967 (1.2501), Acc@1: 0.9375 (0.7094), Acc@5: 0.9688 (0.9154)
2021-12-30 16:06:34,365 Val Step[0550/1563], Loss: 0.9456 (1.2210), Acc@1: 0.7500 (0.7175), Acc@5: 0.9688 (0.9183)
2021-12-30 16:06:35,652 Val Step[0600/1563], Loss: 0.9267 (1.2239), Acc@1: 0.8125 (0.7183), Acc@5: 0.9062 (0.9177)
2021-12-30 16:06:36,969 Val Step[0650/1563], Loss: 0.7912 (1.2438), Acc@1: 0.8438 (0.7144), Acc@5: 1.0000 (0.9146)
2021-12-30 16:06:38,361 Val Step[0700/1563], Loss: 1.3097 (1.2729), Acc@1: 0.7188 (0.7086), Acc@5: 0.9062 (0.9102)
2021-12-30 16:06:39,757 Val Step[0750/1563], Loss: 1.6062 (1.3133), Acc@1: 0.6562 (0.7002), Acc@5: 0.8438 (0.9044)
2021-12-30 16:06:41,166 Val Step[0800/1563], Loss: 1.0786 (1.3534), Acc@1: 0.7812 (0.6909), Acc@5: 0.9688 (0.8988)
2021-12-30 16:06:42,699 Val Step[0850/1563], Loss: 1.5251 (1.3849), Acc@1: 0.5938 (0.6840), Acc@5: 0.8750 (0.8944)
2021-12-30 16:06:44,115 Val Step[0900/1563], Loss: 0.5998 (1.3845), Acc@1: 0.9375 (0.6859), Acc@5: 0.9375 (0.8937)
2021-12-30 16:06:45,508 Val Step[0950/1563], Loss: 1.7028 (1.4114), Acc@1: 0.6875 (0.6802), Acc@5: 0.8750 (0.8894)
2021-12-30 16:06:46,805 Val Step[1000/1563], Loss: 0.9039 (1.4376), Acc@1: 0.8750 (0.6740), Acc@5: 0.9688 (0.8858)
2021-12-30 16:06:48,168 Val Step[1050/1563], Loss: 0.4465 (1.4542), Acc@1: 0.9688 (0.6708), Acc@5: 0.9688 (0.8836)
2021-12-30 16:06:49,478 Val Step[1100/1563], Loss: 1.2889 (1.4710), Acc@1: 0.7500 (0.6678), Acc@5: 0.9375 (0.8810)
2021-12-30 16:06:50,786 Val Step[1150/1563], Loss: 1.6732 (1.4899), Acc@1: 0.6875 (0.6646), Acc@5: 0.7500 (0.8782)
2021-12-30 16:06:52,079 Val Step[1200/1563], Loss: 1.1953 (1.5055), Acc@1: 0.7812 (0.6619), Acc@5: 0.8438 (0.8757)
2021-12-30 16:06:53,436 Val Step[1250/1563], Loss: 0.9828 (1.5181), Acc@1: 0.8750 (0.6603), Acc@5: 0.9062 (0.8730)
2021-12-30 16:06:54,736 Val Step[1300/1563], Loss: 1.0196 (1.5292), Acc@1: 0.7812 (0.6575), Acc@5: 0.9062 (0.8716)
2021-12-30 16:06:56,113 Val Step[1350/1563], Loss: 2.4168 (1.5489), Acc@1: 0.3125 (0.6530), Acc@5: 0.8438 (0.8684)
2021-12-30 16:06:57,447 Val Step[1400/1563], Loss: 1.1108 (1.5586), Acc@1: 0.7500 (0.6510), Acc@5: 0.9375 (0.8671)
2021-12-30 16:06:58,821 Val Step[1450/1563], Loss: 1.8905 (1.5658), Acc@1: 0.5000 (0.6494), Acc@5: 0.8750 (0.8663)
2021-12-30 16:07:00,234 Val Step[1500/1563], Loss: 2.1700 (1.5544), Acc@1: 0.5000 (0.6517), Acc@5: 0.8438 (0.8679)
2021-12-30 16:07:01,641 Val Step[1550/1563], Loss: 0.8738 (1.5532), Acc@1: 0.8750 (0.6518), Acc@5: 0.9062 (0.8682)
2021-12-30 16:07:02,417 ----- Epoch[124/310], Validation Loss: 1.5507, Validation Acc@1: 0.6522, Validation Acc@5: 0.8686, time: 95.97
2021-12-30 16:07:02,417 ----- Epoch[124/310], Train Loss: 4.0578, Train Acc: 0.3107, time: 1581.94, Best Val(epoch124) Acc@1: 0.6522
2021-12-30 16:07:02,604 Max accuracy so far: 0.6522 at epoch_124
2021-12-30 16:07:02,605 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 16:07:02,605 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 16:07:02,708 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 16:07:03,093 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 16:07:03,093 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 16:07:03,224 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 16:07:03,224 Now training epoch 125. LR=0.000631
2021-12-30 16:08:16,263 Epoch[125/310], Step[0000/1251], Loss: 4.2354(4.2354), Acc: 0.3203(0.3203)
2021-12-30 16:09:16,788 Epoch[125/310], Step[0050/1251], Loss: 3.9836(4.0603), Acc: 0.2490(0.2826)
2021-12-30 16:10:16,660 Epoch[125/310], Step[0100/1251], Loss: 4.1948(4.0821), Acc: 0.3857(0.2924)
2021-12-30 16:11:15,330 Epoch[125/310], Step[0150/1251], Loss: 4.3732(4.0908), Acc: 0.1504(0.3006)
2021-12-30 16:12:12,986 Epoch[125/310], Step[0200/1251], Loss: 4.1714(4.0708), Acc: 0.3877(0.3023)
2021-12-30 16:13:11,864 Epoch[125/310], Step[0250/1251], Loss: 3.8881(4.0623), Acc: 0.3975(0.3060)
2021-12-30 16:14:09,750 Epoch[125/310], Step[0300/1251], Loss: 3.2972(4.0568), Acc: 0.2705(0.3135)
2021-12-30 16:15:09,288 Epoch[125/310], Step[0350/1251], Loss: 4.0006(4.0695), Acc: 0.2959(0.3076)
2021-12-30 16:16:07,653 Epoch[125/310], Step[0400/1251], Loss: 4.2346(4.0619), Acc: 0.2354(0.3099)
2021-12-30 16:17:06,665 Epoch[125/310], Step[0450/1251], Loss: 3.6902(4.0692), Acc: 0.1943(0.3095)
2021-12-30 16:18:07,020 Epoch[125/310], Step[0500/1251], Loss: 3.8679(4.0736), Acc: 0.4014(0.3078)
2021-12-30 16:19:07,736 Epoch[125/310], Step[0550/1251], Loss: 3.9231(4.0741), Acc: 0.1494(0.3086)
2021-12-30 16:20:09,326 Epoch[125/310], Step[0600/1251], Loss: 3.7498(4.0737), Acc: 0.3652(0.3074)
2021-12-30 16:21:10,478 Epoch[125/310], Step[0650/1251], Loss: 3.7230(4.0705), Acc: 0.4297(0.3076)
2021-12-30 16:22:11,682 Epoch[125/310], Step[0700/1251], Loss: 4.1445(4.0692), Acc: 0.3203(0.3069)
2021-12-30 16:23:12,590 Epoch[125/310], Step[0750/1251], Loss: 3.9219(4.0675), Acc: 0.4062(0.3060)
2021-12-30 16:24:13,677 Epoch[125/310], Step[0800/1251], Loss: 3.4470(4.0720), Acc: 0.4971(0.3051)
2021-12-30 16:25:13,885 Epoch[125/310], Step[0850/1251], Loss: 4.1840(4.0722), Acc: 0.4434(0.3071)
2021-12-30 16:26:15,081 Epoch[125/310], Step[0900/1251], Loss: 4.3203(4.0719), Acc: 0.2158(0.3080)
2021-12-30 16:27:15,914 Epoch[125/310], Step[0950/1251], Loss: 4.3202(4.0722), Acc: 0.2539(0.3073)
2021-12-30 16:28:16,690 Epoch[125/310], Step[1000/1251], Loss: 3.9318(4.0732), Acc: 0.2334(0.3070)
2021-12-30 16:29:17,933 Epoch[125/310], Step[1050/1251], Loss: 3.9316(4.0701), Acc: 0.3555(0.3073)
2021-12-30 16:30:17,299 Epoch[125/310], Step[1100/1251], Loss: 4.0111(4.0714), Acc: 0.3789(0.3076)
2021-12-30 16:31:17,311 Epoch[125/310], Step[1150/1251], Loss: 4.2531(4.0726), Acc: 0.1885(0.3067)
2021-12-30 16:32:16,613 Epoch[125/310], Step[1200/1251], Loss: 3.8950(4.0730), Acc: 0.0879(0.3073)
2021-12-30 16:33:17,052 Epoch[125/310], Step[1250/1251], Loss: 4.2359(4.0756), Acc: 0.3887(0.3076)
2021-12-30 16:33:18,592 ----- Epoch[125/310], Train Loss: 4.0756, Train Acc: 0.3076, time: 1575.36, Best Val(epoch124) Acc@1: 0.6522
2021-12-30 16:33:18,772 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 16:33:18,773 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 16:33:18,880 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 16:33:18,880 Now training epoch 126. LR=0.000626
2021-12-30 16:34:28,966 Epoch[126/310], Step[0000/1251], Loss: 3.6535(3.6535), Acc: 0.4326(0.4326)
2021-12-30 16:35:28,920 Epoch[126/310], Step[0050/1251], Loss: 4.2433(3.9611), Acc: 0.3818(0.3347)
2021-12-30 16:36:28,259 Epoch[126/310], Step[0100/1251], Loss: 3.9312(3.9838), Acc: 0.3311(0.3226)
2021-12-30 16:37:25,880 Epoch[126/310], Step[0150/1251], Loss: 4.7674(3.9989), Acc: 0.2637(0.3267)
2021-12-30 16:38:25,268 Epoch[126/310], Step[0200/1251], Loss: 3.9731(4.0230), Acc: 0.3125(0.3161)
2021-12-30 16:39:25,385 Epoch[126/310], Step[0250/1251], Loss: 3.2782(4.0197), Acc: 0.2129(0.3101)
2021-12-30 16:40:25,836 Epoch[126/310], Step[0300/1251], Loss: 4.3958(4.0285), Acc: 0.2939(0.3129)
2021-12-30 16:41:24,897 Epoch[126/310], Step[0350/1251], Loss: 3.8090(4.0365), Acc: 0.4805(0.3145)
2021-12-30 16:42:24,721 Epoch[126/310], Step[0400/1251], Loss: 4.2156(4.0307), Acc: 0.3262(0.3181)
2021-12-30 16:43:25,246 Epoch[126/310], Step[0450/1251], Loss: 3.8415(4.0356), Acc: 0.4902(0.3200)
2021-12-30 16:44:24,741 Epoch[126/310], Step[0500/1251], Loss: 4.4559(4.0313), Acc: 0.2607(0.3204)
2021-12-30 16:45:25,268 Epoch[126/310], Step[0550/1251], Loss: 3.8694(4.0350), Acc: 0.2148(0.3199)
2021-12-30 16:46:25,377 Epoch[126/310], Step[0600/1251], Loss: 4.2449(4.0373), Acc: 0.3779(0.3172)
2021-12-30 16:47:25,998 Epoch[126/310], Step[0650/1251], Loss: 4.6360(4.0388), Acc: 0.2881(0.3169)
2021-12-30 16:48:26,893 Epoch[126/310], Step[0700/1251], Loss: 3.8034(4.0384), Acc: 0.1562(0.3171)
2021-12-30 16:49:26,612 Epoch[126/310], Step[0750/1251], Loss: 4.1538(4.0316), Acc: 0.0273(0.3184)
2021-12-30 16:50:26,430 Epoch[126/310], Step[0800/1251], Loss: 4.0490(4.0320), Acc: 0.4141(0.3179)
2021-12-30 16:51:26,396 Epoch[126/310], Step[0850/1251], Loss: 4.2206(4.0318), Acc: 0.3799(0.3174)
2021-12-30 16:52:25,779 Epoch[126/310], Step[0900/1251], Loss: 4.3186(4.0360), Acc: 0.3496(0.3157)
2021-12-30 16:53:26,251 Epoch[126/310], Step[0950/1251], Loss: 3.5695(4.0357), Acc: 0.2959(0.3148)
2021-12-30 16:54:26,323 Epoch[126/310], Step[1000/1251], Loss: 3.5772(4.0352), Acc: 0.3750(0.3147)
2021-12-30 16:55:26,563 Epoch[126/310], Step[1050/1251], Loss: 4.4285(4.0327), Acc: 0.3564(0.3158)
2021-12-30 16:56:26,253 Epoch[126/310], Step[1100/1251], Loss: 4.1574(4.0386), Acc: 0.3594(0.3154)
2021-12-30 16:57:26,981 Epoch[126/310], Step[1150/1251], Loss: 4.1292(4.0414), Acc: 0.2979(0.3144)
2021-12-30 16:58:26,626 Epoch[126/310], Step[1200/1251], Loss: 4.1043(4.0412), Acc: 0.2998(0.3153)
2021-12-30 16:59:26,851 Epoch[126/310], Step[1250/1251], Loss: 3.9440(4.0428), Acc: 0.3086(0.3140)
2021-12-30 16:59:28,428 ----- Validation after Epoch: 126
2021-12-30 17:00:17,558 Val Step[0000/1563], Loss: 0.8673 (0.8673), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-30 17:00:19,021 Val Step[0050/1563], Loss: 2.1484 (0.9149), Acc@1: 0.4688 (0.8113), Acc@5: 0.7812 (0.9498)
2021-12-30 17:00:20,552 Val Step[0100/1563], Loss: 2.2410 (1.2444), Acc@1: 0.3750 (0.7234), Acc@5: 0.8125 (0.9165)
2021-12-30 17:00:21,983 Val Step[0150/1563], Loss: 0.7540 (1.1665), Acc@1: 0.7812 (0.7388), Acc@5: 0.9688 (0.9209)
2021-12-30 17:00:23,430 Val Step[0200/1563], Loss: 1.3211 (1.1812), Acc@1: 0.6875 (0.7379), Acc@5: 0.9375 (0.9184)
2021-12-30 17:00:24,870 Val Step[0250/1563], Loss: 1.1266 (1.1326), Acc@1: 0.6875 (0.7490), Acc@5: 0.9688 (0.9236)
2021-12-30 17:00:26,298 Val Step[0300/1563], Loss: 1.9147 (1.2000), Acc@1: 0.4375 (0.7281), Acc@5: 0.8438 (0.9188)
2021-12-30 17:00:27,862 Val Step[0350/1563], Loss: 1.1279 (1.2130), Acc@1: 0.7812 (0.7233), Acc@5: 0.9062 (0.9193)
2021-12-30 17:00:29,281 Val Step[0400/1563], Loss: 1.3822 (1.2192), Acc@1: 0.6250 (0.7177), Acc@5: 0.9688 (0.9207)
2021-12-30 17:00:30,740 Val Step[0450/1563], Loss: 0.7538 (1.2249), Acc@1: 0.8125 (0.7156), Acc@5: 1.0000 (0.9218)
2021-12-30 17:00:32,251 Val Step[0500/1563], Loss: 0.4149 (1.2141), Acc@1: 0.9375 (0.7186), Acc@5: 1.0000 (0.9232)
2021-12-30 17:00:33,742 Val Step[0550/1563], Loss: 0.8932 (1.1903), Acc@1: 0.7500 (0.7256), Acc@5: 0.9688 (0.9258)
2021-12-30 17:00:35,210 Val Step[0600/1563], Loss: 0.7108 (1.1949), Acc@1: 0.8750 (0.7247), Acc@5: 0.9688 (0.9248)
2021-12-30 17:00:36,636 Val Step[0650/1563], Loss: 0.6850 (1.2146), Acc@1: 0.8125 (0.7212), Acc@5: 1.0000 (0.9216)
2021-12-30 17:00:38,068 Val Step[0700/1563], Loss: 1.4051 (1.2503), Acc@1: 0.7812 (0.7128), Acc@5: 0.8750 (0.9168)
2021-12-30 17:00:39,490 Val Step[0750/1563], Loss: 1.9782 (1.2881), Acc@1: 0.5938 (0.7054), Acc@5: 0.7812 (0.9107)
2021-12-30 17:00:40,845 Val Step[0800/1563], Loss: 1.2694 (1.3294), Acc@1: 0.7500 (0.6964), Acc@5: 0.9062 (0.9049)
2021-12-30 17:00:42,194 Val Step[0850/1563], Loss: 1.4526 (1.3579), Acc@1: 0.6875 (0.6903), Acc@5: 0.9062 (0.9007)
2021-12-30 17:00:43,511 Val Step[0900/1563], Loss: 0.7381 (1.3606), Acc@1: 0.9062 (0.6916), Acc@5: 0.9375 (0.8993)
2021-12-30 17:00:44,925 Val Step[0950/1563], Loss: 1.5387 (1.3851), Acc@1: 0.7188 (0.6876), Acc@5: 0.9375 (0.8955)
2021-12-30 17:00:46,210 Val Step[1000/1563], Loss: 0.5738 (1.4121), Acc@1: 0.9688 (0.6817), Acc@5: 0.9688 (0.8917)
2021-12-30 17:00:47,517 Val Step[1050/1563], Loss: 0.5674 (1.4306), Acc@1: 0.9062 (0.6780), Acc@5: 0.9688 (0.8892)
2021-12-30 17:00:48,821 Val Step[1100/1563], Loss: 1.4812 (1.4480), Acc@1: 0.6875 (0.6739), Acc@5: 0.8750 (0.8867)
2021-12-30 17:00:50,093 Val Step[1150/1563], Loss: 1.8023 (1.4661), Acc@1: 0.5938 (0.6712), Acc@5: 0.7812 (0.8838)
2021-12-30 17:00:51,374 Val Step[1200/1563], Loss: 1.4945 (1.4836), Acc@1: 0.6875 (0.6675), Acc@5: 0.8750 (0.8808)
2021-12-30 17:00:52,728 Val Step[1250/1563], Loss: 1.1903 (1.4986), Acc@1: 0.8125 (0.6651), Acc@5: 0.9062 (0.8780)
2021-12-30 17:00:54,091 Val Step[1300/1563], Loss: 1.2780 (1.5097), Acc@1: 0.7500 (0.6630), Acc@5: 0.9062 (0.8767)
2021-12-30 17:00:55,436 Val Step[1350/1563], Loss: 2.3724 (1.5311), Acc@1: 0.2500 (0.6584), Acc@5: 0.7500 (0.8733)
2021-12-30 17:00:56,918 Val Step[1400/1563], Loss: 0.9284 (1.5387), Acc@1: 0.8438 (0.6563), Acc@5: 0.9375 (0.8724)
2021-12-30 17:00:58,366 Val Step[1450/1563], Loss: 2.0329 (1.5461), Acc@1: 0.4688 (0.6544), Acc@5: 0.9062 (0.8719)
2021-12-30 17:00:59,806 Val Step[1500/1563], Loss: 2.1198 (1.5355), Acc@1: 0.5000 (0.6570), Acc@5: 0.8750 (0.8734)
2021-12-30 17:01:01,152 Val Step[1550/1563], Loss: 1.0418 (1.5354), Acc@1: 0.8750 (0.6568), Acc@5: 0.9062 (0.8733)
2021-12-30 17:01:01,938 ----- Epoch[126/310], Validation Loss: 1.5329, Validation Acc@1: 0.6574, Validation Acc@5: 0.8735, time: 93.51
2021-12-30 17:01:01,938 ----- Epoch[126/310], Train Loss: 4.0428, Train Acc: 0.3140, time: 1569.54, Best Val(epoch126) Acc@1: 0.6574
2021-12-30 17:01:02,125 Max accuracy so far: 0.6574 at epoch_126
2021-12-30 17:01:02,125 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 17:01:02,125 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 17:01:02,230 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 17:01:02,623 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 17:01:02,623 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 17:01:02,752 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 17:01:02,753 Now training epoch 127. LR=0.000621
2021-12-30 17:02:12,157 Epoch[127/310], Step[0000/1251], Loss: 4.0804(4.0804), Acc: 0.1836(0.1836)
2021-12-30 17:03:11,590 Epoch[127/310], Step[0050/1251], Loss: 4.0066(4.1139), Acc: 0.2715(0.3028)
2021-12-30 17:04:11,156 Epoch[127/310], Step[0100/1251], Loss: 4.2988(4.0603), Acc: 0.4111(0.3262)
2021-12-30 17:05:10,444 Epoch[127/310], Step[0150/1251], Loss: 4.7731(4.0662), Acc: 0.2715(0.3145)
2021-12-30 17:06:11,008 Epoch[127/310], Step[0200/1251], Loss: 4.2264(4.0661), Acc: 0.4219(0.3119)
2021-12-30 17:07:11,250 Epoch[127/310], Step[0250/1251], Loss: 4.5656(4.0682), Acc: 0.2646(0.3153)
2021-12-30 17:08:10,540 Epoch[127/310], Step[0300/1251], Loss: 4.2987(4.0680), Acc: 0.3008(0.3162)
2021-12-30 17:09:10,307 Epoch[127/310], Step[0350/1251], Loss: 4.6595(4.0736), Acc: 0.1660(0.3129)
2021-12-30 17:10:08,641 Epoch[127/310], Step[0400/1251], Loss: 3.8071(4.0781), Acc: 0.4219(0.3140)
2021-12-30 17:11:06,669 Epoch[127/310], Step[0450/1251], Loss: 4.7663(4.0804), Acc: 0.2598(0.3153)
2021-12-30 17:12:06,705 Epoch[127/310], Step[0500/1251], Loss: 3.9537(4.0809), Acc: 0.4561(0.3142)
2021-12-30 17:13:04,436 Epoch[127/310], Step[0550/1251], Loss: 4.2918(4.0716), Acc: 0.3662(0.3158)
2021-12-30 17:14:04,454 Epoch[127/310], Step[0600/1251], Loss: 3.6815(4.0666), Acc: 0.1953(0.3165)
2021-12-30 17:15:03,423 Epoch[127/310], Step[0650/1251], Loss: 4.2779(4.0647), Acc: 0.1084(0.3168)
2021-12-30 17:16:02,582 Epoch[127/310], Step[0700/1251], Loss: 3.8792(4.0586), Acc: 0.4609(0.3179)
2021-12-30 17:17:02,713 Epoch[127/310], Step[0750/1251], Loss: 3.8275(4.0549), Acc: 0.4141(0.3171)
2021-12-30 17:18:01,818 Epoch[127/310], Step[0800/1251], Loss: 4.5071(4.0502), Acc: 0.2266(0.3175)
2021-12-30 17:19:02,715 Epoch[127/310], Step[0850/1251], Loss: 4.8556(4.0518), Acc: 0.1787(0.3165)
2021-12-30 17:20:03,296 Epoch[127/310], Step[0900/1251], Loss: 4.3598(4.0539), Acc: 0.3252(0.3156)
2021-12-30 17:21:03,911 Epoch[127/310], Step[0950/1251], Loss: 4.0058(4.0525), Acc: 0.4541(0.3154)
2021-12-30 17:22:03,856 Epoch[127/310], Step[1000/1251], Loss: 4.4905(4.0570), Acc: 0.3213(0.3149)
2021-12-30 17:23:03,415 Epoch[127/310], Step[1050/1251], Loss: 3.7708(4.0559), Acc: 0.2090(0.3135)
2021-12-30 17:24:04,229 Epoch[127/310], Step[1100/1251], Loss: 4.3816(4.0603), Acc: 0.4111(0.3130)
2021-12-30 17:25:04,621 Epoch[127/310], Step[1150/1251], Loss: 4.3208(4.0577), Acc: 0.2852(0.3122)
2021-12-30 17:26:05,281 Epoch[127/310], Step[1200/1251], Loss: 4.0346(4.0570), Acc: 0.4355(0.3116)
2021-12-30 17:27:05,968 Epoch[127/310], Step[1250/1251], Loss: 4.5715(4.0550), Acc: 0.3174(0.3111)
2021-12-30 17:27:07,584 ----- Epoch[127/310], Train Loss: 4.0550, Train Acc: 0.3111, time: 1564.83, Best Val(epoch126) Acc@1: 0.6574
2021-12-30 17:27:07,757 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 17:27:07,758 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 17:27:07,865 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 17:27:07,865 Now training epoch 128. LR=0.000616
2021-12-30 17:28:20,950 Epoch[128/310], Step[0000/1251], Loss: 3.7382(3.7382), Acc: 0.4492(0.4492)
2021-12-30 17:29:20,780 Epoch[128/310], Step[0050/1251], Loss: 4.1092(3.9815), Acc: 0.4258(0.3064)
2021-12-30 17:30:19,914 Epoch[128/310], Step[0100/1251], Loss: 4.2632(3.9957), Acc: 0.3330(0.3224)
2021-12-30 17:31:18,525 Epoch[128/310], Step[0150/1251], Loss: 4.5758(3.9835), Acc: 0.2969(0.3157)
2021-12-30 17:32:17,794 Epoch[128/310], Step[0200/1251], Loss: 3.8125(3.9861), Acc: 0.1748(0.3079)
2021-12-30 17:33:17,667 Epoch[128/310], Step[0250/1251], Loss: 4.0402(3.9992), Acc: 0.2822(0.3050)
2021-12-30 17:34:16,315 Epoch[128/310], Step[0300/1251], Loss: 3.6814(4.0107), Acc: 0.4697(0.3082)
2021-12-30 17:35:15,730 Epoch[128/310], Step[0350/1251], Loss: 4.0089(4.0191), Acc: 0.2695(0.3111)
2021-12-30 17:36:13,484 Epoch[128/310], Step[0400/1251], Loss: 3.8743(4.0267), Acc: 0.1787(0.3099)
2021-12-30 17:37:13,249 Epoch[128/310], Step[0450/1251], Loss: 4.0540(4.0290), Acc: 0.3164(0.3063)
2021-12-30 17:38:13,559 Epoch[128/310], Step[0500/1251], Loss: 4.3892(4.0338), Acc: 0.2275(0.3050)
2021-12-30 17:39:13,626 Epoch[128/310], Step[0550/1251], Loss: 4.5626(4.0429), Acc: 0.2939(0.3029)
2021-12-30 17:40:13,564 Epoch[128/310], Step[0600/1251], Loss: 3.9754(4.0411), Acc: 0.4072(0.3034)
2021-12-30 17:41:13,716 Epoch[128/310], Step[0650/1251], Loss: 4.0887(4.0398), Acc: 0.3047(0.3028)
2021-12-30 17:42:13,882 Epoch[128/310], Step[0700/1251], Loss: 4.1177(4.0404), Acc: 0.1816(0.3039)
2021-12-30 17:43:14,339 Epoch[128/310], Step[0750/1251], Loss: 3.7414(4.0440), Acc: 0.1553(0.3052)
2021-12-30 17:44:13,370 Epoch[128/310], Step[0800/1251], Loss: 3.9408(4.0409), Acc: 0.2705(0.3063)
2021-12-30 17:45:13,489 Epoch[128/310], Step[0850/1251], Loss: 3.6991(4.0396), Acc: 0.4482(0.3082)
2021-12-30 17:46:15,106 Epoch[128/310], Step[0900/1251], Loss: 4.3139(4.0419), Acc: 0.3740(0.3078)
2021-12-30 17:47:16,067 Epoch[128/310], Step[0950/1251], Loss: 3.9806(4.0404), Acc: 0.2080(0.3088)
2021-12-30 17:48:15,973 Epoch[128/310], Step[1000/1251], Loss: 4.0826(4.0415), Acc: 0.4062(0.3090)
2021-12-30 17:49:16,921 Epoch[128/310], Step[1050/1251], Loss: 4.0917(4.0413), Acc: 0.3691(0.3091)
2021-12-30 17:50:16,986 Epoch[128/310], Step[1100/1251], Loss: 3.4407(4.0410), Acc: 0.5098(0.3093)
2021-12-30 17:51:17,640 Epoch[128/310], Step[1150/1251], Loss: 3.8830(4.0403), Acc: 0.4297(0.3091)
2021-12-30 17:52:19,155 Epoch[128/310], Step[1200/1251], Loss: 4.1778(4.0395), Acc: 0.3291(0.3080)
2021-12-30 17:53:19,819 Epoch[128/310], Step[1250/1251], Loss: 3.9915(4.0369), Acc: 0.3799(0.3083)
2021-12-30 17:53:21,993 ----- Validation after Epoch: 128
2021-12-30 17:54:10,194 Val Step[0000/1563], Loss: 0.6564 (0.6564), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2021-12-30 17:54:11,727 Val Step[0050/1563], Loss: 2.6352 (0.9119), Acc@1: 0.3750 (0.8070), Acc@5: 0.7500 (0.9430)
2021-12-30 17:54:13,245 Val Step[0100/1563], Loss: 2.4188 (1.2594), Acc@1: 0.3125 (0.7126), Acc@5: 0.8125 (0.9087)
2021-12-30 17:54:14,660 Val Step[0150/1563], Loss: 0.5946 (1.1831), Acc@1: 0.9062 (0.7308), Acc@5: 0.9375 (0.9151)
2021-12-30 17:54:16,116 Val Step[0200/1563], Loss: 1.2274 (1.1971), Acc@1: 0.7188 (0.7309), Acc@5: 0.9062 (0.9112)
2021-12-30 17:54:17,543 Val Step[0250/1563], Loss: 0.6484 (1.1246), Acc@1: 0.8438 (0.7465), Acc@5: 1.0000 (0.9194)
2021-12-30 17:54:19,036 Val Step[0300/1563], Loss: 1.5775 (1.1939), Acc@1: 0.5625 (0.7240), Acc@5: 0.9062 (0.9140)
2021-12-30 17:54:20,495 Val Step[0350/1563], Loss: 1.4647 (1.2028), Acc@1: 0.5625 (0.7185), Acc@5: 0.9375 (0.9169)
2021-12-30 17:54:21,922 Val Step[0400/1563], Loss: 1.0855 (1.1994), Acc@1: 0.7812 (0.7158), Acc@5: 0.9688 (0.9186)
2021-12-30 17:54:23,275 Val Step[0450/1563], Loss: 1.2400 (1.2034), Acc@1: 0.5312 (0.7135), Acc@5: 1.0000 (0.9196)
2021-12-30 17:54:24,728 Val Step[0500/1563], Loss: 0.3687 (1.1906), Acc@1: 0.9375 (0.7168), Acc@5: 1.0000 (0.9214)
2021-12-30 17:54:26,249 Val Step[0550/1563], Loss: 0.9076 (1.1669), Acc@1: 0.7188 (0.7235), Acc@5: 0.9688 (0.9235)
2021-12-30 17:54:27,532 Val Step[0600/1563], Loss: 0.8366 (1.1725), Acc@1: 0.8750 (0.7235), Acc@5: 0.9688 (0.9226)
2021-12-30 17:54:28,828 Val Step[0650/1563], Loss: 0.7591 (1.1941), Acc@1: 0.8750 (0.7193), Acc@5: 1.0000 (0.9194)
2021-12-30 17:54:30,158 Val Step[0700/1563], Loss: 1.3629 (1.2290), Acc@1: 0.7500 (0.7123), Acc@5: 0.8750 (0.9146)
2021-12-30 17:54:31,619 Val Step[0750/1563], Loss: 1.9610 (1.2701), Acc@1: 0.5312 (0.7051), Acc@5: 0.7500 (0.9085)
2021-12-30 17:54:33,128 Val Step[0800/1563], Loss: 1.3376 (1.3127), Acc@1: 0.6875 (0.6958), Acc@5: 0.9062 (0.9025)
2021-12-30 17:54:34,466 Val Step[0850/1563], Loss: 1.9840 (1.3417), Acc@1: 0.5000 (0.6893), Acc@5: 0.8438 (0.8984)
2021-12-30 17:54:35,866 Val Step[0900/1563], Loss: 0.3628 (1.3443), Acc@1: 0.9688 (0.6904), Acc@5: 0.9688 (0.8976)
2021-12-30 17:54:37,308 Val Step[0950/1563], Loss: 1.5502 (1.3719), Acc@1: 0.6562 (0.6852), Acc@5: 0.8438 (0.8932)
2021-12-30 17:54:38,582 Val Step[1000/1563], Loss: 0.6204 (1.3975), Acc@1: 0.9375 (0.6794), Acc@5: 1.0000 (0.8897)
2021-12-30 17:54:39,861 Val Step[1050/1563], Loss: 0.4009 (1.4122), Acc@1: 0.9688 (0.6762), Acc@5: 0.9688 (0.8878)
2021-12-30 17:54:41,134 Val Step[1100/1563], Loss: 1.1918 (1.4302), Acc@1: 0.7500 (0.6734), Acc@5: 0.9375 (0.8850)
2021-12-30 17:54:42,635 Val Step[1150/1563], Loss: 1.7994 (1.4480), Acc@1: 0.6562 (0.6700), Acc@5: 0.7812 (0.8824)
2021-12-30 17:54:44,024 Val Step[1200/1563], Loss: 1.2435 (1.4657), Acc@1: 0.7812 (0.6661), Acc@5: 0.8750 (0.8794)
2021-12-30 17:54:45,356 Val Step[1250/1563], Loss: 0.7419 (1.4801), Acc@1: 0.8750 (0.6641), Acc@5: 0.9062 (0.8770)
2021-12-30 17:54:46,736 Val Step[1300/1563], Loss: 1.0259 (1.4899), Acc@1: 0.7812 (0.6623), Acc@5: 0.8750 (0.8757)
2021-12-30 17:54:48,083 Val Step[1350/1563], Loss: 2.3809 (1.5126), Acc@1: 0.3125 (0.6574), Acc@5: 0.6875 (0.8717)
2021-12-30 17:54:49,424 Val Step[1400/1563], Loss: 1.1933 (1.5244), Acc@1: 0.6875 (0.6546), Acc@5: 0.9062 (0.8700)
2021-12-30 17:54:50,853 Val Step[1450/1563], Loss: 1.8080 (1.5304), Acc@1: 0.6250 (0.6536), Acc@5: 0.9062 (0.8693)
2021-12-30 17:54:52,330 Val Step[1500/1563], Loss: 2.3434 (1.5177), Acc@1: 0.3750 (0.6563), Acc@5: 0.6562 (0.8710)
2021-12-30 17:54:53,735 Val Step[1550/1563], Loss: 0.9800 (1.5158), Acc@1: 0.8750 (0.6561), Acc@5: 0.9375 (0.8714)
2021-12-30 17:54:54,562 ----- Epoch[128/310], Validation Loss: 1.5136, Validation Acc@1: 0.6565, Validation Acc@5: 0.8717, time: 92.57
2021-12-30 17:54:54,563 ----- Epoch[128/310], Train Loss: 4.0369, Train Acc: 0.3083, time: 1574.12, Best Val(epoch126) Acc@1: 0.6574
2021-12-30 17:54:54,751 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 17:54:54,752 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 17:54:54,859 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 17:54:54,860 Now training epoch 129. LR=0.000611
2021-12-30 17:56:04,246 Epoch[129/310], Step[0000/1251], Loss: 3.8362(3.8362), Acc: 0.2266(0.2266)
2021-12-30 17:57:03,986 Epoch[129/310], Step[0050/1251], Loss: 3.6295(4.0526), Acc: 0.4443(0.3256)
2021-12-30 17:58:04,207 Epoch[129/310], Step[0100/1251], Loss: 4.4593(4.0905), Acc: 0.1367(0.3147)
2021-12-30 17:59:03,030 Epoch[129/310], Step[0150/1251], Loss: 4.3091(4.1039), Acc: 0.2822(0.3126)
2021-12-30 18:00:04,040 Epoch[129/310], Step[0200/1251], Loss: 3.8203(4.0895), Acc: 0.3975(0.3083)
2021-12-30 18:01:04,550 Epoch[129/310], Step[0250/1251], Loss: 4.5262(4.0925), Acc: 0.2939(0.3110)
2021-12-30 18:02:04,703 Epoch[129/310], Step[0300/1251], Loss: 3.8843(4.0887), Acc: 0.2549(0.3129)
2021-12-30 18:03:03,156 Epoch[129/310], Step[0350/1251], Loss: 4.1469(4.0850), Acc: 0.4238(0.3108)
2021-12-30 18:04:03,222 Epoch[129/310], Step[0400/1251], Loss: 4.6575(4.0719), Acc: 0.2285(0.3111)
2021-12-30 18:05:03,731 Epoch[129/310], Step[0450/1251], Loss: 4.2184(4.0590), Acc: 0.4307(0.3136)
2021-12-30 18:06:03,029 Epoch[129/310], Step[0500/1251], Loss: 3.6112(4.0600), Acc: 0.3164(0.3143)
2021-12-30 18:07:02,428 Epoch[129/310], Step[0550/1251], Loss: 4.4257(4.0533), Acc: 0.0947(0.3141)
2021-12-30 18:08:00,725 Epoch[129/310], Step[0600/1251], Loss: 3.9988(4.0565), Acc: 0.0303(0.3132)
2021-12-30 18:09:00,719 Epoch[129/310], Step[0650/1251], Loss: 3.7477(4.0599), Acc: 0.3613(0.3112)
2021-12-30 18:09:59,716 Epoch[129/310], Step[0700/1251], Loss: 4.1346(4.0640), Acc: 0.4043(0.3115)
2021-12-30 18:10:59,635 Epoch[129/310], Step[0750/1251], Loss: 3.6550(4.0622), Acc: 0.4873(0.3119)
2021-12-30 18:11:57,225 Epoch[129/310], Step[0800/1251], Loss: 4.1986(4.0663), Acc: 0.2217(0.3113)
2021-12-30 18:12:55,936 Epoch[129/310], Step[0850/1251], Loss: 3.8081(4.0670), Acc: 0.4990(0.3122)
2021-12-30 18:13:54,807 Epoch[129/310], Step[0900/1251], Loss: 3.8854(4.0658), Acc: 0.4473(0.3130)
2021-12-30 18:14:53,182 Epoch[129/310], Step[0950/1251], Loss: 4.5080(4.0638), Acc: 0.3330(0.3143)
2021-12-30 18:15:54,346 Epoch[129/310], Step[1000/1251], Loss: 3.9931(4.0610), Acc: 0.3154(0.3140)
2021-12-30 18:16:56,103 Epoch[129/310], Step[1050/1251], Loss: 3.7444(4.0625), Acc: 0.4873(0.3138)
2021-12-30 18:17:56,132 Epoch[129/310], Step[1100/1251], Loss: 3.9094(4.0637), Acc: 0.4424(0.3132)
2021-12-30 18:18:55,739 Epoch[129/310], Step[1150/1251], Loss: 4.4256(4.0641), Acc: 0.2080(0.3127)
2021-12-30 18:19:56,120 Epoch[129/310], Step[1200/1251], Loss: 4.2442(4.0664), Acc: 0.3691(0.3130)
2021-12-30 18:20:56,016 Epoch[129/310], Step[1250/1251], Loss: 3.8549(4.0657), Acc: 0.2178(0.3142)
2021-12-30 18:20:57,537 ----- Epoch[129/310], Train Loss: 4.0657, Train Acc: 0.3142, time: 1562.67, Best Val(epoch126) Acc@1: 0.6574
2021-12-30 18:20:57,929 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 18:20:57,929 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 18:20:57,980 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 18:20:57,981 Now training epoch 130. LR=0.000606
2021-12-30 18:22:06,208 Epoch[130/310], Step[0000/1251], Loss: 3.8990(3.8990), Acc: 0.0430(0.0430)
2021-12-30 18:23:04,715 Epoch[130/310], Step[0050/1251], Loss: 3.9930(3.9883), Acc: 0.3896(0.3209)
2021-12-30 18:24:04,134 Epoch[130/310], Step[0100/1251], Loss: 4.4388(3.9818), Acc: 0.1377(0.3195)
2021-12-30 18:25:02,893 Epoch[130/310], Step[0150/1251], Loss: 4.0739(4.0243), Acc: 0.4863(0.3198)
2021-12-30 18:26:03,508 Epoch[130/310], Step[0200/1251], Loss: 4.0459(4.0381), Acc: 0.3809(0.3115)
2021-12-30 18:27:03,099 Epoch[130/310], Step[0250/1251], Loss: 4.5636(4.0443), Acc: 0.2012(0.3088)
2021-12-30 18:28:03,226 Epoch[130/310], Step[0300/1251], Loss: 4.3612(4.0315), Acc: 0.2041(0.3135)
2021-12-30 18:29:03,505 Epoch[130/310], Step[0350/1251], Loss: 3.7079(4.0410), Acc: 0.3301(0.3114)
2021-12-30 18:30:03,828 Epoch[130/310], Step[0400/1251], Loss: 3.8861(4.0459), Acc: 0.3223(0.3074)
2021-12-30 18:31:04,440 Epoch[130/310], Step[0450/1251], Loss: 3.9096(4.0418), Acc: 0.2363(0.3091)
2021-12-30 18:32:04,130 Epoch[130/310], Step[0500/1251], Loss: 4.1209(4.0443), Acc: 0.4160(0.3092)
2021-12-30 18:33:03,191 Epoch[130/310], Step[0550/1251], Loss: 4.6616(4.0485), Acc: 0.2930(0.3106)
2021-12-30 18:34:02,467 Epoch[130/310], Step[0600/1251], Loss: 3.5501(4.0409), Acc: 0.3789(0.3087)
2021-12-30 18:35:02,676 Epoch[130/310], Step[0650/1251], Loss: 3.9734(4.0487), Acc: 0.1924(0.3093)
2021-12-30 18:36:03,714 Epoch[130/310], Step[0700/1251], Loss: 4.2653(4.0522), Acc: 0.1172(0.3082)
2021-12-30 18:37:04,096 Epoch[130/310], Step[0750/1251], Loss: 3.1746(4.0547), Acc: 0.5166(0.3086)
2021-12-30 18:38:04,150 Epoch[130/310], Step[0800/1251], Loss: 3.8302(4.0545), Acc: 0.4219(0.3091)
2021-12-30 18:39:04,069 Epoch[130/310], Step[0850/1251], Loss: 3.6727(4.0542), Acc: 0.3672(0.3098)
2021-12-30 18:40:02,173 Epoch[130/310], Step[0900/1251], Loss: 4.2093(4.0572), Acc: 0.1445(0.3096)
2021-12-30 18:41:00,317 Epoch[130/310], Step[0950/1251], Loss: 4.5184(4.0594), Acc: 0.2324(0.3109)
2021-12-30 18:41:59,738 Epoch[130/310], Step[1000/1251], Loss: 3.9062(4.0577), Acc: 0.4326(0.3113)
2021-12-30 18:42:59,384 Epoch[130/310], Step[1050/1251], Loss: 3.3437(4.0578), Acc: 0.3789(0.3125)
2021-12-30 18:43:57,850 Epoch[130/310], Step[1100/1251], Loss: 4.0235(4.0579), Acc: 0.3965(0.3120)
2021-12-30 18:44:57,013 Epoch[130/310], Step[1150/1251], Loss: 4.3378(4.0583), Acc: 0.3057(0.3113)
2021-12-30 18:45:57,211 Epoch[130/310], Step[1200/1251], Loss: 3.9225(4.0592), Acc: 0.4189(0.3104)
2021-12-30 18:46:57,807 Epoch[130/310], Step[1250/1251], Loss: 4.3186(4.0567), Acc: 0.1084(0.3115)
2021-12-30 18:46:59,369 ----- Validation after Epoch: 130
2021-12-30 18:47:52,564 Val Step[0000/1563], Loss: 0.7967 (0.7967), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-30 18:47:53,946 Val Step[0050/1563], Loss: 2.2065 (0.9514), Acc@1: 0.4375 (0.8094), Acc@5: 0.7812 (0.9430)
2021-12-30 18:47:55,337 Val Step[0100/1563], Loss: 2.3401 (1.3029), Acc@1: 0.4375 (0.7138), Acc@5: 0.8125 (0.8985)
2021-12-30 18:47:56,890 Val Step[0150/1563], Loss: 0.5920 (1.2131), Acc@1: 0.8438 (0.7351), Acc@5: 0.9688 (0.9098)
2021-12-30 18:47:58,285 Val Step[0200/1563], Loss: 1.2920 (1.2250), Acc@1: 0.7188 (0.7385), Acc@5: 0.9375 (0.9076)
2021-12-30 18:47:59,636 Val Step[0250/1563], Loss: 1.1119 (1.1604), Acc@1: 0.7500 (0.7505), Acc@5: 0.9375 (0.9156)
2021-12-30 18:48:01,026 Val Step[0300/1563], Loss: 1.4174 (1.2286), Acc@1: 0.5625 (0.7284), Acc@5: 0.9688 (0.9107)
2021-12-30 18:48:02,304 Val Step[0350/1563], Loss: 1.1916 (1.2562), Acc@1: 0.7500 (0.7173), Acc@5: 0.9375 (0.9115)
2021-12-30 18:48:03,692 Val Step[0400/1563], Loss: 1.2134 (1.2607), Acc@1: 0.7500 (0.7119), Acc@5: 0.9375 (0.9130)
2021-12-30 18:48:05,098 Val Step[0450/1563], Loss: 1.2380 (1.2615), Acc@1: 0.5000 (0.7090), Acc@5: 1.0000 (0.9150)
2021-12-30 18:48:06,426 Val Step[0500/1563], Loss: 0.3983 (1.2519), Acc@1: 0.9062 (0.7116), Acc@5: 1.0000 (0.9172)
2021-12-30 18:48:07,948 Val Step[0550/1563], Loss: 0.9210 (1.2242), Acc@1: 0.7812 (0.7196), Acc@5: 0.9375 (0.9200)
2021-12-30 18:48:09,338 Val Step[0600/1563], Loss: 0.8280 (1.2278), Acc@1: 0.7812 (0.7187), Acc@5: 0.9688 (0.9194)
2021-12-30 18:48:10,764 Val Step[0650/1563], Loss: 0.4628 (1.2484), Acc@1: 0.9688 (0.7145), Acc@5: 1.0000 (0.9162)
2021-12-30 18:48:12,137 Val Step[0700/1563], Loss: 1.5334 (1.2825), Acc@1: 0.7188 (0.7068), Acc@5: 0.8750 (0.9116)
2021-12-30 18:48:13,541 Val Step[0750/1563], Loss: 1.4171 (1.3177), Acc@1: 0.7188 (0.7003), Acc@5: 0.9062 (0.9063)
2021-12-30 18:48:14,910 Val Step[0800/1563], Loss: 0.8905 (1.3562), Acc@1: 0.8125 (0.6924), Acc@5: 1.0000 (0.9010)
2021-12-30 18:48:16,197 Val Step[0850/1563], Loss: 1.6399 (1.3843), Acc@1: 0.5625 (0.6874), Acc@5: 0.8750 (0.8964)
2021-12-30 18:48:17,528 Val Step[0900/1563], Loss: 0.4852 (1.3845), Acc@1: 0.9375 (0.6889), Acc@5: 0.9688 (0.8953)
2021-12-30 18:48:18,957 Val Step[0950/1563], Loss: 1.5676 (1.4090), Acc@1: 0.7188 (0.6839), Acc@5: 0.9062 (0.8918)
2021-12-30 18:48:20,254 Val Step[1000/1563], Loss: 0.6250 (1.4325), Acc@1: 1.0000 (0.6783), Acc@5: 1.0000 (0.8885)
2021-12-30 18:48:21,679 Val Step[1050/1563], Loss: 0.4296 (1.4471), Acc@1: 0.9688 (0.6753), Acc@5: 0.9688 (0.8863)
2021-12-30 18:48:22,994 Val Step[1100/1563], Loss: 1.4713 (1.4639), Acc@1: 0.6875 (0.6714), Acc@5: 0.8438 (0.8838)
2021-12-30 18:48:24,336 Val Step[1150/1563], Loss: 1.5118 (1.4813), Acc@1: 0.7500 (0.6683), Acc@5: 0.8125 (0.8809)
2021-12-30 18:48:25,653 Val Step[1200/1563], Loss: 1.1913 (1.4969), Acc@1: 0.7812 (0.6649), Acc@5: 0.8438 (0.8784)
2021-12-30 18:48:27,033 Val Step[1250/1563], Loss: 0.9223 (1.5094), Acc@1: 0.8750 (0.6631), Acc@5: 0.9062 (0.8759)
2021-12-30 18:48:28,394 Val Step[1300/1563], Loss: 1.1671 (1.5220), Acc@1: 0.7812 (0.6604), Acc@5: 0.9062 (0.8745)
2021-12-30 18:48:29,717 Val Step[1350/1563], Loss: 2.3531 (1.5394), Acc@1: 0.4062 (0.6565), Acc@5: 0.6562 (0.8718)
2021-12-30 18:48:31,024 Val Step[1400/1563], Loss: 1.5684 (1.5486), Acc@1: 0.6562 (0.6544), Acc@5: 0.9062 (0.8706)
2021-12-30 18:48:32,361 Val Step[1450/1563], Loss: 1.7589 (1.5568), Acc@1: 0.5625 (0.6525), Acc@5: 0.9062 (0.8696)
2021-12-30 18:48:33,687 Val Step[1500/1563], Loss: 2.1605 (1.5447), Acc@1: 0.5312 (0.6551), Acc@5: 0.8125 (0.8716)
2021-12-30 18:48:34,972 Val Step[1550/1563], Loss: 1.0889 (1.5445), Acc@1: 0.8438 (0.6552), Acc@5: 0.9062 (0.8717)
2021-12-30 18:48:35,794 ----- Epoch[130/310], Validation Loss: 1.5417, Validation Acc@1: 0.6558, Validation Acc@5: 0.8721, time: 96.42
2021-12-30 18:48:35,794 ----- Epoch[130/310], Train Loss: 4.0567, Train Acc: 0.3115, time: 1561.38, Best Val(epoch126) Acc@1: 0.6574
2021-12-30 18:48:35,959 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-130-Loss-4.034123094057103.pdparams
2021-12-30 18:48:35,959 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-130-Loss-4.034123094057103.pdopt
2021-12-30 18:48:35,999 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-130-Loss-4.034123094057103-EMA.pdparams
2021-12-30 18:48:36,000 Now training epoch 131. LR=0.000601
2021-12-30 18:49:48,147 Epoch[131/310], Step[0000/1251], Loss: 4.1829(4.1829), Acc: 0.1719(0.1719)
2021-12-30 18:50:48,852 Epoch[131/310], Step[0050/1251], Loss: 3.8454(4.0001), Acc: 0.3896(0.3010)
2021-12-30 18:51:47,425 Epoch[131/310], Step[0100/1251], Loss: 3.7659(4.0099), Acc: 0.2393(0.3114)
2021-12-30 18:52:47,410 Epoch[131/310], Step[0150/1251], Loss: 4.5599(4.0362), Acc: 0.2100(0.3135)
2021-12-30 18:53:47,115 Epoch[131/310], Step[0200/1251], Loss: 3.8354(4.0317), Acc: 0.3428(0.3137)
2021-12-30 18:54:45,341 Epoch[131/310], Step[0250/1251], Loss: 3.9286(4.0358), Acc: 0.3115(0.3142)
2021-12-30 18:55:43,835 Epoch[131/310], Step[0300/1251], Loss: 4.4264(4.0425), Acc: 0.3203(0.3145)
2021-12-30 18:56:42,939 Epoch[131/310], Step[0350/1251], Loss: 3.8250(4.0535), Acc: 0.2754(0.3154)
2021-12-30 18:57:43,054 Epoch[131/310], Step[0400/1251], Loss: 3.8928(4.0508), Acc: 0.3428(0.3147)
2021-12-30 18:58:43,547 Epoch[131/310], Step[0450/1251], Loss: 3.9186(4.0479), Acc: 0.3086(0.3131)
2021-12-30 18:59:43,133 Epoch[131/310], Step[0500/1251], Loss: 4.0586(4.0535), Acc: 0.3682(0.3142)
2021-12-30 19:00:42,995 Epoch[131/310], Step[0550/1251], Loss: 3.2451(4.0509), Acc: 0.1338(0.3139)
2021-12-30 19:01:42,221 Epoch[131/310], Step[0600/1251], Loss: 4.0972(4.0501), Acc: 0.3496(0.3150)
2021-12-30 19:02:42,261 Epoch[131/310], Step[0650/1251], Loss: 4.1303(4.0576), Acc: 0.4590(0.3139)
2021-12-30 19:03:42,530 Epoch[131/310], Step[0700/1251], Loss: 3.3592(4.0552), Acc: 0.2568(0.3126)
2021-12-30 19:04:41,637 Epoch[131/310], Step[0750/1251], Loss: 4.7044(4.0532), Acc: 0.2910(0.3126)
2021-12-30 19:05:41,153 Epoch[131/310], Step[0800/1251], Loss: 3.8280(4.0537), Acc: 0.2041(0.3134)
2021-12-30 19:06:40,069 Epoch[131/310], Step[0850/1251], Loss: 4.4244(4.0559), Acc: 0.2803(0.3145)
2021-12-30 19:07:39,740 Epoch[131/310], Step[0900/1251], Loss: 4.7134(4.0594), Acc: 0.3193(0.3140)
2021-12-30 19:08:39,545 Epoch[131/310], Step[0950/1251], Loss: 3.9761(4.0584), Acc: 0.4609(0.3144)
2021-12-30 19:09:38,992 Epoch[131/310], Step[1000/1251], Loss: 4.5109(4.0599), Acc: 0.3076(0.3135)
2021-12-30 19:10:37,434 Epoch[131/310], Step[1050/1251], Loss: 3.2215(4.0555), Acc: 0.3945(0.3149)
2021-12-30 19:11:37,951 Epoch[131/310], Step[1100/1251], Loss: 3.2991(4.0520), Acc: 0.4258(0.3157)
2021-12-30 19:12:38,980 Epoch[131/310], Step[1150/1251], Loss: 3.8403(4.0515), Acc: 0.1787(0.3159)
2021-12-30 19:13:38,517 Epoch[131/310], Step[1200/1251], Loss: 4.4262(4.0551), Acc: 0.3535(0.3160)
2021-12-30 19:14:40,644 Epoch[131/310], Step[1250/1251], Loss: 3.7997(4.0560), Acc: 0.4727(0.3154)
2021-12-30 19:14:42,581 ----- Epoch[131/310], Train Loss: 4.0560, Train Acc: 0.3154, time: 1566.58, Best Val(epoch126) Acc@1: 0.6574
2021-12-30 19:14:42,755 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 19:14:42,756 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 19:14:42,857 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 19:14:42,857 Now training epoch 132. LR=0.000596
2021-12-30 19:15:55,732 Epoch[132/310], Step[0000/1251], Loss: 3.5325(3.5325), Acc: 0.3633(0.3633)
2021-12-30 19:16:56,233 Epoch[132/310], Step[0050/1251], Loss: 3.7807(3.9092), Acc: 0.2285(0.3328)
2021-12-30 19:17:56,368 Epoch[132/310], Step[0100/1251], Loss: 3.7910(3.9451), Acc: 0.2109(0.3218)
2021-12-30 19:18:57,228 Epoch[132/310], Step[0150/1251], Loss: 3.8264(3.9566), Acc: 0.1172(0.3231)
2021-12-30 19:19:56,979 Epoch[132/310], Step[0200/1251], Loss: 3.4293(3.9742), Acc: 0.2383(0.3223)
2021-12-30 19:20:56,972 Epoch[132/310], Step[0250/1251], Loss: 4.3290(3.9775), Acc: 0.3594(0.3195)
2021-12-30 19:21:57,202 Epoch[132/310], Step[0300/1251], Loss: 3.7164(3.9891), Acc: 0.3633(0.3193)
2021-12-30 19:22:57,412 Epoch[132/310], Step[0350/1251], Loss: 4.3593(3.9927), Acc: 0.3096(0.3205)
2021-12-30 19:23:56,344 Epoch[132/310], Step[0400/1251], Loss: 3.9045(3.9988), Acc: 0.3643(0.3190)
2021-12-30 19:24:56,994 Epoch[132/310], Step[0450/1251], Loss: 4.2001(4.0092), Acc: 0.1680(0.3184)
2021-12-30 19:25:57,375 Epoch[132/310], Step[0500/1251], Loss: 4.6005(4.0104), Acc: 0.1768(0.3211)
2021-12-30 19:26:58,077 Epoch[132/310], Step[0550/1251], Loss: 4.7003(4.0183), Acc: 0.2383(0.3186)
2021-12-30 19:27:58,113 Epoch[132/310], Step[0600/1251], Loss: 3.8028(4.0157), Acc: 0.3232(0.3196)
2021-12-30 19:28:57,462 Epoch[132/310], Step[0650/1251], Loss: 3.8266(4.0168), Acc: 0.4678(0.3232)
2021-12-30 19:29:56,287 Epoch[132/310], Step[0700/1251], Loss: 4.4260(4.0197), Acc: 0.3311(0.3224)
2021-12-30 19:30:55,823 Epoch[132/310], Step[0750/1251], Loss: 3.9796(4.0229), Acc: 0.2266(0.3213)
2021-12-30 19:31:55,039 Epoch[132/310], Step[0800/1251], Loss: 4.5391(4.0257), Acc: 0.3223(0.3208)
2021-12-30 19:32:54,485 Epoch[132/310], Step[0850/1251], Loss: 3.9876(4.0259), Acc: 0.2256(0.3201)
2021-12-30 19:33:54,783 Epoch[132/310], Step[0900/1251], Loss: 4.0823(4.0235), Acc: 0.3828(0.3196)
2021-12-30 19:34:54,962 Epoch[132/310], Step[0950/1251], Loss: 4.6591(4.0277), Acc: 0.2090(0.3195)
2021-12-30 19:35:56,029 Epoch[132/310], Step[1000/1251], Loss: 3.7346(4.0320), Acc: 0.2158(0.3189)
2021-12-30 19:36:56,332 Epoch[132/310], Step[1050/1251], Loss: 3.7114(4.0298), Acc: 0.4326(0.3199)
2021-12-30 19:37:57,007 Epoch[132/310], Step[1100/1251], Loss: 4.1192(4.0293), Acc: 0.3770(0.3195)
2021-12-30 19:38:58,078 Epoch[132/310], Step[1150/1251], Loss: 4.4871(4.0271), Acc: 0.2227(0.3191)
2021-12-30 19:39:58,056 Epoch[132/310], Step[1200/1251], Loss: 3.7715(4.0260), Acc: 0.2842(0.3180)
2021-12-30 19:40:58,215 Epoch[132/310], Step[1250/1251], Loss: 4.0777(4.0254), Acc: 0.3779(0.3174)
2021-12-30 19:41:00,065 ----- Validation after Epoch: 132
2021-12-30 19:41:51,835 Val Step[0000/1563], Loss: 1.0062 (1.0062), Acc@1: 0.8125 (0.8125), Acc@5: 0.9688 (0.9688)
2021-12-30 19:41:53,181 Val Step[0050/1563], Loss: 2.3779 (0.9562), Acc@1: 0.4062 (0.8027), Acc@5: 0.8438 (0.9479)
2021-12-30 19:41:54,470 Val Step[0100/1563], Loss: 2.3658 (1.2930), Acc@1: 0.4375 (0.7144), Acc@5: 0.7500 (0.9072)
2021-12-30 19:41:55,806 Val Step[0150/1563], Loss: 0.7149 (1.2067), Acc@1: 0.8750 (0.7353), Acc@5: 0.9688 (0.9164)
2021-12-30 19:41:57,076 Val Step[0200/1563], Loss: 1.2339 (1.2296), Acc@1: 0.6875 (0.7327), Acc@5: 0.9062 (0.9108)
2021-12-30 19:41:58,388 Val Step[0250/1563], Loss: 1.1482 (1.1719), Acc@1: 0.6875 (0.7460), Acc@5: 0.9688 (0.9189)
2021-12-30 19:41:59,665 Val Step[0300/1563], Loss: 1.5063 (1.2346), Acc@1: 0.6562 (0.7275), Acc@5: 0.9375 (0.9141)
2021-12-30 19:42:01,075 Val Step[0350/1563], Loss: 1.3197 (1.2437), Acc@1: 0.6875 (0.7245), Acc@5: 0.9375 (0.9162)
2021-12-30 19:42:02,414 Val Step[0400/1563], Loss: 1.2615 (1.2473), Acc@1: 0.7500 (0.7186), Acc@5: 0.9688 (0.9178)
2021-12-30 19:42:03,712 Val Step[0450/1563], Loss: 1.0268 (1.2532), Acc@1: 0.6562 (0.7160), Acc@5: 1.0000 (0.9180)
2021-12-30 19:42:05,016 Val Step[0500/1563], Loss: 0.4226 (1.2411), Acc@1: 0.9688 (0.7196), Acc@5: 1.0000 (0.9198)
2021-12-30 19:42:06,460 Val Step[0550/1563], Loss: 0.9703 (1.2162), Acc@1: 0.7500 (0.7259), Acc@5: 0.9688 (0.9221)
2021-12-30 19:42:07,747 Val Step[0600/1563], Loss: 0.8460 (1.2198), Acc@1: 0.8125 (0.7256), Acc@5: 0.9062 (0.9208)
2021-12-30 19:42:09,029 Val Step[0650/1563], Loss: 0.8435 (1.2435), Acc@1: 0.8125 (0.7205), Acc@5: 1.0000 (0.9178)
2021-12-30 19:42:10,322 Val Step[0700/1563], Loss: 1.3980 (1.2796), Acc@1: 0.7188 (0.7123), Acc@5: 0.8750 (0.9128)
2021-12-30 19:42:11,637 Val Step[0750/1563], Loss: 1.6541 (1.3178), Acc@1: 0.6562 (0.7046), Acc@5: 0.8438 (0.9070)
2021-12-30 19:42:12,919 Val Step[0800/1563], Loss: 0.8659 (1.3577), Acc@1: 0.7812 (0.6961), Acc@5: 1.0000 (0.9016)
2021-12-30 19:42:14,209 Val Step[0850/1563], Loss: 1.8824 (1.3868), Acc@1: 0.5000 (0.6892), Acc@5: 0.9062 (0.8975)
2021-12-30 19:42:15,547 Val Step[0900/1563], Loss: 0.3638 (1.3874), Acc@1: 0.9375 (0.6904), Acc@5: 0.9688 (0.8966)
2021-12-30 19:42:16,972 Val Step[0950/1563], Loss: 1.9710 (1.4145), Acc@1: 0.6250 (0.6852), Acc@5: 0.8438 (0.8925)
2021-12-30 19:42:18,314 Val Step[1000/1563], Loss: 0.9176 (1.4427), Acc@1: 0.8438 (0.6786), Acc@5: 0.9688 (0.8883)
2021-12-30 19:42:19,615 Val Step[1050/1563], Loss: 0.5766 (1.4607), Acc@1: 0.9375 (0.6744), Acc@5: 0.9688 (0.8862)
2021-12-30 19:42:20,931 Val Step[1100/1563], Loss: 1.4096 (1.4799), Acc@1: 0.7188 (0.6704), Acc@5: 0.8750 (0.8835)
2021-12-30 19:42:22,246 Val Step[1150/1563], Loss: 1.5590 (1.4975), Acc@1: 0.7500 (0.6677), Acc@5: 0.7812 (0.8806)
2021-12-30 19:42:23,519 Val Step[1200/1563], Loss: 1.2165 (1.5143), Acc@1: 0.8125 (0.6642), Acc@5: 0.8750 (0.8777)
2021-12-30 19:42:24,839 Val Step[1250/1563], Loss: 0.9261 (1.5286), Acc@1: 0.8438 (0.6622), Acc@5: 0.9062 (0.8752)
2021-12-30 19:42:26,196 Val Step[1300/1563], Loss: 1.4279 (1.5406), Acc@1: 0.7812 (0.6598), Acc@5: 0.9062 (0.8737)
2021-12-30 19:42:27,534 Val Step[1350/1563], Loss: 1.9546 (1.5598), Acc@1: 0.3750 (0.6552), Acc@5: 0.8125 (0.8704)
2021-12-30 19:42:28,937 Val Step[1400/1563], Loss: 1.7999 (1.5690), Acc@1: 0.6250 (0.6533), Acc@5: 0.8750 (0.8688)
2021-12-30 19:42:30,330 Val Step[1450/1563], Loss: 1.8437 (1.5762), Acc@1: 0.6562 (0.6519), Acc@5: 0.9062 (0.8680)
2021-12-30 19:42:31,670 Val Step[1500/1563], Loss: 2.0839 (1.5652), Acc@1: 0.5000 (0.6543), Acc@5: 0.8438 (0.8696)
2021-12-30 19:42:33,018 Val Step[1550/1563], Loss: 1.0980 (1.5630), Acc@1: 0.8750 (0.6545), Acc@5: 0.9062 (0.8701)
2021-12-30 19:42:33,835 ----- Epoch[132/310], Validation Loss: 1.5607, Validation Acc@1: 0.6550, Validation Acc@5: 0.8704, time: 93.77
2021-12-30 19:42:33,836 ----- Epoch[132/310], Train Loss: 4.0254, Train Acc: 0.3174, time: 1577.20, Best Val(epoch126) Acc@1: 0.6574
2021-12-30 19:42:34,036 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 19:42:34,036 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 19:42:34,145 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 19:42:34,146 Now training epoch 133. LR=0.000591
2021-12-30 19:43:43,983 Epoch[133/310], Step[0000/1251], Loss: 4.1798(4.1798), Acc: 0.2305(0.2305)
2021-12-30 19:44:43,223 Epoch[133/310], Step[0050/1251], Loss: 3.9747(3.9644), Acc: 0.4346(0.3309)
2021-12-30 19:45:41,163 Epoch[133/310], Step[0100/1251], Loss: 4.4978(4.0058), Acc: 0.2490(0.3213)
2021-12-30 19:46:38,491 Epoch[133/310], Step[0150/1251], Loss: 4.0855(4.0423), Acc: 0.4434(0.3159)
2021-12-30 19:47:36,282 Epoch[133/310], Step[0200/1251], Loss: 3.8307(4.0520), Acc: 0.3115(0.3148)
2021-12-30 19:48:34,877 Epoch[133/310], Step[0250/1251], Loss: 4.2262(4.0391), Acc: 0.3105(0.3170)
2021-12-30 19:49:33,788 Epoch[133/310], Step[0300/1251], Loss: 4.2036(4.0443), Acc: 0.3672(0.3180)
2021-12-30 19:50:31,289 Epoch[133/310], Step[0350/1251], Loss: 3.9459(4.0475), Acc: 0.2041(0.3151)
2021-12-30 19:51:29,170 Epoch[133/310], Step[0400/1251], Loss: 4.0850(4.0401), Acc: 0.4268(0.3156)
2021-12-30 19:52:28,887 Epoch[133/310], Step[0450/1251], Loss: 3.6185(4.0417), Acc: 0.3428(0.3156)
2021-12-30 19:53:27,491 Epoch[133/310], Step[0500/1251], Loss: 3.8735(4.0394), Acc: 0.3877(0.3148)
2021-12-30 19:54:27,226 Epoch[133/310], Step[0550/1251], Loss: 4.1384(4.0420), Acc: 0.3594(0.3148)
2021-12-30 19:55:25,080 Epoch[133/310], Step[0600/1251], Loss: 4.2834(4.0416), Acc: 0.2900(0.3150)
2021-12-30 19:56:24,604 Epoch[133/310], Step[0650/1251], Loss: 3.8942(4.0420), Acc: 0.3994(0.3143)
2021-12-30 19:57:22,915 Epoch[133/310], Step[0700/1251], Loss: 3.5296(4.0392), Acc: 0.4229(0.3149)
2021-12-30 19:58:23,384 Epoch[133/310], Step[0750/1251], Loss: 3.9587(4.0428), Acc: 0.4766(0.3155)
2021-12-30 19:59:24,016 Epoch[133/310], Step[0800/1251], Loss: 3.7931(4.0449), Acc: 0.3994(0.3152)
2021-12-30 20:00:23,470 Epoch[133/310], Step[0850/1251], Loss: 4.5920(4.0434), Acc: 0.1768(0.3156)
2021-12-30 20:01:22,262 Epoch[133/310], Step[0900/1251], Loss: 3.7395(4.0492), Acc: 0.3291(0.3154)
2021-12-30 20:02:22,676 Epoch[133/310], Step[0950/1251], Loss: 4.0328(4.0480), Acc: 0.2715(0.3142)
2021-12-30 20:03:22,004 Epoch[133/310], Step[1000/1251], Loss: 4.4918(4.0486), Acc: 0.3633(0.3137)
2021-12-30 20:04:22,366 Epoch[133/310], Step[1050/1251], Loss: 4.5330(4.0517), Acc: 0.2646(0.3140)
2021-12-30 20:05:21,636 Epoch[133/310], Step[1100/1251], Loss: 3.5966(4.0490), Acc: 0.3447(0.3137)
2021-12-30 20:06:21,865 Epoch[133/310], Step[1150/1251], Loss: 3.8679(4.0463), Acc: 0.3018(0.3153)
2021-12-30 20:07:22,695 Epoch[133/310], Step[1200/1251], Loss: 3.8459(4.0430), Acc: 0.1338(0.3153)
2021-12-30 20:08:21,690 Epoch[133/310], Step[1250/1251], Loss: 3.7133(4.0428), Acc: 0.3252(0.3148)
2021-12-30 20:08:23,411 ----- Epoch[133/310], Train Loss: 4.0428, Train Acc: 0.3148, time: 1549.26, Best Val(epoch126) Acc@1: 0.6574
2021-12-30 20:08:23,590 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 20:08:23,590 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 20:08:23,708 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 20:08:23,708 Now training epoch 134. LR=0.000585
2021-12-30 20:09:41,610 Epoch[134/310], Step[0000/1251], Loss: 4.1197(4.1197), Acc: 0.3193(0.3193)
2021-12-30 20:10:41,452 Epoch[134/310], Step[0050/1251], Loss: 4.3758(4.0678), Acc: 0.3896(0.3108)
2021-12-30 20:11:40,591 Epoch[134/310], Step[0100/1251], Loss: 4.1638(4.0745), Acc: 0.3809(0.3172)
2021-12-30 20:12:39,986 Epoch[134/310], Step[0150/1251], Loss: 3.8235(4.0555), Acc: 0.1719(0.3146)
2021-12-30 20:13:39,930 Epoch[134/310], Step[0200/1251], Loss: 4.2141(4.0536), Acc: 0.3066(0.3073)
2021-12-30 20:14:40,276 Epoch[134/310], Step[0250/1251], Loss: 3.8037(4.0534), Acc: 0.2217(0.3055)
2021-12-30 20:15:41,237 Epoch[134/310], Step[0300/1251], Loss: 4.1464(4.0516), Acc: 0.2383(0.3070)
2021-12-30 20:16:39,455 Epoch[134/310], Step[0350/1251], Loss: 3.8920(4.0503), Acc: 0.4561(0.3101)
2021-12-30 20:17:39,039 Epoch[134/310], Step[0400/1251], Loss: 4.0788(4.0419), Acc: 0.3184(0.3111)
2021-12-30 20:18:38,939 Epoch[134/310], Step[0450/1251], Loss: 4.1909(4.0419), Acc: 0.3467(0.3102)
2021-12-30 20:19:38,296 Epoch[134/310], Step[0500/1251], Loss: 3.4543(4.0438), Acc: 0.3965(0.3120)
2021-12-30 20:20:38,550 Epoch[134/310], Step[0550/1251], Loss: 4.3190(4.0449), Acc: 0.2725(0.3117)
2021-12-30 20:21:38,371 Epoch[134/310], Step[0600/1251], Loss: 4.2065(4.0434), Acc: 0.2822(0.3128)
2021-12-30 20:22:38,370 Epoch[134/310], Step[0650/1251], Loss: 3.9216(4.0407), Acc: 0.4229(0.3125)
2021-12-30 20:23:38,047 Epoch[134/310], Step[0700/1251], Loss: 4.2714(4.0388), Acc: 0.1836(0.3131)
2021-12-30 20:24:38,766 Epoch[134/310], Step[0750/1251], Loss: 4.1156(4.0407), Acc: 0.1924(0.3102)
2021-12-30 20:25:38,322 Epoch[134/310], Step[0800/1251], Loss: 3.8696(4.0420), Acc: 0.4189(0.3086)
2021-12-30 20:26:38,694 Epoch[134/310], Step[0850/1251], Loss: 3.8401(4.0399), Acc: 0.4922(0.3098)
2021-12-30 20:27:39,626 Epoch[134/310], Step[0900/1251], Loss: 3.9550(4.0376), Acc: 0.3320(0.3106)
2021-12-30 20:28:39,818 Epoch[134/310], Step[0950/1251], Loss: 4.1672(4.0409), Acc: 0.3145(0.3112)
2021-12-30 20:29:38,822 Epoch[134/310], Step[1000/1251], Loss: 3.9337(4.0400), Acc: 0.3818(0.3116)
2021-12-30 20:30:38,001 Epoch[134/310], Step[1050/1251], Loss: 4.2315(4.0407), Acc: 0.3252(0.3120)
2021-12-30 20:31:37,807 Epoch[134/310], Step[1100/1251], Loss: 3.9709(4.0437), Acc: 0.2783(0.3112)
2021-12-30 20:32:37,224 Epoch[134/310], Step[1150/1251], Loss: 4.2624(4.0449), Acc: 0.3799(0.3115)
2021-12-30 20:33:36,718 Epoch[134/310], Step[1200/1251], Loss: 4.4936(4.0438), Acc: 0.4326(0.3120)
2021-12-30 20:34:34,664 Epoch[134/310], Step[1250/1251], Loss: 4.3179(4.0423), Acc: 0.2334(0.3136)
2021-12-30 20:34:36,408 ----- Validation after Epoch: 134
2021-12-30 20:35:28,784 Val Step[0000/1563], Loss: 0.6778 (0.6778), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2021-12-30 20:35:30,230 Val Step[0050/1563], Loss: 2.3289 (0.9465), Acc@1: 0.4062 (0.8100), Acc@5: 0.8750 (0.9467)
2021-12-30 20:35:31,627 Val Step[0100/1563], Loss: 1.9991 (1.2904), Acc@1: 0.5312 (0.7113), Acc@5: 0.8125 (0.9106)
2021-12-30 20:35:32,983 Val Step[0150/1563], Loss: 0.9343 (1.2097), Acc@1: 0.8125 (0.7332), Acc@5: 0.9688 (0.9158)
2021-12-30 20:35:34,303 Val Step[0200/1563], Loss: 1.7358 (1.2156), Acc@1: 0.5000 (0.7345), Acc@5: 0.9375 (0.9148)
2021-12-30 20:35:35,667 Val Step[0250/1563], Loss: 1.0597 (1.1533), Acc@1: 0.7500 (0.7480), Acc@5: 0.9375 (0.9226)
2021-12-30 20:35:36,938 Val Step[0300/1563], Loss: 1.5889 (1.2301), Acc@1: 0.5938 (0.7245), Acc@5: 0.8438 (0.9151)
2021-12-30 20:35:38,232 Val Step[0350/1563], Loss: 1.3987 (1.2364), Acc@1: 0.6562 (0.7220), Acc@5: 0.9375 (0.9169)
2021-12-30 20:35:39,534 Val Step[0400/1563], Loss: 1.4079 (1.2390), Acc@1: 0.7188 (0.7165), Acc@5: 0.9688 (0.9194)
2021-12-30 20:35:40,996 Val Step[0450/1563], Loss: 1.1270 (1.2399), Acc@1: 0.7188 (0.7149), Acc@5: 1.0000 (0.9212)
2021-12-30 20:35:42,326 Val Step[0500/1563], Loss: 0.3207 (1.2322), Acc@1: 1.0000 (0.7173), Acc@5: 1.0000 (0.9231)
2021-12-30 20:35:43,732 Val Step[0550/1563], Loss: 0.6665 (1.2064), Acc@1: 0.8438 (0.7248), Acc@5: 1.0000 (0.9256)
2021-12-30 20:35:45,116 Val Step[0600/1563], Loss: 0.8114 (1.2129), Acc@1: 0.8750 (0.7251), Acc@5: 0.9688 (0.9249)
2021-12-30 20:35:46,429 Val Step[0650/1563], Loss: 0.7344 (1.2366), Acc@1: 0.9062 (0.7210), Acc@5: 1.0000 (0.9210)
2021-12-30 20:35:47,714 Val Step[0700/1563], Loss: 1.4940 (1.2682), Acc@1: 0.7188 (0.7136), Acc@5: 0.9062 (0.9161)
2021-12-30 20:35:49,001 Val Step[0750/1563], Loss: 1.8852 (1.3080), Acc@1: 0.6562 (0.7060), Acc@5: 0.8125 (0.9095)
2021-12-30 20:35:50,276 Val Step[0800/1563], Loss: 1.0694 (1.3500), Acc@1: 0.7188 (0.6961), Acc@5: 0.9375 (0.9034)
2021-12-30 20:35:51,647 Val Step[0850/1563], Loss: 1.6196 (1.3794), Acc@1: 0.5938 (0.6898), Acc@5: 0.8438 (0.8989)
2021-12-30 20:35:52,968 Val Step[0900/1563], Loss: 0.3923 (1.3798), Acc@1: 0.9062 (0.6909), Acc@5: 1.0000 (0.8983)
2021-12-30 20:35:54,358 Val Step[0950/1563], Loss: 1.5417 (1.4019), Acc@1: 0.6875 (0.6870), Acc@5: 0.8438 (0.8947)
2021-12-30 20:35:55,643 Val Step[1000/1563], Loss: 0.8803 (1.4269), Acc@1: 0.8750 (0.6813), Acc@5: 0.9688 (0.8911)
2021-12-30 20:35:57,054 Val Step[1050/1563], Loss: 0.4726 (1.4429), Acc@1: 0.9688 (0.6778), Acc@5: 0.9688 (0.8888)
2021-12-30 20:35:58,480 Val Step[1100/1563], Loss: 1.2375 (1.4603), Acc@1: 0.7500 (0.6743), Acc@5: 0.9062 (0.8862)
2021-12-30 20:35:59,880 Val Step[1150/1563], Loss: 1.4308 (1.4789), Acc@1: 0.7188 (0.6706), Acc@5: 0.8438 (0.8832)
2021-12-30 20:36:01,327 Val Step[1200/1563], Loss: 1.4053 (1.4957), Acc@1: 0.7500 (0.6672), Acc@5: 0.8438 (0.8803)
2021-12-30 20:36:02,648 Val Step[1250/1563], Loss: 0.7803 (1.5094), Acc@1: 0.8750 (0.6652), Acc@5: 0.9062 (0.8775)
2021-12-30 20:36:03,930 Val Step[1300/1563], Loss: 1.0336 (1.5206), Acc@1: 0.8125 (0.6631), Acc@5: 0.8750 (0.8762)
2021-12-30 20:36:05,228 Val Step[1350/1563], Loss: 2.0076 (1.5404), Acc@1: 0.4375 (0.6588), Acc@5: 0.7812 (0.8730)
2021-12-30 20:36:06,496 Val Step[1400/1563], Loss: 1.6065 (1.5469), Acc@1: 0.6250 (0.6575), Acc@5: 0.8438 (0.8719)
2021-12-30 20:36:07,903 Val Step[1450/1563], Loss: 1.5421 (1.5548), Acc@1: 0.5938 (0.6558), Acc@5: 0.9375 (0.8711)
2021-12-30 20:36:09,334 Val Step[1500/1563], Loss: 1.5855 (1.5426), Acc@1: 0.5938 (0.6584), Acc@5: 0.9375 (0.8728)
2021-12-30 20:36:10,854 Val Step[1550/1563], Loss: 1.0818 (1.5440), Acc@1: 0.8750 (0.6579), Acc@5: 0.9062 (0.8727)
2021-12-30 20:36:11,652 ----- Epoch[134/310], Validation Loss: 1.5423, Validation Acc@1: 0.6583, Validation Acc@5: 0.8729, time: 95.24
2021-12-30 20:36:11,652 ----- Epoch[134/310], Train Loss: 4.0423, Train Acc: 0.3136, time: 1572.70, Best Val(epoch134) Acc@1: 0.6583
2021-12-30 20:36:11,840 Max accuracy so far: 0.6583 at epoch_134
2021-12-30 20:36:11,840 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 20:36:11,841 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 20:36:11,949 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 20:36:12,473 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 20:36:12,473 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 20:36:12,524 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 20:36:12,524 Now training epoch 135. LR=0.000580
2021-12-30 20:37:27,441 Epoch[135/310], Step[0000/1251], Loss: 3.5680(3.5680), Acc: 0.4824(0.4824)
2021-12-30 20:38:27,135 Epoch[135/310], Step[0050/1251], Loss: 4.3500(3.9703), Acc: 0.3721(0.3220)
2021-12-30 20:39:26,091 Epoch[135/310], Step[0100/1251], Loss: 4.2838(3.9923), Acc: 0.3691(0.3022)
2021-12-30 20:40:25,237 Epoch[135/310], Step[0150/1251], Loss: 3.9092(4.0032), Acc: 0.3789(0.3009)
2021-12-30 20:41:23,734 Epoch[135/310], Step[0200/1251], Loss: 4.3640(4.0304), Acc: 0.2295(0.2974)
2021-12-30 20:42:22,408 Epoch[135/310], Step[0250/1251], Loss: 3.9442(4.0252), Acc: 0.3311(0.3016)
2021-12-30 20:43:21,730 Epoch[135/310], Step[0300/1251], Loss: 4.7748(4.0196), Acc: 0.3047(0.3036)
2021-12-30 20:44:21,277 Epoch[135/310], Step[0350/1251], Loss: 4.3286(4.0215), Acc: 0.2246(0.3051)
2021-12-30 20:45:21,082 Epoch[135/310], Step[0400/1251], Loss: 3.7699(4.0296), Acc: 0.4092(0.3048)
2021-12-30 20:46:20,094 Epoch[135/310], Step[0450/1251], Loss: 3.8425(4.0350), Acc: 0.3857(0.3069)
2021-12-30 20:47:18,587 Epoch[135/310], Step[0500/1251], Loss: 4.2688(4.0307), Acc: 0.2529(0.3073)
2021-12-30 20:48:18,294 Epoch[135/310], Step[0550/1251], Loss: 3.6161(4.0333), Acc: 0.3779(0.3080)
2021-12-30 20:49:18,780 Epoch[135/310], Step[0600/1251], Loss: 4.5522(4.0343), Acc: 0.3076(0.3072)
2021-12-30 20:50:18,744 Epoch[135/310], Step[0650/1251], Loss: 3.8436(4.0340), Acc: 0.2529(0.3070)
2021-12-30 20:51:16,955 Epoch[135/310], Step[0700/1251], Loss: 3.9196(4.0337), Acc: 0.3633(0.3077)
2021-12-30 20:52:17,091 Epoch[135/310], Step[0750/1251], Loss: 4.3379(4.0319), Acc: 0.1035(0.3091)
2021-12-30 20:53:17,202 Epoch[135/310], Step[0800/1251], Loss: 4.2042(4.0328), Acc: 0.2373(0.3106)
2021-12-30 20:54:17,131 Epoch[135/310], Step[0850/1251], Loss: 3.8296(4.0328), Acc: 0.2891(0.3116)
2021-12-30 20:55:16,970 Epoch[135/310], Step[0900/1251], Loss: 4.0167(4.0294), Acc: 0.3721(0.3126)
2021-12-30 20:56:18,528 Epoch[135/310], Step[0950/1251], Loss: 3.6722(4.0248), Acc: 0.3135(0.3129)
2021-12-30 20:57:19,296 Epoch[135/310], Step[1000/1251], Loss: 3.9010(4.0255), Acc: 0.3115(0.3129)
2021-12-30 20:58:18,876 Epoch[135/310], Step[1050/1251], Loss: 4.5535(4.0266), Acc: 0.2598(0.3131)
2021-12-30 20:59:17,866 Epoch[135/310], Step[1100/1251], Loss: 3.8166(4.0266), Acc: 0.2627(0.3123)
2021-12-30 21:00:16,751 Epoch[135/310], Step[1150/1251], Loss: 4.1460(4.0272), Acc: 0.3350(0.3133)
2021-12-30 21:01:17,703 Epoch[135/310], Step[1200/1251], Loss: 4.0734(4.0272), Acc: 0.2373(0.3138)
2021-12-30 21:02:17,304 Epoch[135/310], Step[1250/1251], Loss: 4.2721(4.0273), Acc: 0.3779(0.3138)
2021-12-30 21:02:18,908 ----- Epoch[135/310], Train Loss: 4.0273, Train Acc: 0.3138, time: 1566.38, Best Val(epoch134) Acc@1: 0.6583
2021-12-30 21:02:19,086 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 21:02:19,087 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 21:02:19,196 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 21:02:19,196 Now training epoch 136. LR=0.000575
2021-12-30 21:03:30,001 Epoch[136/310], Step[0000/1251], Loss: 4.0090(4.0090), Acc: 0.2285(0.2285)
2021-12-30 21:04:29,888 Epoch[136/310], Step[0050/1251], Loss: 3.5042(4.0106), Acc: 0.4004(0.3172)
2021-12-30 21:05:31,169 Epoch[136/310], Step[0100/1251], Loss: 4.2096(4.0018), Acc: 0.3164(0.3243)
2021-12-30 21:06:30,888 Epoch[136/310], Step[0150/1251], Loss: 3.9727(3.9923), Acc: 0.3848(0.3214)
2021-12-30 21:07:30,361 Epoch[136/310], Step[0200/1251], Loss: 3.7473(3.9933), Acc: 0.2529(0.3210)
2021-12-30 21:08:30,138 Epoch[136/310], Step[0250/1251], Loss: 3.9212(4.0013), Acc: 0.3232(0.3188)
2021-12-30 21:09:29,647 Epoch[136/310], Step[0300/1251], Loss: 4.7209(4.0079), Acc: 0.1982(0.3225)
2021-12-30 21:10:30,063 Epoch[136/310], Step[0350/1251], Loss: 3.6842(4.0117), Acc: 0.2715(0.3209)
2021-12-30 21:11:28,848 Epoch[136/310], Step[0400/1251], Loss: 4.7953(4.0145), Acc: 0.2363(0.3216)
2021-12-30 21:12:28,392 Epoch[136/310], Step[0450/1251], Loss: 4.3481(4.0191), Acc: 0.2197(0.3201)
2021-12-30 21:13:28,303 Epoch[136/310], Step[0500/1251], Loss: 3.7905(4.0219), Acc: 0.0605(0.3187)
2021-12-30 21:14:27,253 Epoch[136/310], Step[0550/1251], Loss: 4.4463(4.0272), Acc: 0.3359(0.3174)
2021-12-30 21:15:25,279 Epoch[136/310], Step[0600/1251], Loss: 3.9096(4.0272), Acc: 0.4160(0.3198)
2021-12-30 21:16:25,338 Epoch[136/310], Step[0650/1251], Loss: 4.4010(4.0320), Acc: 0.2930(0.3197)
2021-12-30 21:17:23,250 Epoch[136/310], Step[0700/1251], Loss: 4.0303(4.0282), Acc: 0.2227(0.3217)
2021-12-30 21:18:20,899 Epoch[136/310], Step[0750/1251], Loss: 4.1045(4.0295), Acc: 0.3340(0.3229)
2021-12-30 21:19:19,659 Epoch[136/310], Step[0800/1251], Loss: 4.0029(4.0310), Acc: 0.1328(0.3219)
2021-12-30 21:20:19,454 Epoch[136/310], Step[0850/1251], Loss: 3.9449(4.0300), Acc: 0.3877(0.3211)
2021-12-30 21:21:18,544 Epoch[136/310], Step[0900/1251], Loss: 4.0861(4.0318), Acc: 0.0576(0.3204)
2021-12-30 21:22:18,587 Epoch[136/310], Step[0950/1251], Loss: 3.6110(4.0343), Acc: 0.3916(0.3190)
2021-12-30 21:23:18,009 Epoch[136/310], Step[1000/1251], Loss: 4.1950(4.0356), Acc: 0.2627(0.3208)
2021-12-30 21:24:17,396 Epoch[136/310], Step[1050/1251], Loss: 4.1345(4.0356), Acc: 0.2998(0.3199)
2021-12-30 21:25:17,775 Epoch[136/310], Step[1100/1251], Loss: 3.9969(4.0342), Acc: 0.4150(0.3212)
2021-12-30 21:26:16,236 Epoch[136/310], Step[1150/1251], Loss: 3.7595(4.0337), Acc: 0.3877(0.3205)
2021-12-30 21:27:17,013 Epoch[136/310], Step[1200/1251], Loss: 3.6493(4.0330), Acc: 0.0137(0.3211)
2021-12-30 21:28:16,599 Epoch[136/310], Step[1250/1251], Loss: 3.9811(4.0322), Acc: 0.4043(0.3210)
2021-12-30 21:28:18,477 ----- Validation after Epoch: 136
2021-12-30 21:29:08,325 Val Step[0000/1563], Loss: 0.8184 (0.8184), Acc@1: 0.9375 (0.9375), Acc@5: 0.9688 (0.9688)
2021-12-30 21:29:09,642 Val Step[0050/1563], Loss: 2.5757 (0.9250), Acc@1: 0.3750 (0.8137), Acc@5: 0.8438 (0.9461)
2021-12-30 21:29:10,993 Val Step[0100/1563], Loss: 2.5538 (1.2787), Acc@1: 0.3125 (0.7200), Acc@5: 0.7500 (0.9066)
2021-12-30 21:29:12,298 Val Step[0150/1563], Loss: 0.7356 (1.2112), Acc@1: 0.8750 (0.7339), Acc@5: 1.0000 (0.9145)
2021-12-30 21:29:13,706 Val Step[0200/1563], Loss: 1.4096 (1.2226), Acc@1: 0.6562 (0.7373), Acc@5: 0.9062 (0.9109)
2021-12-30 21:29:15,029 Val Step[0250/1563], Loss: 0.8002 (1.1614), Acc@1: 0.8750 (0.7510), Acc@5: 0.9688 (0.9186)
2021-12-30 21:29:16,451 Val Step[0300/1563], Loss: 1.2511 (1.2330), Acc@1: 0.7188 (0.7290), Acc@5: 0.9375 (0.9126)
2021-12-30 21:29:17,908 Val Step[0350/1563], Loss: 1.4711 (1.2372), Acc@1: 0.6562 (0.7241), Acc@5: 0.9062 (0.9158)
2021-12-30 21:29:19,276 Val Step[0400/1563], Loss: 1.2123 (1.2471), Acc@1: 0.7500 (0.7183), Acc@5: 0.9375 (0.9158)
2021-12-30 21:29:20,650 Val Step[0450/1563], Loss: 0.8732 (1.2525), Acc@1: 0.7500 (0.7159), Acc@5: 1.0000 (0.9169)
2021-12-30 21:29:22,003 Val Step[0500/1563], Loss: 0.3877 (1.2444), Acc@1: 0.9688 (0.7187), Acc@5: 1.0000 (0.9185)
2021-12-30 21:29:23,461 Val Step[0550/1563], Loss: 0.9992 (1.2223), Acc@1: 0.7500 (0.7265), Acc@5: 0.9688 (0.9205)
2021-12-30 21:29:24,782 Val Step[0600/1563], Loss: 0.7543 (1.2293), Acc@1: 0.8750 (0.7253), Acc@5: 0.9688 (0.9201)
2021-12-30 21:29:26,078 Val Step[0650/1563], Loss: 0.9902 (1.2494), Acc@1: 0.7188 (0.7215), Acc@5: 1.0000 (0.9170)
2021-12-30 21:29:27,366 Val Step[0700/1563], Loss: 1.2549 (1.2894), Acc@1: 0.7812 (0.7117), Acc@5: 0.9375 (0.9116)
2021-12-30 21:29:28,791 Val Step[0750/1563], Loss: 1.7192 (1.3240), Acc@1: 0.6250 (0.7051), Acc@5: 0.8438 (0.9071)
2021-12-30 21:29:30,155 Val Step[0800/1563], Loss: 1.5280 (1.3665), Acc@1: 0.7188 (0.6956), Acc@5: 0.9062 (0.9010)
2021-12-30 21:29:31,425 Val Step[0850/1563], Loss: 1.7648 (1.3965), Acc@1: 0.5625 (0.6889), Acc@5: 0.9062 (0.8970)
2021-12-30 21:29:32,770 Val Step[0900/1563], Loss: 0.4857 (1.3968), Acc@1: 0.9375 (0.6906), Acc@5: 1.0000 (0.8963)
2021-12-30 21:29:34,328 Val Step[0950/1563], Loss: 1.8654 (1.4187), Acc@1: 0.5938 (0.6867), Acc@5: 0.8438 (0.8927)
2021-12-30 21:29:35,714 Val Step[1000/1563], Loss: 1.0101 (1.4461), Acc@1: 0.8750 (0.6805), Acc@5: 1.0000 (0.8889)
2021-12-30 21:29:37,046 Val Step[1050/1563], Loss: 0.4364 (1.4612), Acc@1: 0.9688 (0.6774), Acc@5: 0.9688 (0.8869)
2021-12-30 21:29:38,343 Val Step[1100/1563], Loss: 1.6358 (1.4817), Acc@1: 0.6562 (0.6733), Acc@5: 0.8438 (0.8836)
2021-12-30 21:29:39,648 Val Step[1150/1563], Loss: 1.4195 (1.4972), Acc@1: 0.7812 (0.6705), Acc@5: 0.7812 (0.8810)
2021-12-30 21:29:40,978 Val Step[1200/1563], Loss: 1.4475 (1.5182), Acc@1: 0.8125 (0.6662), Acc@5: 0.8438 (0.8774)
2021-12-30 21:29:42,402 Val Step[1250/1563], Loss: 1.1830 (1.5346), Acc@1: 0.8125 (0.6638), Acc@5: 0.8125 (0.8747)
2021-12-30 21:29:43,755 Val Step[1300/1563], Loss: 0.9543 (1.5447), Acc@1: 0.8750 (0.6621), Acc@5: 0.9375 (0.8735)
2021-12-30 21:29:45,100 Val Step[1350/1563], Loss: 2.6017 (1.5643), Acc@1: 0.2812 (0.6580), Acc@5: 0.6562 (0.8702)
2021-12-30 21:29:46,404 Val Step[1400/1563], Loss: 1.9376 (1.5730), Acc@1: 0.5000 (0.6556), Acc@5: 0.7812 (0.8689)
2021-12-30 21:29:47,711 Val Step[1450/1563], Loss: 1.7636 (1.5802), Acc@1: 0.5625 (0.6542), Acc@5: 0.8750 (0.8680)
2021-12-30 21:29:49,040 Val Step[1500/1563], Loss: 1.8530 (1.5679), Acc@1: 0.5938 (0.6569), Acc@5: 0.9062 (0.8699)
2021-12-30 21:29:50,357 Val Step[1550/1563], Loss: 1.1861 (1.5665), Acc@1: 0.8750 (0.6570), Acc@5: 0.9062 (0.8703)
2021-12-30 21:29:51,146 ----- Epoch[136/310], Validation Loss: 1.5649, Validation Acc@1: 0.6573, Validation Acc@5: 0.8705, time: 92.67
2021-12-30 21:29:51,146 ----- Epoch[136/310], Train Loss: 4.0322, Train Acc: 0.3210, time: 1559.28, Best Val(epoch134) Acc@1: 0.6583
2021-12-30 21:29:51,335 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 21:29:51,335 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 21:29:51,443 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 21:29:51,443 Now training epoch 137. LR=0.000570
2021-12-30 21:31:00,064 Epoch[137/310], Step[0000/1251], Loss: 3.7317(3.7317), Acc: 0.3848(0.3848)
2021-12-30 21:31:58,815 Epoch[137/310], Step[0050/1251], Loss: 4.2037(4.0676), Acc: 0.1592(0.3049)
2021-12-30 21:32:57,335 Epoch[137/310], Step[0100/1251], Loss: 3.7935(4.0379), Acc: 0.3184(0.3202)
2021-12-30 21:33:56,110 Epoch[137/310], Step[0150/1251], Loss: 4.3477(4.0317), Acc: 0.2881(0.3235)
2021-12-30 21:34:56,372 Epoch[137/310], Step[0200/1251], Loss: 4.2534(4.0380), Acc: 0.1807(0.3187)
2021-12-30 21:35:55,668 Epoch[137/310], Step[0250/1251], Loss: 4.3087(4.0381), Acc: 0.4199(0.3214)
2021-12-30 21:36:55,134 Epoch[137/310], Step[0300/1251], Loss: 3.5413(4.0414), Acc: 0.2578(0.3192)
2021-12-30 21:37:53,176 Epoch[137/310], Step[0350/1251], Loss: 4.8201(4.0360), Acc: 0.1846(0.3206)
2021-12-30 21:38:52,877 Epoch[137/310], Step[0400/1251], Loss: 4.4984(4.0334), Acc: 0.2383(0.3196)
2021-12-30 21:39:52,380 Epoch[137/310], Step[0450/1251], Loss: 4.1959(4.0259), Acc: 0.3008(0.3182)
2021-12-30 21:40:52,948 Epoch[137/310], Step[0500/1251], Loss: 4.3191(4.0246), Acc: 0.2764(0.3179)
2021-12-30 21:41:51,494 Epoch[137/310], Step[0550/1251], Loss: 4.0901(4.0265), Acc: 0.3076(0.3168)
2021-12-30 21:42:50,667 Epoch[137/310], Step[0600/1251], Loss: 3.8674(4.0234), Acc: 0.2148(0.3176)
2021-12-30 21:43:50,778 Epoch[137/310], Step[0650/1251], Loss: 4.3231(4.0202), Acc: 0.2754(0.3180)
2021-12-30 21:44:51,792 Epoch[137/310], Step[0700/1251], Loss: 4.0099(4.0173), Acc: 0.2100(0.3168)
2021-12-30 21:45:52,118 Epoch[137/310], Step[0750/1251], Loss: 3.9119(4.0162), Acc: 0.2754(0.3175)
2021-12-30 21:46:51,560 Epoch[137/310], Step[0800/1251], Loss: 4.2205(4.0195), Acc: 0.3662(0.3166)
2021-12-30 21:47:50,057 Epoch[137/310], Step[0850/1251], Loss: 4.1771(4.0175), Acc: 0.3242(0.3173)
2021-12-30 21:48:50,394 Epoch[137/310], Step[0900/1251], Loss: 4.0606(4.0164), Acc: 0.3584(0.3167)
2021-12-30 21:49:50,279 Epoch[137/310], Step[0950/1251], Loss: 4.1495(4.0162), Acc: 0.4102(0.3168)
2021-12-30 21:50:50,800 Epoch[137/310], Step[1000/1251], Loss: 4.3460(4.0178), Acc: 0.3760(0.3172)
2021-12-30 21:51:51,828 Epoch[137/310], Step[1050/1251], Loss: 3.4688(4.0223), Acc: 0.4805(0.3172)
2021-12-30 21:52:51,928 Epoch[137/310], Step[1100/1251], Loss: 4.2845(4.0216), Acc: 0.2363(0.3174)
2021-12-30 21:53:53,325 Epoch[137/310], Step[1150/1251], Loss: 4.4921(4.0214), Acc: 0.3438(0.3162)
2021-12-30 21:54:52,971 Epoch[137/310], Step[1200/1251], Loss: 4.2006(4.0233), Acc: 0.3984(0.3157)
2021-12-30 21:55:53,421 Epoch[137/310], Step[1250/1251], Loss: 3.9267(4.0236), Acc: 0.4453(0.3157)
2021-12-30 21:55:54,974 ----- Epoch[137/310], Train Loss: 4.0236, Train Acc: 0.3157, time: 1563.53, Best Val(epoch134) Acc@1: 0.6583
2021-12-30 21:55:55,155 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 21:55:55,155 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 21:55:55,245 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 21:55:55,245 Now training epoch 138. LR=0.000565
2021-12-30 21:57:05,488 Epoch[138/310], Step[0000/1251], Loss: 4.5976(4.5976), Acc: 0.3105(0.3105)
2021-12-30 21:58:05,331 Epoch[138/310], Step[0050/1251], Loss: 3.8812(4.0664), Acc: 0.3691(0.3136)
2021-12-30 21:59:05,497 Epoch[138/310], Step[0100/1251], Loss: 4.4630(4.0160), Acc: 0.3076(0.3254)
2021-12-30 22:00:05,349 Epoch[138/310], Step[0150/1251], Loss: 4.2240(4.0078), Acc: 0.0986(0.3220)
2021-12-30 22:01:04,088 Epoch[138/310], Step[0200/1251], Loss: 3.6483(4.0059), Acc: 0.4111(0.3259)
2021-12-30 22:02:03,038 Epoch[138/310], Step[0250/1251], Loss: 4.1890(4.0035), Acc: 0.3535(0.3257)
2021-12-30 22:03:03,724 Epoch[138/310], Step[0300/1251], Loss: 3.7722(4.0034), Acc: 0.4922(0.3252)
2021-12-30 22:04:03,573 Epoch[138/310], Step[0350/1251], Loss: 4.4484(4.0039), Acc: 0.3027(0.3248)
2021-12-30 22:05:04,007 Epoch[138/310], Step[0400/1251], Loss: 4.2882(4.0099), Acc: 0.3955(0.3244)
2021-12-30 22:06:05,028 Epoch[138/310], Step[0450/1251], Loss: 3.7336(4.0116), Acc: 0.1982(0.3242)
2021-12-30 22:07:04,673 Epoch[138/310], Step[0500/1251], Loss: 4.0358(4.0139), Acc: 0.4082(0.3255)
2021-12-30 22:08:05,214 Epoch[138/310], Step[0550/1251], Loss: 3.7970(4.0208), Acc: 0.4727(0.3229)
2021-12-30 22:09:05,166 Epoch[138/310], Step[0600/1251], Loss: 3.8592(4.0150), Acc: 0.4053(0.3221)
2021-12-30 22:10:04,384 Epoch[138/310], Step[0650/1251], Loss: 3.7519(4.0226), Acc: 0.2920(0.3212)
2021-12-30 22:11:05,292 Epoch[138/310], Step[0700/1251], Loss: 4.7300(4.0232), Acc: 0.2803(0.3202)
2021-12-30 22:12:07,144 Epoch[138/310], Step[0750/1251], Loss: 3.8761(4.0210), Acc: 0.4766(0.3186)
2021-12-30 22:13:07,450 Epoch[138/310], Step[0800/1251], Loss: 4.2920(4.0183), Acc: 0.2773(0.3185)
2021-12-30 22:14:08,671 Epoch[138/310], Step[0850/1251], Loss: 3.9462(4.0158), Acc: 0.3672(0.3187)
2021-12-30 22:15:09,964 Epoch[138/310], Step[0900/1251], Loss: 3.2708(4.0149), Acc: 0.2568(0.3177)
2021-12-30 22:16:10,762 Epoch[138/310], Step[0950/1251], Loss: 4.2273(4.0154), Acc: 0.3691(0.3180)
2021-12-30 22:17:12,573 Epoch[138/310], Step[1000/1251], Loss: 4.1794(4.0139), Acc: 0.1904(0.3173)
2021-12-30 22:18:13,484 Epoch[138/310], Step[1050/1251], Loss: 3.7342(4.0133), Acc: 0.5205(0.3164)
2021-12-30 22:19:14,120 Epoch[138/310], Step[1100/1251], Loss: 3.9547(4.0136), Acc: 0.2803(0.3163)
2021-12-30 22:20:14,643 Epoch[138/310], Step[1150/1251], Loss: 4.2486(4.0167), Acc: 0.2139(0.3156)
2021-12-30 22:21:14,827 Epoch[138/310], Step[1200/1251], Loss: 3.7085(4.0166), Acc: 0.4355(0.3149)
2021-12-30 22:22:14,999 Epoch[138/310], Step[1250/1251], Loss: 4.0388(4.0160), Acc: 0.2568(0.3149)
2021-12-30 22:22:16,704 ----- Validation after Epoch: 138
2021-12-30 22:23:07,150 Val Step[0000/1563], Loss: 0.8041 (0.8041), Acc@1: 0.8438 (0.8438), Acc@5: 0.9375 (0.9375)
2021-12-30 22:23:08,582 Val Step[0050/1563], Loss: 2.3839 (0.9106), Acc@1: 0.4375 (0.8051), Acc@5: 0.8125 (0.9375)
2021-12-30 22:23:09,906 Val Step[0100/1563], Loss: 1.9509 (1.2525), Acc@1: 0.5938 (0.7110), Acc@5: 0.8125 (0.9019)
2021-12-30 22:23:11,277 Val Step[0150/1563], Loss: 0.6729 (1.1874), Acc@1: 0.8125 (0.7258), Acc@5: 0.9688 (0.9091)
2021-12-30 22:23:12,623 Val Step[0200/1563], Loss: 0.9284 (1.1888), Acc@1: 0.7812 (0.7324), Acc@5: 0.9375 (0.9089)
2021-12-30 22:23:14,019 Val Step[0250/1563], Loss: 0.6881 (1.1284), Acc@1: 0.8750 (0.7464), Acc@5: 1.0000 (0.9173)
2021-12-30 22:23:15,322 Val Step[0300/1563], Loss: 1.4861 (1.1800), Acc@1: 0.5625 (0.7277), Acc@5: 0.9688 (0.9138)
2021-12-30 22:23:16,609 Val Step[0350/1563], Loss: 1.1869 (1.1846), Acc@1: 0.7188 (0.7255), Acc@5: 0.9375 (0.9160)
2021-12-30 22:23:17,890 Val Step[0400/1563], Loss: 1.1066 (1.1887), Acc@1: 0.7500 (0.7214), Acc@5: 0.9688 (0.9170)
2021-12-30 22:23:19,245 Val Step[0450/1563], Loss: 1.0179 (1.1955), Acc@1: 0.7188 (0.7186), Acc@5: 1.0000 (0.9171)
2021-12-30 22:23:20,541 Val Step[0500/1563], Loss: 0.6160 (1.1828), Acc@1: 0.9062 (0.7223), Acc@5: 1.0000 (0.9190)
2021-12-30 22:23:22,041 Val Step[0550/1563], Loss: 0.8137 (1.1602), Acc@1: 0.7812 (0.7294), Acc@5: 1.0000 (0.9213)
2021-12-30 22:23:23,524 Val Step[0600/1563], Loss: 0.8947 (1.1647), Acc@1: 0.7812 (0.7293), Acc@5: 0.9375 (0.9211)
2021-12-30 22:23:24,966 Val Step[0650/1563], Loss: 1.0386 (1.1839), Acc@1: 0.7188 (0.7252), Acc@5: 0.9688 (0.9187)
2021-12-30 22:23:26,441 Val Step[0700/1563], Loss: 1.6021 (1.2151), Acc@1: 0.6562 (0.7180), Acc@5: 0.8438 (0.9147)
2021-12-30 22:23:27,783 Val Step[0750/1563], Loss: 1.7702 (1.2550), Acc@1: 0.6250 (0.7097), Acc@5: 0.8125 (0.9094)
2021-12-30 22:23:29,100 Val Step[0800/1563], Loss: 1.2745 (1.2945), Acc@1: 0.7500 (0.7010), Acc@5: 1.0000 (0.9041)
2021-12-30 22:23:30,418 Val Step[0850/1563], Loss: 1.5586 (1.3248), Acc@1: 0.5625 (0.6941), Acc@5: 0.9062 (0.8999)
2021-12-30 22:23:31,809 Val Step[0900/1563], Loss: 0.5208 (1.3256), Acc@1: 0.8750 (0.6955), Acc@5: 0.9375 (0.8992)
2021-12-30 22:23:33,321 Val Step[0950/1563], Loss: 1.9403 (1.3502), Acc@1: 0.5938 (0.6909), Acc@5: 0.8438 (0.8953)
2021-12-30 22:23:34,779 Val Step[1000/1563], Loss: 0.5420 (1.3766), Acc@1: 0.9062 (0.6848), Acc@5: 0.9688 (0.8918)
2021-12-30 22:23:36,161 Val Step[1050/1563], Loss: 0.3585 (1.3919), Acc@1: 0.9688 (0.6811), Acc@5: 0.9688 (0.8899)
2021-12-30 22:23:37,609 Val Step[1100/1563], Loss: 1.2337 (1.4119), Acc@1: 0.7500 (0.6769), Acc@5: 0.8750 (0.8870)
2021-12-30 22:23:38,947 Val Step[1150/1563], Loss: 1.4601 (1.4296), Acc@1: 0.7812 (0.6739), Acc@5: 0.7812 (0.8841)
2021-12-30 22:23:40,235 Val Step[1200/1563], Loss: 1.3360 (1.4449), Acc@1: 0.7188 (0.6710), Acc@5: 0.8750 (0.8816)
2021-12-30 22:23:41,509 Val Step[1250/1563], Loss: 0.8748 (1.4581), Acc@1: 0.8438 (0.6695), Acc@5: 0.9062 (0.8792)
2021-12-30 22:23:42,837 Val Step[1300/1563], Loss: 0.9742 (1.4690), Acc@1: 0.8125 (0.6668), Acc@5: 0.9062 (0.8782)
2021-12-30 22:23:44,215 Val Step[1350/1563], Loss: 2.1438 (1.4891), Acc@1: 0.3750 (0.6624), Acc@5: 0.7812 (0.8747)
2021-12-30 22:23:45,622 Val Step[1400/1563], Loss: 1.0642 (1.4956), Acc@1: 0.7812 (0.6612), Acc@5: 0.9062 (0.8737)
2021-12-30 22:23:47,035 Val Step[1450/1563], Loss: 1.9523 (1.5004), Acc@1: 0.5312 (0.6600), Acc@5: 0.9062 (0.8734)
2021-12-30 22:23:48,406 Val Step[1500/1563], Loss: 2.5960 (1.4883), Acc@1: 0.4062 (0.6626), Acc@5: 0.6562 (0.8751)
2021-12-30 22:23:49,699 Val Step[1550/1563], Loss: 0.9328 (1.4882), Acc@1: 0.8750 (0.6625), Acc@5: 0.9062 (0.8750)
2021-12-30 22:23:50,478 ----- Epoch[138/310], Validation Loss: 1.4858, Validation Acc@1: 0.6630, Validation Acc@5: 0.8752, time: 93.77
2021-12-30 22:23:50,478 ----- Epoch[138/310], Train Loss: 4.0160, Train Acc: 0.3149, time: 1581.45, Best Val(epoch138) Acc@1: 0.6630
2021-12-30 22:23:50,665 Max accuracy so far: 0.6630 at epoch_138
2021-12-30 22:23:50,666 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 22:23:50,666 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 22:23:50,775 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 22:23:51,160 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 22:23:51,161 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 22:23:51,293 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 22:23:51,294 Now training epoch 139. LR=0.000560
2021-12-30 22:25:01,278 Epoch[139/310], Step[0000/1251], Loss: 4.0868(4.0868), Acc: 0.2754(0.2754)
2021-12-30 22:25:59,517 Epoch[139/310], Step[0050/1251], Loss: 3.8035(3.9172), Acc: 0.3174(0.3225)
2021-12-30 22:26:57,319 Epoch[139/310], Step[0100/1251], Loss: 4.1092(3.9481), Acc: 0.2988(0.3337)
2021-12-30 22:27:56,471 Epoch[139/310], Step[0150/1251], Loss: 4.2942(3.9562), Acc: 0.4199(0.3242)
2021-12-30 22:28:55,081 Epoch[139/310], Step[0200/1251], Loss: 4.0819(3.9776), Acc: 0.3965(0.3203)
2021-12-30 22:29:54,444 Epoch[139/310], Step[0250/1251], Loss: 4.0425(3.9942), Acc: 0.4131(0.3224)
2021-12-30 22:30:54,515 Epoch[139/310], Step[0300/1251], Loss: 3.8545(4.0032), Acc: 0.4111(0.3238)
2021-12-30 22:31:53,955 Epoch[139/310], Step[0350/1251], Loss: 3.9563(4.0163), Acc: 0.3105(0.3212)
2021-12-30 22:32:53,129 Epoch[139/310], Step[0400/1251], Loss: 3.6916(4.0131), Acc: 0.3467(0.3229)
2021-12-30 22:33:53,560 Epoch[139/310], Step[0450/1251], Loss: 4.1142(4.0114), Acc: 0.3154(0.3200)
2021-12-30 22:34:53,981 Epoch[139/310], Step[0500/1251], Loss: 4.1143(4.0156), Acc: 0.4043(0.3173)
2021-12-30 22:35:54,558 Epoch[139/310], Step[0550/1251], Loss: 4.5397(4.0196), Acc: 0.3428(0.3181)
2021-12-30 22:36:53,962 Epoch[139/310], Step[0600/1251], Loss: 4.8615(4.0214), Acc: 0.1855(0.3172)
2021-12-30 22:37:53,728 Epoch[139/310], Step[0650/1251], Loss: 3.7729(4.0195), Acc: 0.4775(0.3169)
2021-12-30 22:38:52,806 Epoch[139/310], Step[0700/1251], Loss: 4.0848(4.0193), Acc: 0.3848(0.3191)
2021-12-30 22:39:52,398 Epoch[139/310], Step[0750/1251], Loss: 4.2665(4.0220), Acc: 0.3535(0.3183)
2021-12-30 22:40:52,415 Epoch[139/310], Step[0800/1251], Loss: 4.4303(4.0243), Acc: 0.2588(0.3173)
2021-12-30 22:41:53,100 Epoch[139/310], Step[0850/1251], Loss: 4.0114(4.0284), Acc: 0.1572(0.3161)
2021-12-30 22:42:53,569 Epoch[139/310], Step[0900/1251], Loss: 3.8244(4.0305), Acc: 0.1855(0.3154)
2021-12-30 22:43:53,821 Epoch[139/310], Step[0950/1251], Loss: 4.0964(4.0341), Acc: 0.3545(0.3146)
2021-12-30 22:44:53,185 Epoch[139/310], Step[1000/1251], Loss: 3.9657(4.0346), Acc: 0.2197(0.3137)
2021-12-30 22:45:52,896 Epoch[139/310], Step[1050/1251], Loss: 3.1895(4.0334), Acc: 0.4375(0.3136)
2021-12-30 22:46:53,602 Epoch[139/310], Step[1100/1251], Loss: 3.6087(4.0342), Acc: 0.3906(0.3141)
2021-12-30 22:47:53,869 Epoch[139/310], Step[1150/1251], Loss: 3.8338(4.0342), Acc: 0.3418(0.3148)
2021-12-30 22:48:54,803 Epoch[139/310], Step[1200/1251], Loss: 4.2821(4.0314), Acc: 0.3564(0.3149)
2021-12-30 22:49:56,138 Epoch[139/310], Step[1250/1251], Loss: 4.2005(4.0306), Acc: 0.3701(0.3143)
2021-12-30 22:49:57,704 ----- Epoch[139/310], Train Loss: 4.0306, Train Acc: 0.3143, time: 1566.41, Best Val(epoch138) Acc@1: 0.6630
2021-12-30 22:49:57,878 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 22:49:57,879 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 22:49:57,984 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 22:49:57,984 Now training epoch 140. LR=0.000555
2021-12-30 22:51:12,367 Epoch[140/310], Step[0000/1251], Loss: 4.0152(4.0152), Acc: 0.3262(0.3262)
2021-12-30 22:52:12,021 Epoch[140/310], Step[0050/1251], Loss: 3.8591(4.0835), Acc: 0.4688(0.3175)
2021-12-30 22:53:11,225 Epoch[140/310], Step[0100/1251], Loss: 4.5542(4.0394), Acc: 0.3818(0.3187)
2021-12-30 22:54:10,214 Epoch[140/310], Step[0150/1251], Loss: 4.2298(4.0146), Acc: 0.2480(0.3239)
2021-12-30 22:55:08,970 Epoch[140/310], Step[0200/1251], Loss: 4.0806(4.0017), Acc: 0.2148(0.3266)
2021-12-30 22:56:07,479 Epoch[140/310], Step[0250/1251], Loss: 3.9206(4.0093), Acc: 0.2666(0.3299)
2021-12-30 22:57:07,026 Epoch[140/310], Step[0300/1251], Loss: 4.3248(4.0199), Acc: 0.3066(0.3287)
2021-12-30 22:58:05,986 Epoch[140/310], Step[0350/1251], Loss: 3.8898(4.0223), Acc: 0.4189(0.3273)
2021-12-30 22:59:06,152 Epoch[140/310], Step[0400/1251], Loss: 3.8344(4.0230), Acc: 0.4561(0.3257)
2021-12-30 23:00:06,308 Epoch[140/310], Step[0450/1251], Loss: 4.4324(4.0228), Acc: 0.3340(0.3265)
2021-12-30 23:01:05,212 Epoch[140/310], Step[0500/1251], Loss: 3.4631(4.0282), Acc: 0.3857(0.3266)
2021-12-30 23:02:05,110 Epoch[140/310], Step[0550/1251], Loss: 4.0745(4.0225), Acc: 0.3682(0.3270)
2021-12-30 23:03:05,342 Epoch[140/310], Step[0600/1251], Loss: 4.4592(4.0263), Acc: 0.2227(0.3260)
2021-12-30 23:04:04,917 Epoch[140/310], Step[0650/1251], Loss: 4.2525(4.0219), Acc: 0.3262(0.3268)
2021-12-30 23:05:04,874 Epoch[140/310], Step[0700/1251], Loss: 4.0964(4.0233), Acc: 0.2324(0.3266)
2021-12-30 23:06:05,205 Epoch[140/310], Step[0750/1251], Loss: 3.7756(4.0242), Acc: 0.1807(0.3270)
2021-12-30 23:07:05,752 Epoch[140/310], Step[0800/1251], Loss: 3.9758(4.0274), Acc: 0.2949(0.3264)
2021-12-30 23:08:05,119 Epoch[140/310], Step[0850/1251], Loss: 3.8700(4.0276), Acc: 0.3135(0.3263)
2021-12-30 23:09:04,527 Epoch[140/310], Step[0900/1251], Loss: 4.0883(4.0287), Acc: 0.4053(0.3254)
2021-12-30 23:10:04,390 Epoch[140/310], Step[0950/1251], Loss: 4.2691(4.0274), Acc: 0.3428(0.3250)
2021-12-30 23:11:05,154 Epoch[140/310], Step[1000/1251], Loss: 3.6169(4.0279), Acc: 0.2686(0.3231)
2021-12-30 23:12:05,933 Epoch[140/310], Step[1050/1251], Loss: 4.1012(4.0293), Acc: 0.2754(0.3232)
2021-12-30 23:13:05,236 Epoch[140/310], Step[1100/1251], Loss: 3.8840(4.0300), Acc: 0.4639(0.3232)
2021-12-30 23:14:03,660 Epoch[140/310], Step[1150/1251], Loss: 4.0558(4.0265), Acc: 0.4053(0.3246)
2021-12-30 23:15:02,347 Epoch[140/310], Step[1200/1251], Loss: 3.7426(4.0271), Acc: 0.4805(0.3251)
2021-12-30 23:16:02,066 Epoch[140/310], Step[1250/1251], Loss: 4.3586(4.0275), Acc: 0.1396(0.3253)
2021-12-30 23:16:03,658 ----- Validation after Epoch: 140
2021-12-30 23:16:53,330 Val Step[0000/1563], Loss: 0.8163 (0.8163), Acc@1: 0.8750 (0.8750), Acc@5: 0.9688 (0.9688)
2021-12-30 23:16:54,797 Val Step[0050/1563], Loss: 2.1473 (0.9075), Acc@1: 0.4375 (0.8094), Acc@5: 0.8438 (0.9430)
2021-12-30 23:16:56,217 Val Step[0100/1563], Loss: 2.0185 (1.2613), Acc@1: 0.5625 (0.7209), Acc@5: 0.8438 (0.9087)
2021-12-30 23:16:57,593 Val Step[0150/1563], Loss: 0.6773 (1.1807), Acc@1: 0.8750 (0.7363), Acc@5: 0.9688 (0.9164)
2021-12-30 23:16:58,958 Val Step[0200/1563], Loss: 1.1571 (1.1969), Acc@1: 0.7500 (0.7374), Acc@5: 0.9375 (0.9142)
2021-12-30 23:17:00,265 Val Step[0250/1563], Loss: 1.4046 (1.1285), Acc@1: 0.6562 (0.7521), Acc@5: 0.9375 (0.9226)
2021-12-30 23:17:01,583 Val Step[0300/1563], Loss: 1.6895 (1.1948), Acc@1: 0.4062 (0.7299), Acc@5: 0.9375 (0.9166)
2021-12-30 23:17:02,892 Val Step[0350/1563], Loss: 1.7605 (1.2039), Acc@1: 0.5312 (0.7240), Acc@5: 0.8750 (0.9199)
2021-12-30 23:17:04,189 Val Step[0400/1563], Loss: 1.1281 (1.2150), Acc@1: 0.7500 (0.7176), Acc@5: 0.9688 (0.9207)
2021-12-30 23:17:05,494 Val Step[0450/1563], Loss: 0.9172 (1.2128), Acc@1: 0.7188 (0.7161), Acc@5: 1.0000 (0.9218)
2021-12-30 23:17:06,794 Val Step[0500/1563], Loss: 0.6380 (1.2005), Acc@1: 0.9062 (0.7189), Acc@5: 1.0000 (0.9239)
2021-12-30 23:17:08,237 Val Step[0550/1563], Loss: 1.0018 (1.1735), Acc@1: 0.7188 (0.7267), Acc@5: 0.9375 (0.9268)
2021-12-30 23:17:09,644 Val Step[0600/1563], Loss: 0.8387 (1.1741), Acc@1: 0.7812 (0.7275), Acc@5: 0.9688 (0.9259)
2021-12-30 23:17:10,960 Val Step[0650/1563], Loss: 0.9360 (1.1977), Acc@1: 0.7812 (0.7233), Acc@5: 1.0000 (0.9219)
2021-12-30 23:17:12,450 Val Step[0700/1563], Loss: 1.2688 (1.2346), Acc@1: 0.7812 (0.7150), Acc@5: 0.9062 (0.9169)
2021-12-30 23:17:13,777 Val Step[0750/1563], Loss: 1.4538 (1.2752), Acc@1: 0.7812 (0.7076), Acc@5: 0.8750 (0.9110)
2021-12-30 23:17:15,079 Val Step[0800/1563], Loss: 1.2519 (1.3203), Acc@1: 0.6562 (0.6978), Acc@5: 0.9688 (0.9043)
2021-12-30 23:17:16,371 Val Step[0850/1563], Loss: 1.6702 (1.3465), Acc@1: 0.6562 (0.6930), Acc@5: 0.8750 (0.9005)
2021-12-30 23:17:17,842 Val Step[0900/1563], Loss: 0.3638 (1.3480), Acc@1: 0.9375 (0.6943), Acc@5: 1.0000 (0.8997)
2021-12-30 23:17:19,290 Val Step[0950/1563], Loss: 1.5860 (1.3689), Acc@1: 0.6875 (0.6912), Acc@5: 0.8750 (0.8965)
2021-12-30 23:17:20,621 Val Step[1000/1563], Loss: 0.6024 (1.3929), Acc@1: 0.9062 (0.6860), Acc@5: 1.0000 (0.8929)
2021-12-30 23:17:22,008 Val Step[1050/1563], Loss: 0.4525 (1.4066), Acc@1: 0.9688 (0.6833), Acc@5: 0.9688 (0.8911)
2021-12-30 23:17:23,334 Val Step[1100/1563], Loss: 1.5427 (1.4265), Acc@1: 0.6250 (0.6793), Acc@5: 0.8438 (0.8884)
2021-12-30 23:17:24,635 Val Step[1150/1563], Loss: 1.4403 (1.4427), Acc@1: 0.7500 (0.6765), Acc@5: 0.8125 (0.8859)
2021-12-30 23:17:25,949 Val Step[1200/1563], Loss: 1.6198 (1.4591), Acc@1: 0.6875 (0.6740), Acc@5: 0.8438 (0.8829)
2021-12-30 23:17:27,309 Val Step[1250/1563], Loss: 1.1676 (1.4750), Acc@1: 0.8125 (0.6714), Acc@5: 0.8750 (0.8805)
2021-12-30 23:17:28,647 Val Step[1300/1563], Loss: 1.1970 (1.4866), Acc@1: 0.7500 (0.6691), Acc@5: 0.9062 (0.8789)
2021-12-30 23:17:30,021 Val Step[1350/1563], Loss: 2.1532 (1.5057), Acc@1: 0.4375 (0.6644), Acc@5: 0.8750 (0.8762)
2021-12-30 23:17:31,398 Val Step[1400/1563], Loss: 1.4621 (1.5155), Acc@1: 0.7188 (0.6625), Acc@5: 0.8438 (0.8746)
2021-12-30 23:17:32,719 Val Step[1450/1563], Loss: 1.8849 (1.5215), Acc@1: 0.5938 (0.6612), Acc@5: 0.9062 (0.8738)
2021-12-30 23:17:34,118 Val Step[1500/1563], Loss: 1.8926 (1.5102), Acc@1: 0.5312 (0.6635), Acc@5: 0.8750 (0.8755)
2021-12-30 23:17:35,617 Val Step[1550/1563], Loss: 1.0587 (1.5116), Acc@1: 0.8750 (0.6633), Acc@5: 0.9062 (0.8752)
2021-12-30 23:17:36,410 ----- Epoch[140/310], Validation Loss: 1.5086, Validation Acc@1: 0.6640, Validation Acc@5: 0.8755, time: 92.75
2021-12-30 23:17:36,410 ----- Epoch[140/310], Train Loss: 4.0275, Train Acc: 0.3253, time: 1565.67, Best Val(epoch140) Acc@1: 0.6640
2021-12-30 23:17:36,595 Max accuracy so far: 0.6640 at epoch_140
2021-12-30 23:17:36,595 ----- Save BEST model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdparams
2021-12-30 23:17:36,595 ----- Save BEST optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT.pdopt
2021-12-30 23:17:36,698 ----- Save BEST ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/Best_PiT-EMA.pdparams
2021-12-30 23:17:36,827 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-140-Loss-4.03374516744789.pdparams
2021-12-30 23:17:36,828 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-140-Loss-4.03374516744789.pdopt
2021-12-30 23:17:36,869 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Epoch-140-Loss-4.03374516744789-EMA.pdparams
2021-12-30 23:17:36,869 Now training epoch 141. LR=0.000549
2021-12-30 23:18:40,829 Epoch[141/310], Step[0000/1251], Loss: 3.6542(3.6542), Acc: 0.3496(0.3496)
2021-12-30 23:19:40,332 Epoch[141/310], Step[0050/1251], Loss: 4.3815(4.0728), Acc: 0.3604(0.3168)
2021-12-30 23:20:40,891 Epoch[141/310], Step[0100/1251], Loss: 4.1921(4.0603), Acc: 0.3848(0.3159)
2021-12-30 23:21:40,796 Epoch[141/310], Step[0150/1251], Loss: 3.6944(4.0408), Acc: 0.4902(0.3195)
2021-12-30 23:22:41,835 Epoch[141/310], Step[0200/1251], Loss: 4.1343(4.0600), Acc: 0.2686(0.3146)
2021-12-30 23:23:40,464 Epoch[141/310], Step[0250/1251], Loss: 3.8118(4.0512), Acc: 0.4785(0.3179)
2021-12-30 23:24:40,138 Epoch[141/310], Step[0300/1251], Loss: 4.0323(4.0565), Acc: 0.4639(0.3190)
2021-12-30 23:25:40,964 Epoch[141/310], Step[0350/1251], Loss: 3.5438(4.0442), Acc: 0.4951(0.3188)
2021-12-30 23:26:40,120 Epoch[141/310], Step[0400/1251], Loss: 4.0663(4.0385), Acc: 0.2451(0.3187)
2021-12-30 23:27:38,918 Epoch[141/310], Step[0450/1251], Loss: 3.8321(4.0317), Acc: 0.2617(0.3187)
2021-12-30 23:28:37,947 Epoch[141/310], Step[0500/1251], Loss: 4.3305(4.0240), Acc: 0.2939(0.3185)
2021-12-30 23:29:37,296 Epoch[141/310], Step[0550/1251], Loss: 3.9175(4.0248), Acc: 0.2656(0.3188)
2021-12-30 23:30:37,457 Epoch[141/310], Step[0600/1251], Loss: 4.3149(4.0308), Acc: 0.3643(0.3181)
2021-12-30 23:31:37,998 Epoch[141/310], Step[0650/1251], Loss: 4.0861(4.0324), Acc: 0.3555(0.3166)
2021-12-30 23:32:36,031 Epoch[141/310], Step[0700/1251], Loss: 3.7152(4.0347), Acc: 0.3701(0.3161)
2021-12-30 23:33:36,069 Epoch[141/310], Step[0750/1251], Loss: 3.7843(4.0355), Acc: 0.4648(0.3173)
2021-12-30 23:34:35,228 Epoch[141/310], Step[0800/1251], Loss: 4.3840(4.0327), Acc: 0.3916(0.3193)
2021-12-30 23:35:35,771 Epoch[141/310], Step[0850/1251], Loss: 4.3865(4.0280), Acc: 0.2383(0.3174)
2021-12-30 23:36:35,969 Epoch[141/310], Step[0900/1251], Loss: 3.6857(4.0290), Acc: 0.3330(0.3170)
2021-12-30 23:37:37,402 Epoch[141/310], Step[0950/1251], Loss: 4.0200(4.0270), Acc: 0.3545(0.3174)
2021-12-30 23:38:37,351 Epoch[141/310], Step[1000/1251], Loss: 3.5697(4.0288), Acc: 0.4629(0.3180)
2021-12-30 23:39:37,799 Epoch[141/310], Step[1050/1251], Loss: 4.2274(4.0290), Acc: 0.3428(0.3179)
2021-12-30 23:40:36,177 Epoch[141/310], Step[1100/1251], Loss: 4.0330(4.0290), Acc: 0.4424(0.3182)
2021-12-30 23:41:36,463 Epoch[141/310], Step[1150/1251], Loss: 4.0658(4.0295), Acc: 0.4023(0.3180)
2021-12-30 23:42:36,668 Epoch[141/310], Step[1200/1251], Loss: 4.2258(4.0271), Acc: 0.2891(0.3177)
2021-12-30 23:43:37,273 Epoch[141/310], Step[1250/1251], Loss: 4.1740(4.0282), Acc: 0.2646(0.3175)
2021-12-30 23:43:38,843 ----- Epoch[141/310], Train Loss: 4.0282, Train Acc: 0.3175, time: 1561.97, Best Val(epoch140) Acc@1: 0.6640
2021-12-30 23:43:39,031 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-30 23:43:39,032 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-30 23:43:39,120 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-30 23:43:39,121 Now training epoch 142. LR=0.000544
2021-12-30 23:44:52,668 Epoch[142/310], Step[0000/1251], Loss: 4.1572(4.1572), Acc: 0.2910(0.2910)
2021-12-30 23:45:53,196 Epoch[142/310], Step[0050/1251], Loss: 4.3096(4.0267), Acc: 0.2471(0.3379)
2021-12-30 23:46:51,444 Epoch[142/310], Step[0100/1251], Loss: 3.8294(4.0054), Acc: 0.3125(0.3274)
2021-12-30 23:47:50,094 Epoch[142/310], Step[0150/1251], Loss: 4.1809(4.0131), Acc: 0.3652(0.3277)
2021-12-30 23:48:48,247 Epoch[142/310], Step[0200/1251], Loss: 4.3952(4.0199), Acc: 0.2314(0.3252)
2021-12-30 23:49:46,219 Epoch[142/310], Step[0250/1251], Loss: 4.0976(4.0038), Acc: 0.3525(0.3266)
2021-12-30 23:50:44,101 Epoch[142/310], Step[0300/1251], Loss: 3.8880(4.0158), Acc: 0.1924(0.3257)
2021-12-30 23:51:44,235 Epoch[142/310], Step[0350/1251], Loss: 4.3753(4.0234), Acc: 0.2607(0.3202)
2021-12-30 23:52:42,446 Epoch[142/310], Step[0400/1251], Loss: 3.5380(4.0193), Acc: 0.5186(0.3193)
2021-12-30 23:53:41,865 Epoch[142/310], Step[0450/1251], Loss: 4.0773(4.0228), Acc: 0.2617(0.3174)
2021-12-30 23:54:41,828 Epoch[142/310], Step[0500/1251], Loss: 3.9488(4.0212), Acc: 0.2764(0.3144)
2021-12-30 23:55:40,524 Epoch[142/310], Step[0550/1251], Loss: 4.1985(4.0266), Acc: 0.2920(0.3140)
2021-12-30 23:56:40,084 Epoch[142/310], Step[0600/1251], Loss: 4.3582(4.0217), Acc: 0.3545(0.3140)
2021-12-30 23:57:38,479 Epoch[142/310], Step[0650/1251], Loss: 3.4471(4.0214), Acc: 0.3477(0.3144)
2021-12-30 23:58:38,668 Epoch[142/310], Step[0700/1251], Loss: 4.3450(4.0196), Acc: 0.2920(0.3148)
2021-12-30 23:59:38,526 Epoch[142/310], Step[0750/1251], Loss: 4.1651(4.0186), Acc: 0.2764(0.3146)
2021-12-31 00:00:38,892 Epoch[142/310], Step[0800/1251], Loss: 4.0647(4.0227), Acc: 0.2646(0.3125)
2021-12-31 00:01:39,073 Epoch[142/310], Step[0850/1251], Loss: 3.7024(4.0257), Acc: 0.3848(0.3121)
2021-12-31 00:02:39,716 Epoch[142/310], Step[0900/1251], Loss: 3.5483(4.0301), Acc: 0.3887(0.3111)
2021-12-31 00:03:40,163 Epoch[142/310], Step[0950/1251], Loss: 3.8513(4.0288), Acc: 0.4658(0.3113)
2021-12-31 00:04:40,030 Epoch[142/310], Step[1000/1251], Loss: 3.8860(4.0271), Acc: 0.0703(0.3119)
2021-12-31 00:05:39,891 Epoch[142/310], Step[1050/1251], Loss: 3.4761(4.0279), Acc: 0.2070(0.3120)
2021-12-31 00:06:40,773 Epoch[142/310], Step[1100/1251], Loss: 3.8701(4.0273), Acc: 0.4277(0.3125)
2021-12-31 00:07:41,386 Epoch[142/310], Step[1150/1251], Loss: 4.0735(4.0309), Acc: 0.3799(0.3122)
2021-12-31 00:08:42,158 Epoch[142/310], Step[1200/1251], Loss: 4.3129(4.0294), Acc: 0.3115(0.3122)
2021-12-31 00:09:40,838 Epoch[142/310], Step[1250/1251], Loss: 3.7364(4.0282), Acc: 0.2139(0.3120)
2021-12-31 00:09:42,474 ----- Validation after Epoch: 142
2021-12-31 00:10:41,019 Val Step[0000/1563], Loss: 0.7788 (0.7788), Acc@1: 0.9062 (0.9062), Acc@5: 0.9688 (0.9688)
2021-12-31 00:10:42,385 Val Step[0050/1563], Loss: 2.5793 (1.0219), Acc@1: 0.4062 (0.7935), Acc@5: 0.7812 (0.9393)
2021-12-31 00:10:43,733 Val Step[0100/1563], Loss: 1.8970 (1.3190), Acc@1: 0.5625 (0.7082), Acc@5: 0.8438 (0.9069)
2021-12-31 00:10:45,109 Val Step[0150/1563], Loss: 0.5931 (1.2294), Acc@1: 0.8750 (0.7318), Acc@5: 0.9688 (0.9151)
2021-12-31 00:10:46,453 Val Step[0200/1563], Loss: 1.1446 (1.2453), Acc@1: 0.7500 (0.7331), Acc@5: 0.9375 (0.9117)
2021-12-31 00:10:47,740 Val Step[0250/1563], Loss: 0.9164 (1.1800), Acc@1: 0.8125 (0.7490), Acc@5: 0.9688 (0.9187)
2021-12-31 00:10:49,031 Val Step[0300/1563], Loss: 1.3815 (1.2437), Acc@1: 0.7188 (0.7289), Acc@5: 0.9375 (0.9153)
2021-12-31 00:10:50,374 Val Step[0350/1563], Loss: 1.2939 (1.2544), Acc@1: 0.7812 (0.7235), Acc@5: 0.9062 (0.9168)
2021-12-31 00:10:51,684 Val Step[0400/1563], Loss: 1.2918 (1.2494), Acc@1: 0.7500 (0.7209), Acc@5: 0.9688 (0.9186)
2021-12-31 00:10:53,080 Val Step[0450/1563], Loss: 1.1319 (1.2473), Acc@1: 0.6562 (0.7201), Acc@5: 1.0000 (0.9205)
2021-12-31 00:10:54,487 Val Step[0500/1563], Loss: 0.5648 (1.2411), Acc@1: 0.9062 (0.7221), Acc@5: 1.0000 (0.9216)
2021-12-31 00:10:55,913 Val Step[0550/1563], Loss: 1.1159 (1.2154), Acc@1: 0.7500 (0.7294), Acc@5: 1.0000 (0.9243)
2021-12-31 00:10:57,292 Val Step[0600/1563], Loss: 1.0794 (1.2208), Acc@1: 0.7500 (0.7295), Acc@5: 0.9062 (0.9239)
2021-12-31 00:10:58,649 Val Step[0650/1563], Loss: 0.8436 (1.2447), Acc@1: 0.8125 (0.7243), Acc@5: 1.0000 (0.9206)
2021-12-31 00:11:00,036 Val Step[0700/1563], Loss: 1.6759 (1.2796), Acc@1: 0.6875 (0.7179), Acc@5: 0.8750 (0.9152)
2021-12-31 00:11:01,372 Val Step[0750/1563], Loss: 1.5417 (1.3153), Acc@1: 0.7188 (0.7110), Acc@5: 0.8438 (0.9097)
2021-12-31 00:11:02,808 Val Step[0800/1563], Loss: 1.0915 (1.3562), Acc@1: 0.7812 (0.7009), Acc@5: 0.9688 (0.9043)
2021-12-31 00:11:04,328 Val Step[0850/1563], Loss: 1.7044 (1.3827), Acc@1: 0.5625 (0.6958), Acc@5: 0.8438 (0.9002)
2021-12-31 00:11:05,737 Val Step[0900/1563], Loss: 0.6211 (1.3837), Acc@1: 0.9062 (0.6966), Acc@5: 0.9688 (0.8988)
2021-12-31 00:11:07,160 Val Step[0950/1563], Loss: 1.6468 (1.4080), Acc@1: 0.6875 (0.6915), Acc@5: 0.9062 (0.8953)
2021-12-31 00:11:08,454 Val Step[1000/1563], Loss: 0.5453 (1.4332), Acc@1: 0.9062 (0.6854), Acc@5: 1.0000 (0.8916)
2021-12-31 00:11:09,782 Val Step[1050/1563], Loss: 0.5175 (1.4472), Acc@1: 0.9688 (0.6821), Acc@5: 0.9688 (0.8898)
2021-12-31 00:11:11,073 Val Step[1100/1563], Loss: 1.2133 (1.4687), Acc@1: 0.7812 (0.6776), Acc@5: 0.8750 (0.8864)
2021-12-31 00:11:12,409 Val Step[1150/1563], Loss: 1.2324 (1.4870), Acc@1: 0.7812 (0.6743), Acc@5: 0.8125 (0.8838)
2021-12-31 00:11:13,705 Val Step[1200/1563], Loss: 1.3999 (1.5052), Acc@1: 0.7812 (0.6702), Acc@5: 0.8438 (0.8807)
2021-12-31 00:11:15,028 Val Step[1250/1563], Loss: 1.2632 (1.5192), Acc@1: 0.8125 (0.6685), Acc@5: 0.8750 (0.8785)
2021-12-31 00:11:16,378 Val Step[1300/1563], Loss: 1.2455 (1.5312), Acc@1: 0.7812 (0.6661), Acc@5: 0.8750 (0.8771)
2021-12-31 00:11:17,781 Val Step[1350/1563], Loss: 2.9218 (1.5503), Acc@1: 0.2500 (0.6621), Acc@5: 0.7188 (0.8742)
2021-12-31 00:11:19,079 Val Step[1400/1563], Loss: 1.6503 (1.5587), Acc@1: 0.6875 (0.6601), Acc@5: 0.8438 (0.8730)
2021-12-31 00:11:20,487 Val Step[1450/1563], Loss: 2.0420 (1.5667), Acc@1: 0.4688 (0.6582), Acc@5: 0.8750 (0.8723)
2021-12-31 00:11:21,931 Val Step[1500/1563], Loss: 2.2150 (1.5546), Acc@1: 0.4062 (0.6607), Acc@5: 0.7812 (0.8739)
2021-12-31 00:11:23,345 Val Step[1550/1563], Loss: 1.1475 (1.5552), Acc@1: 0.8750 (0.6606), Acc@5: 0.9062 (0.8738)
2021-12-31 00:11:24,197 ----- Epoch[142/310], Validation Loss: 1.5540, Validation Acc@1: 0.6610, Validation Acc@5: 0.8739, time: 101.72
2021-12-31 00:11:24,198 ----- Epoch[142/310], Train Loss: 4.0282, Train Acc: 0.3120, time: 1563.35, Best Val(epoch140) Acc@1: 0.6640
2021-12-31 00:11:24,395 ----- Save model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdparams
2021-12-31 00:11:24,396 ----- Save optim: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest.pdopt
2021-12-31 00:11:24,503 ----- Save ema model: /root/paddlejob/workspace/output/train-20211229-17-42-20/PiT-Latest-EMA.pdparams
2021-12-31 00:11:24,503 Now training epoch 143. LR=0.000539
2021-12-31 00:12:35,127 Epoch[143/310], Step[0000/1251], Loss: 3.8392(3.8392), Acc: 0.3467(0.3467)
2021-12-31 00:13:33,706 Epoch[143/310], Step[0050/1251], Loss: 4.5831(4.0309), Acc: 0.2275(0.3303)
2021-12-31 00:14:33,458 Epoch[143/310], Step[0100/1251], Loss: 3.3196(4.0047), Acc: 0.3916(0.3204)
2021-12-31 00:15:32,340 Epoch[143/310], Step[0150/1251], Loss: 4.1081(3.9941), Acc: 0.1777(0.3199)
2021-12-31 00:16:32,486 Epoch[143/310], Step[0200/1251], Loss: 4.1058(4.0101), Acc: 0.4072(0.3145)
2021-12-31 00:17:31,875 Epoch[143/310], Step[0250/1251], Loss: 3.5757(3.9995), Acc: 0.3594(0.3152)
2021-12-31 00:18:31,465 Epoch[143/310], Step[0300/1251], Loss: 4.1669(4.0015), Acc: 0.2197(0.3152)
2021-12-31 00:19:31,229 Epoch[143/310], Step[0350/1251], Loss: 4.5716(3.9981), Acc: 0.2109(0.3144)
2021-12-31 00:20:31,225 Epoch[143/310], Step[0400/1251], Loss: 4.5375(4.0027), Acc: 0.2812(0.3156)
2021-12-31 00:21:30,785 Epoch[143/310], Step[0450/1251], Loss: 3.7022(3.9920), Acc: 0.2197(0.3189)
2021-12-31 00:22:31,738 Epoch[143/310], Step[0500/1251], Loss: 4.1987(3.9999), Acc: 0.3105(0.3182)
2021-12-31 00:23:31,880 Epoch[143/310], Step[0550/1251], Loss: 3.9185(4.0007), Acc: 0.2080(0.3171)
2021-12-31 00:24:32,613 Epoch[143/310], Step[0600/1251], Loss: 3.7118(4.0024), Acc: 0.4170(0.3180)
2021-12-31 00:25:33,059 Epoch[143/310], Step[0650/1251], Loss: 4.2322(4.0085), Acc: 0.3076(0.3176)
2021-12-31 00:26:32,329 Epoch[143/310], Step[0700/1251], Loss: 4.3101(4.0106), Acc: 0.1797(0.3172)
2021-12-31 00:27:33,099 Epoch[143/310], Step[0750/1251], Loss: 4.1350(4.0071), Acc: 0.2734(0.3173)
2021-12-31 00:28:33,414 Epoch[143/310], Step[0800/1251], Loss: 3.7707(4.0057), Acc: 0.4424(0.3169)
2021-12-31 00:29:33,538 Epoch[143/310], Step[0850/1251], Loss: 4.0541(4.0008), Acc: 0.4014(0.3172)
2021-12-31 00:30:32,931 Epoch[143/310], Step[0900/1251], Loss: 3.8011(3.9998), Acc: 0.4434(0.3170)
2021-12-31 00:31:31,728 Epoch[143/310], Step[0950/1251], Loss: 4.2159(4.0044), Acc: 0.3408(0.3167)
2021-12-31 00:32:31,995 Epoch[143/310], Step[1000/1251], Loss: 4.6042(4.0033), Acc: 0.3428(0.3167)
2021-12-31 00:33:31,792 Epoch[143/310], Step[1050/1251], Loss: 3.9048(4.0037), Acc: 0.2393(0.3170)
